[
    {
        "paper": {
            "id": "2512.05150",
            "authors": [
                {
                    "_id": "69365dca3962c926cf6807f7",
                    "name": "Zhenglin Cheng",
                    "hidden": false
                },
                {
                    "_id": "69365dca3962c926cf6807f8",
                    "name": "Peng Sun",
                    "hidden": false
                },
                {
                    "_id": "69365dca3962c926cf6807f9",
                    "name": "Jianguo Li",
                    "hidden": false
                },
                {
                    "_id": "69365dca3962c926cf6807fa",
                    "name": "Tao Lin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-03T07:45:46.000Z",
            "submittedOnDailyAt": "2025-12-08T02:49:09.906Z",
            "title": "TwinFlow: Realizing One-step Generation on Large Models with Self-adversarial Flows",
            "submittedOnDailyBy": {
                "_id": "65028e8389707f182386588c",
                "avatarUrl": "/avatars/86a748a3264e6e0f4ee5eaf8f7032ecb.svg",
                "isPro": true,
                "fullname": "Zhenglin Cheng (SII)",
                "user": "kenshinn",
                "type": "user"
            },
            "summary": "Recent advances in large multi-modal generative models have demonstrated impressive capabilities in multi-modal generation, including image and video generation. These models are typically built upon multi-step frameworks like diffusion and flow matching, which inherently limits their inference efficiency (requiring 40-100 Number of Function Evaluations (NFEs)). While various few-step methods aim to accelerate the inference, existing solutions have clear limitations. Prominent distillation-based methods, such as progressive and consistency distillation, either require an iterative distillation procedure or show significant degradation at very few steps (< 4-NFE). Meanwhile, integrating adversarial training into distillation (e.g., DMD/DMD2 and SANA-Sprint) to enhance performance introduces training instability, added complexity, and high GPU memory overhead due to the auxiliary trained models. To this end, we propose TwinFlow, a simple yet effective framework for training 1-step generative models that bypasses the need of fixed pretrained teacher models and avoids standard adversarial networks during training, making it ideal for building large-scale, efficient models. On text-to-image tasks, our method achieves a GenEval score of 0.83 in 1-NFE, outperforming strong baselines like SANA-Sprint (a GAN loss-based framework) and RCGM (a consistency-based framework). Notably, we demonstrate the scalability of TwinFlow by full-parameter training on Qwen-Image-20B and transform it into an efficient few-step generator. With just 1-NFE, our approach matches the performance of the original 100-NFE model on both the GenEval and DPG-Bench benchmarks, reducing computational cost by 100times with minor quality degradation. Project page is available at https://zhenglin-cheng.com/twinflow.",
            "upvotes": 46,
            "discussionId": "69365dcb3962c926cf6807fb",
            "ai_summary": "TwinFlow is a 1-step generative model framework that enhances inference efficiency without requiring fixed pretrained teacher models or standard adversarial networks, achieving high performance on text-to-image tasks and scaling efficiently.",
            "ai_keywords": [
                "diffusion",
                "flow matching",
                "inference efficiency",
                "Number of Function Evaluations",
                "few-step methods",
                "progressive distillation",
                "consistency distillation",
                "adversarial training",
                "DMD/DMD2",
                "SANA-Sprint",
                "TwinFlow",
                "GenEval",
                "RCGM",
                "Qwen-Image-20B",
                "DPG-Bench"
            ],
            "organization": {
                "_id": "67aea5c8f086ab0f70ed97c9",
                "name": "inclusionAI",
                "fullname": "inclusionAI",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/fyKuazRifqiaIO34xrhhm.jpeg"
            }
        },
        "publishedAt": "2025-12-03T02:45:46.000Z",
        "title": "TwinFlow: Realizing One-step Generation on Large Models with Self-adversarial Flows",
        "summary": "Recent advances in large multi-modal generative models have demonstrated impressive capabilities in multi-modal generation, including image and video generation. These models are typically built upon multi-step frameworks like diffusion and flow matching, which inherently limits their inference efficiency (requiring 40-100 Number of Function Evaluations (NFEs)). While various few-step methods aim to accelerate the inference, existing solutions have clear limitations. Prominent distillation-based methods, such as progressive and consistency distillation, either require an iterative distillation procedure or show significant degradation at very few steps (< 4-NFE). Meanwhile, integrating adversarial training into distillation (e.g., DMD/DMD2 and SANA-Sprint) to enhance performance introduces training instability, added complexity, and high GPU memory overhead due to the auxiliary trained models. To this end, we propose TwinFlow, a simple yet effective framework for training 1-step generative models that bypasses the need of fixed pretrained teacher models and avoids standard adversarial networks during training, making it ideal for building large-scale, efficient models. On text-to-image tasks, our method achieves a GenEval score of 0.83 in 1-NFE, outperforming strong baselines like SANA-Sprint (a GAN loss-based framework) and RCGM (a consistency-based framework). Notably, we demonstrate the scalability of TwinFlow by full-parameter training on Qwen-Image-20B and transform it into an efficient few-step generator. With just 1-NFE, our approach matches the performance of the original 100-NFE model on both the GenEval and DPG-Bench benchmarks, reducing computational cost by 100times with minor quality degradation. Project page is available at https://zhenglin-cheng.com/twinflow.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.05150.png",
        "numComments": 5,
        "submittedBy": {
            "_id": "65028e8389707f182386588c",
            "avatarUrl": "/avatars/86a748a3264e6e0f4ee5eaf8f7032ecb.svg",
            "fullname": "Zhenglin Cheng (SII)",
            "name": "kenshinn",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 14
        },
        "organization": {
            "_id": "67aea5c8f086ab0f70ed97c9",
            "name": "inclusionAI",
            "fullname": "inclusionAI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/fyKuazRifqiaIO34xrhhm.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.05965",
            "authors": [
                {
                    "_id": "693638ba3962c926cf680724",
                    "user": {
                        "_id": "66fd43e80cde4879f9aeca01",
                        "avatarUrl": "/avatars/7bc9afa5e023e00820333e8d18dc4bc5.svg",
                        "isPro": false,
                        "fullname": "Hongyu Li",
                        "user": "appletea2333",
                        "type": "user"
                    },
                    "name": "Hongyu Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-08T08:30:17.282Z",
                    "hidden": false
                },
                {
                    "_id": "693638ba3962c926cf680725",
                    "name": "Manyuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "693638ba3962c926cf680726",
                    "user": {
                        "_id": "67e60ae6ac37824273d74389",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/YvPKZ_0gyJnvNwM1zK3JS.png",
                        "isPro": false,
                        "fullname": "Dian Zheng",
                        "user": "zhengli1013",
                        "type": "user"
                    },
                    "name": "Dian Zheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-08T08:30:20.419Z",
                    "hidden": false
                },
                {
                    "_id": "693638ba3962c926cf680727",
                    "name": "Ziyu Guo",
                    "hidden": false
                },
                {
                    "_id": "693638ba3962c926cf680728",
                    "name": "Yimeng Jia",
                    "hidden": false
                },
                {
                    "_id": "693638ba3962c926cf680729",
                    "name": "Kaituo Feng",
                    "hidden": false
                },
                {
                    "_id": "693638ba3962c926cf68072a",
                    "user": {
                        "_id": "669205f1ccca14aa8f13f770",
                        "avatarUrl": "/avatars/11ce274e93345fe3790ac9fa687e2bcb.svg",
                        "isPro": false,
                        "fullname": "Hao Yu",
                        "user": "Longin-Yu",
                        "type": "user"
                    },
                    "name": "Hao Yu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-08T08:30:14.976Z",
                    "hidden": false
                },
                {
                    "_id": "693638ba3962c926cf68072b",
                    "name": "Yexin Liu",
                    "hidden": false
                },
                {
                    "_id": "693638ba3962c926cf68072c",
                    "name": "Yan Feng",
                    "hidden": false
                },
                {
                    "_id": "693638ba3962c926cf68072d",
                    "name": "Peng Pei",
                    "hidden": false
                },
                {
                    "_id": "693638ba3962c926cf68072e",
                    "name": "Xunliang Cai",
                    "hidden": false
                },
                {
                    "_id": "693638ba3962c926cf68072f",
                    "name": "Linjiang Huang",
                    "hidden": false
                },
                {
                    "_id": "693638ba3962c926cf680730",
                    "name": "Hongsheng Li",
                    "hidden": false
                },
                {
                    "_id": "693638ba3962c926cf680731",
                    "name": "Si Liu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/8f-me49cubcrpFx-hfaoM.gif"
            ],
            "publishedAt": "2025-12-05T18:58:09.000Z",
            "submittedOnDailyAt": "2025-12-08T00:03:16.853Z",
            "title": "EditThinker: Unlocking Iterative Reasoning for Any Image Editor",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Instruction-based image editing has emerged as a prominent research area, which, benefiting from image generation foundation models, have achieved high aesthetic quality, making instruction-following capability the primary challenge. Existing approaches improve instruction adherence via supervised or reinforcement learning, yet single-turn success rates remain limited due to inherent stochasticity and a lack of deliberation. In this work, we propose a deliberative editing framework to 'think' while they edit, which simulates the human cognitive loop by iteratively executing a Think-while-Edit cycle: Critiquing results and Refining instructions , followed by Repeating the generation until satisfactory. Specifically, we train a single MLLM, EditThinker, to act as the reasoning engine of this framework, which jointly produce the critique score, reasoning process, and refined instructions. We employ reinforcement learning to align the EditThinker's thinking with its editing, thereby generating more targeted instruction improvements. Extensive experiments on four benchmarks demonstrate that our approach significantly improves the instruction-following capability of any image editing model by a large margin. We will release our data construction framework, datasets, and models to benefit the community.",
            "upvotes": 32,
            "discussionId": "693638ba3962c926cf680732",
            "projectPage": "https://appletea233.github.io/think-while-edit/",
            "githubRepo": "https://github.com/appletea233/EditThinker",
            "ai_summary": "A deliberative editing framework with a reasoning engine improves instruction-following in image editing through iterative critique and refinement, significantly enhancing performance.",
            "ai_keywords": [
                "MLLM",
                "EditThinker",
                "reinforcement learning",
                "Think-while-Edit cycle",
                "Critiquing",
                "Refining instructions",
                "generation",
                "image editing",
                "instruction-following capability",
                "data construction framework"
            ],
            "githubStars": 25
        },
        "publishedAt": "2025-12-05T13:58:09.000Z",
        "title": "EditThinker: Unlocking Iterative Reasoning for Any Image Editor",
        "summary": "Instruction-based image editing has emerged as a prominent research area, which, benefiting from image generation foundation models, have achieved high aesthetic quality, making instruction-following capability the primary challenge. Existing approaches improve instruction adherence via supervised or reinforcement learning, yet single-turn success rates remain limited due to inherent stochasticity and a lack of deliberation. In this work, we propose a deliberative editing framework to 'think' while they edit, which simulates the human cognitive loop by iteratively executing a Think-while-Edit cycle: Critiquing results and Refining instructions , followed by Repeating the generation until satisfactory. Specifically, we train a single MLLM, EditThinker, to act as the reasoning engine of this framework, which jointly produce the critique score, reasoning process, and refined instructions. We employ reinforcement learning to align the EditThinker's thinking with its editing, thereby generating more targeted instruction improvements. Extensive experiments on four benchmarks demonstrate that our approach significantly improves the instruction-following capability of any image editing model by a large margin. We will release our data construction framework, datasets, and models to benefit the community.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/8f-me49cubcrpFx-hfaoM.gif"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.05965.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 178
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.02580",
            "authors": [
                {
                    "_id": "6934d2ba3962c926cf680575",
                    "name": "Changpeng Yang",
                    "hidden": false
                },
                {
                    "_id": "6934d2ba3962c926cf680576",
                    "name": "Jinyang Wu",
                    "hidden": false
                },
                {
                    "_id": "6934d2ba3962c926cf680577",
                    "name": "Yuchen Liu",
                    "hidden": false
                },
                {
                    "_id": "6934d2ba3962c926cf680578",
                    "name": "Shuai Zhang",
                    "hidden": false
                },
                {
                    "_id": "6934d2ba3962c926cf680579",
                    "name": "Yang Li",
                    "hidden": false
                },
                {
                    "_id": "6934d2ba3962c926cf68057a",
                    "name": "Qiliang Liang",
                    "hidden": false
                },
                {
                    "_id": "6934d2ba3962c926cf68057b",
                    "name": "Hongzhen Wang",
                    "hidden": false
                },
                {
                    "_id": "6934d2ba3962c926cf68057c",
                    "name": "Shuai Nie",
                    "hidden": false
                },
                {
                    "_id": "6934d2ba3962c926cf68057d",
                    "name": "Jiaming Xu",
                    "hidden": false
                },
                {
                    "_id": "6934d2ba3962c926cf68057e",
                    "name": "Runyu Shi",
                    "hidden": false
                },
                {
                    "_id": "6934d2ba3962c926cf68057f",
                    "name": "Ying Huang",
                    "hidden": false
                },
                {
                    "_id": "6934d2ba3962c926cf680580",
                    "name": "Guoquan Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-02T09:48:57.000Z",
            "submittedOnDailyAt": "2025-12-08T01:16:07.593Z",
            "title": "From Imitation to Discrimination: Toward A Generalized Curriculum Advantage Mechanism Enhancing Cross-Domain Reasoning Tasks",
            "submittedOnDailyBy": {
                "_id": "6747de57f8cab58c22ec94a2",
                "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg",
                "isPro": false,
                "fullname": "Jinyang Wu",
                "user": "Jinyang23",
                "type": "user"
            },
            "summary": "Reinforcement learning has emerged as a paradigm for post-training large language models, boosting their reasoning capabilities. Such approaches compute an advantage value for each sample, reflecting better or worse performance than expected, thereby yielding both positive and negative signals for training. However, the indiscriminate mixing of the two signals in existing methods, especially from the early stages, may lead to ambiguous guidance and limited gains. To address this issue, we propose **CAPO** (**C**urriculum **A**dvantage **P**olicy **O**ptimization), an adaptive curriculum mechanism based on advantage signals. The proposed mechanism bootstraps imitation learning with positive-only advantage samples to establish robust foundations, and subsequently introduces negative signals to cultivate discriminative capabilities, thereby improving generalization across complex scenarios. Compatible with diverse optimization methods including GRPO, PPO, RLOO, and Reinforce++, our method consistently achieves stable and significant improvements in mathematical reasoning tasks, and further generalizes effectively to multimodal Graphical User Interface (GUI) reasoning scenarios, establishing itself as a versatile and robust optimization framework.",
            "upvotes": 25,
            "discussionId": "6934d2ba3962c926cf680581",
            "ai_summary": "CAPO, a curriculum advantage policy optimization, enhances reinforcement learning for large language models by strategically introducing positive and negative advantage signals, improving reasoning capabilities and generalization.",
            "ai_keywords": [
                "reinforcement learning",
                "post-training",
                "large language models",
                "reasoning capabilities",
                "advantage value",
                "positive signals",
                "negative signals",
                "curriculum mechanism",
                "imitation learning",
                "discriminative capabilities",
                "generalization",
                "GRPO",
                "PPO",
                "RLOO",
                "Reinforce++",
                "mathematical reasoning tasks",
                "multimodal Graphical User Interface (GUI) reasoning scenarios",
                "optimization framework"
            ]
        },
        "publishedAt": "2025-12-02T04:48:57.000Z",
        "title": "From Imitation to Discrimination: Toward A Generalized Curriculum Advantage Mechanism Enhancing Cross-Domain Reasoning Tasks",
        "summary": "Reinforcement learning has emerged as a paradigm for post-training large language models, boosting their reasoning capabilities. Such approaches compute an advantage value for each sample, reflecting better or worse performance than expected, thereby yielding both positive and negative signals for training. However, the indiscriminate mixing of the two signals in existing methods, especially from the early stages, may lead to ambiguous guidance and limited gains. To address this issue, we propose **CAPO** (**C**urriculum **A**dvantage **P**olicy **O**ptimization), an adaptive curriculum mechanism based on advantage signals. The proposed mechanism bootstraps imitation learning with positive-only advantage samples to establish robust foundations, and subsequently introduces negative signals to cultivate discriminative capabilities, thereby improving generalization across complex scenarios. Compatible with diverse optimization methods including GRPO, PPO, RLOO, and Reinforce++, our method consistently achieves stable and significant improvements in mathematical reasoning tasks, and further generalizes effectively to multimodal Graphical User Interface (GUI) reasoning scenarios, establishing itself as a versatile and robust optimization framework.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02580.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6747de57f8cab58c22ec94a2",
            "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg",
            "fullname": "Jinyang Wu",
            "name": "Jinyang23",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.04810",
            "authors": [
                {
                    "_id": "6932753c6d1060ca587a27b3",
                    "name": "Xin He",
                    "hidden": false
                },
                {
                    "_id": "6932753c6d1060ca587a27b4",
                    "name": "Longhui Wei",
                    "hidden": false
                },
                {
                    "_id": "6932753c6d1060ca587a27b5",
                    "name": "Jianbo Ouyang",
                    "hidden": false
                },
                {
                    "_id": "6932753c6d1060ca587a27b6",
                    "name": "Lingxi Xie",
                    "hidden": false
                },
                {
                    "_id": "6932753c6d1060ca587a27b7",
                    "name": "Qi Tian",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-04T14:01:53.000Z",
            "submittedOnDailyAt": "2025-12-08T11:07:32.487Z",
            "title": "EMMA: Efficient Multimodal Understanding, Generation, and Editing with a Unified Architecture",
            "submittedOnDailyBy": {
                "_id": "66bb8186150fad8b4e3b1f8c",
                "avatarUrl": "/avatars/1e95838ed1418e9c52338931b9f9f92e.svg",
                "isPro": false,
                "fullname": "Longhui Wei",
                "user": "Joohnzxcv",
                "type": "user"
            },
            "summary": "We propose EMMA, an efficient and unified architecture for multimodal understanding, generation and editing. Specifically, EMMA primarily consists of 1) An efficient autoencoder with a 32x compression ratio, which significantly reduces the number of tokens required for generation. This also ensures the training balance between understanding and generation tasks by applying the same compression ratio to images. 2) Channel-wise concatenation instead of token-wise concatenation among visual understanding and generation tokens, which further reduces the visual tokens in unified architectures. 3) A shared-and-decoupled network that enables mutual improvements across tasks while meeting the task-specific modeling requirements. 4) A mixture-of-experts mechanism adopted for visual understanding encoder, which substantially improves perceptual capabilities with a few parameters increase. Extensive experiments have shown that EMMA-4B can significantly outperform state-of-the-art unified multimodal approaches (e.g., BAGEL-7B) in both efficiency and performance, while also achieving competitive results compared to recent multimodal understanding and generation experts (e.g., Qwen3-VL and Qwen-Image). We believe that EMMA lays a solid foundation for the future development of unified multimodal architectures.",
            "upvotes": 22,
            "discussionId": "6932753c6d1060ca587a27b8",
            "projectPage": "https://emma-umm.github.io/emma/",
            "githubRepo": "https://github.com/umm-emma/emma",
            "ai_summary": "EMMA is an efficient unified architecture for multimodal tasks that uses autoencoders, channel-wise concatenation, shared-and-decoupled networks, and mixture-of-experts to achieve superior performance and efficiency.",
            "ai_keywords": [
                "autoencoder",
                "channel-wise concatenation",
                "shared-and-decoupled network",
                "mixture-of-experts",
                "multimodal understanding",
                "multimodal generation",
                "perceptual capabilities"
            ],
            "githubStars": 13
        },
        "publishedAt": "2025-12-04T09:01:53.000Z",
        "title": "EMMA: Efficient Multimodal Understanding, Generation, and Editing with a Unified Architecture",
        "summary": "We propose EMMA, an efficient and unified architecture for multimodal understanding, generation and editing. Specifically, EMMA primarily consists of 1) An efficient autoencoder with a 32x compression ratio, which significantly reduces the number of tokens required for generation. This also ensures the training balance between understanding and generation tasks by applying the same compression ratio to images. 2) Channel-wise concatenation instead of token-wise concatenation among visual understanding and generation tokens, which further reduces the visual tokens in unified architectures. 3) A shared-and-decoupled network that enables mutual improvements across tasks while meeting the task-specific modeling requirements. 4) A mixture-of-experts mechanism adopted for visual understanding encoder, which substantially improves perceptual capabilities with a few parameters increase. Extensive experiments have shown that EMMA-4B can significantly outperform state-of-the-art unified multimodal approaches (e.g., BAGEL-7B) in both efficiency and performance, while also achieving competitive results compared to recent multimodal understanding and generation experts (e.g., Qwen3-VL and Qwen-Image). We believe that EMMA lays a solid foundation for the future development of unified multimodal architectures.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04810.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "66bb8186150fad8b4e3b1f8c",
            "avatarUrl": "/avatars/1e95838ed1418e9c52338931b9f9f92e.svg",
            "fullname": "Longhui Wei",
            "name": "Joohnzxcv",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.04784",
            "authors": [
                {
                    "_id": "693385573962c926cf68049c",
                    "name": "Bowen Ping",
                    "hidden": false
                },
                {
                    "_id": "693385573962c926cf68049d",
                    "user": {
                        "_id": "6602548a68d519ed324b47c5",
                        "avatarUrl": "/avatars/5ab411f87440cc2a98c7a1c6a3ed5548.svg",
                        "isPro": false,
                        "fullname": "ChengyouJia",
                        "user": "ChengyouJia",
                        "type": "user"
                    },
                    "name": "Chengyou Jia",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-08T08:31:17.418Z",
                    "hidden": false
                },
                {
                    "_id": "693385573962c926cf68049e",
                    "name": "Minnan Luo",
                    "hidden": false
                },
                {
                    "_id": "693385573962c926cf68049f",
                    "name": "Changliang Xia",
                    "hidden": false
                },
                {
                    "_id": "693385573962c926cf6804a0",
                    "name": "Xin Shen",
                    "hidden": false
                },
                {
                    "_id": "693385573962c926cf6804a1",
                    "name": "Zhuohang Dang",
                    "hidden": false
                },
                {
                    "_id": "693385573962c926cf6804a2",
                    "name": "Hangwei Qian",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-02T13:39:03.000Z",
            "submittedOnDailyAt": "2025-12-08T01:22:57.723Z",
            "title": "PaCo-RL: Advancing Reinforcement Learning for Consistent Image Generation with Pairwise Reward Modeling",
            "submittedOnDailyBy": {
                "_id": "6602548a68d519ed324b47c5",
                "avatarUrl": "/avatars/5ab411f87440cc2a98c7a1c6a3ed5548.svg",
                "isPro": false,
                "fullname": "ChengyouJia",
                "user": "ChengyouJia",
                "type": "user"
            },
            "summary": "Consistent image generation requires faithfully preserving identities, styles, and logical coherence across multiple images, which is essential for applications such as storytelling and character design. Supervised training approaches struggle with this task due to the lack of large-scale datasets capturing visual consistency and the complexity of modeling human perceptual preferences. In this paper, we argue that reinforcement learning (RL) offers a promising alternative by enabling models to learn complex and subjective visual criteria in a data-free manner. To achieve this, we introduce PaCo-RL, a comprehensive framework that combines a specialized consistency reward model with an efficient RL algorithm. The first component, PaCo-Reward, is a pairwise consistency evaluator trained on a large-scale dataset constructed via automated sub-figure pairing. It evaluates consistency through a generative, autoregressive scoring mechanism enhanced by task-aware instructions and CoT reasons. The second component, PaCo-GRPO, leverages a novel resolution-decoupled optimization strategy to substantially reduce RL cost, alongside a log-tamed multi-reward aggregation mechanism that ensures balanced and stable reward optimization. Extensive experiments across the two representative subtasks show that PaCo-Reward significantly improves alignment with human perceptions of visual consistency, and PaCo-GRPO achieves state-of-the-art consistency performance with improved training efficiency and stability. Together, these results highlight the promise of PaCo-RL as a practical and scalable solution for consistent image generation. The project page is available at https://x-gengroup.github.io/HomePage_PaCo-RL/.",
            "upvotes": 20,
            "discussionId": "693385583962c926cf6804a3",
            "projectPage": "https://x-gengroup.github.io/HomePage_PaCo-RL/",
            "githubRepo": "https://github.com/X-GenGroup/PaCo-RL",
            "ai_summary": "PaCo-RL combines reinforcement learning with a specialized consistency reward model and an efficient optimization strategy to improve consistent image generation.",
            "ai_keywords": [
                "reinforcement learning",
                "PaCo-RL",
                "PaCo-Reward",
                "pairwise consistency evaluator",
                "generative scoring mechanism",
                "autoregressive scoring mechanism",
                "task-aware instructions",
                "CoT reasons",
                "PaCo-GRPO",
                "resolution-decoupled optimization",
                "log-tamed multi-reward aggregation",
                "consistency performance",
                "training efficiency",
                "stability"
            ],
            "githubStars": 14,
            "organization": {
                "_id": "692ea0d54f38e991f11118e3",
                "name": "X-GenGroup",
                "fullname": "X-Gen Group",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/692e9b90b58acaf7e3a46774/rr1cR56-fHn0WHPm6dYwd.jpeg"
            }
        },
        "publishedAt": "2025-12-02T08:39:03.000Z",
        "title": "PaCo-RL: Advancing Reinforcement Learning for Consistent Image Generation with Pairwise Reward Modeling",
        "summary": "Consistent image generation requires faithfully preserving identities, styles, and logical coherence across multiple images, which is essential for applications such as storytelling and character design. Supervised training approaches struggle with this task due to the lack of large-scale datasets capturing visual consistency and the complexity of modeling human perceptual preferences. In this paper, we argue that reinforcement learning (RL) offers a promising alternative by enabling models to learn complex and subjective visual criteria in a data-free manner. To achieve this, we introduce PaCo-RL, a comprehensive framework that combines a specialized consistency reward model with an efficient RL algorithm. The first component, PaCo-Reward, is a pairwise consistency evaluator trained on a large-scale dataset constructed via automated sub-figure pairing. It evaluates consistency through a generative, autoregressive scoring mechanism enhanced by task-aware instructions and CoT reasons. The second component, PaCo-GRPO, leverages a novel resolution-decoupled optimization strategy to substantially reduce RL cost, alongside a log-tamed multi-reward aggregation mechanism that ensures balanced and stable reward optimization. Extensive experiments across the two representative subtasks show that PaCo-Reward significantly improves alignment with human perceptions of visual consistency, and PaCo-GRPO achieves state-of-the-art consistency performance with improved training efficiency and stability. Together, these results highlight the promise of PaCo-RL as a practical and scalable solution for consistent image generation. The project page is available at https://x-gengroup.github.io/HomePage_PaCo-RL/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04784.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6602548a68d519ed324b47c5",
            "avatarUrl": "/avatars/5ab411f87440cc2a98c7a1c6a3ed5548.svg",
            "fullname": "ChengyouJia",
            "name": "ChengyouJia",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "organization": {
            "_id": "692ea0d54f38e991f11118e3",
            "name": "X-GenGroup",
            "fullname": "X-Gen Group",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/692e9b90b58acaf7e3a46774/rr1cR56-fHn0WHPm6dYwd.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.05905",
            "authors": [
                {
                    "_id": "69363c253962c926cf680776",
                    "name": "Wenhao Yan",
                    "hidden": false
                },
                {
                    "_id": "69363c253962c926cf680777",
                    "name": "Sheng Ye",
                    "hidden": false
                },
                {
                    "_id": "69363c253962c926cf680778",
                    "name": "Zhuoyi Yang",
                    "hidden": false
                },
                {
                    "_id": "69363c253962c926cf680779",
                    "user": {
                        "_id": "65228733377bffdc59a10117",
                        "avatarUrl": "/avatars/6eec07553658ab22f8058caa0bfbed49.svg",
                        "isPro": false,
                        "fullname": "tengjiayan",
                        "user": "tengjiayan",
                        "type": "user"
                    },
                    "name": "Jiayan Teng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-08T09:01:04.763Z",
                    "hidden": false
                },
                {
                    "_id": "69363c253962c926cf68077a",
                    "name": "ZhenHui Dong",
                    "hidden": false
                },
                {
                    "_id": "69363c253962c926cf68077b",
                    "user": {
                        "_id": "66604ca821f3ae1d9e7ef3ef",
                        "avatarUrl": "/avatars/e2351897678b502554c0f217f4a5b9df.svg",
                        "isPro": false,
                        "fullname": "Wen Kairui",
                        "user": "SKearbvaanl",
                        "type": "user"
                    },
                    "name": "Kairui Wen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-08T09:00:44.740Z",
                    "hidden": false
                },
                {
                    "_id": "69363c253962c926cf68077c",
                    "user": {
                        "_id": "6438bca4a5e10f6d58694b47",
                        "avatarUrl": "/avatars/3aeb25fbc73c5cab1265e13d11adfb76.svg",
                        "isPro": false,
                        "fullname": "Xiaotao Gu",
                        "user": "xgeric",
                        "type": "user"
                    },
                    "name": "Xiaotao Gu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-08T09:00:34.434Z",
                    "hidden": false
                },
                {
                    "_id": "69363c253962c926cf68077d",
                    "name": "Yong-Jin Liu",
                    "hidden": false
                },
                {
                    "_id": "69363c253962c926cf68077e",
                    "user": {
                        "_id": "640dff05474aa6f89556677e",
                        "avatarUrl": "/avatars/1b4591c7322d649c797b3125148f1915.svg",
                        "isPro": false,
                        "fullname": "Jie Tang",
                        "user": "jerytang",
                        "type": "user"
                    },
                    "name": "Jie Tang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-08T09:00:18.245Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63a369d98c0c89dcae3b8329/WXrZe3fv7C3YRAazHHfZM.qt"
            ],
            "publishedAt": "2025-12-05T17:38:55.000Z",
            "submittedOnDailyAt": "2025-12-08T05:59:29.481Z",
            "title": "SCAIL: Towards Studio-Grade Character Animation via In-Context Learning of 3D-Consistent Pose Representations",
            "submittedOnDailyBy": {
                "_id": "63a369d98c0c89dcae3b8329",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg",
                "isPro": false,
                "fullname": "Adina Yakefu",
                "user": "AdinaY",
                "type": "user"
            },
            "summary": "Achieving character animation that meets studio-grade production standards remains challenging despite recent progress. Existing approaches can transfer motion from a driving video to a reference image, but often fail to preserve structural fidelity and temporal consistency in wild scenarios involving complex motion and cross-identity animations. In this work, we present SCAIL (Studio-grade Character Animation via In-context Learning), a framework designed to address these challenges from two key innovations. First, we propose a novel 3D pose representation, providing a more robust and flexible motion signal. Second, we introduce a full-context pose injection mechanism within a diffusion-transformer architecture, enabling effective spatio-temporal reasoning over full motion sequences. To align with studio-level requirements, we develop a curated data pipeline ensuring both diversity and quality, and establish a comprehensive benchmark for systematic evaluation. Experiments show that SCAIL achieves state-of-the-art performance and advances character animation toward studio-grade reliability and realism.",
            "upvotes": 13,
            "discussionId": "69363c263962c926cf68077f",
            "ai_summary": "SCAIL framework improves character animation by using a novel 3D pose representation and a diffusion-transformer architecture with full-context pose injection, achieving studio-grade quality and realism.",
            "ai_keywords": [
                "3D pose representation",
                "diffusion-transformer architecture",
                "full-context pose injection",
                "studio-grade animation",
                "spatio-temporal reasoning",
                "motion sequences"
            ],
            "organization": {
                "_id": "62ad27f19096e7f9ecb1853a",
                "name": "zai-org",
                "fullname": "Z.ai",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62dc173789b4cf157d36ebee/i_pxzM2ZDo3Ub-BEgIkE9.png"
            }
        },
        "publishedAt": "2025-12-05T12:38:55.000Z",
        "title": "SCAIL: Towards Studio-Grade Character Animation via In-Context Learning of 3D-Consistent Pose Representations",
        "summary": "Achieving character animation that meets studio-grade production standards remains challenging despite recent progress. Existing approaches can transfer motion from a driving video to a reference image, but often fail to preserve structural fidelity and temporal consistency in wild scenarios involving complex motion and cross-identity animations. In this work, we present SCAIL (Studio-grade Character Animation via In-context Learning), a framework designed to address these challenges from two key innovations. First, we propose a novel 3D pose representation, providing a more robust and flexible motion signal. Second, we introduce a full-context pose injection mechanism within a diffusion-transformer architecture, enabling effective spatio-temporal reasoning over full motion sequences. To align with studio-level requirements, we develop a curated data pipeline ensuring both diversity and quality, and establish a comprehensive benchmark for systematic evaluation. Experiments show that SCAIL achieves state-of-the-art performance and advances character animation toward studio-grade reliability and realism.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63a369d98c0c89dcae3b8329/WXrZe3fv7C3YRAazHHfZM.qt"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.05905.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63a369d98c0c89dcae3b8329",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg",
            "fullname": "Adina Yakefu",
            "name": "AdinaY",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1187
        },
        "organization": {
            "_id": "62ad27f19096e7f9ecb1853a",
            "name": "zai-org",
            "fullname": "Z.ai",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62dc173789b4cf157d36ebee/i_pxzM2ZDo3Ub-BEgIkE9.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.05591",
            "authors": [
                {
                    "_id": "6936451a3962c926cf6807d6",
                    "user": {
                        "_id": "61c2cf8d1172fa7969904d99",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61c2cf8d1172fa7969904d99/R10G5h3d9Q_YQ__Hc-H4k.jpeg",
                        "isPro": false,
                        "fullname": "suu",
                        "user": "Suu",
                        "type": "user"
                    },
                    "name": "Zhenpeng Su",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-08T08:29:49.837Z",
                    "hidden": false
                },
                {
                    "_id": "6936451a3962c926cf6807d7",
                    "name": "Leiyu Pan",
                    "hidden": false
                },
                {
                    "_id": "6936451a3962c926cf6807d8",
                    "name": "Minxuan Lv",
                    "hidden": false
                },
                {
                    "_id": "6936451a3962c926cf6807d9",
                    "name": "Tiehua Mei",
                    "hidden": false
                },
                {
                    "_id": "6936451a3962c926cf6807da",
                    "name": "Zijia Lin",
                    "hidden": false
                },
                {
                    "_id": "6936451a3962c926cf6807db",
                    "name": "Yuntao Li",
                    "hidden": false
                },
                {
                    "_id": "6936451a3962c926cf6807dc",
                    "name": "Wenping Hu",
                    "hidden": false
                },
                {
                    "_id": "6936451a3962c926cf6807dd",
                    "name": "Ruiming Tang",
                    "hidden": false
                },
                {
                    "_id": "6936451a3962c926cf6807de",
                    "name": "Kun Gai",
                    "hidden": false
                },
                {
                    "_id": "6936451a3962c926cf6807df",
                    "name": "Guorui Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-05T10:26:32.000Z",
            "submittedOnDailyAt": "2025-12-08T00:57:20.286Z",
            "title": "Entropy Ratio Clipping as a Soft Global Constraint for Stable Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "61c2cf8d1172fa7969904d99",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61c2cf8d1172fa7969904d99/R10G5h3d9Q_YQ__Hc-H4k.jpeg",
                "isPro": false,
                "fullname": "suu",
                "user": "Suu",
                "type": "user"
            },
            "summary": "Large language model post-training relies on reinforcement learning to improve model capability and alignment quality. However, the off-policy training paradigm introduces distribution shift, which often pushes the policy beyond the trust region, leading to training instabilities manifested as fluctuations in policy entropy and unstable gradients. Although PPO-Clip mitigates this issue through importance clipping, it still overlooks the global distributional shift of actions. To address these challenges, we propose using the entropy ratio between the current and previous policies as a new global metric that effectively quantifies the relative change in policy exploration throughout updates. Building on this metric, we introduce an Entropy Ratio Clipping (ERC) mechanism that imposes bidirectional constraints on the entropy ratio. This stabilizes policy updates at the global distribution level and compensates for the inability of PPO-clip to regulate probability shifts of un-sampled actions. We integrate ERC into both DAPO and GPPO reinforcement learning algorithms. Experiments across multiple benchmarks show that ERC consistently improves performance.",
            "upvotes": 13,
            "discussionId": "6936451b3962c926cf6807e0",
            "projectPage": "https://huggingface.co/collections/Kwai-Klear/klearreasoner",
            "ai_summary": "Entropy Ratio Clipping (ERC) mechanism stabilizes policy updates in reinforcement learning by addressing global distributional shifts, improving performance over existing methods.",
            "ai_keywords": [
                "reinforcement learning",
                "distribution shift",
                "trust region",
                "policy entropy",
                "unstable gradients",
                "PPO-Clip",
                "entropy ratio",
                "Entropy Ratio Clipping",
                "ERC",
                "DAPO",
                "GPPO"
            ]
        },
        "publishedAt": "2025-12-05T05:26:32.000Z",
        "title": "Entropy Ratio Clipping as a Soft Global Constraint for Stable Reinforcement Learning",
        "summary": "Large language model post-training relies on reinforcement learning to improve model capability and alignment quality. However, the off-policy training paradigm introduces distribution shift, which often pushes the policy beyond the trust region, leading to training instabilities manifested as fluctuations in policy entropy and unstable gradients. Although PPO-Clip mitigates this issue through importance clipping, it still overlooks the global distributional shift of actions. To address these challenges, we propose using the entropy ratio between the current and previous policies as a new global metric that effectively quantifies the relative change in policy exploration throughout updates. Building on this metric, we introduce an Entropy Ratio Clipping (ERC) mechanism that imposes bidirectional constraints on the entropy ratio. This stabilizes policy updates at the global distribution level and compensates for the inability of PPO-clip to regulate probability shifts of un-sampled actions. We integrate ERC into both DAPO and GPPO reinforcement learning algorithms. Experiments across multiple benchmarks show that ERC consistently improves performance.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.05591.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "61c2cf8d1172fa7969904d99",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61c2cf8d1172fa7969904d99/R10G5h3d9Q_YQ__Hc-H4k.jpeg",
            "fullname": "suu",
            "name": "Suu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.05044",
            "authors": [
                {
                    "_id": "69363ff73962c926cf680790",
                    "user": {
                        "_id": "661cfae9a853782abad2a495",
                        "avatarUrl": "/avatars/39723a07bf9efed8278e009fe966d044.svg",
                        "isPro": false,
                        "fullname": "Yanran Zhang",
                        "user": "Yanran21",
                        "type": "user"
                    },
                    "name": "Yanran Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-08T08:30:00.324Z",
                    "hidden": false
                },
                {
                    "_id": "69363ff73962c926cf680791",
                    "name": "Ziyi Wang",
                    "hidden": false
                },
                {
                    "_id": "69363ff73962c926cf680792",
                    "name": "Wenzhao Zheng",
                    "hidden": false
                },
                {
                    "_id": "69363ff73962c926cf680793",
                    "name": "Zheng Zhu",
                    "hidden": false
                },
                {
                    "_id": "69363ff73962c926cf680794",
                    "name": "Jie Zhou",
                    "hidden": false
                },
                {
                    "_id": "69363ff73962c926cf680795",
                    "name": "Jiwen Lu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-04T17:59:10.000Z",
            "submittedOnDailyAt": "2025-12-08T00:40:23.601Z",
            "title": "Joint 3D Geometry Reconstruction and Motion Generation for 4D Synthesis from a Single Image",
            "submittedOnDailyBy": {
                "_id": "661cfae9a853782abad2a495",
                "avatarUrl": "/avatars/39723a07bf9efed8278e009fe966d044.svg",
                "isPro": false,
                "fullname": "Yanran Zhang",
                "user": "Yanran21",
                "type": "user"
            },
            "summary": "Generating interactive and dynamic 4D scenes from a single static image remains a core challenge. Most existing generate-then-reconstruct and reconstruct-then-generate methods decouple geometry from motion, causing spatiotemporal inconsistencies and poor generalization. To address these, we extend the reconstruct-then-generate framework to jointly perform Motion generation and geometric Reconstruction for 4D Synthesis (MoRe4D). We first introduce TrajScene-60K, a large-scale dataset of 60,000 video samples with dense point trajectories, addressing the scarcity of high-quality 4D scene data. Based on this, we propose a diffusion-based 4D Scene Trajectory Generator (4D-STraG) to jointly generate geometrically consistent and motion-plausible 4D point trajectories. To leverage single-view priors, we design a depth-guided motion normalization strategy and a motion-aware module for effective geometry and dynamics integration. We then propose a 4D View Synthesis Module (4D-ViSM) to render videos with arbitrary camera trajectories from 4D point track representations. Experiments show that MoRe4D generates high-quality 4D scenes with multi-view consistency and rich dynamic details from a single image. Code: https://github.com/Zhangyr2022/MoRe4D.",
            "upvotes": 12,
            "discussionId": "69363ff83962c926cf680796",
            "projectPage": "https://ivg-yanranzhang.github.io/MoRe4D/",
            "githubRepo": "https://github.com/Zhangyr2022/MoRe4D",
            "ai_summary": "MoRe4D generates high-quality 4D scenes with multi-view consistency and dynamic details from a single image using a diffusion-based trajectory generator and depth-guided motion normalization.",
            "ai_keywords": [
                "TrajScene-60K",
                "diffusion-based 4D Scene Trajectory Generator",
                "4D-STraG",
                "depth-guided motion normalization",
                "motion-aware module",
                "4D View Synthesis Module",
                "4D-ViSM",
                "multi-view consistency",
                "dynamic details"
            ],
            "githubStars": 37,
            "organization": {
                "_id": "693649ff6df58c411109e13e",
                "name": "Tsinghua-IVG",
                "fullname": "Tsinghua-IVG",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/661cfae9a853782abad2a495/RskTCzyJeWVIs5-RJA2QR.png"
            }
        },
        "publishedAt": "2025-12-04T12:59:10.000Z",
        "title": "Joint 3D Geometry Reconstruction and Motion Generation for 4D Synthesis from a Single Image",
        "summary": "Generating interactive and dynamic 4D scenes from a single static image remains a core challenge. Most existing generate-then-reconstruct and reconstruct-then-generate methods decouple geometry from motion, causing spatiotemporal inconsistencies and poor generalization. To address these, we extend the reconstruct-then-generate framework to jointly perform Motion generation and geometric Reconstruction for 4D Synthesis (MoRe4D). We first introduce TrajScene-60K, a large-scale dataset of 60,000 video samples with dense point trajectories, addressing the scarcity of high-quality 4D scene data. Based on this, we propose a diffusion-based 4D Scene Trajectory Generator (4D-STraG) to jointly generate geometrically consistent and motion-plausible 4D point trajectories. To leverage single-view priors, we design a depth-guided motion normalization strategy and a motion-aware module for effective geometry and dynamics integration. We then propose a 4D View Synthesis Module (4D-ViSM) to render videos with arbitrary camera trajectories from 4D point track representations. Experiments show that MoRe4D generates high-quality 4D scenes with multi-view consistency and rich dynamic details from a single image. Code: https://github.com/Zhangyr2022/MoRe4D.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.05044.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "661cfae9a853782abad2a495",
            "avatarUrl": "/avatars/39723a07bf9efed8278e009fe966d044.svg",
            "fullname": "Yanran Zhang",
            "name": "Yanran21",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "693649ff6df58c411109e13e",
            "name": "Tsinghua-IVG",
            "fullname": "Tsinghua-IVG",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/661cfae9a853782abad2a495/RskTCzyJeWVIs5-RJA2QR.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.04563",
            "authors": [
                {
                    "_id": "693530fb3962c926cf6805ef",
                    "user": {
                        "_id": "642c2dcec3694d2b74565c48",
                        "avatarUrl": "/avatars/31243bb505f8c511ebd7492eaf3ea1a9.svg",
                        "isPro": false,
                        "fullname": "zhangzef",
                        "user": "Starrrrrry",
                        "type": "user"
                    },
                    "name": "Zefeng Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-08T08:30:43.319Z",
                    "hidden": false
                },
                {
                    "_id": "693530fb3962c926cf6805f0",
                    "user": {
                        "_id": "64ec6025b96ff0e175728ac0",
                        "avatarUrl": "/avatars/a1acbe22b5e5105703e7912da2cfced2.svg",
                        "isPro": false,
                        "fullname": "hxz",
                        "user": "CUDAOUTOFMEMORY",
                        "type": "user"
                    },
                    "name": "Xiangzhao Hao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-08T08:56:20.670Z",
                    "hidden": false
                },
                {
                    "_id": "693530fb3962c926cf6805f1",
                    "name": "Hengzhu Tang",
                    "hidden": false
                },
                {
                    "_id": "693530fb3962c926cf6805f2",
                    "name": "Zhenyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "693530fb3962c926cf6805f3",
                    "name": "Jiawei Sheng",
                    "hidden": false
                },
                {
                    "_id": "693530fb3962c926cf6805f4",
                    "name": "Xiaodong Li",
                    "hidden": false
                },
                {
                    "_id": "693530fb3962c926cf6805f5",
                    "name": "Zhenyang Li",
                    "hidden": false
                },
                {
                    "_id": "693530fb3962c926cf6805f6",
                    "name": "Li Gao",
                    "hidden": false
                },
                {
                    "_id": "693530fb3962c926cf6805f7",
                    "name": "Daiting Shi",
                    "hidden": false
                },
                {
                    "_id": "693530fb3962c926cf6805f8",
                    "name": "Dawei Yin",
                    "hidden": false
                },
                {
                    "_id": "693530fb3962c926cf6805f9",
                    "name": "Tingwen Liu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/642c2dcec3694d2b74565c48/J9N--2Y_pvsvuy2qjS63y.png",
                "https://cdn-uploads.huggingface.co/production/uploads/642c2dcec3694d2b74565c48/Te_kKXnGIMDd7njmIkXQ3.png"
            ],
            "publishedAt": "2025-12-04T08:26:04.000Z",
            "submittedOnDailyAt": "2025-12-08T01:11:05.761Z",
            "title": "COOPER: A Unified Model for Cooperative Perception and Reasoning in Spatial Intelligence",
            "submittedOnDailyBy": {
                "_id": "642c2dcec3694d2b74565c48",
                "avatarUrl": "/avatars/31243bb505f8c511ebd7492eaf3ea1a9.svg",
                "isPro": false,
                "fullname": "zhangzef",
                "user": "Starrrrrry",
                "type": "user"
            },
            "summary": "Visual Spatial Reasoning is crucial for enabling Multimodal Large Language Models (MLLMs) to understand object properties and spatial relationships, yet current models still struggle with 3D-aware reasoning. Existing approaches typically enhance either perception, by augmenting RGB inputs with auxiliary modalities such as depth and segmentation, or reasoning, by training on spatial VQA datasets and applying reinforcement learning, and thus treat these two aspects in isolation. In this work, we investigate whether a unified MLLM can develop an intrinsic ability to enhance spatial perception and, through adaptive interleaved reasoning, achieve stronger spatial intelligence. We propose COOPER, a unified MLLM that leverages depth and segmentation as auxiliary modalities and is trained in two stages to acquire auxiliary modality generation and adaptive, interleaved reasoning capabilities. COOPER achieves an average 6.91\\% improvement in spatial reasoning while maintaining general performance. Moreover, even a variant trained only for auxiliary modality generation attains a 7.92\\% gain on distance and size estimation, suggesting that learning to generate auxiliary modalities helps internalize spatial knowledge and strengthen spatial understanding.",
            "upvotes": 11,
            "discussionId": "693530fc3962c926cf6805fa",
            "githubRepo": "https://github.com/zhangzef/COOPER",
            "ai_summary": "A unified multimodal large language model (MLLM) that integrates depth and segmentation modalities enhances spatial reasoning and perception through adaptive interleaved reasoning, improving spatial intelligence and general performance.",
            "ai_keywords": [
                "Multimodal Large Language Models (MLLMs)",
                "visual spatial reasoning",
                "3D-aware reasoning",
                "spatial VQA datasets",
                "reinforcement learning",
                "auxiliary modalities",
                "depth",
                "segmentation",
                "adaptive interleaved reasoning",
                "spatial perception",
                "spatial intelligence",
                "distance estimation",
                "size estimation"
            ],
            "githubStars": 5
        },
        "publishedAt": "2025-12-04T03:26:04.000Z",
        "title": "COOPER: A Unified Model for Cooperative Perception and Reasoning in Spatial Intelligence",
        "summary": "Visual Spatial Reasoning is crucial for enabling Multimodal Large Language Models (MLLMs) to understand object properties and spatial relationships, yet current models still struggle with 3D-aware reasoning. Existing approaches typically enhance either perception, by augmenting RGB inputs with auxiliary modalities such as depth and segmentation, or reasoning, by training on spatial VQA datasets and applying reinforcement learning, and thus treat these two aspects in isolation. In this work, we investigate whether a unified MLLM can develop an intrinsic ability to enhance spatial perception and, through adaptive interleaved reasoning, achieve stronger spatial intelligence. We propose COOPER, a unified MLLM that leverages depth and segmentation as auxiliary modalities and is trained in two stages to acquire auxiliary modality generation and adaptive, interleaved reasoning capabilities. COOPER achieves an average 6.91\\% improvement in spatial reasoning while maintaining general performance. Moreover, even a variant trained only for auxiliary modality generation attains a 7.92\\% gain on distance and size estimation, suggesting that learning to generate auxiliary modalities helps internalize spatial knowledge and strengthen spatial understanding.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/642c2dcec3694d2b74565c48/J9N--2Y_pvsvuy2qjS63y.png",
            "https://cdn-uploads.huggingface.co/production/uploads/642c2dcec3694d2b74565c48/Te_kKXnGIMDd7njmIkXQ3.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04563.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "642c2dcec3694d2b74565c48",
            "avatarUrl": "/avatars/31243bb505f8c511ebd7492eaf3ea1a9.svg",
            "fullname": "zhangzef",
            "name": "Starrrrrry",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.00473",
            "authors": [
                {
                    "_id": "693655753962c926cf6807eb",
                    "name": "Junyan Ye",
                    "hidden": false
                },
                {
                    "_id": "693655753962c926cf6807ec",
                    "name": "Leiqi Zhu",
                    "hidden": false
                },
                {
                    "_id": "693655753962c926cf6807ed",
                    "name": "Yuncheng Guo",
                    "hidden": false
                },
                {
                    "_id": "693655753962c926cf6807ee",
                    "name": "Dongzhi Jiang",
                    "hidden": false
                },
                {
                    "_id": "693655753962c926cf6807ef",
                    "name": "Zilong Huang",
                    "hidden": false
                },
                {
                    "_id": "693655753962c926cf6807f0",
                    "name": "Yifan Zhang",
                    "hidden": false
                },
                {
                    "_id": "693655753962c926cf6807f1",
                    "name": "Zhiyuan Yan",
                    "hidden": false
                },
                {
                    "_id": "693655753962c926cf6807f2",
                    "name": "Haohuan Fu",
                    "hidden": false
                },
                {
                    "_id": "693655753962c926cf6807f3",
                    "name": "Conghui He",
                    "hidden": false
                },
                {
                    "_id": "693655753962c926cf6807f4",
                    "name": "Weijia Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-29T12:52:26.000Z",
            "submittedOnDailyAt": "2025-12-08T02:06:14.909Z",
            "title": "RealGen: Photorealistic Text-to-Image Generation via Detector-Guided Rewards",
            "submittedOnDailyBy": {
                "_id": "6487e158f675b4a7867f45fa",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6487e158f675b4a7867f45fa/J0sls6zZ682o-SH7iQs7B.jpeg",
                "isPro": false,
                "fullname": "Zilong Huang",
                "user": "SereinH",
                "type": "user"
            },
            "summary": "With the continuous advancement of image generation technology, advanced models such as GPT-Image-1 and Qwen-Image have achieved remarkable text-to-image consistency and world knowledge However, these models still fall short in photorealistic image generation. Even on simple T2I tasks, they tend to produce \" fake\" images with distinct AI artifacts, often characterized by \"overly smooth skin\" and \"oily facial sheens\". To recapture the original goal of \"indistinguishable-from-reality\" generation, we propose RealGen, a photorealistic text-to-image framework. RealGen integrates an LLM component for prompt optimization and a diffusion model for realistic image generation. Inspired by adversarial generation, RealGen introduces a \"Detector Reward\" mechanism, which quantifies artifacts and assesses realism using both semantic-level and feature-level synthetic image detectors. We leverage this reward signal with the GRPO algorithm to optimize the entire generation pipeline, significantly enhancing image realism and detail. Furthermore, we propose RealBench, an automated evaluation benchmark employing Detector-Scoring and Arena-Scoring. It enables human-free photorealism assessment, yielding results that are more accurate and aligned with real user experience. Experiments demonstrate that RealGen significantly outperforms general models like GPT-Image-1 and Qwen-Image, as well as specialized photorealistic models like FLUX-Krea, in terms of realism, detail, and aesthetics. The code is available at https://github.com/yejy53/RealGen.",
            "upvotes": 10,
            "discussionId": "693655763962c926cf6807f5",
            "projectPage": "https://yejy53.github.io/RealGen/",
            "githubRepo": "https://github.com/yejy53/RealGen",
            "ai_summary": "RealGen is a photorealistic text-to-image framework that uses an LLM for prompt optimization and a diffusion model for image generation, enhanced by a Detector Reward mechanism and RealBench for automated evaluation.",
            "ai_keywords": [
                "LLM",
                "diffusion model",
                "Detector Reward",
                "semantic-level detectors",
                "feature-level detectors",
                "GRPO algorithm",
                "RealBench",
                "Detector-Scoring",
                "Arena-Scoring"
            ],
            "githubStars": 65
        },
        "publishedAt": "2025-11-29T07:52:26.000Z",
        "title": "RealGen: Photorealistic Text-to-Image Generation via Detector-Guided Rewards",
        "summary": "With the continuous advancement of image generation technology, advanced models such as GPT-Image-1 and Qwen-Image have achieved remarkable text-to-image consistency and world knowledge However, these models still fall short in photorealistic image generation. Even on simple T2I tasks, they tend to produce \" fake\" images with distinct AI artifacts, often characterized by \"overly smooth skin\" and \"oily facial sheens\". To recapture the original goal of \"indistinguishable-from-reality\" generation, we propose RealGen, a photorealistic text-to-image framework. RealGen integrates an LLM component for prompt optimization and a diffusion model for realistic image generation. Inspired by adversarial generation, RealGen introduces a \"Detector Reward\" mechanism, which quantifies artifacts and assesses realism using both semantic-level and feature-level synthetic image detectors. We leverage this reward signal with the GRPO algorithm to optimize the entire generation pipeline, significantly enhancing image realism and detail. Furthermore, we propose RealBench, an automated evaluation benchmark employing Detector-Scoring and Arena-Scoring. It enables human-free photorealism assessment, yielding results that are more accurate and aligned with real user experience. Experiments demonstrate that RealGen significantly outperforms general models like GPT-Image-1 and Qwen-Image, as well as specialized photorealistic models like FLUX-Krea, in terms of realism, detail, and aesthetics. The code is available at https://github.com/yejy53/RealGen.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.00473.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6487e158f675b4a7867f45fa",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6487e158f675b4a7867f45fa/J0sls6zZ682o-SH7iQs7B.jpeg",
            "fullname": "Zilong Huang",
            "name": "SereinH",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.05145",
            "authors": [
                {
                    "_id": "693638b23962c926cf68071a",
                    "name": "Inna Wanyin Lin",
                    "hidden": false
                },
                {
                    "_id": "693638b23962c926cf68071b",
                    "name": "Yushi Hu",
                    "hidden": false
                },
                {
                    "_id": "693638b23962c926cf68071c",
                    "name": "Shuyue Stella Li",
                    "hidden": false
                },
                {
                    "_id": "693638b23962c926cf68071d",
                    "name": "Scott Geng",
                    "hidden": false
                },
                {
                    "_id": "693638b23962c926cf68071e",
                    "name": "Pang Wei Koh",
                    "hidden": false
                },
                {
                    "_id": "693638b23962c926cf68071f",
                    "name": "Luke Zettlemoyer",
                    "hidden": false
                },
                {
                    "_id": "693638b23962c926cf680720",
                    "name": "Tim Althoff",
                    "hidden": false
                },
                {
                    "_id": "693638b23962c926cf680721",
                    "name": "Marjan Ghazvininejad",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-02T20:52:19.000Z",
            "submittedOnDailyAt": "2025-12-08T00:02:34.517Z",
            "title": "Self-Improving VLM Judges Without Human Annotations",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Effective judges of Vision-Language Models (VLMs) are crucial for model development. Current methods for training VLM judges mainly rely on large-scale human preference annotations. However, such an approach is costly, and the annotations easily become obsolete as models rapidly improve. In this work, we present a framework to self-train a VLM judge model without any human preference annotations, using only self-synthesized data. Our method is iterative and has three stages: (1) generate diverse multimodal instruction-response pairs at varying quality levels, (2) generate reasoning traces and judgments for each pair, removing the ones that do not match our expected quality levels, and (3) training on correct judge answers and their reasoning traces. We evaluate the resulting judge on Multimodal RewardBench and VL-RewardBench across domains: correctness, preference, reasoning, safety, and visual question-answering. Our method improves a Llama-3.2-11B multimodal judge from 0.38 to 0.51 in overall accuracy on VL-RewardBench, often outperforming much larger models including Llama-3.2-90B, GPT-4o, and Claude 3.5 Sonnet, with particularly strong gains in general, hallucination, and reasoning dimensions. The overall strength of these human-annotation-free results suggest the potential for a future self-judge that evolves alongside rapidly improving VLM capabilities.",
            "upvotes": 9,
            "discussionId": "693638b33962c926cf680722",
            "ai_summary": "A framework for self-training a Vision-Language Model (VLM) judge using self-synthesized data improves judge accuracy on VL-RewardBench, surpassing larger models in several dimensions.",
            "ai_keywords": [
                "Vision-Language Models",
                "VLM judges",
                "multimodal instruction-response pairs",
                "reasoning traces",
                "Multimodal RewardBench",
                "VL-RewardBench",
                "correctness",
                "preference",
                "reasoning",
                "safety",
                "visual question-answering",
                "hallucination"
            ],
            "organization": {
                "_id": "66b54027408752ae16404b05",
                "name": "metaresearch",
                "fullname": "Meta Research",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66b25f3f58babfaeb76112dc/2GmiaF075AZ7BcE538oPk.png"
            }
        },
        "publishedAt": "2025-12-02T15:52:19.000Z",
        "title": "Self-Improving VLM Judges Without Human Annotations",
        "summary": "Effective judges of Vision-Language Models (VLMs) are crucial for model development. Current methods for training VLM judges mainly rely on large-scale human preference annotations. However, such an approach is costly, and the annotations easily become obsolete as models rapidly improve. In this work, we present a framework to self-train a VLM judge model without any human preference annotations, using only self-synthesized data. Our method is iterative and has three stages: (1) generate diverse multimodal instruction-response pairs at varying quality levels, (2) generate reasoning traces and judgments for each pair, removing the ones that do not match our expected quality levels, and (3) training on correct judge answers and their reasoning traces. We evaluate the resulting judge on Multimodal RewardBench and VL-RewardBench across domains: correctness, preference, reasoning, safety, and visual question-answering. Our method improves a Llama-3.2-11B multimodal judge from 0.38 to 0.51 in overall accuracy on VL-RewardBench, often outperforming much larger models including Llama-3.2-90B, GPT-4o, and Claude 3.5 Sonnet, with particularly strong gains in general, hallucination, and reasoning dimensions. The overall strength of these human-annotation-free results suggest the potential for a future self-judge that evolves alongside rapidly improving VLM capabilities.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.05145.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 178
        },
        "organization": {
            "_id": "66b54027408752ae16404b05",
            "name": "metaresearch",
            "fullname": "Meta Research",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66b25f3f58babfaeb76112dc/2GmiaF075AZ7BcE538oPk.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.05927",
            "authors": [
                {
                    "_id": "6936388c3962c926cf680713",
                    "name": "Zhiting Mei",
                    "hidden": false
                },
                {
                    "_id": "6936388c3962c926cf680714",
                    "name": "Tenny Yin",
                    "hidden": false
                },
                {
                    "_id": "6936388c3962c926cf680715",
                    "name": "Micah Baker",
                    "hidden": false
                },
                {
                    "_id": "6936388c3962c926cf680716",
                    "name": "Ola Shorinwa",
                    "hidden": false
                },
                {
                    "_id": "6936388c3962c926cf680717",
                    "name": "Anirudha Majumdar",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/uFiYGO8LB_HsPAbDr5LFO.mp4"
            ],
            "publishedAt": "2025-12-05T18:06:18.000Z",
            "submittedOnDailyAt": "2025-12-08T00:01:47.963Z",
            "title": "World Models That Know When They Don't Know: Controllable Video Generation with Calibrated Uncertainty",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Recent advances in generative video models have led to significant breakthroughs in high-fidelity video synthesis, specifically in controllable video generation where the generated video is conditioned on text and action inputs, e.g., in instruction-guided video editing and world modeling in robotics. Despite these exceptional capabilities, controllable video models often hallucinate - generating future video frames that are misaligned with physical reality - which raises serious concerns in many tasks such as robot policy evaluation and planning. However, state-of-the-art video models lack the ability to assess and express their confidence, impeding hallucination mitigation. To rigorously address this challenge, we propose C3, an uncertainty quantification (UQ) method for training continuous-scale calibrated controllable video models for dense confidence estimation at the subpatch level, precisely localizing the uncertainty in each generated video frame. Our UQ method introduces three core innovations to empower video models to estimate their uncertainty. First, our method develops a novel framework that trains video models for correctness and calibration via strictly proper scoring rules. Second, we estimate the video model's uncertainty in latent space, avoiding training instability and prohibitive training costs associated with pixel-space approaches. Third, we map the dense latent-space uncertainty to interpretable pixel-level uncertainty in the RGB space for intuitive visualization, providing high-resolution uncertainty heatmaps that identify untrustworthy regions. Through extensive experiments on large-scale robot learning datasets (Bridge and DROID) and real-world evaluations, we demonstrate that our method not only provides calibrated uncertainty estimates within the training distribution, but also enables effective out-of-distribution detection.",
            "upvotes": 8,
            "discussionId": "6936388c3962c926cf680718",
            "projectPage": "https://c-cubed-uq.github.io/",
            "ai_summary": "C3 is an uncertainty quantification method for training controllable video models that provides dense confidence estimation and out-of-distribution detection, addressing hallucination issues.",
            "ai_keywords": [
                "generative video models",
                "controllable video models",
                "instruction-guided video editing",
                "robot policy evaluation",
                "planning",
                "uncertainty quantification",
                "UQ",
                "continuous-scale calibrated",
                "dense confidence estimation",
                "subpatch level",
                "latent space",
                "pixel-space",
                "calibrated uncertainty estimates",
                "training distribution",
                "out-of-distribution detection"
            ]
        },
        "publishedAt": "2025-12-05T13:06:18.000Z",
        "title": "World Models That Know When They Don't Know: Controllable Video Generation with Calibrated Uncertainty",
        "summary": "Recent advances in generative video models have led to significant breakthroughs in high-fidelity video synthesis, specifically in controllable video generation where the generated video is conditioned on text and action inputs, e.g., in instruction-guided video editing and world modeling in robotics. Despite these exceptional capabilities, controllable video models often hallucinate - generating future video frames that are misaligned with physical reality - which raises serious concerns in many tasks such as robot policy evaluation and planning. However, state-of-the-art video models lack the ability to assess and express their confidence, impeding hallucination mitigation. To rigorously address this challenge, we propose C3, an uncertainty quantification (UQ) method for training continuous-scale calibrated controllable video models for dense confidence estimation at the subpatch level, precisely localizing the uncertainty in each generated video frame. Our UQ method introduces three core innovations to empower video models to estimate their uncertainty. First, our method develops a novel framework that trains video models for correctness and calibration via strictly proper scoring rules. Second, we estimate the video model's uncertainty in latent space, avoiding training instability and prohibitive training costs associated with pixel-space approaches. Third, we map the dense latent-space uncertainty to interpretable pixel-level uncertainty in the RGB space for intuitive visualization, providing high-resolution uncertainty heatmaps that identify untrustworthy regions. Through extensive experiments on large-scale robot learning datasets (Bridge and DROID) and real-world evaluations, we demonstrate that our method not only provides calibrated uncertainty estimates within the training distribution, but also enables effective out-of-distribution detection.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/uFiYGO8LB_HsPAbDr5LFO.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.05927.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 178
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.05343",
            "authors": [
                {
                    "_id": "693643b03962c926cf6807c6",
                    "user": {
                        "_id": "62df0ae635a8bfe88fc4ac80",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62df0ae635a8bfe88fc4ac80/yWu5Kfq_88Ni2uy3HvnLe.jpeg",
                        "isPro": false,
                        "fullname": "elisabetta fedele",
                        "user": "efedele",
                        "type": "user"
                    },
                    "name": "Elisabetta Fedele",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-08T08:29:57.199Z",
                    "hidden": false
                },
                {
                    "_id": "693643b03962c926cf6807c7",
                    "name": "Francis Engelmann",
                    "hidden": false
                },
                {
                    "_id": "693643b03962c926cf6807c8",
                    "name": "Ian Huang",
                    "hidden": false
                },
                {
                    "_id": "693643b03962c926cf6807c9",
                    "name": "Or Litany",
                    "hidden": false
                },
                {
                    "_id": "693643b03962c926cf6807ca",
                    "name": "Marc Pollefeys",
                    "hidden": false
                },
                {
                    "_id": "693643b03962c926cf6807cb",
                    "name": "Leonidas Guibas",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/m60vjvviA7kv37fhMKGVV.mp4"
            ],
            "publishedAt": "2025-12-05T00:54:48.000Z",
            "submittedOnDailyAt": "2025-12-08T00:49:20.320Z",
            "title": "SpaceControl: Introducing Test-Time Spatial Control to 3D Generative Modeling",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Generative methods for 3D assets have recently achieved remarkable progress, yet providing intuitive and precise control over the object geometry remains a key challenge. Existing approaches predominantly rely on text or image prompts, which often fall short in geometric specificity: language can be ambiguous, and images are cumbersome to edit. In this work, we introduce SpaceControl, a training-free test-time method for explicit spatial control of 3D generation. Our approach accepts a wide range of geometric inputs, from coarse primitives to detailed meshes, and integrates seamlessly with modern pre-trained generative models without requiring any additional training. A controllable parameter lets users trade off between geometric fidelity and output realism. Extensive quantitative evaluation and user studies demonstrate that SpaceControl outperforms both training-based and optimization-based baselines in geometric faithfulness while preserving high visual quality. Finally, we present an interactive user interface that enables online editing of superquadrics for direct conversion into textured 3D assets, facilitating practical deployment in creative workflows. Find our project page at https://spacecontrol3d.github.io/",
            "upvotes": 8,
            "discussionId": "693643b03962c926cf6807cc",
            "projectPage": "https://spacecontrol3d.github.io",
            "ai_summary": "SpaceControl enables explicit spatial control of 3D generation using various geometric inputs, outperforming existing methods in geometric faithfulness while maintaining visual quality.",
            "ai_keywords": [
                "3D assets",
                "generative methods",
                "text prompts",
                "image prompts",
                "SpaceControl",
                "geometric inputs",
                "primitives",
                "meshes",
                "pre-trained generative models",
                "geometric fidelity",
                "visual quality",
                "superquadrics",
                "interactive user interface"
            ]
        },
        "publishedAt": "2025-12-04T19:54:48.000Z",
        "title": "SpaceControl: Introducing Test-Time Spatial Control to 3D Generative Modeling",
        "summary": "Generative methods for 3D assets have recently achieved remarkable progress, yet providing intuitive and precise control over the object geometry remains a key challenge. Existing approaches predominantly rely on text or image prompts, which often fall short in geometric specificity: language can be ambiguous, and images are cumbersome to edit. In this work, we introduce SpaceControl, a training-free test-time method for explicit spatial control of 3D generation. Our approach accepts a wide range of geometric inputs, from coarse primitives to detailed meshes, and integrates seamlessly with modern pre-trained generative models without requiring any additional training. A controllable parameter lets users trade off between geometric fidelity and output realism. Extensive quantitative evaluation and user studies demonstrate that SpaceControl outperforms both training-based and optimization-based baselines in geometric faithfulness while preserving high visual quality. Finally, we present an interactive user interface that enables online editing of superquadrics for direct conversion into textured 3D assets, facilitating practical deployment in creative workflows. Find our project page at https://spacecontrol3d.github.io/",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/m60vjvviA7kv37fhMKGVV.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.05343.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 178
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.02835",
            "authors": [
                {
                    "_id": "69325b966d1060ca587a275f",
                    "user": {
                        "_id": "68aebbb80fdaa186aa530e8e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/2WI2GgcHoZzYSM-879ejO.png",
                        "isPro": false,
                        "fullname": "Yifan Li(SII)",
                        "user": "Tangerine24",
                        "type": "user"
                    },
                    "name": "Yifan Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-05T08:29:16.888Z",
                    "hidden": false
                },
                {
                    "_id": "69325b966d1060ca587a2760",
                    "name": "Yingda Yin",
                    "hidden": false
                },
                {
                    "_id": "69325b966d1060ca587a2761",
                    "name": "Lingting Zhu",
                    "hidden": false
                },
                {
                    "_id": "69325b966d1060ca587a2762",
                    "name": "Weikai Chen",
                    "hidden": false
                },
                {
                    "_id": "69325b966d1060ca587a2763",
                    "name": "Shengju Qian",
                    "hidden": false
                },
                {
                    "_id": "69325b966d1060ca587a2764",
                    "name": "Xin Wang",
                    "hidden": false
                },
                {
                    "_id": "69325b966d1060ca587a2765",
                    "name": "Yanwei Fu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-02T14:44:12.000Z",
            "submittedOnDailyAt": "2025-12-08T00:07:36.304Z",
            "title": "ReVSeg: Incentivizing the Reasoning Chain for Video Segmentation with Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "68aebbb80fdaa186aa530e8e",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/2WI2GgcHoZzYSM-879ejO.png",
                "isPro": false,
                "fullname": "Yifan Li(SII)",
                "user": "Tangerine24",
                "type": "user"
            },
            "summary": "Reasoning-centric video object segmentation is an inherently complex task: the query often refers to dynamics, causality, and temporal interactions, rather than static appearances. Yet existing solutions generally collapse these factors into simplified reasoning with latent embeddings, rendering the reasoning chain opaque and essentially intractable. We therefore adopt an explicit decomposition perspective and introduce ReVSeg, which executes reasoning as sequential decisions in the native interface of pretrained vision language models (VLMs). Rather than folding all reasoning into a single-step prediction, ReVSeg executes three explicit operations -- semantics interpretation, temporal evidence selection, and spatial grounding -- aligning pretrained capabilities. We further employ reinforcement learning to optimize the multi-step reasoning chain, enabling the model to self-refine its decision quality from outcome-driven signals. Experimental results demonstrate that ReVSeg attains state-of-the-art performances on standard video object segmentation benchmarks and yields interpretable reasoning trajectories. Project page is available at https://clementine24.github.io/ReVSeg/ .",
            "upvotes": 8,
            "discussionId": "69325b966d1060ca587a2766",
            "projectPage": "https://clementine24.github.io/ReVSeg/",
            "githubRepo": "https://github.com/Clementine24/ReVSeg",
            "ai_summary": "ReVSeg, a reasoning-centric video object segmentation framework, uses sequential decision-making in pretrained vision language models and reinforcement learning to achieve state-of-the-art performance and interpretable reasoning.",
            "ai_keywords": [
                "video object segmentation",
                "dynamics",
                "causality",
                "temporal interactions",
                "latent embeddings",
                "pretrained vision language models",
                "semantics interpretation",
                "temporal evidence selection",
                "spatial grounding",
                "reinforcement learning",
                "reasoning trajectories"
            ],
            "githubStars": 5
        },
        "publishedAt": "2025-12-02T09:44:12.000Z",
        "title": "ReVSeg: Incentivizing the Reasoning Chain for Video Segmentation with Reinforcement Learning",
        "summary": "Reasoning-centric video object segmentation is an inherently complex task: the query often refers to dynamics, causality, and temporal interactions, rather than static appearances. Yet existing solutions generally collapse these factors into simplified reasoning with latent embeddings, rendering the reasoning chain opaque and essentially intractable. We therefore adopt an explicit decomposition perspective and introduce ReVSeg, which executes reasoning as sequential decisions in the native interface of pretrained vision language models (VLMs). Rather than folding all reasoning into a single-step prediction, ReVSeg executes three explicit operations -- semantics interpretation, temporal evidence selection, and spatial grounding -- aligning pretrained capabilities. We further employ reinforcement learning to optimize the multi-step reasoning chain, enabling the model to self-refine its decision quality from outcome-driven signals. Experimental results demonstrate that ReVSeg attains state-of-the-art performances on standard video object segmentation benchmarks and yields interpretable reasoning trajectories. Project page is available at https://clementine24.github.io/ReVSeg/ .",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02835.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "68aebbb80fdaa186aa530e8e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/2WI2GgcHoZzYSM-879ejO.png",
            "fullname": "Yifan Li(SII)",
            "name": "Tangerine24",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.05356",
            "authors": [
                {
                    "_id": "693642683962c926cf680798",
                    "name": "Jason Weston",
                    "hidden": false
                },
                {
                    "_id": "693642683962c926cf680799",
                    "name": "Jakob Foerster",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-05T01:50:23.000Z",
            "submittedOnDailyAt": "2025-12-08T00:44:58.133Z",
            "title": "AI & Human Co-Improvement for Safer Co-Superintelligence",
            "submittedOnDailyBy": {
                "_id": "62f023a36a027498eaa2f9cc",
                "avatarUrl": "/avatars/8ac1c5c74d0957e3c6cc94b3a7795c37.svg",
                "isPro": false,
                "fullname": "Jason Weston",
                "user": "spermwhale",
                "type": "user"
            },
            "summary": "Self-improvement is a goal currently exciting the field of AI, but is fraught with danger, and may take time to fully achieve. We advocate that a more achievable and better goal for humanity is to maximize co-improvement: collaboration between human researchers and AIs to achieve co-superintelligence. That is, specifically targeting improving AI systems' ability to work with human researchers to conduct AI research together, from ideation to experimentation, in order to both accelerate AI research and to generally endow both AIs and humans with safer superintelligence through their symbiosis. Focusing on including human research improvement in the loop will both get us there faster, and more safely.",
            "upvotes": 7,
            "discussionId": "693642683962c926cf68079a",
            "ai_summary": "The focus should be on collaborative co-improvement between humans and AI systems to achieve safer and accelerated AI research and development.",
            "ai_keywords": [
                ""
            ],
            "organization": {
                "_id": "66b54027408752ae16404b05",
                "name": "metaresearch",
                "fullname": "Meta Research",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66b25f3f58babfaeb76112dc/2GmiaF075AZ7BcE538oPk.png"
            }
        },
        "publishedAt": "2025-12-04T20:50:23.000Z",
        "title": "AI & Human Co-Improvement for Safer Co-Superintelligence",
        "summary": "Self-improvement is a goal currently exciting the field of AI, but is fraught with danger, and may take time to fully achieve. We advocate that a more achievable and better goal for humanity is to maximize co-improvement: collaboration between human researchers and AIs to achieve co-superintelligence. That is, specifically targeting improving AI systems' ability to work with human researchers to conduct AI research together, from ideation to experimentation, in order to both accelerate AI research and to generally endow both AIs and humans with safer superintelligence through their symbiosis. Focusing on including human research improvement in the loop will both get us there faster, and more safely.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.05356.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62f023a36a027498eaa2f9cc",
            "avatarUrl": "/avatars/8ac1c5c74d0957e3c6cc94b3a7795c37.svg",
            "fullname": "Jason Weston",
            "name": "spermwhale",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 19
        },
        "organization": {
            "_id": "66b54027408752ae16404b05",
            "name": "metaresearch",
            "fullname": "Meta Research",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66b25f3f58babfaeb76112dc/2GmiaF075AZ7BcE538oPk.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.03514",
            "authors": [
                {
                    "_id": "69311bb22d1e5b0a7d84da33",
                    "user": {
                        "_id": "6442d975ad54813badc1ddf7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6442d975ad54813badc1ddf7/JIAN8B1dLOXN8fWTk8_mI.jpeg",
                        "isPro": false,
                        "fullname": "Adithya S K",
                        "user": "AdithyaSK",
                        "type": "user"
                    },
                    "name": "Adithya S Kolavi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-08T08:32:49.573Z",
                    "hidden": false
                },
                {
                    "_id": "69311bb22d1e5b0a7d84da34",
                    "user": {
                        "_id": "665d52766c20caf090adc94e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/665d52766c20caf090adc94e/-xnnLMCI6HCBfvhny7MgP.jpeg",
                        "isPro": false,
                        "fullname": "VyoJ",
                        "user": "VyoJ",
                        "type": "user"
                    },
                    "name": "Vyoman Jain",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-08T08:32:54.078Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6442d975ad54813badc1ddf7/QhECg0m-b7rdBumTuaKSU.png",
                "https://cdn-uploads.huggingface.co/production/uploads/6442d975ad54813badc1ddf7/iMimQnI2B5CTbYRkuYwQX.png",
                "https://cdn-uploads.huggingface.co/production/uploads/6442d975ad54813badc1ddf7/4px9x61ChZVgI6dM63vDA.png",
                "https://cdn-uploads.huggingface.co/production/uploads/6442d975ad54813badc1ddf7/kjetYxjQIBeJ9jgfg8N9g.png"
            ],
            "publishedAt": "2025-12-03T07:17:59.000Z",
            "submittedOnDailyAt": "2025-12-08T09:46:39.499Z",
            "title": "M3DR: Towards Universal Multilingual Multimodal Document Retrieval",
            "submittedOnDailyBy": {
                "_id": "6442d975ad54813badc1ddf7",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6442d975ad54813badc1ddf7/JIAN8B1dLOXN8fWTk8_mI.jpeg",
                "isPro": false,
                "fullname": "Adithya S K",
                "user": "AdithyaSK",
                "type": "user"
            },
            "summary": "Multimodal document retrieval systems have shown strong progress in aligning visual and textual content for semantic search. However, most existing approaches remain heavily English-centric, limiting their effectiveness in multilingual contexts. In this work, we present M3DR (Multilingual Multimodal Document Retrieval), a framework designed to bridge this gap across languages, enabling applicability across diverse linguistic and cultural contexts. M3DR leverages synthetic multilingual document data and generalizes across different vision-language architectures and model sizes, enabling robust cross-lingual and cross-modal alignment. Using contrastive training, our models learn unified representations for text and document images that transfer effectively across languages. We validate this capability on 22 typologically diverse languages, demonstrating consistent performance and adaptability across linguistic and script variations. We further introduce a comprehensive benchmark that captures real-world multilingual scenarios, evaluating models under monolingual, multilingual, and mixed-language settings. M3DR generalizes across both single dense vector and ColBERT-style token-level multi-vector retrieval paradigms. Our models, NetraEmbed and ColNetraEmbed achieve state-of-the-art performance with ~150% relative improvements on cross-lingual retrieval.",
            "upvotes": 6,
            "discussionId": "69311bb22d1e5b0a7d84da35",
            "projectPage": "https://www.cognitivelab.in/blog/introducing-netraembed",
            "githubRepo": "https://github.com/adithya-s-k/colpali",
            "ai_summary": "M3DR is a multilingual multimodal document retrieval framework using contrastive training to achieve robust cross-lingual and cross-modal alignment across diverse languages and document types.",
            "ai_keywords": [
                "M3DR",
                "synthetic multilingual document data",
                "vision-language architectures",
                "contrastive training",
                "unified representations",
                "ColBERT-style token-level multi-vector",
                "NetraEmbed",
                "ColNetraEmbed"
            ],
            "githubStars": 1,
            "organization": {
                "_id": "6596a7360255def580480373",
                "name": "Cognitive-Lab",
                "fullname": "CognitiveLab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6442d975ad54813badc1ddf7/fHXqLTEawvDFRAVdSAvW0.png"
            }
        },
        "publishedAt": "2025-12-03T02:17:59.000Z",
        "title": "M3DR: Towards Universal Multilingual Multimodal Document Retrieval",
        "summary": "Multimodal document retrieval systems have shown strong progress in aligning visual and textual content for semantic search. However, most existing approaches remain heavily English-centric, limiting their effectiveness in multilingual contexts. In this work, we present M3DR (Multilingual Multimodal Document Retrieval), a framework designed to bridge this gap across languages, enabling applicability across diverse linguistic and cultural contexts. M3DR leverages synthetic multilingual document data and generalizes across different vision-language architectures and model sizes, enabling robust cross-lingual and cross-modal alignment. Using contrastive training, our models learn unified representations for text and document images that transfer effectively across languages. We validate this capability on 22 typologically diverse languages, demonstrating consistent performance and adaptability across linguistic and script variations. We further introduce a comprehensive benchmark that captures real-world multilingual scenarios, evaluating models under monolingual, multilingual, and mixed-language settings. M3DR generalizes across both single dense vector and ColBERT-style token-level multi-vector retrieval paradigms. Our models, NetraEmbed and ColNetraEmbed achieve state-of-the-art performance with ~150% relative improvements on cross-lingual retrieval.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6442d975ad54813badc1ddf7/QhECg0m-b7rdBumTuaKSU.png",
            "https://cdn-uploads.huggingface.co/production/uploads/6442d975ad54813badc1ddf7/iMimQnI2B5CTbYRkuYwQX.png",
            "https://cdn-uploads.huggingface.co/production/uploads/6442d975ad54813badc1ddf7/4px9x61ChZVgI6dM63vDA.png",
            "https://cdn-uploads.huggingface.co/production/uploads/6442d975ad54813badc1ddf7/kjetYxjQIBeJ9jgfg8N9g.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.03514.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6442d975ad54813badc1ddf7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6442d975ad54813badc1ddf7/JIAN8B1dLOXN8fWTk8_mI.jpeg",
            "fullname": "Adithya S K",
            "name": "AdithyaSK",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 23
        },
        "organization": {
            "_id": "6596a7360255def580480373",
            "name": "Cognitive-Lab",
            "fullname": "CognitiveLab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6442d975ad54813badc1ddf7/fHXqLTEawvDFRAVdSAvW0.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.05277",
            "authors": [
                {
                    "_id": "693717fe19d912300c34a0fb",
                    "name": "Kevin Cannons",
                    "hidden": false
                },
                {
                    "_id": "693717fe19d912300c34a0fc",
                    "user": {
                        "_id": "67f8267b33ef0ce3cdc24ce8",
                        "avatarUrl": "/avatars/f23e5e3ec86b450f7f29548253f217b6.svg",
                        "isPro": false,
                        "fullname": "Saeed Ranjbar Alvar",
                        "user": "saeedranjbar12",
                        "type": "user"
                    },
                    "name": "Saeed Ranjbar Alvar",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-08T19:38:14.138Z",
                    "hidden": false
                },
                {
                    "_id": "693717fe19d912300c34a0fd",
                    "name": "Mohammad Asiful Hossain",
                    "hidden": false
                },
                {
                    "_id": "693717fe19d912300c34a0fe",
                    "name": "Ahmad Rezaei",
                    "hidden": false
                },
                {
                    "_id": "693717fe19d912300c34a0ff",
                    "name": "Mohsen Gholami",
                    "hidden": false
                },
                {
                    "_id": "693717fe19d912300c34a100",
                    "name": "Alireza Heidarikhazaei",
                    "hidden": false
                },
                {
                    "_id": "693717fe19d912300c34a101",
                    "name": "Zhou Weimin",
                    "hidden": false
                },
                {
                    "_id": "693717fe19d912300c34a102",
                    "name": "Yong Zhang",
                    "hidden": false
                },
                {
                    "_id": "693717fe19d912300c34a103",
                    "user": {
                        "_id": "6545976cb8ac1a89ffa8d6cb",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fIBp6wUQ8Ln3gIDwvSv8-.png",
                        "isPro": false,
                        "fullname": "Mohammad Akbari",
                        "user": "moak7",
                        "type": "user"
                    },
                    "name": "Mohammad Akbari",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-08T19:38:16.211Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/67f8267b33ef0ce3cdc24ce8/tf5aiwrnQboykSHddVcQV.png"
            ],
            "publishedAt": "2025-12-04T21:57:10.000Z",
            "submittedOnDailyAt": "2025-12-08T15:59:22.304Z",
            "title": "From Segments to Scenes: Temporal Understanding in Autonomous Driving via Vision-Language Model",
            "submittedOnDailyBy": {
                "_id": "67f8267b33ef0ce3cdc24ce8",
                "avatarUrl": "/avatars/f23e5e3ec86b450f7f29548253f217b6.svg",
                "isPro": false,
                "fullname": "Saeed Ranjbar Alvar",
                "user": "saeedranjbar12",
                "type": "user"
            },
            "summary": "Temporal understanding in autonomous driving (AD) remains a significant challenge, even for recent state-of-the-art (SoTA) Vision-Language Models (VLMs). Prior work has introduced datasets and benchmarks aimed at improving temporal reasoning, but these have emphasized other video content, including sports, cooking, and movies. No existing benchmark focuses exclusively on the unique challenges of temporal understanding in ego-centric AD footage. To fill this gap, the Temporal Understanding in Autonomous Driving (TAD) benchmark is presented, which evaluates VLMs' ability to capture the dynamic relationships between actions in AD. TAD comprises nearly 6,000 question-answer (QA) pairs, spanning 7 human-designed tasks. In addition, an evaluation is performed that consists of 9 closed- and open-source generalist models as well as SoTA AD specialist models. When applied to TAD, current SoTA models demonstrated substandard accuracies, largely due to imperfect fine-grained motion understanding. To improve motion understanding and overall accuracy on TAD, two novel training-free solutions are proposed: Scene-CoT, that leverages Chain-of-Thought (CoT) and TCogMap, which incorporates an ego-centric temporal cognitive map. The proposed approaches are integrated with existing VLMs and improve average accuracy on TAD by up to 17.72%. By introducing TAD, benchmarking multiple SoTA models, and proposing effective enhancements, this work aims to catalyze future research on temporal understanding in AD. The benchmark and evaluation code are available at https://huggingface.co/datasets/vbdai/TAD{Hugging Face} and https://github.com/vbdi/tad_bench{Github}, respectively.",
            "upvotes": 4,
            "discussionId": "693717fe19d912300c34a104",
            "githubRepo": "https://github.com/vbdi/tad_bench",
            "ai_summary": "The TAD benchmark evaluates Vision-Language Models on temporal understanding in autonomous driving footage, revealing substandard performance and proposing training-free solutions to improve accuracy.",
            "ai_keywords": [
                "Vision-Language Models",
                "temporal reasoning",
                "TAD benchmark",
                "question-answer pairs",
                "Chain-of-Thought",
                "TCogMap",
                "ego-centric temporal cognitive map"
            ],
            "githubStars": 1,
            "organization": {
                "_id": "68ae0f09570f0a1176411a35",
                "name": "vbdai",
                "fullname": "Huawei's Vancouver VBDAI Lab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6545976cb8ac1a89ffa8d6cb/RqPqXMUOkpd6O4rVJACOF.png"
            }
        },
        "publishedAt": "2025-12-04T16:57:10.000Z",
        "title": "From Segments to Scenes: Temporal Understanding in Autonomous Driving via Vision-Language Model",
        "summary": "Temporal understanding in autonomous driving (AD) remains a significant challenge, even for recent state-of-the-art (SoTA) Vision-Language Models (VLMs). Prior work has introduced datasets and benchmarks aimed at improving temporal reasoning, but these have emphasized other video content, including sports, cooking, and movies. No existing benchmark focuses exclusively on the unique challenges of temporal understanding in ego-centric AD footage. To fill this gap, the Temporal Understanding in Autonomous Driving (TAD) benchmark is presented, which evaluates VLMs' ability to capture the dynamic relationships between actions in AD. TAD comprises nearly 6,000 question-answer (QA) pairs, spanning 7 human-designed tasks. In addition, an evaluation is performed that consists of 9 closed- and open-source generalist models as well as SoTA AD specialist models. When applied to TAD, current SoTA models demonstrated substandard accuracies, largely due to imperfect fine-grained motion understanding. To improve motion understanding and overall accuracy on TAD, two novel training-free solutions are proposed: Scene-CoT, that leverages Chain-of-Thought (CoT) and TCogMap, which incorporates an ego-centric temporal cognitive map. The proposed approaches are integrated with existing VLMs and improve average accuracy on TAD by up to 17.72%. By introducing TAD, benchmarking multiple SoTA models, and proposing effective enhancements, this work aims to catalyze future research on temporal understanding in AD. The benchmark and evaluation code are available at https://huggingface.co/datasets/vbdai/TAD{Hugging Face} and https://github.com/vbdi/tad_bench{Github}, respectively.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/67f8267b33ef0ce3cdc24ce8/tf5aiwrnQboykSHddVcQV.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.05277.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67f8267b33ef0ce3cdc24ce8",
            "avatarUrl": "/avatars/f23e5e3ec86b450f7f29548253f217b6.svg",
            "fullname": "Saeed Ranjbar Alvar",
            "name": "saeedranjbar12",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "68ae0f09570f0a1176411a35",
            "name": "vbdai",
            "fullname": "Huawei's Vancouver VBDAI Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6545976cb8ac1a89ffa8d6cb/RqPqXMUOkpd6O4rVJACOF.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.05564",
            "authors": [
                {
                    "_id": "693639283962c926cf68073e",
                    "name": "Zijun Wang",
                    "hidden": false
                },
                {
                    "_id": "693639283962c926cf68073f",
                    "name": "Panwen Hu",
                    "hidden": false
                },
                {
                    "_id": "693639283962c926cf680740",
                    "name": "Jing Wang",
                    "hidden": false
                },
                {
                    "_id": "693639283962c926cf680741",
                    "name": "Terry Jingchen Zhang",
                    "hidden": false
                },
                {
                    "_id": "693639283962c926cf680742",
                    "name": "Yuhao Cheng",
                    "hidden": false
                },
                {
                    "_id": "693639283962c926cf680743",
                    "name": "Long Chen",
                    "hidden": false
                },
                {
                    "_id": "693639283962c926cf680744",
                    "name": "Yiqiang Yan",
                    "hidden": false
                },
                {
                    "_id": "693639283962c926cf680745",
                    "name": "Zutao Jiang",
                    "hidden": false
                },
                {
                    "_id": "693639283962c926cf680746",
                    "name": "Hanhui Li",
                    "hidden": false
                },
                {
                    "_id": "693639283962c926cf680747",
                    "name": "Xiaodan Liang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-05T09:39:26.000Z",
            "submittedOnDailyAt": "2025-12-08T00:04:23.307Z",
            "title": "ProPhy: Progressive Physical Alignment for Dynamic World Simulation",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Recent advances in video generation have shown remarkable potential for constructing world simulators. However, current models still struggle to produce physically consistent results, particularly when handling large-scale or complex dynamics. This limitation arises primarily because existing approaches respond isotropically to physical prompts and neglect the fine-grained alignment between generated content and localized physical cues. To address these challenges, we propose ProPhy, a Progressive Physical Alignment Framework that enables explicit physics-aware conditioning and anisotropic generation. ProPhy employs a two-stage Mixture-of-Physics-Experts (MoPE) mechanism for discriminative physical prior extraction, where Semantic Experts infer semantic-level physical principles from textual descriptions, and Refinement Experts capture token-level physical dynamics. This mechanism allows the model to learn fine-grained, physics-aware video representations that better reflect underlying physical laws. Furthermore, we introduce a physical alignment strategy that transfers the physical reasoning capabilities of vision-language models (VLMs) into the Refinement Experts, facilitating a more accurate representation of dynamic physical phenomena. Extensive experiments on physics-aware video generation benchmarks demonstrate that ProPhy produces more realistic, dynamic, and physically coherent results than existing state-of-the-art methods.",
            "upvotes": 3,
            "discussionId": "693639293962c926cf680748",
            "ai_summary": "ProPhy, a two-stage framework with Semantic and Refinement Experts, enhances video generation by incorporating physics-aware conditioning and anisotropic generation, improving physical consistency and realism.",
            "ai_keywords": [
                "video generation",
                "world simulators",
                "physical consistency",
                "large-scale dynamics",
                "complex dynamics",
                "ProPhy",
                "Progressive Physical Alignment Framework",
                "Mixture-of-Physics-Experts (MoPE)",
                "Semantic Experts",
                "Refinement Experts",
                "semantic-level physical principles",
                "token-level physical dynamics",
                "physical reasoning capabilities",
                "vision-language models (VLMs)",
                "physics-aware video representations",
                "dynamic physical phenomena"
            ]
        },
        "publishedAt": "2025-12-05T04:39:26.000Z",
        "title": "ProPhy: Progressive Physical Alignment for Dynamic World Simulation",
        "summary": "Recent advances in video generation have shown remarkable potential for constructing world simulators. However, current models still struggle to produce physically consistent results, particularly when handling large-scale or complex dynamics. This limitation arises primarily because existing approaches respond isotropically to physical prompts and neglect the fine-grained alignment between generated content and localized physical cues. To address these challenges, we propose ProPhy, a Progressive Physical Alignment Framework that enables explicit physics-aware conditioning and anisotropic generation. ProPhy employs a two-stage Mixture-of-Physics-Experts (MoPE) mechanism for discriminative physical prior extraction, where Semantic Experts infer semantic-level physical principles from textual descriptions, and Refinement Experts capture token-level physical dynamics. This mechanism allows the model to learn fine-grained, physics-aware video representations that better reflect underlying physical laws. Furthermore, we introduce a physical alignment strategy that transfers the physical reasoning capabilities of vision-language models (VLMs) into the Refinement Experts, facilitating a more accurate representation of dynamic physical phenomena. Extensive experiments on physics-aware video generation benchmarks demonstrate that ProPhy produces more realistic, dynamic, and physically coherent results than existing state-of-the-art methods.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.05564.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 178
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.05409",
            "authors": [
                {
                    "_id": "6936a2fc3962c926cf68088c",
                    "name": "Ruixuan Huang",
                    "hidden": false
                },
                {
                    "_id": "6936a2fc3962c926cf68088d",
                    "name": "Hao Zeng",
                    "hidden": false
                },
                {
                    "_id": "6936a2fc3962c926cf68088e",
                    "name": "Hantao Huang",
                    "hidden": false
                },
                {
                    "_id": "6936a2fc3962c926cf68088f",
                    "name": "Jinyuan Shi",
                    "hidden": false
                },
                {
                    "_id": "6936a2fc3962c926cf680890",
                    "name": "Minghui Yu",
                    "hidden": false
                },
                {
                    "_id": "6936a2fc3962c926cf680891",
                    "name": "Ian En-Hsu Yen",
                    "hidden": false
                },
                {
                    "_id": "6936a2fc3962c926cf680892",
                    "name": "Shuai Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-05T03:58:04.000Z",
            "submittedOnDailyAt": "2025-12-08T08:52:23.635Z",
            "title": "SQ-format: A Unified Sparse-Quantized Hardware-friendly Data Format for LLMs",
            "submittedOnDailyBy": {
                "_id": "687620b15a6752969a51f75f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QkNF4z2C_IBM9gxQ1XED8.png",
                "isPro": false,
                "fullname": "Sprout Huang",
                "user": "HRXUST",
                "type": "user"
            },
            "summary": "Post-training quantization (PTQ) plays a crucial role in the democratization of large language models (LLMs). However, existing low-bit quantization and sparsification techniques are difficult to balance accuracy and efficiency due to the limited hardware support. For example, W4A8 can only achieve the same peak TOPS as W8A8 whereas the GPU-supported sparse data format (2:4 semi-structure sparse) is seldomly adopted due to the loss of accuracy. To bridge this gap, in this paper, we propose the Sparse-Quantized Format (SQ-format), which is a unified data format for quantization and sparsification potentially easily supported by new hardware and existing GPUs. SQ-format makes use of the fact that sparse matrix can be accelerated in high-precision, and low-precision matrix multiplication can also be accelerated accordingly. As such, SQ-format is proposed to achieve Pareto improvement between performance and throughput. This format is particularly suitable for activations with outlier inequality status and makes their static compression possible. We show the state-of-the-art PTQ performance with SQ-format, propose the hardware required to support it, and further offer the design exploration and insights for the next-generation AI accelerators.",
            "upvotes": 2,
            "discussionId": "6936a2fd3962c926cf680893",
            "ai_summary": "A Sparse-Quantized Format (SQ-format) is proposed to improve the balance between accuracy and efficiency in post-training quantization of large language models by leveraging sparse and low-precision matrix multiplications.",
            "ai_keywords": [
                "Post-training quantization",
                "large language models",
                "low-bit quantization",
                "sparsification",
                "W4A8",
                "W8A8",
                "2:4 semi-structure sparse",
                "sparse matrix",
                "low-precision matrix multiplication",
                "SQ-format",
                "Pareto improvement",
                "performance",
                "throughput",
                "activations",
                "outlier inequality status",
                "static compression",
                "AI accelerators"
            ]
        },
        "publishedAt": "2025-12-04T22:58:04.000Z",
        "title": "SQ-format: A Unified Sparse-Quantized Hardware-friendly Data Format for LLMs",
        "summary": "Post-training quantization (PTQ) plays a crucial role in the democratization of large language models (LLMs). However, existing low-bit quantization and sparsification techniques are difficult to balance accuracy and efficiency due to the limited hardware support. For example, W4A8 can only achieve the same peak TOPS as W8A8 whereas the GPU-supported sparse data format (2:4 semi-structure sparse) is seldomly adopted due to the loss of accuracy. To bridge this gap, in this paper, we propose the Sparse-Quantized Format (SQ-format), which is a unified data format for quantization and sparsification potentially easily supported by new hardware and existing GPUs. SQ-format makes use of the fact that sparse matrix can be accelerated in high-precision, and low-precision matrix multiplication can also be accelerated accordingly. As such, SQ-format is proposed to achieve Pareto improvement between performance and throughput. This format is particularly suitable for activations with outlier inequality status and makes their static compression possible. We show the state-of-the-art PTQ performance with SQ-format, propose the hardware required to support it, and further offer the design exploration and insights for the next-generation AI accelerators.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.05409.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "687620b15a6752969a51f75f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QkNF4z2C_IBM9gxQ1XED8.png",
            "fullname": "Sprout Huang",
            "name": "HRXUST",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.04694",
            "authors": [
                {
                    "_id": "693344ee3962c926cf680475",
                    "user": {
                        "_id": "62978d6eb5a5a72ea2090c8b",
                        "avatarUrl": "/avatars/67b2c7ea10d2a77f5bf7c98ce6edfa1c.svg",
                        "isPro": false,
                        "fullname": "Yilmaz",
                        "user": "Barisylmz",
                        "type": "user"
                    },
                    "name": "Baris Yilmaz",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-08T08:31:19.869Z",
                    "hidden": false
                },
                {
                    "_id": "693344ee3962c926cf680476",
                    "name": "Bevan Deniz Cilgin",
                    "hidden": false
                },
                {
                    "_id": "693344ee3962c926cf680477",
                    "name": "Erdem Akagndz",
                    "hidden": false
                },
                {
                    "_id": "693344ee3962c926cf680478",
                    "name": "Salih Tileylioglu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/62978d6eb5a5a72ea2090c8b/nfNLl4pC5c7qnCiHW8jrV.qt"
            ],
            "publishedAt": "2025-12-04T11:44:13.000Z",
            "submittedOnDailyAt": "2025-12-08T07:05:05.209Z",
            "title": "TimesNet-Gen: Deep Learning-based Site Specific Strong Motion Generation",
            "submittedOnDailyBy": {
                "_id": "62978d6eb5a5a72ea2090c8b",
                "avatarUrl": "/avatars/67b2c7ea10d2a77f5bf7c98ce6edfa1c.svg",
                "isPro": false,
                "fullname": "Yilmaz",
                "user": "Barisylmz",
                "type": "user"
            },
            "summary": "Effective earthquake risk reduction relies on accurate site-specific evaluations. This requires models that can represent the influence of local site conditions on ground motion characteristics. In this context, data driven approaches that learn site controlled signatures from recorded ground motions offer a promising direction. We address strong ground motion generation from time-domain accelerometer records and introduce the TimesNet-Gen, a time-domain conditional generator. The approach uses a station specific latent bottleneck. We evaluate generation by comparing HVSR curves and fundamental site-frequency f_0 distributions between real and generated records per station, and summarize station specificity with a score based on the f_0 distribution confusion matrices. TimesNet-Gen achieves strong station-wise alignment and compares favorably with a spectrogram-based conditional VAE baseline for site-specific strong motion synthesis. Our codes are available via https://github.com/brsylmz23/TimesNet-Gen.",
            "upvotes": 2,
            "discussionId": "693344ee3962c926cf680479",
            "projectPage": "https://huggingface.co/spaces/Barisylmz/TimesNet-Gen",
            "githubRepo": "https://github.com/brsylmz23/TimesNet-Gen/tree/main",
            "ai_summary": "TimesNet-Gen, a time-domain conditional generator with a station-specific latent bottleneck, effectively synthesizes site-specific strong ground motion records, outperforming a spectrogram-based conditional VAE baseline.",
            "ai_keywords": [
                "TimesNet-Gen",
                "time-domain generator",
                "latent bottleneck",
                "HVSR curves",
                "fundamental site-frequency",
                "f distribution",
                "strong ground motion synthesis",
                "conditional VAE"
            ],
            "githubStars": 0
        },
        "publishedAt": "2025-12-04T06:44:13.000Z",
        "title": "TimesNet-Gen: Deep Learning-based Site Specific Strong Motion Generation",
        "summary": "Effective earthquake risk reduction relies on accurate site-specific evaluations. This requires models that can represent the influence of local site conditions on ground motion characteristics. In this context, data driven approaches that learn site controlled signatures from recorded ground motions offer a promising direction. We address strong ground motion generation from time-domain accelerometer records and introduce the TimesNet-Gen, a time-domain conditional generator. The approach uses a station specific latent bottleneck. We evaluate generation by comparing HVSR curves and fundamental site-frequency f_0 distributions between real and generated records per station, and summarize station specificity with a score based on the f_0 distribution confusion matrices. TimesNet-Gen achieves strong station-wise alignment and compares favorably with a spectrogram-based conditional VAE baseline for site-specific strong motion synthesis. Our codes are available via https://github.com/brsylmz23/TimesNet-Gen.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/62978d6eb5a5a72ea2090c8b/nfNLl4pC5c7qnCiHW8jrV.qt"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04694.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62978d6eb5a5a72ea2090c8b",
            "avatarUrl": "/avatars/67b2c7ea10d2a77f5bf7c98ce6edfa1c.svg",
            "fullname": "Yilmaz",
            "name": "Barisylmz",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.03667",
            "authors": [
                {
                    "_id": "6936a2f03962c926cf680886",
                    "name": "Ge-Peng Ji",
                    "hidden": false
                },
                {
                    "_id": "6936a2f03962c926cf680887",
                    "name": "Jingyi Liu",
                    "hidden": false
                },
                {
                    "_id": "6936a2f03962c926cf680888",
                    "name": "Deng-Ping Fan",
                    "hidden": false
                },
                {
                    "_id": "6936a2f03962c926cf680889",
                    "name": "Nick Barnes",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-03T10:55:07.000Z",
            "submittedOnDailyAt": "2025-12-08T18:53:12.959Z",
            "title": "Colon-X: Advancing Intelligent Colonoscopy from Multimodal Understanding to Clinical Reasoning",
            "submittedOnDailyBy": {
                "_id": "65c9f61f6f1355cb7f619466",
                "avatarUrl": "/avatars/76179d54e780dce5cee749062d6b12ac.svg",
                "isPro": false,
                "fullname": "Daniel Ji",
                "user": "DanielJi",
                "type": "user"
            },
            "summary": "In this study, we present Colon-X, an open initiative aimed at advancing multimodal intelligence in colonoscopy. We begin by constructing ColonVQA, the most comprehensive multimodal dataset ever built for colonoscopy, featuring over 1.1M+ visual question answering entries across 76 clinical findings and 18 multimodal tasks. Beyond serving as a community-wide data foundation, we further investigate a critical yet underexplored transition in colonoscopy - evolving from multimodal understanding to clinical reasoning: (a) To capture the current landscape of multimodal understanding behaviors, we systematically assess the generalizability of 22 multimodal large language models and examine their reliability under human-induced perturbations. The results reveal that clinical outputs from leading MLLMs remain far from robust and trustworthy. (b) To narrow this gap, we further explore reasoning-centric intelligence tailored for colonoscopy. Specifically, we curate ColonReason, a clinically grounded reasoning dataset annotated through a multi-expert debating pipeline, and develop ColonR1, the first R1-styled model incorporating task-adaptive rewarding and gradient-stable optimization techniques. Under data-scarce conditions, our ColonR1 achieves 56.61% overall accuracy, outperforming supervised fine-tuning by 25.22%, and sets a new reasoning-enabled baseline for multimodal colonoscopy analysis. All data and model resources are publicly available at https://github.com/ai4colonoscopy/Colon-X.",
            "upvotes": 2,
            "discussionId": "6936a2f03962c926cf68088a",
            "githubRepo": "https://github.com/ai4colonoscopy/Colon-X",
            "ai_summary": "Colon-X advances multimodal intelligence in colonoscopy by constructing comprehensive datasets and developing reasoning-centric models that outperform traditional methods under data scarcity.",
            "ai_keywords": [
                "multimodal intelligence",
                "ColonVQA",
                "visual question answering",
                "multimodal large language models",
                "human-induced perturbations",
                "ColonReason",
                "ColonR1",
                "R1-styled model",
                "task-adaptive rewarding",
                "gradient-stable optimization"
            ],
            "githubStars": 3,
            "organization": {
                "_id": "68e49855d52d48135dfa6d71",
                "name": "AustralianNationalUniversity",
                "fullname": "The Australian National University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/666a97bbed959c9b9bae82d7/r8VztluJdv-8hOtWDGDrB.png"
            }
        },
        "publishedAt": "2025-12-03T05:55:07.000Z",
        "title": "Colon-X: Advancing Intelligent Colonoscopy from Multimodal Understanding to Clinical Reasoning",
        "summary": "In this study, we present Colon-X, an open initiative aimed at advancing multimodal intelligence in colonoscopy. We begin by constructing ColonVQA, the most comprehensive multimodal dataset ever built for colonoscopy, featuring over 1.1M+ visual question answering entries across 76 clinical findings and 18 multimodal tasks. Beyond serving as a community-wide data foundation, we further investigate a critical yet underexplored transition in colonoscopy - evolving from multimodal understanding to clinical reasoning: (a) To capture the current landscape of multimodal understanding behaviors, we systematically assess the generalizability of 22 multimodal large language models and examine their reliability under human-induced perturbations. The results reveal that clinical outputs from leading MLLMs remain far from robust and trustworthy. (b) To narrow this gap, we further explore reasoning-centric intelligence tailored for colonoscopy. Specifically, we curate ColonReason, a clinically grounded reasoning dataset annotated through a multi-expert debating pipeline, and develop ColonR1, the first R1-styled model incorporating task-adaptive rewarding and gradient-stable optimization techniques. Under data-scarce conditions, our ColonR1 achieves 56.61% overall accuracy, outperforming supervised fine-tuning by 25.22%, and sets a new reasoning-enabled baseline for multimodal colonoscopy analysis. All data and model resources are publicly available at https://github.com/ai4colonoscopy/Colon-X.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.03667.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65c9f61f6f1355cb7f619466",
            "avatarUrl": "/avatars/76179d54e780dce5cee749062d6b12ac.svg",
            "fullname": "Daniel Ji",
            "name": "DanielJi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "68e49855d52d48135dfa6d71",
            "name": "AustralianNationalUniversity",
            "fullname": "The Australian National University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/666a97bbed959c9b9bae82d7/r8VztluJdv-8hOtWDGDrB.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.05774",
            "authors": [
                {
                    "_id": "6937077219d912300c34a0d1",
                    "name": "Ziyang Wang",
                    "hidden": false
                },
                {
                    "_id": "6937077219d912300c34a0d2",
                    "name": "Honglu Zhou",
                    "hidden": false
                },
                {
                    "_id": "6937077219d912300c34a0d3",
                    "name": "Shijie Wang",
                    "hidden": false
                },
                {
                    "_id": "6937077219d912300c34a0d4",
                    "name": "Junnan Li",
                    "hidden": false
                },
                {
                    "_id": "6937077219d912300c34a0d5",
                    "name": "Caiming Xiong",
                    "hidden": false
                },
                {
                    "_id": "6937077219d912300c34a0d6",
                    "name": "Silvio Savarese",
                    "hidden": false
                },
                {
                    "_id": "6937077219d912300c34a0d7",
                    "name": "Mohit Bansal",
                    "hidden": false
                },
                {
                    "_id": "6937077219d912300c34a0d8",
                    "name": "Michael S. Ryoo",
                    "hidden": false
                },
                {
                    "_id": "6937077219d912300c34a0d9",
                    "name": "Juan Carlos Niebles",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-05T15:03:48.000Z",
            "submittedOnDailyAt": "2025-12-08T14:49:28.693Z",
            "title": "Active Video Perception: Iterative Evidence Seeking for Agentic Long Video Understanding",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": true,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "Long video understanding (LVU) is challenging because answering real-world queries often depends on sparse, temporally dispersed cues buried in hours of mostly redundant and irrelevant content. While agentic pipelines improve video reasoning capabilities, prevailing frameworks rely on a query-agnostic captioner to perceive video information, which wastes computation on irrelevant content and blurs fine-grained temporal and spatial information. Motivated by active perception theory, we argue that LVU agents should actively decide what, when, and where to observe, and continuously assess whether the current observation is sufficient to answer the query. We present Active Video Perception (AVP), an evidence-seeking framework that treats the video as an interactive environment and acquires compact, queryrelevant evidence directly from pixels. Concretely, AVP runs an iterative plan-observe-reflect process with MLLM agents. In each round, a planner proposes targeted video interactions, an observer executes them to extract time-stamped evidence, and a reflector evaluates the sufficiency of the evidence for the query, either halting with an answer or triggering further observation. Across five LVU benchmarks, AVP achieves highest performance with significant improvements. Notably, AVP outperforms the best agentic method by 5.7% in average accuracy while only requires 18.4% inference time and 12.4% input tokens.",
            "upvotes": 1,
            "discussionId": "6937077219d912300c34a0da",
            "ai_summary": "Active Video Perception (AVP) improves long video understanding by iteratively selecting and evaluating query-relevant video evidence, achieving higher accuracy with reduced computational cost.",
            "ai_keywords": [
                "long video understanding",
                "agentic pipelines",
                "query-agnostic captioner",
                "active perception theory",
                "Active Video Perception",
                "MLLM agents",
                "plan-observe-reflect process",
                "time-stamped evidence"
            ]
        },
        "publishedAt": "2025-12-05T10:03:48.000Z",
        "title": "Active Video Perception: Iterative Evidence Seeking for Agentic Long Video Understanding",
        "summary": "Long video understanding (LVU) is challenging because answering real-world queries often depends on sparse, temporally dispersed cues buried in hours of mostly redundant and irrelevant content. While agentic pipelines improve video reasoning capabilities, prevailing frameworks rely on a query-agnostic captioner to perceive video information, which wastes computation on irrelevant content and blurs fine-grained temporal and spatial information. Motivated by active perception theory, we argue that LVU agents should actively decide what, when, and where to observe, and continuously assess whether the current observation is sufficient to answer the query. We present Active Video Perception (AVP), an evidence-seeking framework that treats the video as an interactive environment and acquires compact, queryrelevant evidence directly from pixels. Concretely, AVP runs an iterative plan-observe-reflect process with MLLM agents. In each round, a planner proposes targeted video interactions, an observer executes them to extract time-stamped evidence, and a reflector evaluates the sufficiency of the evidence for the query, either halting with an answer or triggering further observation. Across five LVU benchmarks, AVP achieves highest performance with significant improvements. Notably, AVP outperforms the best agentic method by 5.7% in average accuracy while only requires 18.4% inference time and 12.4% input tokens.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.05774.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": true,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8869
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.04142",
            "authors": [
                {
                    "_id": "6932b58d6d1060ca587a2880",
                    "user": {
                        "_id": "6707cd417d4ff45be291f80a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/sQmeE4mk1xLsCAvYG9OYb.png",
                        "isPro": false,
                        "fullname": "Sophia Falk",
                        "user": "sophia-falk",
                        "type": "user"
                    },
                    "name": "Sophia Falk",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-05T15:14:36.786Z",
                    "hidden": false
                },
                {
                    "_id": "6932b58d6d1060ca587a2881",
                    "user": {
                        "_id": "62e1cc43926f4892a4ca2ff9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e1cc43926f4892a4ca2ff9/KqZUpkRRjGlZ7FawmdLq2.png",
                        "isPro": true,
                        "fullname": "Nicholas Kluge Corra",
                        "user": "nicholasKluge",
                        "type": "user"
                    },
                    "name": "Nicholas Kluge Corra",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-05T15:14:40.077Z",
                    "hidden": false
                },
                {
                    "_id": "6932b58d6d1060ca587a2882",
                    "name": "Sasha Luccioni",
                    "hidden": false
                },
                {
                    "_id": "6932b58d6d1060ca587a2883",
                    "name": "Lisa Biber-Freudenberger",
                    "hidden": false
                },
                {
                    "_id": "6932b58d6d1060ca587a2884",
                    "name": "Aimee van Wynsberghe",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-03T17:01:46.000Z",
            "submittedOnDailyAt": "2025-12-08T11:44:35.222Z",
            "title": "From FLOPs to Footprints: The Resource Cost of Artificial Intelligence",
            "submittedOnDailyBy": {
                "_id": "62e1cc43926f4892a4ca2ff9",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e1cc43926f4892a4ca2ff9/KqZUpkRRjGlZ7FawmdLq2.png",
                "isPro": true,
                "fullname": "Nicholas Kluge Corra",
                "user": "nicholasKluge",
                "type": "user"
            },
            "summary": "As computational demands continue to rise, assessing the environmental footprint of AI requires moving beyond energy and water consumption to include the material demands of specialized hardware. This study quantifies the material footprint of AI training by linking computational workloads to physical hardware needs. The elemental composition of the Nvidia A100 SXM 40 GB graphics processing unit (GPU) was analyzed using inductively coupled plasma optical emission spectroscopy, which identified 32 elements. The results show that AI hardware consists of about 90% heavy metals and only trace amounts of precious metals. The elements copper, iron, tin, silicon, and nickel dominate the GPU composition by mass. In a multi-step methodology, we integrate these measurements with computational throughput per GPU across varying lifespans, accounting for the computational requirements of training specific AI models at different training efficiency regimes. Scenario-based analyses reveal that, depending on Model FLOPs Utilization (MFU) and hardware lifespan, training GPT-4 requires between 1,174 and 8,800 A100 GPUs, corresponding to the extraction and eventual disposal of up to 7 tons of toxic elements. Combined software and hardware optimization strategies can reduce material demands: increasing MFU from 20% to 60% lowers GPU requirements by 67%, while extending lifespan from 1 to 3 years yields comparable savings; implementing both measures together reduces GPU needs by up to 93%. Our findings highlight that incremental performance gains, such as those observed between GPT-3.5 and GPT-4, come at disproportionately high material costs. The study underscores the necessity of incorporating material resource considerations into discussions of AI scalability, emphasizing that future progress in AI must align with principles of resource efficiency and environmental responsibility.",
            "upvotes": 1,
            "discussionId": "6932b58d6d1060ca587a2885",
            "ai_summary": "The study quantifies the material footprint of AI training, focusing on the Nvidia A100 GPU, and examines the environmental impact of training models like GPT-4, emphasizing the need for resource-efficient strategies.",
            "ai_keywords": [
                "inductively coupled plasma optical emission spectroscopy",
                "graphics processing unit",
                "Model FLOPs Utilization",
                "GPT-4",
                "computational throughput",
                "material resource considerations",
                "resource efficiency",
                "environmental responsibility"
            ]
        },
        "publishedAt": "2025-12-03T12:01:46.000Z",
        "title": "From FLOPs to Footprints: The Resource Cost of Artificial Intelligence",
        "summary": "As computational demands continue to rise, assessing the environmental footprint of AI requires moving beyond energy and water consumption to include the material demands of specialized hardware. This study quantifies the material footprint of AI training by linking computational workloads to physical hardware needs. The elemental composition of the Nvidia A100 SXM 40 GB graphics processing unit (GPU) was analyzed using inductively coupled plasma optical emission spectroscopy, which identified 32 elements. The results show that AI hardware consists of about 90% heavy metals and only trace amounts of precious metals. The elements copper, iron, tin, silicon, and nickel dominate the GPU composition by mass. In a multi-step methodology, we integrate these measurements with computational throughput per GPU across varying lifespans, accounting for the computational requirements of training specific AI models at different training efficiency regimes. Scenario-based analyses reveal that, depending on Model FLOPs Utilization (MFU) and hardware lifespan, training GPT-4 requires between 1,174 and 8,800 A100 GPUs, corresponding to the extraction and eventual disposal of up to 7 tons of toxic elements. Combined software and hardware optimization strategies can reduce material demands: increasing MFU from 20% to 60% lowers GPU requirements by 67%, while extending lifespan from 1 to 3 years yields comparable savings; implementing both measures together reduces GPU needs by up to 93%. Our findings highlight that incremental performance gains, such as those observed between GPT-3.5 and GPT-4, come at disproportionately high material costs. The study underscores the necessity of incorporating material resource considerations into discussions of AI scalability, emphasizing that future progress in AI must align with principles of resource efficiency and environmental responsibility.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04142.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62e1cc43926f4892a4ca2ff9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e1cc43926f4892a4ca2ff9/KqZUpkRRjGlZ7FawmdLq2.png",
            "fullname": "Nicholas Kluge Corra",
            "name": "nicholasKluge",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 46
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.05339",
            "authors": [
                {
                    "_id": "69370e3a19d912300c34a0dc",
                    "name": "Mahesh Kumar Nandwana",
                    "hidden": false
                },
                {
                    "_id": "69370e3a19d912300c34a0dd",
                    "name": "Youngwan Lim",
                    "hidden": false
                },
                {
                    "_id": "69370e3a19d912300c34a0de",
                    "name": "Joseph Liu",
                    "hidden": false
                },
                {
                    "_id": "69370e3a19d912300c34a0df",
                    "name": "Alex Yang",
                    "hidden": false
                },
                {
                    "_id": "69370e3a19d912300c34a0e0",
                    "name": "Varun Notibala",
                    "hidden": false
                },
                {
                    "_id": "69370e3a19d912300c34a0e1",
                    "name": "Nishchaie Khanna",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-05T00:43:55.000Z",
            "submittedOnDailyAt": "2025-12-08T15:14:45.026Z",
            "title": "Taxonomy-Adaptive Moderation Model with Robust Guardrails for Large Language Models",
            "submittedOnDailyBy": {
                "_id": "667f437aa94430cd2e723346",
                "avatarUrl": "/avatars/c7d71f6789756082e129b0b9cf9dcfda.svg",
                "isPro": false,
                "fullname": "Mahesh Nandwana",
                "user": "mnandwana",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) are typically aligned for safety during the post-training phase; however, they may still generate inappropriate outputs that could potentially pose risks to users. This challenge underscores the need for robust safeguards that operate across both model inputs and outputs. In this work, we introduce Roblox Guard 1.0, a state-of-the-art instruction fine-tuned LLM designed to enhance the safety of LLM systems through comprehensive input-output moderation, using a pipeline of LLMs to enhance moderation capability. Built on the Llama-3.1-8B-Instruct backbone, our model is instruction fine-tuned to generalize across previously unseen safety taxonomies and demonstrates strong performance on out-of-domain safety benchmarks. The instruction fine-tuning process uses a mix of synthetic and open-source safety datasets, augmented with chain-of-thought (CoT) rationales and input inversion to enhance contextual understanding and decision making. To support systematic evaluation, we also release RobloxGuard-Eval, a new benchmark featuring an extensible safety taxonomy to assess the effectiveness of LLM guardrails and moderation frameworks.",
            "upvotes": 0,
            "discussionId": "69370e3a19d912300c34a0e2",
            "ai_summary": "Roblox Guard 1.0 is an instruction fine-tuned LLM that enhances safety through comprehensive input-output moderation using a pipeline of LLMs and demonstrates strong performance on out-of-domain safety benchmarks.",
            "ai_keywords": [
                "instruction fine-tuned",
                "LLM",
                "safety taxonomies",
                "chain-of-thought",
                "input inversion",
                "contextual understanding",
                "RobloxGuard-Eval",
                "safety benchmarks"
            ],
            "organization": {
                "_id": "62a3a163b5db00777be6a438",
                "name": "Roblox",
                "fullname": "Roblox Corporation",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6438428526c25444413c54d1/zHEWe9jE0_osXQZ1f4tRl.png"
            }
        },
        "publishedAt": "2025-12-04T19:43:55.000Z",
        "title": "Taxonomy-Adaptive Moderation Model with Robust Guardrails for Large Language Models",
        "summary": "Large Language Models (LLMs) are typically aligned for safety during the post-training phase; however, they may still generate inappropriate outputs that could potentially pose risks to users. This challenge underscores the need for robust safeguards that operate across both model inputs and outputs. In this work, we introduce Roblox Guard 1.0, a state-of-the-art instruction fine-tuned LLM designed to enhance the safety of LLM systems through comprehensive input-output moderation, using a pipeline of LLMs to enhance moderation capability. Built on the Llama-3.1-8B-Instruct backbone, our model is instruction fine-tuned to generalize across previously unseen safety taxonomies and demonstrates strong performance on out-of-domain safety benchmarks. The instruction fine-tuning process uses a mix of synthetic and open-source safety datasets, augmented with chain-of-thought (CoT) rationales and input inversion to enhance contextual understanding and decision making. To support systematic evaluation, we also release RobloxGuard-Eval, a new benchmark featuring an extensible safety taxonomy to assess the effectiveness of LLM guardrails and moderation frameworks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.05339.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "667f437aa94430cd2e723346",
            "avatarUrl": "/avatars/c7d71f6789756082e129b0b9cf9dcfda.svg",
            "fullname": "Mahesh Nandwana",
            "name": "mnandwana",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "62a3a163b5db00777be6a438",
            "name": "Roblox",
            "fullname": "Roblox Corporation",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6438428526c25444413c54d1/zHEWe9jE0_osXQZ1f4tRl.png"
        },
        "isAuthorParticipating": false
    }
]