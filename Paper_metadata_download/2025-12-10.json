[
    {
        "paper": {
            "id": "2512.08765",
            "authors": [
                {
                    "_id": "6938da63dfc35938ba129f3c",
                    "user": {
                        "_id": "642e3bcb958faf258a40e89c",
                        "avatarUrl": "/avatars/dad142df2217f8eed1f45c9e7287d3ea.svg",
                        "isPro": false,
                        "fullname": "Ruihang Chu",
                        "user": "Ruihang",
                        "type": "user"
                    },
                    "name": "Ruihang Chu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-10T09:42:07.767Z",
                    "hidden": false
                },
                {
                    "_id": "6938da63dfc35938ba129f3d",
                    "name": "Yefei He",
                    "hidden": false
                },
                {
                    "_id": "6938da63dfc35938ba129f3e",
                    "user": {
                        "_id": "62d812e143df7719860d05d1",
                        "avatarUrl": "/avatars/412f7ec5c9f54990f4b562652d3e2c59.svg",
                        "isPro": false,
                        "fullname": "zhekai chen",
                        "user": "Azily",
                        "type": "user"
                    },
                    "name": "Zhekai Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-10T09:42:00.513Z",
                    "hidden": false
                },
                {
                    "_id": "6938da63dfc35938ba129f3f",
                    "name": "Shiwei Zhang",
                    "hidden": false
                },
                {
                    "_id": "6938da63dfc35938ba129f40",
                    "user": {
                        "_id": "637ee45b2438d7485b8d8f6a",
                        "avatarUrl": "/avatars/11b7d29b6fa6c1b392641e0cd4002863.svg",
                        "isPro": false,
                        "fullname": "Xiaogang Xu",
                        "user": "xiaogang00",
                        "type": "user"
                    },
                    "name": "Xiaogang Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-10T09:41:51.241Z",
                    "hidden": false
                },
                {
                    "_id": "6938da63dfc35938ba129f41",
                    "name": "Bin Xia",
                    "hidden": false
                },
                {
                    "_id": "6938da63dfc35938ba129f42",
                    "name": "Dingdong Wang",
                    "hidden": false
                },
                {
                    "_id": "6938da63dfc35938ba129f43",
                    "name": "Hongwei Yi",
                    "hidden": false
                },
                {
                    "_id": "6938da63dfc35938ba129f44",
                    "user": {
                        "_id": "65d5ec74cd05bc1eaa125040",
                        "avatarUrl": "/avatars/2de1b1539a86452c2c89570eeb02f5ab.svg",
                        "isPro": false,
                        "fullname": "Xihui Liu",
                        "user": "XihuiLiu",
                        "type": "user"
                    },
                    "name": "Xihui Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-10T09:41:32.582Z",
                    "hidden": false
                },
                {
                    "_id": "6938da63dfc35938ba129f45",
                    "user": {
                        "_id": "690090cca41c454e4786c0e5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/690090cca41c454e4786c0e5/ykyy4gV7EV_xfv4glxC1m.png",
                        "isPro": false,
                        "fullname": "Hengshuang Zhao",
                        "user": "Hengshuang",
                        "type": "user"
                    },
                    "name": "Hengshuang Zhao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-10T09:41:26.372Z",
                    "hidden": false
                },
                {
                    "_id": "6938da63dfc35938ba129f46",
                    "name": "Yu Liu",
                    "hidden": false
                },
                {
                    "_id": "6938da63dfc35938ba129f47",
                    "name": "Yingya Zhang",
                    "hidden": false
                },
                {
                    "_id": "6938da63dfc35938ba129f48",
                    "user": {
                        "_id": "64ca1fe838837b12d5e529b7",
                        "avatarUrl": "/avatars/44a3ad9e59318784ac531993b5f69f6b.svg",
                        "isPro": false,
                        "fullname": "Yujiu Yang",
                        "user": "Thu-redrobot",
                        "type": "user"
                    },
                    "name": "Yujiu Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-10T09:41:10.566Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/GRHR-g0jymQza9KMw0iLn.png",
                "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/K8nyGDyLuL_hxp15wJdMk.png",
                "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/4b8dLB_xvGpTjJlWP8oeh.mp4"
            ],
            "publishedAt": "2025-12-09T16:13:55.000Z",
            "submittedOnDailyAt": "2025-12-10T00:20:18.797Z",
            "title": "Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.",
            "upvotes": 94,
            "discussionId": "6938da64dfc35938ba129f49",
            "githubRepo": "https://github.com/ali-vilab/Wan-Move",
            "githubRepoAddedBy": "user",
            "ai_summary": "Wan-Move enhances motion control in video generative models by integrating motion-aware features into latent space, enabling high-quality and scalable video synthesis.",
            "ai_keywords": [
                "motion control",
                "video generative models",
                "dense point trajectories",
                "latent space",
                "spatiotemporal feature map",
                "motion guidance",
                "image-to-video model",
                "auxiliary motion encoders",
                "fine-tuning",
                "MoveBench",
                "motion annotations"
            ],
            "githubStars": 197,
            "organization": {
                "_id": "67d15cca6e2cf0e062dbfb54",
                "name": "AlibabaTongyiLab",
                "fullname": "TongyiLab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
            }
        },
        "publishedAt": "2025-12-09T11:13:55.000Z",
        "title": "Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance",
        "summary": "We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/GRHR-g0jymQza9KMw0iLn.png",
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/K8nyGDyLuL_hxp15wJdMk.png",
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/4b8dLB_xvGpTjJlWP8oeh.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.08765.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 181
        },
        "organization": {
            "_id": "67d15cca6e2cf0e062dbfb54",
            "name": "AlibabaTongyiLab",
            "fullname": "TongyiLab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.08478",
            "authors": [
                {
                    "_id": "6938e00fdfc35938ba129f4f",
                    "name": "Yuning Gong",
                    "hidden": false
                },
                {
                    "_id": "6938e00fdfc35938ba129f50",
                    "name": "Yifei Liu",
                    "hidden": false
                },
                {
                    "_id": "6938e00fdfc35938ba129f51",
                    "name": "Yifan Zhan",
                    "hidden": false
                },
                {
                    "_id": "6938e00fdfc35938ba129f52",
                    "name": "Muyao Niu",
                    "hidden": false
                },
                {
                    "_id": "6938e00fdfc35938ba129f53",
                    "name": "Xueying Li",
                    "hidden": false
                },
                {
                    "_id": "6938e00fdfc35938ba129f54",
                    "name": "Yuanjun Liao",
                    "hidden": false
                },
                {
                    "_id": "6938e00fdfc35938ba129f55",
                    "name": "Jiaming Chen",
                    "hidden": false
                },
                {
                    "_id": "6938e00fdfc35938ba129f56",
                    "name": "Yuanyuan Gao",
                    "hidden": false
                },
                {
                    "_id": "6938e00fdfc35938ba129f57",
                    "name": "Jiaqi Chen",
                    "hidden": false
                },
                {
                    "_id": "6938e00fdfc35938ba129f58",
                    "name": "Minming Chen",
                    "hidden": false
                },
                {
                    "_id": "6938e00fdfc35938ba129f59",
                    "name": "Li Zhou",
                    "hidden": false
                },
                {
                    "_id": "6938e00fdfc35938ba129f5a",
                    "name": "Yuning Zhang",
                    "hidden": false
                },
                {
                    "_id": "6938e00fdfc35938ba129f5b",
                    "name": "Wei Wang",
                    "hidden": false
                },
                {
                    "_id": "6938e00fdfc35938ba129f5c",
                    "name": "Xiaoqing Hou",
                    "hidden": false
                },
                {
                    "_id": "6938e00fdfc35938ba129f5d",
                    "name": "Huaxi Huang",
                    "hidden": false
                },
                {
                    "_id": "6938e00fdfc35938ba129f5e",
                    "name": "Shixiang Tang",
                    "hidden": false
                },
                {
                    "_id": "6938e00fdfc35938ba129f5f",
                    "name": "Le Ma",
                    "hidden": false
                },
                {
                    "_id": "6938e00fdfc35938ba129f60",
                    "name": "Dingwen Zhang",
                    "hidden": false
                },
                {
                    "_id": "6938e00fdfc35938ba129f61",
                    "name": "Xue Yang",
                    "hidden": false
                },
                {
                    "_id": "6938e00fdfc35938ba129f62",
                    "name": "Junchi Yan",
                    "hidden": false
                },
                {
                    "_id": "6938e00fdfc35938ba129f63",
                    "name": "Yanchi Zhang",
                    "hidden": false
                },
                {
                    "_id": "6938e00fdfc35938ba129f64",
                    "name": "Yinqiang Zheng",
                    "hidden": false
                },
                {
                    "_id": "6938e00fdfc35938ba129f65",
                    "name": "Xiao Sun",
                    "hidden": false
                },
                {
                    "_id": "6938e00fdfc35938ba129f66",
                    "user": {
                        "_id": "6938f4de790b5cd0f6df6462",
                        "avatarUrl": "/avatars/4f22f0499d96bb749af7e8dba2b0b533.svg",
                        "isPro": false,
                        "fullname": "Zhihang Zhong",
                        "user": "Zuica96",
                        "type": "user"
                    },
                    "name": "Zhihang Zhong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-10T08:56:28.162Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6938f4de790b5cd0f6df6462/OZHh1MEcn5fqR-GNW7m_m.mp4"
            ],
            "publishedAt": "2025-12-09T10:54:58.000Z",
            "submittedOnDailyAt": "2025-12-10T07:43:37.566Z",
            "title": "Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform",
            "submittedOnDailyBy": {
                "_id": "6938f4de790b5cd0f6df6462",
                "avatarUrl": "/avatars/4f22f0499d96bb749af7e8dba2b0b533.svg",
                "isPro": false,
                "fullname": "Zhihang Zhong",
                "user": "Zuica96",
                "type": "user"
            },
            "summary": "Neural rendering, particularly 3D Gaussian Splatting (3DGS), has evolved rapidly and become a key component for building world models. However, existing viewer solutions remain fragmented, heavy, or constrained by legacy pipelines, resulting in high deployment friction and limited support for dynamic content and generative models. In this work, we present Visionary, an open, web-native platform for real-time various Gaussian Splatting and meshes rendering. Built on an efficient WebGPU renderer with per-frame ONNX inference, Visionary enables dynamic neural processing while maintaining a lightweight, \"click-to-run\" browser experience. It introduces a standardized Gaussian Generator contract, which not only supports standard 3DGS rendering but also allows plug-and-play algorithms to generate or update Gaussians each frame. Such inference also enables us to apply feedforward generative post-processing. The platform further offers a plug in three.js library with a concise TypeScript API for seamless integration into existing web applications. Experiments show that, under identical 3DGS assets, Visionary achieves superior rendering efficiency compared to current Web viewers due to GPU-based primitive sorting. It already supports multiple variants, including MLP-based 3DGS, 4DGS, neural avatars, and style transformation or enhancement networks. By unifying inference and rendering directly in the browser, Visionary significantly lowers the barrier to reproduction, comparison, and deployment of 3DGS-family methods, serving as a unified World Model Carrier for both reconstructive and generative paradigms.",
            "upvotes": 65,
            "discussionId": "6938e00fdfc35938ba129f67",
            "projectPage": "https://visionary-laboratory.github.io/visionary/",
            "githubRepo": "https://github.com/Visionary-Laboratory/visionary",
            "githubRepoAddedBy": "user",
            "ai_summary": "Visionary is an open web-native platform enabling real-time rendering of 3D Gaussian Splatting and meshes with efficient GPU-based inference, supporting dynamic content and generative models.",
            "ai_keywords": [
                "Neural rendering",
                "3D Gaussian Splatting",
                "3DGS",
                "WebGPU",
                "ONNX inference",
                "Gaussian Generator contract",
                "three.js",
                "TypeScript API",
                "MLP-based 3DGS",
                "4DGS",
                "neural avatars",
                "style transformation",
                "GPU-based primitive sorting",
                "World Model Carrier"
            ],
            "githubStars": 162
        },
        "publishedAt": "2025-12-09T05:54:58.000Z",
        "title": "Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform",
        "summary": "Neural rendering, particularly 3D Gaussian Splatting (3DGS), has evolved rapidly and become a key component for building world models. However, existing viewer solutions remain fragmented, heavy, or constrained by legacy pipelines, resulting in high deployment friction and limited support for dynamic content and generative models. In this work, we present Visionary, an open, web-native platform for real-time various Gaussian Splatting and meshes rendering. Built on an efficient WebGPU renderer with per-frame ONNX inference, Visionary enables dynamic neural processing while maintaining a lightweight, \"click-to-run\" browser experience. It introduces a standardized Gaussian Generator contract, which not only supports standard 3DGS rendering but also allows plug-and-play algorithms to generate or update Gaussians each frame. Such inference also enables us to apply feedforward generative post-processing. The platform further offers a plug in three.js library with a concise TypeScript API for seamless integration into existing web applications. Experiments show that, under identical 3DGS assets, Visionary achieves superior rendering efficiency compared to current Web viewers due to GPU-based primitive sorting. It already supports multiple variants, including MLP-based 3DGS, 4DGS, neural avatars, and style transformation or enhancement networks. By unifying inference and rendering directly in the browser, Visionary significantly lowers the barrier to reproduction, comparison, and deployment of 3DGS-family methods, serving as a unified World Model Carrier for both reconstructive and generative paradigms.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6938f4de790b5cd0f6df6462/OZHh1MEcn5fqR-GNW7m_m.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.08478.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "6938f4de790b5cd0f6df6462",
            "avatarUrl": "/avatars/4f22f0499d96bb749af7e8dba2b0b533.svg",
            "fullname": "Zhihang Zhong",
            "name": "Zuica96",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.07951",
            "authors": [
                {
                    "_id": "6938e892dfc35938ba129ff5",
                    "name": "Zekai Luo",
                    "hidden": false
                },
                {
                    "_id": "6938e892dfc35938ba129ff6",
                    "name": "Zongze Du",
                    "hidden": false
                },
                {
                    "_id": "6938e892dfc35938ba129ff7",
                    "name": "Zhouhang Zhu",
                    "hidden": false
                },
                {
                    "_id": "6938e892dfc35938ba129ff8",
                    "name": "Hao Zhong",
                    "hidden": false
                },
                {
                    "_id": "6938e892dfc35938ba129ff9",
                    "user": {
                        "_id": "632179745fc60c44fd91fc33",
                        "avatarUrl": "/avatars/37d4fefbcc19f091dccffefec9706de2.svg",
                        "isPro": false,
                        "fullname": "zhumuzhi",
                        "user": "Z-MU-Z",
                        "type": "user"
                    },
                    "name": "Muzhi Zhu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-10T10:51:43.541Z",
                    "hidden": false
                },
                {
                    "_id": "6938e892dfc35938ba129ffa",
                    "name": "Wen Wang",
                    "hidden": false
                },
                {
                    "_id": "6938e892dfc35938ba129ffb",
                    "name": "Yuling Xi",
                    "hidden": false
                },
                {
                    "_id": "6938e892dfc35938ba129ffc",
                    "name": "Chenchen Jing",
                    "hidden": false
                },
                {
                    "_id": "6938e892dfc35938ba129ffd",
                    "name": "Hao Chen",
                    "hidden": false
                },
                {
                    "_id": "6938e892dfc35938ba129ffe",
                    "name": "Chunhua Shen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-08T19:00:04.000Z",
            "submittedOnDailyAt": "2025-12-10T00:59:39.366Z",
            "title": "Preserving Source Video Realism: High-Fidelity Face Swapping for Cinematic Quality",
            "submittedOnDailyBy": {
                "_id": "632179745fc60c44fd91fc33",
                "avatarUrl": "/avatars/37d4fefbcc19f091dccffefec9706de2.svg",
                "isPro": false,
                "fullname": "zhumuzhi",
                "user": "Z-MU-Z",
                "type": "user"
            },
            "summary": "Video face swapping is crucial in film and entertainment production, where achieving high fidelity and temporal consistency over long and complex video sequences remains a significant challenge. Inspired by recent advances in reference-guided image editing, we explore whether rich visual attributes from source videos can be similarly leveraged to enhance both fidelity and temporal coherence in video face swapping. Building on this insight, this work presents LivingSwap, the first video reference guided face swapping model. Our approach employs keyframes as conditioning signals to inject the target identity, enabling flexible and controllable editing. By combining keyframe conditioning with video reference guidance, the model performs temporal stitching to ensure stable identity preservation and high-fidelity reconstruction across long video sequences. To address the scarcity of data for reference-guided training, we construct a paired face-swapping dataset, Face2Face, and further reverse the data pairs to ensure reliable ground-truth supervision. Extensive experiments demonstrate that our method achieves state-of-the-art results, seamlessly integrating the target identity with the source video's expressions, lighting, and motion, while significantly reducing manual effort in production workflows. Project webpage: https://aim-uofa.github.io/LivingSwap",
            "upvotes": 40,
            "discussionId": "6938e892dfc35938ba129fff",
            "projectPage": "https://aim-uofa.github.io/LivingSwap",
            "ai_summary": "LivingSwap enhances video face swapping by using keyframes and reference guidance to maintain identity and fidelity over long sequences, reducing manual effort and achieving state-of-the-art results.",
            "ai_keywords": [
                "keyframe conditioning",
                "video reference guidance",
                "temporal stitching",
                "identity preservation",
                "high-fidelity reconstruction",
                "Face2Face dataset"
            ],
            "organization": {
                "_id": "61bac2af530e5c78d7b99667",
                "name": "zju",
                "fullname": "Zhejiang University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"
            }
        },
        "publishedAt": "2025-12-08T14:00:04.000Z",
        "title": "Preserving Source Video Realism: High-Fidelity Face Swapping for Cinematic Quality",
        "summary": "Video face swapping is crucial in film and entertainment production, where achieving high fidelity and temporal consistency over long and complex video sequences remains a significant challenge. Inspired by recent advances in reference-guided image editing, we explore whether rich visual attributes from source videos can be similarly leveraged to enhance both fidelity and temporal coherence in video face swapping. Building on this insight, this work presents LivingSwap, the first video reference guided face swapping model. Our approach employs keyframes as conditioning signals to inject the target identity, enabling flexible and controllable editing. By combining keyframe conditioning with video reference guidance, the model performs temporal stitching to ensure stable identity preservation and high-fidelity reconstruction across long video sequences. To address the scarcity of data for reference-guided training, we construct a paired face-swapping dataset, Face2Face, and further reverse the data pairs to ensure reliable ground-truth supervision. Extensive experiments demonstrate that our method achieves state-of-the-art results, seamlessly integrating the target identity with the source video's expressions, lighting, and motion, while significantly reducing manual effort in production workflows. Project webpage: https://aim-uofa.github.io/LivingSwap",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.07951.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "632179745fc60c44fd91fc33",
            "avatarUrl": "/avatars/37d4fefbcc19f091dccffefec9706de2.svg",
            "fullname": "zhumuzhi",
            "name": "Z-MU-Z",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "organization": {
            "_id": "61bac2af530e5c78d7b99667",
            "name": "zju",
            "fullname": "Zhejiang University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.07802",
            "authors": [
                {
                    "_id": "69392ceedfc35938ba12a187",
                    "user": {
                        "_id": "65e5eae6958b39864e8b683e",
                        "avatarUrl": "/avatars/b6a857e7b725767197dd95bc876f8ad1.svg",
                        "isPro": true,
                        "fullname": "Zhaochong An",
                        "user": "ZhaochongAn",
                        "type": "user"
                    },
                    "name": "Zhaochong An",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-10T13:07:50.839Z",
                    "hidden": false
                },
                {
                    "_id": "69392ceedfc35938ba12a188",
                    "name": "Menglin Jia",
                    "hidden": false
                },
                {
                    "_id": "69392ceedfc35938ba12a189",
                    "name": "Haonan Qiu",
                    "hidden": false
                },
                {
                    "_id": "69392ceedfc35938ba12a18a",
                    "name": "Zijian Zhou",
                    "hidden": false
                },
                {
                    "_id": "69392ceedfc35938ba12a18b",
                    "name": "Xiaoke Huang",
                    "hidden": false
                },
                {
                    "_id": "69392ceedfc35938ba12a18c",
                    "name": "Zhiheng Liu",
                    "hidden": false
                },
                {
                    "_id": "69392ceedfc35938ba12a18d",
                    "name": "Weiming Ren",
                    "hidden": false
                },
                {
                    "_id": "69392ceedfc35938ba12a18e",
                    "user": {
                        "_id": "65a12ec4c9873890d15c4ab9",
                        "avatarUrl": "/avatars/ce4d46f575ba757f78eabdb25b394171.svg",
                        "isPro": false,
                        "fullname": "Kumara Kahatapitiya",
                        "user": "kumarak",
                        "type": "user"
                    },
                    "name": "Kumara Kahatapitiya",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-10T10:51:00.011Z",
                    "hidden": false
                },
                {
                    "_id": "69392ceedfc35938ba12a18f",
                    "name": "Ding Liu",
                    "hidden": false
                },
                {
                    "_id": "69392ceedfc35938ba12a190",
                    "name": "Sen He",
                    "hidden": false
                },
                {
                    "_id": "69392ceedfc35938ba12a191",
                    "name": "Chenyang Zhang",
                    "hidden": false
                },
                {
                    "_id": "69392ceedfc35938ba12a192",
                    "name": "Tao Xiang",
                    "hidden": false
                },
                {
                    "_id": "69392ceedfc35938ba12a193",
                    "name": "Fanny Yang",
                    "hidden": false
                },
                {
                    "_id": "69392ceedfc35938ba12a194",
                    "name": "Serge Belongie",
                    "hidden": false
                },
                {
                    "_id": "69392ceedfc35938ba12a195",
                    "name": "Tian Xie",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/65e5eae6958b39864e8b683e/iJ5L2BbZWW8XiE7ORZuFK.mp4"
            ],
            "publishedAt": "2025-12-08T18:32:24.000Z",
            "submittedOnDailyAt": "2025-12-10T06:06:45.364Z",
            "title": "OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory",
            "submittedOnDailyBy": {
                "_id": "65e5eae6958b39864e8b683e",
                "avatarUrl": "/avatars/b6a857e7b725767197dd95bc876f8ad1.svg",
                "isPro": true,
                "fullname": "Zhaochong An",
                "user": "ZhaochongAn",
                "type": "user"
            },
            "summary": "Storytelling in real-world videos often unfolds through multiple shots -- discontinuous yet semantically connected clips that together convey a coherent narrative. However, existing multi-shot video generation (MSV) methods struggle to effectively model long-range cross-shot context, as they rely on limited temporal windows or single keyframe conditioning, leading to degraded performance under complex narratives. In this work, we propose OneStory, enabling global yet compact cross-shot context modeling for consistent and scalable narrative generation. OneStory reformulates MSV as a next-shot generation task, enabling autoregressive shot synthesis while leveraging pretrained image-to-video (I2V) models for strong visual conditioning. We introduce two key modules: a Frame Selection module that constructs a semantically-relevant global memory based on informative frames from prior shots, and an Adaptive Conditioner that performs importance-guided patchification to generate compact context for direct conditioning. We further curate a high-quality multi-shot dataset with referential captions to mirror real-world storytelling patterns, and design effective training strategies under the next-shot paradigm. Finetuned from a pretrained I2V model on our curated 60K dataset, OneStory achieves state-of-the-art narrative coherence across diverse and complex scenes in both text- and image-conditioned settings, enabling controllable and immersive long-form video storytelling.",
            "upvotes": 31,
            "discussionId": "69392ceedfc35938ba12a196",
            "projectPage": "https://zhaochongan.github.io/projects/OneStory/",
            "ai_summary": "OneStory generates coherent multi-shot videos by modeling global cross-shot context through a Frame Selection module and an Adaptive Conditioner, leveraging pretrained image-to-video models and a curated dataset.",
            "ai_keywords": [
                "multi-shot video generation",
                "next-shot generation",
                "autoregressive shot synthesis",
                "pretrained image-to-video models",
                "Frame Selection module",
                "Adaptive Conditioner",
                "semantically-relevant global memory",
                "importance-guided patchification",
                "referential captions",
                "text-conditioned",
                "image-conditioned"
            ],
            "organization": {
                "_id": "5e63d8713071d5be688861b8",
                "name": "facebook",
                "fullname": "AI at Meta",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
            }
        },
        "publishedAt": "2025-12-08T13:32:24.000Z",
        "title": "OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory",
        "summary": "Storytelling in real-world videos often unfolds through multiple shots -- discontinuous yet semantically connected clips that together convey a coherent narrative. However, existing multi-shot video generation (MSV) methods struggle to effectively model long-range cross-shot context, as they rely on limited temporal windows or single keyframe conditioning, leading to degraded performance under complex narratives. In this work, we propose OneStory, enabling global yet compact cross-shot context modeling for consistent and scalable narrative generation. OneStory reformulates MSV as a next-shot generation task, enabling autoregressive shot synthesis while leveraging pretrained image-to-video (I2V) models for strong visual conditioning. We introduce two key modules: a Frame Selection module that constructs a semantically-relevant global memory based on informative frames from prior shots, and an Adaptive Conditioner that performs importance-guided patchification to generate compact context for direct conditioning. We further curate a high-quality multi-shot dataset with referential captions to mirror real-world storytelling patterns, and design effective training strategies under the next-shot paradigm. Finetuned from a pretrained I2V model on our curated 60K dataset, OneStory achieves state-of-the-art narrative coherence across diverse and complex scenes in both text- and image-conditioned settings, enabling controllable and immersive long-form video storytelling.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/65e5eae6958b39864e8b683e/iJ5L2BbZWW8XiE7ORZuFK.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.07802.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65e5eae6958b39864e8b683e",
            "avatarUrl": "/avatars/b6a857e7b725767197dd95bc876f8ad1.svg",
            "fullname": "Zhaochong An",
            "name": "ZhaochongAn",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "organization": {
            "_id": "5e63d8713071d5be688861b8",
            "name": "facebook",
            "fullname": "AI at Meta",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.07843",
            "authors": [
                {
                    "_id": "6938e51ddfc35938ba129fb2",
                    "user": {
                        "_id": "63797c273f575acc2f6893c0",
                        "avatarUrl": "/avatars/32d7a6a8881c8c4d80a097b732ed24b6.svg",
                        "isPro": true,
                        "fullname": "Long(Tony) Lian",
                        "user": "longlian",
                        "type": "user"
                    },
                    "name": "Long Lian",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-10T08:56:15.315Z",
                    "hidden": false
                },
                {
                    "_id": "6938e51ddfc35938ba129fb3",
                    "name": "Sida Wang",
                    "hidden": false
                },
                {
                    "_id": "6938e51ddfc35938ba129fb4",
                    "user": {
                        "_id": "6417cf37dce1e4c0229f17b1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6417cf37dce1e4c0229f17b1/7h-ZCB5f4wif7TsnF-B1M.jpeg",
                        "isPro": false,
                        "fullname": "Felix Xu",
                        "user": "katanaxu",
                        "type": "user"
                    },
                    "name": "Felix Juefei-Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-10T13:07:56.324Z",
                    "hidden": false
                },
                {
                    "_id": "6938e51ddfc35938ba129fb5",
                    "name": "Tsu-Jui Fu",
                    "hidden": false
                },
                {
                    "_id": "6938e51ddfc35938ba129fb6",
                    "name": "Xiuyu Li",
                    "hidden": false
                },
                {
                    "_id": "6938e51ddfc35938ba129fb7",
                    "name": "Adam Yala",
                    "hidden": false
                },
                {
                    "_id": "6938e51ddfc35938ba129fb8",
                    "name": "Trevor Darrell",
                    "hidden": false
                },
                {
                    "_id": "6938e51ddfc35938ba129fb9",
                    "name": "Alane Suhr",
                    "hidden": false
                },
                {
                    "_id": "6938e51ddfc35938ba129fba",
                    "name": "Yuandong Tian",
                    "hidden": false
                },
                {
                    "_id": "6938e51ddfc35938ba129fbb",
                    "name": "Xi Victoria Lin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-24T18:55:59.000Z",
            "submittedOnDailyAt": "2025-12-10T02:14:20.520Z",
            "title": "ThreadWeaver: Adaptive Threading for Efficient Parallel Reasoning in Language Models",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": true,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "Scaling inference-time computation has enabled Large Language Models (LLMs) to achieve strong reasoning performance, but inherently sequential decoding leads to substantial latency, especially on complex tasks. Recent work on adaptive parallel reasoning aims to improve inference efficiency by decomposing the problem-solving process into concurrent reasoning threads when beneficial. However, existing methods on realistic tasks are either limited to supervised behavior cloning or exhibit significant accuracy drops compared to widely-used sequential long chain-of-thought (CoT) baselines. Moreover, many require customized inference engines, complicating deployment. We introduce ThreadWeaver, a framework for adaptive parallel reasoning that achieves accuracy on par with popular sequential reasoning models of comparable size while significantly reducing inference latency. ThreadWeaver's performance stems from three key innovations: 1) a two-stage parallel trajectory generator that produces large-scale, high-quality CoT data with parallel annotations for supervised fine-tuning; 2) a trie-based training-inference co-design that enables parallel reasoning on any off-the-shelf autoregressive inference engine without modifying position embeddings or KV caches; and 3) a parallelization-aware reinforcement learning framework that teaches the model to balance accuracy with effective parallelization. Across six challenging mathematical reasoning benchmarks, ThreadWeaver trained atop Qwen3-8B achieves accuracy comparable to cutting-edge sequential reasoning models (71.9% on average and 79.9% on AIME24) while delivering up to 1.53x average speedup in token latency, establishing a new Pareto frontier between accuracy and efficiency.",
            "upvotes": 18,
            "discussionId": "6938e51edfc35938ba129fbc",
            "ai_summary": "ThreadWeaver, a framework for adaptive parallel reasoning, achieves accuracy comparable to sequential models while reducing inference latency through parallel trajectory generation, trie-based training-inference co-design, and parallelization-aware reinforcement learning.",
            "ai_keywords": [
                "Large Language Models",
                "sequential decoding",
                "adaptive parallel reasoning",
                "long chain-of-thought",
                "CoT",
                "ThreadWeaver",
                "parallel trajectory generator",
                "trie-based training-inference co-design",
                "parallelization-aware reinforcement learning",
                "Qwen3-8B",
                "AIME24"
            ]
        },
        "publishedAt": "2025-11-24T13:55:59.000Z",
        "title": "ThreadWeaver: Adaptive Threading for Efficient Parallel Reasoning in Language Models",
        "summary": "Scaling inference-time computation has enabled Large Language Models (LLMs) to achieve strong reasoning performance, but inherently sequential decoding leads to substantial latency, especially on complex tasks. Recent work on adaptive parallel reasoning aims to improve inference efficiency by decomposing the problem-solving process into concurrent reasoning threads when beneficial. However, existing methods on realistic tasks are either limited to supervised behavior cloning or exhibit significant accuracy drops compared to widely-used sequential long chain-of-thought (CoT) baselines. Moreover, many require customized inference engines, complicating deployment. We introduce ThreadWeaver, a framework for adaptive parallel reasoning that achieves accuracy on par with popular sequential reasoning models of comparable size while significantly reducing inference latency. ThreadWeaver's performance stems from three key innovations: 1) a two-stage parallel trajectory generator that produces large-scale, high-quality CoT data with parallel annotations for supervised fine-tuning; 2) a trie-based training-inference co-design that enables parallel reasoning on any off-the-shelf autoregressive inference engine without modifying position embeddings or KV caches; and 3) a parallelization-aware reinforcement learning framework that teaches the model to balance accuracy with effective parallelization. Across six challenging mathematical reasoning benchmarks, ThreadWeaver trained atop Qwen3-8B achieves accuracy comparable to cutting-edge sequential reasoning models (71.9% on average and 79.9% on AIME24) while delivering up to 1.53x average speedup in token latency, establishing a new Pareto frontier between accuracy and efficiency.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.07843.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": true,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8882
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.06864",
            "authors": [
                {
                    "_id": "69394fefdfc35938ba12a212",
                    "name": "Kaixuan Lu",
                    "hidden": false
                },
                {
                    "_id": "69394fefdfc35938ba12a213",
                    "user": {
                        "_id": "63be9021da08ed0544f36c38",
                        "avatarUrl": "/avatars/42511d8b1d3a3ef99bc154c98b72dfba.svg",
                        "isPro": false,
                        "fullname": "onurcan",
                        "user": "monurcan",
                        "type": "user"
                    },
                    "name": "Mehmet Onurcan Kaya",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-10T15:44:47.317Z",
                    "hidden": false
                },
                {
                    "_id": "69394fefdfc35938ba12a214",
                    "name": "Dim P. Papadopoulos",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-07T14:37:12.000Z",
            "submittedOnDailyAt": "2025-12-10T08:59:10.180Z",
            "title": "Boosting Unsupervised Video Instance Segmentation with Automatic Quality-Guided Self-Training",
            "submittedOnDailyBy": {
                "_id": "63be9021da08ed0544f36c38",
                "avatarUrl": "/avatars/42511d8b1d3a3ef99bc154c98b72dfba.svg",
                "isPro": false,
                "fullname": "onurcan",
                "user": "monurcan",
                "type": "user"
            },
            "summary": "Video Instance Segmentation (VIS) faces significant annotation challenges due to its dual requirements of pixel-level masks and temporal consistency labels. While recent unsupervised methods like VideoCutLER eliminate optical flow dependencies through synthetic data, they remain constrained by the synthetic-to-real domain gap. We present AutoQ-VIS, a novel unsupervised framework that bridges this gap through quality-guided self-training. Our approach establishes a closed-loop system between pseudo-label generation and automatic quality assessment, enabling progressive adaptation from synthetic to real videos. Experiments demonstrate state-of-the-art performance with 52.6 AP_{50} on YouTubeVIS-2019 val set, surpassing the previous state-of-the-art VideoCutLER by 4.4%, while requiring no human annotations. This demonstrates the viability of quality-aware self-training for unsupervised VIS. We will release the code at https://github.com/wcbup/AutoQ-VIS.",
            "upvotes": 12,
            "discussionId": "69394fefdfc35938ba12a215",
            "githubRepo": "https://github.com/wcbup/AutoQ-VIS/",
            "githubRepoAddedBy": "user",
            "ai_summary": "AutoQ-VIS achieves state-of-the-art results in unsupervised Video Instance Segmentation using quality-guided self-training to bridge the synthetic-to-real domain gap.",
            "ai_keywords": [
                "unsupervised methods",
                "VideoCutLER",
                "quality-guided self-training",
                "closed-loop system",
                "pseudo-label generation",
                "automatic quality assessment",
                "YouTubeVIS-2019",
                "AP50"
            ],
            "githubStars": 3
        },
        "publishedAt": "2025-12-07T09:37:12.000Z",
        "title": "Boosting Unsupervised Video Instance Segmentation with Automatic Quality-Guided Self-Training",
        "summary": "Video Instance Segmentation (VIS) faces significant annotation challenges due to its dual requirements of pixel-level masks and temporal consistency labels. While recent unsupervised methods like VideoCutLER eliminate optical flow dependencies through synthetic data, they remain constrained by the synthetic-to-real domain gap. We present AutoQ-VIS, a novel unsupervised framework that bridges this gap through quality-guided self-training. Our approach establishes a closed-loop system between pseudo-label generation and automatic quality assessment, enabling progressive adaptation from synthetic to real videos. Experiments demonstrate state-of-the-art performance with 52.6 AP_{50} on YouTubeVIS-2019 val set, surpassing the previous state-of-the-art VideoCutLER by 4.4%, while requiring no human annotations. This demonstrates the viability of quality-aware self-training for unsupervised VIS. We will release the code at https://github.com/wcbup/AutoQ-VIS.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.06864.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63be9021da08ed0544f36c38",
            "avatarUrl": "/avatars/42511d8b1d3a3ef99bc154c98b72dfba.svg",
            "fullname": "onurcan",
            "name": "monurcan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.05033",
            "authors": [
                {
                    "_id": "693919e9dfc35938ba12a151",
                    "name": "Monishwaran Maheswaran",
                    "hidden": false
                },
                {
                    "_id": "693919e9dfc35938ba12a152",
                    "name": "Rishabh Tiwari",
                    "hidden": false
                },
                {
                    "_id": "693919e9dfc35938ba12a153",
                    "user": {
                        "_id": "67dc66fe55c24fc4f981a4ab",
                        "avatarUrl": "/avatars/7bd900ade802d99db7c562ad6c2f6661.svg",
                        "isPro": false,
                        "fullname": "Yuezhou Hu",
                        "user": "yuezhouhu",
                        "type": "user"
                    },
                    "name": "Yuezhou Hu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-10T08:39:25.344Z",
                    "hidden": false
                },
                {
                    "_id": "693919e9dfc35938ba12a154",
                    "name": "Kerem Dilmen",
                    "hidden": false
                },
                {
                    "_id": "693919e9dfc35938ba12a155",
                    "name": "Coleman Hooper",
                    "hidden": false
                },
                {
                    "_id": "693919e9dfc35938ba12a156",
                    "name": "Haocheng Xi",
                    "hidden": false
                },
                {
                    "_id": "693919e9dfc35938ba12a157",
                    "name": "Nicholas Lee",
                    "hidden": false
                },
                {
                    "_id": "693919e9dfc35938ba12a158",
                    "name": "Mehrdad Farajtabar",
                    "hidden": false
                },
                {
                    "_id": "693919e9dfc35938ba12a159",
                    "name": "Michael W. Mahoney",
                    "hidden": false
                },
                {
                    "_id": "693919e9dfc35938ba12a15a",
                    "name": "Kurt Keutzer",
                    "hidden": false
                },
                {
                    "_id": "693919e9dfc35938ba12a15b",
                    "name": "Amir Gholami",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-04T17:50:53.000Z",
            "submittedOnDailyAt": "2025-12-10T18:56:28.645Z",
            "title": "Arbitrage: Efficient Reasoning via Advantage-Aware Speculation",
            "submittedOnDailyBy": {
                "_id": "67dc66fe55c24fc4f981a4ab",
                "avatarUrl": "/avatars/7bd900ade802d99db7c562ad6c2f6661.svg",
                "isPro": false,
                "fullname": "Yuezhou Hu",
                "user": "yuezhouhu",
                "type": "user"
            },
            "summary": "Modern Large Language Models achieve impressive reasoning capabilities with long Chain of Thoughts, but they incur substantial computational cost during inference, and this motivates techniques to improve the performance-cost ratio. Among these techniques, Speculative Decoding accelerates inference by employing a fast but inaccurate draft model to autoregressively propose tokens, which are then verified in parallel by a more capable target model. However, due to unnecessary rejections caused by token mismatches in semantically equivalent steps, traditional token-level Speculative Decoding struggles in reasoning tasks. Although recent works have shifted to step-level semantic verification, which improve efficiency by accepting or rejecting entire reasoning steps, existing step-level methods still regenerate many rejected steps with little improvement, wasting valuable target compute. To address this challenge, we propose Arbitrage, a novel step-level speculative generation framework that routes generation dynamically based on the relative advantage between draft and target models. Instead of applying a fixed acceptance threshold, Arbitrage uses a lightweight router trained to predict when the target model is likely to produce a meaningfully better step. This routing approximates an ideal Arbitrage Oracle that always chooses the higher-quality step, achieving near-optimal efficiency-accuracy trade-offs. Across multiple mathematical reasoning benchmarks, Arbitrage consistently surpasses prior step-level Speculative Decoding baselines, reducing inference latency by up to sim2times at matched accuracy.",
            "upvotes": 12,
            "discussionId": "693919e9dfc35938ba12a15c",
            "projectPage": "https://www.monishwaran.com/arbitrage.html",
            "githubRepo": "https://github.com/SqueezeAILab/Arbitrage",
            "githubRepoAddedBy": "user",
            "ai_summary": "Arbitrage is a dynamic routing framework for speculative decoding that improves efficiency in large language model inference by predicting when the target model will provide a better step, outperforming traditional step-level methods.",
            "ai_keywords": [
                "Speculative Decoding",
                "draft model",
                "target model",
                "token-level Speculative Decoding",
                "step-level semantic verification",
                "Arbitrage",
                "router",
                "Arbitrage Oracle",
                "inference latency"
            ],
            "githubStars": 2,
            "organization": {
                "_id": "66b1baeff10262fc4fa61961",
                "name": "UCBerkeley",
                "fullname": "University of California, Berkeley",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f425c3a096536aeab42dea/bxNKEkprdm5JI1wkjmNAL.png"
            }
        },
        "publishedAt": "2025-12-04T12:50:53.000Z",
        "title": "Arbitrage: Efficient Reasoning via Advantage-Aware Speculation",
        "summary": "Modern Large Language Models achieve impressive reasoning capabilities with long Chain of Thoughts, but they incur substantial computational cost during inference, and this motivates techniques to improve the performance-cost ratio. Among these techniques, Speculative Decoding accelerates inference by employing a fast but inaccurate draft model to autoregressively propose tokens, which are then verified in parallel by a more capable target model. However, due to unnecessary rejections caused by token mismatches in semantically equivalent steps, traditional token-level Speculative Decoding struggles in reasoning tasks. Although recent works have shifted to step-level semantic verification, which improve efficiency by accepting or rejecting entire reasoning steps, existing step-level methods still regenerate many rejected steps with little improvement, wasting valuable target compute. To address this challenge, we propose Arbitrage, a novel step-level speculative generation framework that routes generation dynamically based on the relative advantage between draft and target models. Instead of applying a fixed acceptance threshold, Arbitrage uses a lightweight router trained to predict when the target model is likely to produce a meaningfully better step. This routing approximates an ideal Arbitrage Oracle that always chooses the higher-quality step, achieving near-optimal efficiency-accuracy trade-offs. Across multiple mathematical reasoning benchmarks, Arbitrage consistently surpasses prior step-level Speculative Decoding baselines, reducing inference latency by up to sim2times at matched accuracy.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.05033.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67dc66fe55c24fc4f981a4ab",
            "avatarUrl": "/avatars/7bd900ade802d99db7c562ad6c2f6661.svg",
            "fullname": "Yuezhou Hu",
            "name": "yuezhouhu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "66b1baeff10262fc4fa61961",
            "name": "UCBerkeley",
            "fullname": "University of California, Berkeley",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f425c3a096536aeab42dea/bxNKEkprdm5JI1wkjmNAL.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.06628",
            "authors": [
                {
                    "_id": "6937b61c19d912300c34a377",
                    "name": "Ruicheng Zhang",
                    "hidden": false
                },
                {
                    "_id": "6937b61c19d912300c34a378",
                    "name": "Mingyang Zhang",
                    "hidden": false
                },
                {
                    "_id": "6937b61c19d912300c34a379",
                    "name": "Jun Zhou",
                    "hidden": false
                },
                {
                    "_id": "6937b61c19d912300c34a37a",
                    "name": "Zhangrui Guo",
                    "hidden": false
                },
                {
                    "_id": "6937b61c19d912300c34a37b",
                    "name": "Xiaofan Liu",
                    "hidden": false
                },
                {
                    "_id": "6937b61c19d912300c34a37c",
                    "name": "Zunnan Xu",
                    "hidden": false
                },
                {
                    "_id": "6937b61c19d912300c34a37d",
                    "name": "Zhizhou Zhong",
                    "hidden": false
                },
                {
                    "_id": "6937b61c19d912300c34a37e",
                    "name": "Puxin Yan",
                    "hidden": false
                },
                {
                    "_id": "6937b61c19d912300c34a37f",
                    "name": "Haocheng Luo",
                    "hidden": false
                },
                {
                    "_id": "6937b61c19d912300c34a380",
                    "name": "Xiu Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-07T02:28:06.000Z",
            "submittedOnDailyAt": "2025-12-10T05:46:39.074Z",
            "title": "MIND-V: Hierarchical Video Generation for Long-Horizon Robotic Manipulation with RL-based Physical Alignment",
            "submittedOnDailyBy": {
                "_id": "6481523b3fb124fc9850afed",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6481523b3fb124fc9850afed/XqXS2k42HF8HjeUXc21RX.jpeg",
                "isPro": false,
                "fullname": "Zunnan Xu",
                "user": "kkakkkka",
                "type": "user"
            },
            "summary": "Embodied imitation learning is constrained by the scarcity of diverse, long-horizon robotic manipulation data. Existing video generation models for this domain are limited to synthesizing short clips of simple actions and often rely on manually defined trajectories. To this end, we introduce MIND-V, a hierarchical framework designed to synthesize physically plausible and logically coherent videos of long-horizon robotic manipulation. Inspired by cognitive science, MIND-V bridges high-level reasoning with pixel-level synthesis through three core components: a Semantic Reasoning Hub (SRH) that leverages a pre-trained vision-language model for task planning; a Behavioral Semantic Bridge (BSB) that translates abstract instructions into domain-invariant representations; and a Motor Video Generator (MVG) for conditional video rendering. MIND-V employs Staged Visual Future Rollouts, a test-time optimization strategy to enhance long-horizon robustness. To align the generated videos with physical laws, we introduce a GRPO reinforcement learning post-training phase guided by a novel Physical Foresight Coherence (PFC) reward. PFC leverages the V-JEPA world model to enforce physical plausibility by aligning the predicted and actual dynamic evolutions in the feature space. MIND-V demonstrates state-of-the-art performance in long-horizon robotic manipulation video generation, establishing a scalable and controllable paradigm for embodied data synthesis.",
            "upvotes": 9,
            "discussionId": "6937b61c19d912300c34a381",
            "projectPage": "https://github.com/Richard-Zhang-AI/MIND-V",
            "githubRepo": "https://github.com/Richard-Zhang-AI/MIND-V",
            "githubRepoAddedBy": "user",
            "ai_summary": "MIND-V generates long-horizon robotic manipulation videos by integrating semantic reasoning, domain-invariant representations, and physical plausibility through a hierarchical framework.",
            "ai_keywords": [
                "Semantic Reasoning Hub",
                "Behavioral Semantic Bridge",
                "Motor Video Generator",
                "Staged Visual Future Rollouts",
                "GRPO reinforcement learning",
                "Physical Foresight Coherence",
                "V-JEPA world model"
            ],
            "githubStars": 13,
            "organization": {
                "_id": "628735cbc83a2d6ab8d14a66",
                "name": "Tsinghua",
                "fullname": "Tsinghua University"
            }
        },
        "publishedAt": "2025-12-06T21:28:06.000Z",
        "title": "MIND-V: Hierarchical Video Generation for Long-Horizon Robotic Manipulation with RL-based Physical Alignment",
        "summary": "Embodied imitation learning is constrained by the scarcity of diverse, long-horizon robotic manipulation data. Existing video generation models for this domain are limited to synthesizing short clips of simple actions and often rely on manually defined trajectories. To this end, we introduce MIND-V, a hierarchical framework designed to synthesize physically plausible and logically coherent videos of long-horizon robotic manipulation. Inspired by cognitive science, MIND-V bridges high-level reasoning with pixel-level synthesis through three core components: a Semantic Reasoning Hub (SRH) that leverages a pre-trained vision-language model for task planning; a Behavioral Semantic Bridge (BSB) that translates abstract instructions into domain-invariant representations; and a Motor Video Generator (MVG) for conditional video rendering. MIND-V employs Staged Visual Future Rollouts, a test-time optimization strategy to enhance long-horizon robustness. To align the generated videos with physical laws, we introduce a GRPO reinforcement learning post-training phase guided by a novel Physical Foresight Coherence (PFC) reward. PFC leverages the V-JEPA world model to enforce physical plausibility by aligning the predicted and actual dynamic evolutions in the feature space. MIND-V demonstrates state-of-the-art performance in long-horizon robotic manipulation video generation, establishing a scalable and controllable paradigm for embodied data synthesis.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.06628.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6481523b3fb124fc9850afed",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6481523b3fb124fc9850afed/XqXS2k42HF8HjeUXc21RX.jpeg",
            "fullname": "Zunnan Xu",
            "name": "kkakkkka",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "organization": {
            "_id": "628735cbc83a2d6ab8d14a66",
            "name": "Tsinghua",
            "fullname": "Tsinghua University"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.02231",
            "authors": [
                {
                    "_id": "692fa66626742347f61dab17",
                    "name": "Le Thien Phuc Nguyen",
                    "hidden": false
                },
                {
                    "_id": "692fa66626742347f61dab18",
                    "name": "Zhuoran Yu",
                    "hidden": false
                },
                {
                    "_id": "692fa66626742347f61dab19",
                    "name": "Samuel Low Yu Hang",
                    "hidden": false
                },
                {
                    "_id": "692fa66626742347f61dab1a",
                    "name": "Subin An",
                    "hidden": false
                },
                {
                    "_id": "692fa66626742347f61dab1b",
                    "name": "Jeongik Lee",
                    "hidden": false
                },
                {
                    "_id": "692fa66626742347f61dab1c",
                    "name": "Yohan Ban",
                    "hidden": false
                },
                {
                    "_id": "692fa66626742347f61dab1d",
                    "name": "SeungEun Chung",
                    "hidden": false
                },
                {
                    "_id": "692fa66626742347f61dab1e",
                    "name": "Thanh-Huy Nguyen",
                    "hidden": false
                },
                {
                    "_id": "692fa66626742347f61dab1f",
                    "name": "JuWan Maeng",
                    "hidden": false
                },
                {
                    "_id": "692fa66626742347f61dab20",
                    "name": "Soochahn Lee",
                    "hidden": false
                },
                {
                    "_id": "692fa66626742347f61dab21",
                    "name": "Yong Jae Lee",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-01T21:57:26.000Z",
            "submittedOnDailyAt": "2025-12-10T17:22:04.669Z",
            "title": "See, Hear, and Understand: Benchmarking Audiovisual Human Speech Understanding in Multimodal Large Language Models",
            "submittedOnDailyBy": {
                "_id": "660047c56ab19a1d21e9d764",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/PFPd87Uh5Z3Y5WoCnJqIx.png",
                "isPro": false,
                "fullname": "Le Thien Phuc Nguyen",
                "user": "plnguyen2908",
                "type": "user"
            },
            "summary": "Multimodal large language models (MLLMs) are expected to jointly interpret vision, audio, and language, yet existing video benchmarks rarely assess fine-grained reasoning about human speech. Many tasks remain visually solvable or only coarsely evaluate speech, offering limited insight into whether models can align who speaks, what is said, and when it occurs. We introduce AV-SpeakerBench, a curated benchmark of 3,212 multiple-choice questions focused on speaker-centric audiovisual reasoning in real-world videos. It features: (1) a speaker-centered formulation that treats speakers-not scenes-as the core reasoning unit; (2) fusion-grounded question design embedding audiovisual dependencies into question semantics; and (3) expert-curated annotations ensuring temporal precision and cross-modal validity. Comprehensive evaluations show that the Gemini family consistently outperforms open-source systems, with Gemini 2.5 Pro achieving the best results. Among open models, Qwen3-Omni-30B approaches Gemini 2.0 Flash but remains far behind Gemini 2.5 Pro, primarily due to weaker audiovisual fusion rather than visual perception. We believe AV-SpeakerBench establishes a rigorous foundation for advancing fine-grained audiovisual reasoning in future multimodal systems.",
            "upvotes": 7,
            "discussionId": "692fa66726742347f61dab22",
            "projectPage": "https://plnguyen2908.github.io/AV-SpeakerBench-project-page/",
            "githubRepo": "https://github.com/plnguyen2908/AV-SpeakerBench",
            "githubRepoAddedBy": "user",
            "ai_summary": "AV-SpeakerBench is a benchmark for evaluating speaker-centric audiovisual reasoning in videos, highlighting the importance of audiovisual fusion in multimodal large language models.",
            "ai_keywords": [
                "multimodal large language models",
                "MLMLs",
                "audiovisual reasoning",
                "AV-SpeakerBench",
                "speaker-centric",
                "fusion-grounded",
                "expert-curated",
                "Gemini family",
                "audiovisual fusion",
                "Qwen3-Omni-30B"
            ],
            "githubStars": 1,
            "organization": {
                "_id": "6318959fda3063b19c1c1d9b",
                "name": "Wisconsin",
                "fullname": "University of Wisconsin - Madison",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/644645655004f2cb3aefc452/UqU99v2mCOrNNsD8hYv5Q.png"
            }
        },
        "publishedAt": "2025-12-01T16:57:26.000Z",
        "title": "See, Hear, and Understand: Benchmarking Audiovisual Human Speech Understanding in Multimodal Large Language Models",
        "summary": "Multimodal large language models (MLLMs) are expected to jointly interpret vision, audio, and language, yet existing video benchmarks rarely assess fine-grained reasoning about human speech. Many tasks remain visually solvable or only coarsely evaluate speech, offering limited insight into whether models can align who speaks, what is said, and when it occurs. We introduce AV-SpeakerBench, a curated benchmark of 3,212 multiple-choice questions focused on speaker-centric audiovisual reasoning in real-world videos. It features: (1) a speaker-centered formulation that treats speakers-not scenes-as the core reasoning unit; (2) fusion-grounded question design embedding audiovisual dependencies into question semantics; and (3) expert-curated annotations ensuring temporal precision and cross-modal validity. Comprehensive evaluations show that the Gemini family consistently outperforms open-source systems, with Gemini 2.5 Pro achieving the best results. Among open models, Qwen3-Omni-30B approaches Gemini 2.0 Flash but remains far behind Gemini 2.5 Pro, primarily due to weaker audiovisual fusion rather than visual perception. We believe AV-SpeakerBench establishes a rigorous foundation for advancing fine-grained audiovisual reasoning in future multimodal systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02231.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "660047c56ab19a1d21e9d764",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/PFPd87Uh5Z3Y5WoCnJqIx.png",
            "fullname": "Le Thien Phuc Nguyen",
            "name": "plnguyen2908",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "6318959fda3063b19c1c1d9b",
            "name": "Wisconsin",
            "fullname": "University of Wisconsin - Madison",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/644645655004f2cb3aefc452/UqU99v2mCOrNNsD8hYv5Q.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.07921",
            "authors": [
                {
                    "_id": "6938e671dfc35938ba129fd5",
                    "name": "Zongwei Li",
                    "hidden": false
                },
                {
                    "_id": "6938e671dfc35938ba129fd6",
                    "name": "Zhonghang Li",
                    "hidden": false
                },
                {
                    "_id": "6938e671dfc35938ba129fd7",
                    "name": "Zirui Guo",
                    "hidden": false
                },
                {
                    "_id": "6938e671dfc35938ba129fd8",
                    "name": "Xubin Ren",
                    "hidden": false
                },
                {
                    "_id": "6938e671dfc35938ba129fd9",
                    "name": "Chao Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-08T16:07:13.000Z",
            "submittedOnDailyAt": "2025-12-10T00:48:19.359Z",
            "title": "DeepCode: Open Agentic Coding",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Recent advances in large language models (LLMs) have given rise to powerful coding agents, making it possible for code assistants to evolve into code engineers. However, existing methods still face significant challenges in achieving high-fidelity document-to-codebase synthesis--such as scientific papers to code--primarily due to a fundamental conflict between information overload and the context bottlenecks of LLMs. In this work, we introduce DeepCode, a fully autonomous framework that fundamentally addresses this challenge through principled information-flow management. By treating repository synthesis as a channel optimization problem, DeepCode seamlessly orchestrates four information operations to maximize task-relevant signals under finite context budgets: source compression via blueprint distillation, structured indexing using stateful code memory, conditional knowledge injection via retrieval-augmented generation, and closed-loop error correction. Extensive evaluations on the PaperBench benchmark demonstrate that DeepCode achieves state-of-the-art performance, decisively outperforming leading commercial agents such as Cursor and Claude Code, and crucially, surpassing PhD-level human experts from top institutes on key reproduction metrics. By systematically transforming paper specifications into production-grade implementations comparable to human expert quality, this work establishes new foundations for autonomous scientific reproduction that can accelerate research evaluation and discovery.",
            "upvotes": 6,
            "discussionId": "6938e672dfc35938ba129fda",
            "githubRepo": "https://github.com/HKUDS/DeepCode",
            "githubRepoAddedBy": "user",
            "ai_summary": "DeepCode, a fully autonomous framework, addresses the challenges of document-to-codebase synthesis by optimizing information flow through source compression, structured indexing, knowledge injection, and error correction, achieving state-of-the-art performance and surpassing human experts.",
            "ai_keywords": [
                "large language models",
                "coding agents",
                "document-to-codebase synthesis",
                "information overload",
                "context bottlenecks",
                "DeepCode",
                "channel optimization",
                "blueprint distillation",
                "stateful code memory",
                "retrieval-augmented generation",
                "closed-loop error correction",
                "PaperBench",
                "autonomous scientific reproduction"
            ],
            "githubStars": 11750
        },
        "publishedAt": "2025-12-08T11:07:13.000Z",
        "title": "DeepCode: Open Agentic Coding",
        "summary": "Recent advances in large language models (LLMs) have given rise to powerful coding agents, making it possible for code assistants to evolve into code engineers. However, existing methods still face significant challenges in achieving high-fidelity document-to-codebase synthesis--such as scientific papers to code--primarily due to a fundamental conflict between information overload and the context bottlenecks of LLMs. In this work, we introduce DeepCode, a fully autonomous framework that fundamentally addresses this challenge through principled information-flow management. By treating repository synthesis as a channel optimization problem, DeepCode seamlessly orchestrates four information operations to maximize task-relevant signals under finite context budgets: source compression via blueprint distillation, structured indexing using stateful code memory, conditional knowledge injection via retrieval-augmented generation, and closed-loop error correction. Extensive evaluations on the PaperBench benchmark demonstrate that DeepCode achieves state-of-the-art performance, decisively outperforming leading commercial agents such as Cursor and Claude Code, and crucially, surpassing PhD-level human experts from top institutes on key reproduction metrics. By systematically transforming paper specifications into production-grade implementations comparable to human expert quality, this work establishes new foundations for autonomous scientific reproduction that can accelerate research evaluation and discovery.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.07921.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 181
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.08153",
            "authors": [
                {
                    "_id": "6938de86dfc35938ba129f4b",
                    "name": "Zheng Ding",
                    "hidden": false
                },
                {
                    "_id": "6938de86dfc35938ba129f4c",
                    "name": "Weirui Ye",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-09T01:17:34.000Z",
            "submittedOnDailyAt": "2025-12-10T00:14:54.011Z",
            "title": "TreeGRPO: Tree-Advantage GRPO for Online RL Post-Training of Diffusion Models",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Reinforcement learning (RL) post-training is crucial for aligning generative models with human preferences, but its prohibitive computational cost remains a major barrier to widespread adoption. We introduce TreeGRPO, a novel RL framework that dramatically improves training efficiency by recasting the denoising process as a search tree. From shared initial noise samples, TreeGRPO strategically branches to generate multiple candidate trajectories while efficiently reusing their common prefixes. This tree-structured approach delivers three key advantages: (1) High sample efficiency, achieving better performance under same training samples (2) Fine-grained credit assignment via reward backpropagation that computes step-specific advantages, overcoming the uniform credit assignment limitation of trajectory-based methods, and (3) Amortized computation where multi-child branching enables multiple policy updates per forward pass. Extensive experiments on both diffusion and flow-based models demonstrate that TreeGRPO achieves 2.4times faster training while establishing a superior Pareto frontier in the efficiency-reward trade-off space. Our method consistently outperforms GRPO baselines across multiple benchmarks and reward models, providing a scalable and effective pathway for RL-based visual generative model alignment. The project website is available at treegrpo.github.io.",
            "upvotes": 4,
            "discussionId": "6938de87dfc35938ba129f4d",
            "ai_summary": "TreeGRPO, a novel RL framework, enhances training efficiency for generative models by using a tree-structured denoising process, leading to faster training and better performance.",
            "ai_keywords": [
                "Reinforcement learning",
                "TreeGRPO",
                "denoising process",
                "search tree",
                "candidate trajectories",
                "sample efficiency",
                "fine-grained credit assignment",
                "reward backpropagation",
                "amortized computation",
                "diffusion models",
                "flow-based models",
                "training efficiency",
                "Pareto frontier",
                "efficiency-reward trade-off"
            ]
        },
        "publishedAt": "2025-12-08T20:17:34.000Z",
        "title": "TreeGRPO: Tree-Advantage GRPO for Online RL Post-Training of Diffusion Models",
        "summary": "Reinforcement learning (RL) post-training is crucial for aligning generative models with human preferences, but its prohibitive computational cost remains a major barrier to widespread adoption. We introduce TreeGRPO, a novel RL framework that dramatically improves training efficiency by recasting the denoising process as a search tree. From shared initial noise samples, TreeGRPO strategically branches to generate multiple candidate trajectories while efficiently reusing their common prefixes. This tree-structured approach delivers three key advantages: (1) High sample efficiency, achieving better performance under same training samples (2) Fine-grained credit assignment via reward backpropagation that computes step-specific advantages, overcoming the uniform credit assignment limitation of trajectory-based methods, and (3) Amortized computation where multi-child branching enables multiple policy updates per forward pass. Extensive experiments on both diffusion and flow-based models demonstrate that TreeGRPO achieves 2.4times faster training while establishing a superior Pareto frontier in the efficiency-reward trade-off space. Our method consistently outperforms GRPO baselines across multiple benchmarks and reward models, providing a scalable and effective pathway for RL-based visual generative model alignment. The project website is available at treegrpo.github.io.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.08153.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 181
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.06776",
            "authors": [
                {
                    "_id": "693925a0dfc35938ba12a166",
                    "name": "Yuchuan Tian",
                    "hidden": false
                },
                {
                    "_id": "693925a0dfc35938ba12a167",
                    "name": "Yuchen Liang",
                    "hidden": false
                },
                {
                    "_id": "693925a0dfc35938ba12a168",
                    "name": "Jiacheng Sun",
                    "hidden": false
                },
                {
                    "_id": "693925a0dfc35938ba12a169",
                    "name": "Shuo Zhang",
                    "hidden": false
                },
                {
                    "_id": "693925a0dfc35938ba12a16a",
                    "name": "Guangwen Yang",
                    "hidden": false
                },
                {
                    "_id": "693925a0dfc35938ba12a16b",
                    "name": "Yingte Shu",
                    "hidden": false
                },
                {
                    "_id": "693925a0dfc35938ba12a16c",
                    "name": "Sibo Fang",
                    "hidden": false
                },
                {
                    "_id": "693925a0dfc35938ba12a16d",
                    "name": "Tianyu Guo",
                    "hidden": false
                },
                {
                    "_id": "693925a0dfc35938ba12a16e",
                    "name": "Kai Han",
                    "hidden": false
                },
                {
                    "_id": "693925a0dfc35938ba12a16f",
                    "name": "Chao Xu",
                    "hidden": false
                },
                {
                    "_id": "693925a0dfc35938ba12a170",
                    "name": "Hanting Chen",
                    "hidden": false
                },
                {
                    "_id": "693925a0dfc35938ba12a171",
                    "name": "Xinghao Chen",
                    "hidden": false
                },
                {
                    "_id": "693925a0dfc35938ba12a172",
                    "name": "Yunhe Wang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64ba7a73a01da43118fce871/7Siv6RPU1ej8LtLmucAF7.gif"
            ],
            "publishedAt": "2025-12-07T10:28:21.000Z",
            "submittedOnDailyAt": "2025-12-10T05:20:49.824Z",
            "title": "From Next-Token to Next-Block: A Principled Adaptation Path for Diffusion LLMs",
            "submittedOnDailyBy": {
                "_id": "64ba7a73a01da43118fce871",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ba7a73a01da43118fce871/x2QDw6N8QRZkwwYOLNLYD.png",
                "isPro": false,
                "fullname": "YuchuanTian",
                "user": "yuchuantian",
                "type": "user"
            },
            "summary": "Large language models (LLMs) excel at generation but dominant autoregressive (AR) decoding is inherently sequential, creating a throughput bottleneck. Diffusion Language Models (DLMs)--especially block-wise variants--enable parallel generation and intra-block bidirectional reasoning, yet training large DLMs from scratch is costly and wastes the knowledge in mature AR checkpoints. Prior \"adaptation\" attempts either modify logits or randomly grow attention masks to full-sequence diffusion, or simply transplant AR weights into a block-diffusion recipe, leaving a fundamental mismatch between AR causality and block-wise bidirectionality unaddressed. We reframe adaptation as a intra-paradigm path from AR to Block-Diffusion by viewing AR as Block-Diffusion with blocksize=1. Concretely, we design the pathway of adaptation as follows: we use a context-causal attention mask (causal in context, bidirectional only within the active block), an efficient parallel adaptation procedure, an auxiliary AR loss to maximize data utilization and retain pretrained knowledge, and gradual increment of the generation block size. The recipe integrates cleanly with masked block-diffusion and maintains train-inference consistency. Built on these components, NBDiff-7B (Base and Instruct) could inherit the long-context modeling and reasoning capabilities, and achieve state-of-the-art performance among the 7B-class DLMs, delivering strong gains on general-knowledge, math, and code benchmarks over strong baselines. These results demonstrate that principled AR-to-block-diffusion adaptation is an effective and compute-efficient alternative to training DLMs from scratch. Codes: https://github.com/YuchuanTian/NBDiff.",
            "upvotes": 4,
            "discussionId": "693925a1dfc35938ba12a173",
            "ai_summary": "Adapting autoregressive models to block-wise diffusion enables parallel generation and retains pretrained knowledge, achieving state-of-the-art performance among 7B-class diffusion language models.",
            "ai_keywords": [
                "Large language models",
                "autoregressive decoding",
                "diffusion language models",
                "block-wise variants",
                "parallel generation",
                "intra-block bidirectional reasoning",
                "context-causal attention mask",
                "parallel adaptation procedure",
                "auxiliary AR loss",
                "masked block-diffusion",
                "train-inference consistency",
                "NBDiff-7B"
            ],
            "organization": {
                "_id": "61dcd8e344f59573371b5cb6",
                "name": "PekingUniversity",
                "fullname": "Peking University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
            }
        },
        "publishedAt": "2025-12-07T05:28:21.000Z",
        "title": "From Next-Token to Next-Block: A Principled Adaptation Path for Diffusion LLMs",
        "summary": "Large language models (LLMs) excel at generation but dominant autoregressive (AR) decoding is inherently sequential, creating a throughput bottleneck. Diffusion Language Models (DLMs)--especially block-wise variants--enable parallel generation and intra-block bidirectional reasoning, yet training large DLMs from scratch is costly and wastes the knowledge in mature AR checkpoints. Prior \"adaptation\" attempts either modify logits or randomly grow attention masks to full-sequence diffusion, or simply transplant AR weights into a block-diffusion recipe, leaving a fundamental mismatch between AR causality and block-wise bidirectionality unaddressed. We reframe adaptation as a intra-paradigm path from AR to Block-Diffusion by viewing AR as Block-Diffusion with blocksize=1. Concretely, we design the pathway of adaptation as follows: we use a context-causal attention mask (causal in context, bidirectional only within the active block), an efficient parallel adaptation procedure, an auxiliary AR loss to maximize data utilization and retain pretrained knowledge, and gradual increment of the generation block size. The recipe integrates cleanly with masked block-diffusion and maintains train-inference consistency. Built on these components, NBDiff-7B (Base and Instruct) could inherit the long-context modeling and reasoning capabilities, and achieve state-of-the-art performance among the 7B-class DLMs, delivering strong gains on general-knowledge, math, and code benchmarks over strong baselines. These results demonstrate that principled AR-to-block-diffusion adaptation is an effective and compute-efficient alternative to training DLMs from scratch. Codes: https://github.com/YuchuanTian/NBDiff.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64ba7a73a01da43118fce871/7Siv6RPU1ej8LtLmucAF7.gif"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.06776.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64ba7a73a01da43118fce871",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ba7a73a01da43118fce871/x2QDw6N8QRZkwwYOLNLYD.png",
            "fullname": "YuchuanTian",
            "name": "yuchuantian",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "organization": {
            "_id": "61dcd8e344f59573371b5cb6",
            "name": "PekingUniversity",
            "fullname": "Peking University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.08924",
            "authors": [
                {
                    "_id": "69395847dfc35938ba12a223",
                    "name": "Chuhan Zhang",
                    "hidden": false
                },
                {
                    "_id": "69395847dfc35938ba12a224",
                    "name": "Guillaume Le Moing",
                    "hidden": false
                },
                {
                    "_id": "69395847dfc35938ba12a225",
                    "name": "Skanda Koppula",
                    "hidden": false
                },
                {
                    "_id": "69395847dfc35938ba12a226",
                    "name": "Ignacio Rocco",
                    "hidden": false
                },
                {
                    "_id": "69395847dfc35938ba12a227",
                    "name": "Liliane Momeni",
                    "hidden": false
                },
                {
                    "_id": "69395847dfc35938ba12a228",
                    "name": "Junyu Xie",
                    "hidden": false
                },
                {
                    "_id": "69395847dfc35938ba12a229",
                    "name": "Shuyang Sun",
                    "hidden": false
                },
                {
                    "_id": "69395847dfc35938ba12a22a",
                    "name": "Rahul Sukthankar",
                    "hidden": false
                },
                {
                    "_id": "69395847dfc35938ba12a22b",
                    "name": "Jolle K Barral",
                    "hidden": false
                },
                {
                    "_id": "69395847dfc35938ba12a22c",
                    "name": "Raia Hadsell",
                    "hidden": false
                },
                {
                    "_id": "69395847dfc35938ba12a22d",
                    "name": "Zoubin Ghahramani",
                    "hidden": false
                },
                {
                    "_id": "69395847dfc35938ba12a22e",
                    "name": "Andrew Zisserman",
                    "hidden": false
                },
                {
                    "_id": "69395847dfc35938ba12a22f",
                    "name": "Junlin Zhang",
                    "hidden": false
                },
                {
                    "_id": "69395847dfc35938ba12a230",
                    "name": "Mehdi SM Sajjadi",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/648b79e790eb9e3c53ca8fdf/loZVrCgY4RN-d3JypotdD.gif"
            ],
            "publishedAt": "2025-12-09T18:57:21.000Z",
            "submittedOnDailyAt": "2025-12-10T08:57:50.560Z",
            "title": "Efficiently Reconstructing Dynamic Scenes One D4RT at a Time",
            "submittedOnDailyBy": {
                "_id": "648b79e790eb9e3c53ca8fdf",
                "avatarUrl": "/avatars/6bad66ce7c5c95fd3a53c91a9a29ea15.svg",
                "isPro": true,
                "fullname": "Arthur Xie",
                "user": "Jyxarthur",
                "type": "user"
            },
            "summary": "Understanding and reconstructing the complex geometry and motion of dynamic scenes from video remains a formidable challenge in computer vision. This paper introduces D4RT, a simple yet powerful feedforward model designed to efficiently solve this task. D4RT utilizes a unified transformer architecture to jointly infer depth, spatio-temporal correspondence, and full camera parameters from a single video. Its core innovation is a novel querying mechanism that sidesteps the heavy computation of dense, per-frame decoding and the complexity of managing multiple, task-specific decoders. Our decoding interface allows the model to independently and flexibly probe the 3D position of any point in space and time. The result is a lightweight and highly scalable method that enables remarkably efficient training and inference. We demonstrate that our approach sets a new state of the art, outperforming previous methods across a wide spectrum of 4D reconstruction tasks. We refer to the project webpage for animated results: https://d4rt-paper.github.io/.",
            "upvotes": 3,
            "discussionId": "69395847dfc35938ba12a231",
            "ai_summary": "D4RT, a unified transformer-based model, efficiently reconstructs 4D scenes from videos by querying 3D positions in space-time, outperforming previous methods.",
            "ai_keywords": [
                "feedforward model",
                "unified transformer architecture",
                "depth inference",
                "spatio-temporal correspondence",
                "camera parameters",
                "querying mechanism",
                "dense decoding",
                "task-specific decoders",
                "4D reconstruction",
                "animated results"
            ],
            "organization": {
                "_id": "5e6aca39878b8b2bf9806447",
                "name": "google",
                "fullname": "Google",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"
            }
        },
        "publishedAt": "2025-12-09T13:57:21.000Z",
        "title": "Efficiently Reconstructing Dynamic Scenes One D4RT at a Time",
        "summary": "Understanding and reconstructing the complex geometry and motion of dynamic scenes from video remains a formidable challenge in computer vision. This paper introduces D4RT, a simple yet powerful feedforward model designed to efficiently solve this task. D4RT utilizes a unified transformer architecture to jointly infer depth, spatio-temporal correspondence, and full camera parameters from a single video. Its core innovation is a novel querying mechanism that sidesteps the heavy computation of dense, per-frame decoding and the complexity of managing multiple, task-specific decoders. Our decoding interface allows the model to independently and flexibly probe the 3D position of any point in space and time. The result is a lightweight and highly scalable method that enables remarkably efficient training and inference. We demonstrate that our approach sets a new state of the art, outperforming previous methods across a wide spectrum of 4D reconstruction tasks. We refer to the project webpage for animated results: https://d4rt-paper.github.io/.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/648b79e790eb9e3c53ca8fdf/loZVrCgY4RN-d3JypotdD.gif"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.08924.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "648b79e790eb9e3c53ca8fdf",
            "avatarUrl": "/avatars/6bad66ce7c5c95fd3a53c91a9a29ea15.svg",
            "fullname": "Arthur Xie",
            "name": "Jyxarthur",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "5e6aca39878b8b2bf9806447",
            "name": "google",
            "fullname": "Google",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.08564",
            "authors": [
                {
                    "_id": "6938fc77dfc35938ba12a12c",
                    "user": {
                        "_id": "6574973c05e573071548922c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6574973c05e573071548922c/KT-KAo4aepdpbZguXfKPv.jpeg",
                        "isPro": false,
                        "fullname": "Mahmoud Afifi",
                        "user": "mafifi",
                        "type": "user"
                    },
                    "name": "Mahmoud Afifi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-10T08:39:32.140Z",
                    "hidden": false
                },
                {
                    "_id": "6938fc77dfc35938ba12a12d",
                    "name": "Zhongling Wang",
                    "hidden": false
                },
                {
                    "_id": "6938fc77dfc35938ba12a12e",
                    "name": "Ran Zhang",
                    "hidden": false
                },
                {
                    "_id": "6938fc77dfc35938ba12a12f",
                    "name": "Michael S. Brown",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-09T13:04:08.000Z",
            "submittedOnDailyAt": "2025-12-10T02:29:57.817Z",
            "title": "Modular Neural Image Signal Processing",
            "submittedOnDailyBy": {
                "_id": "6574973c05e573071548922c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6574973c05e573071548922c/KT-KAo4aepdpbZguXfKPv.jpeg",
                "isPro": false,
                "fullname": "Mahmoud Afifi",
                "user": "mafifi",
                "type": "user"
            },
            "summary": "This paper presents a modular neural image signal processing (ISP) framework that processes raw inputs and renders high-quality display-referred images. Unlike prior neural ISP designs, our method introduces a high degree of modularity, providing full control over multiple intermediate stages of the rendering process.~This modular design not only achieves high rendering accuracy but also improves scalability, debuggability, generalization to unseen cameras, and flexibility to match different user-preference styles. To demonstrate the advantages of this design, we built a user-interactive photo-editing tool that leverages our neural ISP to support diverse editing operations and picture styles. The tool is carefully engineered to take advantage of the high-quality rendering of our neural ISP and to enable unlimited post-editable re-rendering. Our method is a fully learning-based framework with variants of different capacities, all of moderate size (ranging from ~0.5 M to ~3.9 M parameters for the entire pipeline), and consistently delivers competitive qualitative and quantitative results across multiple test sets. Watch the supplemental video at: https://youtu.be/ByhQjQSjxVM",
            "upvotes": 3,
            "discussionId": "6938fc78dfc35938ba12a130",
            "githubRepo": "https://github.com/mahmoudnafifi/modular_neural_isp",
            "githubRepoAddedBy": "user",
            "ai_summary": "A modular neural ISP framework provides high rendering accuracy, scalability, and flexibility for diverse photo-editing operations with competitive results.",
            "ai_keywords": [
                "neural ISP",
                "modular design",
                "rendering accuracy",
                "scalability",
                "debuggability",
                "generalization",
                "user-preference styles",
                "photo-editing tool",
                "post-editable re-rendering"
            ],
            "githubStars": 1
        },
        "publishedAt": "2025-12-09T08:04:08.000Z",
        "title": "Modular Neural Image Signal Processing",
        "summary": "This paper presents a modular neural image signal processing (ISP) framework that processes raw inputs and renders high-quality display-referred images. Unlike prior neural ISP designs, our method introduces a high degree of modularity, providing full control over multiple intermediate stages of the rendering process.~This modular design not only achieves high rendering accuracy but also improves scalability, debuggability, generalization to unseen cameras, and flexibility to match different user-preference styles. To demonstrate the advantages of this design, we built a user-interactive photo-editing tool that leverages our neural ISP to support diverse editing operations and picture styles. The tool is carefully engineered to take advantage of the high-quality rendering of our neural ISP and to enable unlimited post-editable re-rendering. Our method is a fully learning-based framework with variants of different capacities, all of moderate size (ranging from ~0.5 M to ~3.9 M parameters for the entire pipeline), and consistently delivers competitive qualitative and quantitative results across multiple test sets. Watch the supplemental video at: https://youtu.be/ByhQjQSjxVM",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.08564.png",
        "numComments": 5,
        "submittedBy": {
            "_id": "6574973c05e573071548922c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6574973c05e573071548922c/KT-KAo4aepdpbZguXfKPv.jpeg",
            "fullname": "Mahmoud Afifi",
            "name": "mafifi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.08186",
            "authors": [
                {
                    "_id": "6938ecf3dfc35938ba12a01e",
                    "name": "Meng Wei",
                    "hidden": false
                },
                {
                    "_id": "6938ecf3dfc35938ba12a01f",
                    "name": "Chenyang Wan",
                    "hidden": false
                },
                {
                    "_id": "6938ecf3dfc35938ba12a020",
                    "name": "Jiaqi Peng",
                    "hidden": false
                },
                {
                    "_id": "6938ecf3dfc35938ba12a021",
                    "name": "Xiqian Yu",
                    "hidden": false
                },
                {
                    "_id": "6938ecf3dfc35938ba12a022",
                    "name": "Yuqiang Yang",
                    "hidden": false
                },
                {
                    "_id": "6938ecf3dfc35938ba12a023",
                    "name": "Delin Feng",
                    "hidden": false
                },
                {
                    "_id": "6938ecf3dfc35938ba12a024",
                    "name": "Wenzhe Cai",
                    "hidden": false
                },
                {
                    "_id": "6938ecf3dfc35938ba12a025",
                    "name": "Chenming Zhu",
                    "hidden": false
                },
                {
                    "_id": "6938ecf3dfc35938ba12a026",
                    "name": "Tai Wang",
                    "hidden": false
                },
                {
                    "_id": "6938ecf3dfc35938ba12a027",
                    "name": "Jiangmiao Pang",
                    "hidden": false
                },
                {
                    "_id": "6938ecf3dfc35938ba12a028",
                    "name": "Xihui Liu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6440fc05603214724eba4766/-paN7Eygd15byTE9X6DIj.mp4"
            ],
            "publishedAt": "2025-12-09T02:29:36.000Z",
            "submittedOnDailyAt": "2025-12-10T02:51:53.938Z",
            "title": "Ground Slow, Move Fast: A Dual-System Foundation Model for Generalizable Vision-and-Language Navigation",
            "submittedOnDailyBy": {
                "_id": "6440fc05603214724eba4766",
                "avatarUrl": "/avatars/1a82a3361c96ba7bfd429dbd3e6f0bad.svg",
                "isPro": false,
                "fullname": "weimeng",
                "user": "mengwei0427",
                "type": "user"
            },
            "summary": "While recent large vision-language models (VLMs) have improved generalization in vision-language navigation (VLN), existing methods typically rely on end-to-end pipelines that map vision-language inputs directly to short-horizon discrete actions. Such designs often produce fragmented motions, incur high latency, and struggle with real-world challenges like dynamic obstacle avoidance. We propose DualVLN, the first dual-system VLN foundation model that synergistically integrates high-level reasoning with low-level action execution. System 2, a VLM-based global planner, \"grounds slowly\" by predicting mid-term waypoint goals via image-grounded reasoning. System 1, a lightweight, multi-modal conditioning Diffusion Transformer policy, \"moves fast\" by leveraging both explicit pixel goals and latent features from System 2 to generate smooth and accurate trajectories. The dual-system design enables robust real-time control and adaptive local decision-making in complex, dynamic environments. By decoupling training, the VLM retains its generalization, while System 1 achieves interpretable and effective local navigation. DualVLN outperforms prior methods across all VLN benchmarks and real-world experiments demonstrate robust long-horizon planning and real-time adaptability in dynamic environments.",
            "upvotes": 3,
            "discussionId": "6938ecf3dfc35938ba12a029",
            "projectPage": "https://internrobotics.github.io/internvla-n1-dualvln.github.io/",
            "githubRepo": "https://github.com/InternRobotics/InternNav",
            "githubRepoAddedBy": "user",
            "ai_summary": "DualVLN integrates high-level reasoning and low-level action execution to improve vision-language navigation in dynamic environments, achieving robust real-time control and long-horizon planning.",
            "ai_keywords": [
                "VLMs",
                "vision-language navigation",
                "VLN",
                "dual-system",
                "global planner",
                "image-grounded reasoning",
                "Diffusion Transformer",
                "pixel goals",
                "latent features",
                "real-time control",
                "adaptive local decision-making",
                "long-horizon planning",
                "real-time adaptability"
            ],
            "githubStars": 455,
            "organization": {
                "_id": "6881c146ff13df8b65153273",
                "name": "InternRobotics",
                "fullname": "Intern Robotics",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65d9f09bbcd15bc5cb255fed/REfA3nEK1_Y-PTfGn_5H1.jpeg"
            }
        },
        "publishedAt": "2025-12-08T21:29:36.000Z",
        "title": "Ground Slow, Move Fast: A Dual-System Foundation Model for Generalizable Vision-and-Language Navigation",
        "summary": "While recent large vision-language models (VLMs) have improved generalization in vision-language navigation (VLN), existing methods typically rely on end-to-end pipelines that map vision-language inputs directly to short-horizon discrete actions. Such designs often produce fragmented motions, incur high latency, and struggle with real-world challenges like dynamic obstacle avoidance. We propose DualVLN, the first dual-system VLN foundation model that synergistically integrates high-level reasoning with low-level action execution. System 2, a VLM-based global planner, \"grounds slowly\" by predicting mid-term waypoint goals via image-grounded reasoning. System 1, a lightweight, multi-modal conditioning Diffusion Transformer policy, \"moves fast\" by leveraging both explicit pixel goals and latent features from System 2 to generate smooth and accurate trajectories. The dual-system design enables robust real-time control and adaptive local decision-making in complex, dynamic environments. By decoupling training, the VLM retains its generalization, while System 1 achieves interpretable and effective local navigation. DualVLN outperforms prior methods across all VLN benchmarks and real-world experiments demonstrate robust long-horizon planning and real-time adaptability in dynamic environments.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6440fc05603214724eba4766/-paN7Eygd15byTE9X6DIj.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.08186.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6440fc05603214724eba4766",
            "avatarUrl": "/avatars/1a82a3361c96ba7bfd429dbd3e6f0bad.svg",
            "fullname": "weimeng",
            "name": "mengwei0427",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "6881c146ff13df8b65153273",
            "name": "InternRobotics",
            "fullname": "Intern Robotics",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65d9f09bbcd15bc5cb255fed/REfA3nEK1_Y-PTfGn_5H1.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.08868",
            "authors": [
                {
                    "_id": "6938e481dfc35938ba129f9c",
                    "name": "Rui Min",
                    "hidden": false
                },
                {
                    "_id": "6938e481dfc35938ba129f9d",
                    "name": "Zile Qiao",
                    "hidden": false
                },
                {
                    "_id": "6938e481dfc35938ba129f9e",
                    "name": "Ze Xu",
                    "hidden": false
                },
                {
                    "_id": "6938e481dfc35938ba129f9f",
                    "name": "Jiawen Zhai",
                    "hidden": false
                },
                {
                    "_id": "6938e481dfc35938ba129fa0",
                    "name": "Wenyu Gao",
                    "hidden": false
                },
                {
                    "_id": "6938e481dfc35938ba129fa1",
                    "user": {
                        "_id": "65e6970d135c27ea806526fe",
                        "avatarUrl": "/avatars/4aced113d9cab055ae06f3945869a280.svg",
                        "isPro": false,
                        "fullname": "Xuanzhong Chen",
                        "user": "chenxz",
                        "type": "user"
                    },
                    "name": "Xuanzhong Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-10T08:56:17.089Z",
                    "hidden": false
                },
                {
                    "_id": "6938e481dfc35938ba129fa2",
                    "name": "Haozhen Sun",
                    "hidden": false
                },
                {
                    "_id": "6938e481dfc35938ba129fa3",
                    "name": "Zhen Zhang",
                    "hidden": false
                },
                {
                    "_id": "6938e481dfc35938ba129fa4",
                    "name": "Xinyu Wang",
                    "hidden": false
                },
                {
                    "_id": "6938e481dfc35938ba129fa5",
                    "name": "Hong Zhou",
                    "hidden": false
                },
                {
                    "_id": "6938e481dfc35938ba129fa6",
                    "name": "Wenbiao Yin",
                    "hidden": false
                },
                {
                    "_id": "6938e481dfc35938ba129fa7",
                    "name": "Xuan Zhou",
                    "hidden": false
                },
                {
                    "_id": "6938e481dfc35938ba129fa8",
                    "name": "Yong Jiang",
                    "hidden": false
                },
                {
                    "_id": "6938e481dfc35938ba129fa9",
                    "name": "Haicheng Liu",
                    "hidden": false
                },
                {
                    "_id": "6938e481dfc35938ba129faa",
                    "name": "Liang Ding",
                    "hidden": false
                },
                {
                    "_id": "6938e481dfc35938ba129fab",
                    "name": "Ling Zou",
                    "hidden": false
                },
                {
                    "_id": "6938e481dfc35938ba129fac",
                    "name": "Yi R.",
                    "hidden": false
                },
                {
                    "_id": "6938e481dfc35938ba129fad",
                    "name": "Fung",
                    "hidden": false
                },
                {
                    "_id": "6938e481dfc35938ba129fae",
                    "name": "Yalong Li",
                    "hidden": false
                },
                {
                    "_id": "6938e481dfc35938ba129faf",
                    "name": "Pengjun Xie",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-09T18:00:26.000Z",
            "submittedOnDailyAt": "2025-12-10T00:40:05.606Z",
            "title": "EcomBench: Towards Holistic Evaluation of Foundation Agents in E-commerce",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Foundation agents have rapidly advanced in their ability to reason and interact with real environments, making the evaluation of their core capabilities increasingly important. While many benchmarks have been developed to assess agent performance, most concentrate on academic settings or artificially designed scenarios while overlooking the challenges that arise in real applications. To address this issue, we focus on a highly practical real-world setting, the e-commerce domain, which involves a large volume of diverse user interactions, dynamic market conditions, and tasks directly tied to real decision-making processes. To this end, we introduce EcomBench, a holistic E-commerce Benchmark designed to evaluate agent performance in realistic e-commerce environments. EcomBench is built from genuine user demands embedded in leading global e-commerce ecosystems and is carefully curated and annotated through human experts to ensure clarity, accuracy, and domain relevance. It covers multiple task categories within e-commerce scenarios and defines three difficulty levels that evaluate agents on key capabilities such as deep information retrieval, multi-step reasoning, and cross-source knowledge integration. By grounding evaluation in real e-commerce contexts, EcomBench provides a rigorous and dynamic testbed for measuring the practical capabilities of agents in modern e-commerce.",
            "upvotes": 2,
            "discussionId": "6938e482dfc35938ba129fb0",
            "ai_summary": "EcomBench is a benchmark that evaluates agent performance in real-world e-commerce environments through deep information retrieval, multi-step reasoning, and cross-source knowledge integration.",
            "ai_keywords": [
                "deep information retrieval",
                "multi-step reasoning",
                "cross-source knowledge integration"
            ],
            "organization": {
                "_id": "67d15cca6e2cf0e062dbfb54",
                "name": "AlibabaTongyiLab",
                "fullname": "TongyiLab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
            }
        },
        "publishedAt": "2025-12-09T13:00:26.000Z",
        "title": "EcomBench: Towards Holistic Evaluation of Foundation Agents in E-commerce",
        "summary": "Foundation agents have rapidly advanced in their ability to reason and interact with real environments, making the evaluation of their core capabilities increasingly important. While many benchmarks have been developed to assess agent performance, most concentrate on academic settings or artificially designed scenarios while overlooking the challenges that arise in real applications. To address this issue, we focus on a highly practical real-world setting, the e-commerce domain, which involves a large volume of diverse user interactions, dynamic market conditions, and tasks directly tied to real decision-making processes. To this end, we introduce EcomBench, a holistic E-commerce Benchmark designed to evaluate agent performance in realistic e-commerce environments. EcomBench is built from genuine user demands embedded in leading global e-commerce ecosystems and is carefully curated and annotated through human experts to ensure clarity, accuracy, and domain relevance. It covers multiple task categories within e-commerce scenarios and defines three difficulty levels that evaluate agents on key capabilities such as deep information retrieval, multi-step reasoning, and cross-source knowledge integration. By grounding evaluation in real e-commerce contexts, EcomBench provides a rigorous and dynamic testbed for measuring the practical capabilities of agents in modern e-commerce.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.08868.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 181
        },
        "organization": {
            "_id": "67d15cca6e2cf0e062dbfb54",
            "name": "AlibabaTongyiLab",
            "fullname": "TongyiLab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.08358",
            "authors": [
                {
                    "_id": "6938e0d4dfc35938ba129f70",
                    "name": "Jiahao Lu",
                    "hidden": false
                },
                {
                    "_id": "6938e0d4dfc35938ba129f71",
                    "user": {
                        "_id": "67bf0cc3dc791d5790d9d6c6",
                        "avatarUrl": "/avatars/c86a991358c3740d99cfa231c9c5a208.svg",
                        "isPro": false,
                        "fullname": "Weitao Xiong",
                        "user": "xwt123",
                        "type": "user"
                    },
                    "name": "Weitao Xiong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-10T15:50:11.073Z",
                    "hidden": false
                },
                {
                    "_id": "6938e0d4dfc35938ba129f72",
                    "name": "Jiacheng Deng",
                    "hidden": false
                },
                {
                    "_id": "6938e0d4dfc35938ba129f73",
                    "name": "Peng Li",
                    "hidden": false
                },
                {
                    "_id": "6938e0d4dfc35938ba129f74",
                    "name": "Tianyu Huang",
                    "hidden": false
                },
                {
                    "_id": "6938e0d4dfc35938ba129f75",
                    "name": "Zhiyang Dou",
                    "hidden": false
                },
                {
                    "_id": "6938e0d4dfc35938ba129f76",
                    "name": "Cheng Lin",
                    "hidden": false
                },
                {
                    "_id": "6938e0d4dfc35938ba129f77",
                    "name": "Sai-Kit Yeung",
                    "hidden": false
                },
                {
                    "_id": "6938e0d4dfc35938ba129f78",
                    "name": "Yuan Liu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/IVOTOtMI7zOMowz45koOL.mp4"
            ],
            "publishedAt": "2025-12-09T08:35:42.000Z",
            "submittedOnDailyAt": "2025-12-10T00:25:01.396Z",
            "title": "TrackingWorld: World-centric Monocular 3D Tracking of Almost All Pixels",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Monocular 3D tracking aims to capture the long-term motion of pixels in 3D space from a single monocular video and has witnessed rapid progress in recent years. However, we argue that the existing monocular 3D tracking methods still fall short in separating the camera motion from foreground dynamic motion and cannot densely track newly emerging dynamic subjects in the videos. To address these two limitations, we propose TrackingWorld, a novel pipeline for dense 3D tracking of almost all pixels within a world-centric 3D coordinate system. First, we introduce a tracking upsampler that efficiently lifts the arbitrary sparse 2D tracks into dense 2D tracks. Then, to generalize the current tracking methods to newly emerging objects, we apply the upsampler to all frames and reduce the redundancy of 2D tracks by eliminating the tracks in overlapped regions. Finally, we present an efficient optimization-based framework to back-project dense 2D tracks into world-centric 3D trajectories by estimating the camera poses and the 3D coordinates of these 2D tracks. Extensive evaluations on both synthetic and real-world datasets demonstrate that our system achieves accurate and dense 3D tracking in a world-centric coordinate frame.",
            "upvotes": 2,
            "discussionId": "6938e0d4dfc35938ba129f79",
            "projectPage": "https://igl-hkust.github.io/TrackingWorld.github.io/",
            "ai_summary": "TrackingWorld provides dense 3D tracking of pixels in a world-centric coordinate system by upsampling sparse 2D tracks and optimizing camera poses and 3D coordinates.",
            "ai_keywords": [
                "monocular 3D tracking",
                "tracking upsampler",
                "dense 3D tracking",
                "world-centric 3D coordinate system",
                "2D tracks",
                "camera poses",
                "3D coordinates",
                "optimization-based framework",
                "synthetic datasets",
                "real-world datasets"
            ]
        },
        "publishedAt": "2025-12-09T03:35:42.000Z",
        "title": "TrackingWorld: World-centric Monocular 3D Tracking of Almost All Pixels",
        "summary": "Monocular 3D tracking aims to capture the long-term motion of pixels in 3D space from a single monocular video and has witnessed rapid progress in recent years. However, we argue that the existing monocular 3D tracking methods still fall short in separating the camera motion from foreground dynamic motion and cannot densely track newly emerging dynamic subjects in the videos. To address these two limitations, we propose TrackingWorld, a novel pipeline for dense 3D tracking of almost all pixels within a world-centric 3D coordinate system. First, we introduce a tracking upsampler that efficiently lifts the arbitrary sparse 2D tracks into dense 2D tracks. Then, to generalize the current tracking methods to newly emerging objects, we apply the upsampler to all frames and reduce the redundancy of 2D tracks by eliminating the tracks in overlapped regions. Finally, we present an efficient optimization-based framework to back-project dense 2D tracks into world-centric 3D trajectories by estimating the camera poses and the 3D coordinates of these 2D tracks. Extensive evaluations on both synthetic and real-world datasets demonstrate that our system achieves accurate and dense 3D tracking in a world-centric coordinate frame.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/IVOTOtMI7zOMowz45koOL.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.08358.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 181
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.07197",
            "authors": [
                {
                    "_id": "69394df3dfc35938ba12a20a",
                    "name": "Seokhyun Youn",
                    "hidden": false
                },
                {
                    "_id": "69394df3dfc35938ba12a20b",
                    "name": "Soohyun Lee",
                    "hidden": false
                },
                {
                    "_id": "69394df3dfc35938ba12a20c",
                    "name": "Geonho Kim",
                    "hidden": false
                },
                {
                    "_id": "69394df3dfc35938ba12a20d",
                    "user": {
                        "_id": "6692506aab6879eceaafb083",
                        "avatarUrl": "/avatars/f7f735faf11f6b4036b164aca90345e6.svg",
                        "isPro": false,
                        "fullname": "WEEYOUNG KWON",
                        "user": "klavna",
                        "type": "user"
                    },
                    "name": "Weeyoung Kwon",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-10T15:48:29.495Z",
                    "hidden": false
                },
                {
                    "_id": "69394df3dfc35938ba12a20e",
                    "user": {
                        "_id": "649bf062f8183583a4f1dec6",
                        "avatarUrl": "/avatars/4317ffc9c2af6739cd27fd80aa1285b3.svg",
                        "isPro": false,
                        "fullname": "Sung Ho Bae",
                        "user": "shbae84",
                        "type": "user"
                    },
                    "name": "Sung-Ho Bae",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-10T15:48:16.797Z",
                    "hidden": false
                },
                {
                    "_id": "69394df3dfc35938ba12a20f",
                    "user": {
                        "_id": "6576b99d58ce19fa1e33eb1d",
                        "avatarUrl": "/avatars/b533e776aa3d95d722b46ef0cd381acd.svg",
                        "isPro": false,
                        "fullname": "Jihyong Oh",
                        "user": "ozbro",
                        "type": "user"
                    },
                    "name": "Jihyong Oh",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-10T15:48:11.162Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-08T06:15:59.000Z",
            "submittedOnDailyAt": "2025-12-10T08:10:33.038Z",
            "title": "SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting",
            "submittedOnDailyBy": {
                "_id": "6576b99d58ce19fa1e33eb1d",
                "avatarUrl": "/avatars/b533e776aa3d95d722b46ef0cd381acd.svg",
                "isPro": false,
                "fullname": "Jihyong Oh",
                "user": "ozbro",
                "type": "user"
            },
            "summary": "3D Gaussian Splatting (3DGS) has emerged as a powerful explicit representation enabling real-time, high-fidelity 3D reconstruction and novel view synthesis. However, its practical use is hindered by the massive memory and computational demands required to store and render millions of Gaussians. These challenges become even more severe in 4D dynamic scenes. To address these issues, the field of Efficient Gaussian Splatting has rapidly evolved, proposing methods that reduce redundancy while preserving reconstruction quality. This survey provides the first unified overview of efficient 3D and 4D Gaussian Splatting techniques. For both 3D and 4D settings, we systematically categorize existing methods into two major directions, Parameter Compression and Restructuring Compression, and comprehensively summarize the core ideas and methodological trends within each category. We further cover widely used datasets, evaluation metrics, and representative benchmark comparisons. Finally, we discuss current limitations and outline promising research directions toward scalable, compact, and real-time Gaussian Splatting for both static and dynamic 3D scene representation.",
            "upvotes": 2,
            "discussionId": "69394df3dfc35938ba12a210",
            "projectPage": "https://cmlab-korea.github.io/Awesome-Efficient-GS/",
            "githubRepo": "https://github.com/CMLab-Korea/Awesome-Efficient-GS",
            "githubRepoAddedBy": "user",
            "ai_summary": "This survey reviews efficient techniques for 3D and 4D Gaussian Splatting, focusing on parameter and restructuring compression methods to improve memory and computational efficiency while maintaining reconstruction quality.",
            "ai_keywords": [
                "Gaussian Splatting",
                "real-time",
                "high-fidelity",
                "novel view synthesis",
                "memory demands",
                "computational demands",
                "dynamic scenes",
                "Efficient Gaussian Splatting",
                "Parameter Compression",
                "Restructuring Compression",
                "datasets",
                "evaluation metrics",
                "benchmark comparisons"
            ],
            "githubStars": 6
        },
        "publishedAt": "2025-12-08T01:15:59.000Z",
        "title": "SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting",
        "summary": "3D Gaussian Splatting (3DGS) has emerged as a powerful explicit representation enabling real-time, high-fidelity 3D reconstruction and novel view synthesis. However, its practical use is hindered by the massive memory and computational demands required to store and render millions of Gaussians. These challenges become even more severe in 4D dynamic scenes. To address these issues, the field of Efficient Gaussian Splatting has rapidly evolved, proposing methods that reduce redundancy while preserving reconstruction quality. This survey provides the first unified overview of efficient 3D and 4D Gaussian Splatting techniques. For both 3D and 4D settings, we systematically categorize existing methods into two major directions, Parameter Compression and Restructuring Compression, and comprehensively summarize the core ideas and methodological trends within each category. We further cover widely used datasets, evaluation metrics, and representative benchmark comparisons. Finally, we discuss current limitations and outline promising research directions toward scalable, compact, and real-time Gaussian Splatting for both static and dynamic 3D scene representation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.07197.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6576b99d58ce19fa1e33eb1d",
            "avatarUrl": "/avatars/b533e776aa3d95d722b46ef0cd381acd.svg",
            "fullname": "Jihyong Oh",
            "name": "ozbro",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.06531",
            "authors": [
                {
                    "_id": "693963d9dfc35938ba12a272",
                    "user": {
                        "_id": "661d949b5421044655cd765d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/661d949b5421044655cd765d/fxCPXm_KXC8Y-JPQxJjqb.webp",
                        "isPro": false,
                        "fullname": "Sayan Das",
                        "user": "Necromancer0912",
                        "type": "user"
                    },
                    "name": "Sayan Das",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-10T19:41:45.164Z",
                    "hidden": false
                },
                {
                    "_id": "693963d9dfc35938ba12a273",
                    "user": {
                        "_id": "673377c611e26ee8dfe52b03",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673377c611e26ee8dfe52b03/KRCd_6PzJ5NqQVXLBgZgt.jpeg",
                        "isPro": false,
                        "fullname": "Arghadip Biswas",
                        "user": "arghadip2002",
                        "type": "user"
                    },
                    "name": "Arghadip Biswas",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-10T13:07:26.209Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/673377c611e26ee8dfe52b03/HPXNXHnjr7gsZ6pIwwF0E.jpeg",
                "https://cdn-uploads.huggingface.co/production/uploads/673377c611e26ee8dfe52b03/Pp7q-yjNIoV8DhA7VLye1.jpeg"
            ],
            "publishedAt": "2025-12-06T18:49:57.000Z",
            "submittedOnDailyAt": "2025-12-10T10:56:52.253Z",
            "title": "Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images",
            "submittedOnDailyBy": {
                "_id": "673377c611e26ee8dfe52b03",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673377c611e26ee8dfe52b03/KRCd_6PzJ5NqQVXLBgZgt.jpeg",
                "isPro": false,
                "fullname": "Arghadip Biswas",
                "user": "arghadip2002",
                "type": "user"
            },
            "summary": "Brain tumors pose a significant threat to human life, therefore it is very much necessary to detect them accurately in the early stages for better diagnosis and treatment. Brain tumors can be detected by the radiologist manually from the MRI scan images of the patients. However, the incidence of brain tumors has risen amongst children and adolescents in recent years, resulting in a substantial volume of data, as a result, it is time-consuming and difficult to detect manually. With the emergence of Artificial intelligence in the modern world and its vast application in the medical field, we can make an approach to the CAD (Computer Aided Diagnosis) system for the early detection of Brain tumors automatically. All the existing models for this task are not completely generalized and perform poorly on the validation data. So, we have proposed two novel Deep Learning Architectures - (a) SAETCN (Self-Attention Enhancement Tumor Classification Network) for the classification of different kinds of brain tumors. We have achieved an accuracy of 99.38% on the validation dataset making it one of the few Novel Deep learning-based architecture that is capable of detecting brain tumors accurately. We have trained the model on the dataset, which contains images of 3 types of tumors (glioma, meningioma, and pituitary tumors) and non-tumor cases. and (b) SAS-Net (Self-Attentive Segmentation Network) for the accurate segmentation of brain tumors. We have achieved an overall pixel accuracy of 99.23%.",
            "upvotes": 2,
            "discussionId": "693963d9dfc35938ba12a274",
            "githubRepo": "https://github.com/arghadip2002/SAETCN-and-SASNET-Architectures",
            "githubRepoAddedBy": "user",
            "ai_summary": "Two novel deep learning architectures, SAETCN and SAS-Net, achieve high accuracy in classifying and segmenting brain tumors from MRI scans.",
            "ai_keywords": [
                "Self-Attention Enhancement Tumor Classification Network",
                "SAETCN",
                "Self-Attentive Segmentation Network",
                "SAS-Net",
                "brain tumor classification",
                "brain tumor segmentation",
                "MRI scans",
                "glioma",
                "meningioma",
                "pituitary tumors"
            ],
            "githubStars": 0
        },
        "publishedAt": "2025-12-06T13:49:57.000Z",
        "title": "Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images",
        "summary": "Brain tumors pose a significant threat to human life, therefore it is very much necessary to detect them accurately in the early stages for better diagnosis and treatment. Brain tumors can be detected by the radiologist manually from the MRI scan images of the patients. However, the incidence of brain tumors has risen amongst children and adolescents in recent years, resulting in a substantial volume of data, as a result, it is time-consuming and difficult to detect manually. With the emergence of Artificial intelligence in the modern world and its vast application in the medical field, we can make an approach to the CAD (Computer Aided Diagnosis) system for the early detection of Brain tumors automatically. All the existing models for this task are not completely generalized and perform poorly on the validation data. So, we have proposed two novel Deep Learning Architectures - (a) SAETCN (Self-Attention Enhancement Tumor Classification Network) for the classification of different kinds of brain tumors. We have achieved an accuracy of 99.38% on the validation dataset making it one of the few Novel Deep learning-based architecture that is capable of detecting brain tumors accurately. We have trained the model on the dataset, which contains images of 3 types of tumors (glioma, meningioma, and pituitary tumors) and non-tumor cases. and (b) SAS-Net (Self-Attentive Segmentation Network) for the accurate segmentation of brain tumors. We have achieved an overall pixel accuracy of 99.23%.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/673377c611e26ee8dfe52b03/HPXNXHnjr7gsZ6pIwwF0E.jpeg",
            "https://cdn-uploads.huggingface.co/production/uploads/673377c611e26ee8dfe52b03/Pp7q-yjNIoV8DhA7VLye1.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.06531.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "673377c611e26ee8dfe52b03",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673377c611e26ee8dfe52b03/KRCd_6PzJ5NqQVXLBgZgt.jpeg",
            "fullname": "Arghadip Biswas",
            "name": "arghadip2002",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.05325",
            "authors": [
                {
                    "_id": "6939403adfc35938ba12a1f1",
                    "user": {
                        "_id": "6425e8f7ba51f8a2136770d7",
                        "avatarUrl": "/avatars/86c96f061e574c6448d9526c52abd15a.svg",
                        "isPro": false,
                        "fullname": "Omer Faruk Akgul",
                        "user": "farukakgul",
                        "type": "user"
                    },
                    "name": "mer Faruk Akgl",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-10T15:48:51.928Z",
                    "hidden": false
                },
                {
                    "_id": "6939403adfc35938ba12a1f2",
                    "name": "Yusuf Hakan Kalayc",
                    "hidden": false
                },
                {
                    "_id": "6939403adfc35938ba12a1f3",
                    "name": "Rajgopal Kannan",
                    "hidden": false
                },
                {
                    "_id": "6939403adfc35938ba12a1f4",
                    "user": {
                        "_id": "644bf65522d211df6444a7f4",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644bf65522d211df6444a7f4/k_ddZdQDg2fzhwjI1EXyx.jpeg",
                        "isPro": false,
                        "fullname": "Willie Neiswanger",
                        "user": "willieneis",
                        "type": "user"
                    },
                    "name": "Willie Neiswanger",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-10T15:49:11.311Z",
                    "hidden": false
                },
                {
                    "_id": "6939403adfc35938ba12a1f5",
                    "name": "Viktor Prasanna",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6425e8f7ba51f8a2136770d7/mUkZSibMBefIYyl9lxPdE.png"
            ],
            "publishedAt": "2025-12-05T00:04:42.000Z",
            "submittedOnDailyAt": "2025-12-10T07:22:31.745Z",
            "title": "LYNX: Learning Dynamic Exits for Confidence-Controlled Reasoning",
            "submittedOnDailyBy": {
                "_id": "6425e8f7ba51f8a2136770d7",
                "avatarUrl": "/avatars/86c96f061e574c6448d9526c52abd15a.svg",
                "isPro": false,
                "fullname": "Omer Faruk Akgul",
                "user": "farukakgul",
                "type": "user"
            },
            "summary": "Large reasoning models achieve strong performance on complex tasks by generating extended chains of thought, but they often \"overthink\": continuing to reason long after they have enough information to answer correctly. This wastes inference-time compute and can hurt accuracy. Existing attempts to stop early either manipulate decoding with extra sampling and heuristics, rely on auxiliary verifier models, or operate only as post-hoc analysis pipelines without formal guarantees. We introduce LYNX, an online early-exit mechanism that turns a model's own hidden-state awareness into confidence-controlled stopping decisions. LYNX attaches exit decisions to naturally occurring reasoning cues (e.g., \"hmm\", \"wait\") during generation, trains a lightweight probe on hidden states at those cue tokens using supervision from forced exits, and wraps the resulting scores in split conformal prediction to obtain distribution-free control over premature exits. Crucially, we train and calibrate this probe once on a generic mathematical corpus and reuse it unchanged across benchmarks, decoding temperatures, and even non-mathematical tasks. Across three model families spanning 1.5B to 32B parameters, a single mathematically trained probe per base model yields strong accuracy--efficiency tradeoffs. On GSM8K, LYNX matches or improves baseline accuracy while reducing tokens by 40--65\\%; on MATH-500 it improves accuracy by up to 12 points with roughly 35--60\\% fewer tokens; on AIME 2024 it recovers baseline accuracy with more than 50\\% token savings; and on CommonsenseQA, a non-math benchmark, it transfers zero-shot with modest accuracy gains and up to 70\\% fewer tokens. Compared to state-of-the-art early-exit methods, LYNX offers competitive or superior Pareto frontiers while remaining fully online, requiring no proxy models at inference, and providing explicit, user-tunable confidence guarantees.",
            "upvotes": 2,
            "discussionId": "6939403adfc35938ba12a1f6",
            "githubRepo": "https://github.com/farukakgul/LYNX",
            "githubRepoAddedBy": "user",
            "ai_summary": "LYNX is an early-exit mechanism that uses hidden-state awareness and confidence-controlled stopping decisions to improve efficiency and accuracy in large reasoning models.",
            "ai_keywords": [
                "hidden-state awareness",
                "confidence-controlled stopping decisions",
                "early-exit mechanism",
                "reasoning cues",
                "lightweight probe",
                "split conformal prediction",
                "distribution-free control",
                "Pareto frontiers"
            ],
            "githubStars": 1,
            "organization": {
                "_id": "63cc6a2ff488db9bb3ca61f3",
                "name": "USC",
                "fullname": "University of Southern California"
            }
        },
        "publishedAt": "2025-12-04T19:04:42.000Z",
        "title": "LYNX: Learning Dynamic Exits for Confidence-Controlled Reasoning",
        "summary": "Large reasoning models achieve strong performance on complex tasks by generating extended chains of thought, but they often \"overthink\": continuing to reason long after they have enough information to answer correctly. This wastes inference-time compute and can hurt accuracy. Existing attempts to stop early either manipulate decoding with extra sampling and heuristics, rely on auxiliary verifier models, or operate only as post-hoc analysis pipelines without formal guarantees. We introduce LYNX, an online early-exit mechanism that turns a model's own hidden-state awareness into confidence-controlled stopping decisions. LYNX attaches exit decisions to naturally occurring reasoning cues (e.g., \"hmm\", \"wait\") during generation, trains a lightweight probe on hidden states at those cue tokens using supervision from forced exits, and wraps the resulting scores in split conformal prediction to obtain distribution-free control over premature exits. Crucially, we train and calibrate this probe once on a generic mathematical corpus and reuse it unchanged across benchmarks, decoding temperatures, and even non-mathematical tasks. Across three model families spanning 1.5B to 32B parameters, a single mathematically trained probe per base model yields strong accuracy--efficiency tradeoffs. On GSM8K, LYNX matches or improves baseline accuracy while reducing tokens by 40--65\\%; on MATH-500 it improves accuracy by up to 12 points with roughly 35--60\\% fewer tokens; on AIME 2024 it recovers baseline accuracy with more than 50\\% token savings; and on CommonsenseQA, a non-math benchmark, it transfers zero-shot with modest accuracy gains and up to 70\\% fewer tokens. Compared to state-of-the-art early-exit methods, LYNX offers competitive or superior Pareto frontiers while remaining fully online, requiring no proxy models at inference, and providing explicit, user-tunable confidence guarantees.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6425e8f7ba51f8a2136770d7/mUkZSibMBefIYyl9lxPdE.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.05325.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6425e8f7ba51f8a2136770d7",
            "avatarUrl": "/avatars/86c96f061e574c6448d9526c52abd15a.svg",
            "fullname": "Omer Faruk Akgul",
            "name": "farukakgul",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "63cc6a2ff488db9bb3ca61f3",
            "name": "USC",
            "fullname": "University of Southern California"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.08406",
            "authors": [
                {
                    "_id": "69395943dfc35938ba12a233",
                    "user": {
                        "_id": "64465292e0dbfe6c21a8af09",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64465292e0dbfe6c21a8af09/89M79YxG8tJEZz1xH23dI.png",
                        "isPro": false,
                        "fullname": "Mingqi Gao",
                        "user": "gaomingqi",
                        "type": "user"
                    },
                    "name": "Mingqi Gao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-10T15:44:26.374Z",
                    "hidden": false
                },
                {
                    "_id": "69395943dfc35938ba12a234",
                    "name": "Yunqi Miao",
                    "hidden": false
                },
                {
                    "_id": "69395943dfc35938ba12a235",
                    "name": "Jungong Han",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64465292e0dbfe6c21a8af09/VZiYVy9tgFWGPbVFnfIZA.gif",
                "https://cdn-uploads.huggingface.co/production/uploads/64465292e0dbfe6c21a8af09/J9FGUxeWSQN5tsASFuBNY.gif"
            ],
            "publishedAt": "2025-12-09T09:37:31.000Z",
            "submittedOnDailyAt": "2025-12-10T11:48:14.193Z",
            "title": "SAM-Body4D: Training-Free 4D Human Body Mesh Recovery from Videos",
            "submittedOnDailyBy": {
                "_id": "64465292e0dbfe6c21a8af09",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64465292e0dbfe6c21a8af09/89M79YxG8tJEZz1xH23dI.png",
                "isPro": false,
                "fullname": "Mingqi Gao",
                "user": "gaomingqi",
                "type": "user"
            },
            "summary": "Human Mesh Recovery (HMR) aims to reconstruct 3D human pose and shape from 2D observations and is fundamental to human-centric understanding in real-world scenarios. While recent image-based HMR methods such as SAM 3D Body achieve strong robustness on in-the-wild images, they rely on per-frame inference when applied to videos, leading to temporal inconsistency and degraded performance under occlusions. We address these issues without extra training by leveraging the inherent human continuity in videos. We propose SAM-Body4D, a training-free framework for temporally consistent and occlusion-robust HMR from videos. We first generate identity-consistent masklets using a promptable video segmentation model, then refine them with an Occlusion-Aware module to recover missing regions. The refined masklets guide SAM 3D Body to produce consistent full-body mesh trajectories, while a padding-based parallel strategy enables efficient multi-human inference. Experimental results demonstrate that SAM-Body4D achieves improved temporal stability and robustness in challenging in-the-wild videos, without any retraining. Our code and demo are available at: https://github.com/gaomingqi/sam-body4d.",
            "upvotes": 1,
            "discussionId": "69395943dfc35938ba12a236",
            "githubRepo": "https://github.com/gaomingqi/sam-body4d",
            "githubRepoAddedBy": "user",
            "ai_summary": "SAM-Body4D is a training-free framework that enhances 3D human mesh recovery from videos by ensuring temporal consistency and robustness to occlusions through masklet generation and refinement.",
            "ai_keywords": [
                "Human Mesh Recovery",
                "HMR",
                "SAM 3D Body",
                "SAM-Body4D",
                "video segmentation",
                "masklets",
                "Occlusion-Aware module",
                "full-body mesh trajectories",
                "parallel strategy",
                "multi-human inference"
            ],
            "githubStars": 15
        },
        "publishedAt": "2025-12-09T04:37:31.000Z",
        "title": "SAM-Body4D: Training-Free 4D Human Body Mesh Recovery from Videos",
        "summary": "Human Mesh Recovery (HMR) aims to reconstruct 3D human pose and shape from 2D observations and is fundamental to human-centric understanding in real-world scenarios. While recent image-based HMR methods such as SAM 3D Body achieve strong robustness on in-the-wild images, they rely on per-frame inference when applied to videos, leading to temporal inconsistency and degraded performance under occlusions. We address these issues without extra training by leveraging the inherent human continuity in videos. We propose SAM-Body4D, a training-free framework for temporally consistent and occlusion-robust HMR from videos. We first generate identity-consistent masklets using a promptable video segmentation model, then refine them with an Occlusion-Aware module to recover missing regions. The refined masklets guide SAM 3D Body to produce consistent full-body mesh trajectories, while a padding-based parallel strategy enables efficient multi-human inference. Experimental results demonstrate that SAM-Body4D achieves improved temporal stability and robustness in challenging in-the-wild videos, without any retraining. Our code and demo are available at: https://github.com/gaomingqi/sam-body4d.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64465292e0dbfe6c21a8af09/VZiYVy9tgFWGPbVFnfIZA.gif",
            "https://cdn-uploads.huggingface.co/production/uploads/64465292e0dbfe6c21a8af09/J9FGUxeWSQN5tsASFuBNY.gif"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.08406.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "64465292e0dbfe6c21a8af09",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64465292e0dbfe6c21a8af09/89M79YxG8tJEZz1xH23dI.png",
            "fullname": "Mingqi Gao",
            "name": "gaomingqi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.04763",
            "authors": [
                {
                    "_id": "693827e119d912300c34a5c0",
                    "user": {
                        "_id": "63f62ee3b29015adc33aafa0",
                        "avatarUrl": "/avatars/b7d7fe1b65333fb698402ed065fc5d13.svg",
                        "isPro": false,
                        "fullname": "Massimo Bini",
                        "user": "mwbini",
                        "type": "user"
                    },
                    "name": "Massimo Bini",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-10T19:41:43.083Z",
                    "hidden": false
                },
                {
                    "_id": "693827e119d912300c34a5c1",
                    "name": "Ondrej Bohdal",
                    "hidden": false
                },
                {
                    "_id": "693827e119d912300c34a5c2",
                    "name": "Umberto Michieli",
                    "hidden": false
                },
                {
                    "_id": "693827e119d912300c34a5c3",
                    "name": "Zeynep Akata",
                    "hidden": false
                },
                {
                    "_id": "693827e119d912300c34a5c4",
                    "name": "Mete Ozay",
                    "hidden": false
                },
                {
                    "_id": "693827e119d912300c34a5c5",
                    "name": "Taha Ceritli",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-04T12:56:30.000Z",
            "submittedOnDailyAt": "2025-12-10T14:47:02.075Z",
            "title": "MemLoRA: Distilling Expert Adapters for On-Device Memory Systems",
            "submittedOnDailyBy": {
                "_id": "63f62ee3b29015adc33aafa0",
                "avatarUrl": "/avatars/b7d7fe1b65333fb698402ed065fc5d13.svg",
                "isPro": false,
                "fullname": "Massimo Bini",
                "user": "mwbini",
                "type": "user"
            },
            "summary": "Memory-augmented Large Language Models (LLMs) have demonstrated remarkable consistency during prolonged dialogues by storing relevant memories and incorporating them as context. Such memory-based personalization is also key in on-device settings that allow users to keep their conversations and data private. However, memory-augmented systems typically rely on LLMs that are too costly for local on-device deployment. Even though Small Language Models (SLMs) are more suitable for on-device inference than LLMs, they cannot achieve sufficient performance. Additionally, these LLM-based systems lack native visual capabilities, limiting their applicability in multimodal contexts. In this paper, we introduce (i) MemLoRA, a novel memory system that enables local deployment by equipping SLMs with specialized memory adapters, and (ii) its vision extension MemLoRA-V, which integrates small Vision-Language Models (SVLMs) to memory systems, enabling native visual understanding. Following knowledge distillation principles, each adapter is trained separately for specific memory operationsx2013knowledge extraction, memory update, and memory-augmented generation. Equipped with memory adapters, small models enable accurate on-device memory operations without cloud dependency. On text-only operations, MemLoRA outperforms 10times larger baseline models (e.g., Gemma2-27B) and achieves performance comparable to 60times larger models (e.g., GPT-OSS-120B) on the LoCoMo benchmark. To evaluate visual understanding operations instead, we extend LoCoMo with challenging Visual Question Answering tasks that require direct visual reasoning. On this, our VLM-integrated MemLoRA-V shows massive improvements over caption-based approaches (81.3 vs. 23.7 accuracy) while keeping strong performance in text-based tasks, demonstrating the efficacy of our method in multimodal contexts.",
            "upvotes": 1,
            "discussionId": "693827e119d912300c34a5c6",
            "ai_summary": "MemLoRA and MemLoRA-V enhance small language and vision-language models with memory adapters, enabling efficient local deployment, improved performance, and native visual understanding in multimodal contexts.",
            "ai_keywords": [
                "Memory-augmented Large Language Models (LLMs)",
                "Small Language Models (SLMs)",
                "MemLoRA",
                "MemLoRA-V",
                "Small Vision-Language Models (SVLMs)",
                "knowledge distillation",
                "knowledge extraction",
                "memory update",
                "memory-augmented generation",
                "visual question answering",
                "multimodal contexts"
            ]
        },
        "publishedAt": "2025-12-04T07:56:30.000Z",
        "title": "MemLoRA: Distilling Expert Adapters for On-Device Memory Systems",
        "summary": "Memory-augmented Large Language Models (LLMs) have demonstrated remarkable consistency during prolonged dialogues by storing relevant memories and incorporating them as context. Such memory-based personalization is also key in on-device settings that allow users to keep their conversations and data private. However, memory-augmented systems typically rely on LLMs that are too costly for local on-device deployment. Even though Small Language Models (SLMs) are more suitable for on-device inference than LLMs, they cannot achieve sufficient performance. Additionally, these LLM-based systems lack native visual capabilities, limiting their applicability in multimodal contexts. In this paper, we introduce (i) MemLoRA, a novel memory system that enables local deployment by equipping SLMs with specialized memory adapters, and (ii) its vision extension MemLoRA-V, which integrates small Vision-Language Models (SVLMs) to memory systems, enabling native visual understanding. Following knowledge distillation principles, each adapter is trained separately for specific memory operationsx2013knowledge extraction, memory update, and memory-augmented generation. Equipped with memory adapters, small models enable accurate on-device memory operations without cloud dependency. On text-only operations, MemLoRA outperforms 10times larger baseline models (e.g., Gemma2-27B) and achieves performance comparable to 60times larger models (e.g., GPT-OSS-120B) on the LoCoMo benchmark. To evaluate visual understanding operations instead, we extend LoCoMo with challenging Visual Question Answering tasks that require direct visual reasoning. On this, our VLM-integrated MemLoRA-V shows massive improvements over caption-based approaches (81.3 vs. 23.7 accuracy) while keeping strong performance in text-based tasks, demonstrating the efficacy of our method in multimodal contexts.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04763.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63f62ee3b29015adc33aafa0",
            "avatarUrl": "/avatars/b7d7fe1b65333fb698402ed065fc5d13.svg",
            "fullname": "Massimo Bini",
            "name": "mwbini",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.04434",
            "authors": [
                {
                    "_id": "6937deda19d912300c34a4d0",
                    "user": {
                        "_id": "665755ba4d33c039d11bc73d",
                        "avatarUrl": "/avatars/e48205e66a7ab81328eabe6ece93c7ff.svg",
                        "isPro": false,
                        "fullname": "Ali Rabeh",
                        "user": "arabeh",
                        "type": "user"
                    },
                    "name": "Ali Rabeh",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-09T09:21:10.610Z",
                    "hidden": false
                },
                {
                    "_id": "6937deda19d912300c34a4d1",
                    "name": "Suresh Murugaiyan",
                    "hidden": false
                },
                {
                    "_id": "6937deda19d912300c34a4d2",
                    "user": {
                        "_id": "68d5581236e2de9610c0a428",
                        "avatarUrl": "/avatars/d4b0132d9d8cd7f02fc61f4bb3644204.svg",
                        "isPro": false,
                        "fullname": "Adarsh Krishnamurthy",
                        "user": "k-adarsh",
                        "type": "user"
                    },
                    "name": "Adarsh Krishnamurthy",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-10T13:23:36.769Z",
                    "hidden": false
                },
                {
                    "_id": "6937deda19d912300c34a4d3",
                    "name": "Baskar Ganapathysubramanian",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/665755ba4d33c039d11bc73d/fL_VnMUmjVJ3Kxqxrm9iW.gif"
            ],
            "publishedAt": "2025-12-04T04:00:22.000Z",
            "submittedOnDailyAt": "2025-12-10T05:27:30.768Z",
            "title": "Predicting Time-Dependent Flow Over Complex Geometries Using Operator Networks",
            "submittedOnDailyBy": {
                "_id": "665755ba4d33c039d11bc73d",
                "avatarUrl": "/avatars/e48205e66a7ab81328eabe6ece93c7ff.svg",
                "isPro": false,
                "fullname": "Ali Rabeh",
                "user": "arabeh",
                "type": "user"
            },
            "summary": "Fast, geometry-generalizing surrogates for unsteady flow remain challenging. We present a time-dependent, geometry-aware Deep Operator Network that predicts velocity fields for moderate-Re flows around parametric and non-parametric shapes. The model encodes geometry via a signed distance field (SDF) trunk and flow history via a CNN branch, trained on 841 high-fidelity simulations. On held-out shapes, it attains sim 5% relative L2 single-step error and up to 1000X speedups over CFD. We provide physics-centric rollout diagnostics, including phase error at probes and divergence norms, to quantify long-horizon fidelity. These reveal accurate near-term transients but error accumulation in fine-scale wakes, most pronounced for sharp-cornered geometries. We analyze failure modes and outline practical mitigations. Code, splits, and scripts are openly released at: https://github.com/baskargroup/TimeDependent-DeepONet to support reproducibility and benchmarking.",
            "upvotes": 1,
            "discussionId": "6937dedb19d912300c34a4d4",
            "githubRepo": "https://github.com/baskargroup/TimeDependent-DeepONet",
            "githubRepoAddedBy": "user",
            "ai_summary": "A geometry-aware Deep Operator Network encodes shape and flow history to predict velocity fields for unsteady flows, achieving significant speedup over CFD simulations.",
            "ai_keywords": [
                "Deep Operator Network",
                "signed distance field",
                "CNN",
                "high-fidelity simulations",
                "relative L2 error",
                "phase error",
                "divergence norms",
                "failure modes",
                "mitigations"
            ],
            "githubStars": 1,
            "organization": {
                "_id": "667c6818b2b8235b6e55a9d7",
                "name": "iowastate",
                "fullname": "iowa state university"
            }
        },
        "publishedAt": "2025-12-03T23:00:22.000Z",
        "title": "Predicting Time-Dependent Flow Over Complex Geometries Using Operator Networks",
        "summary": "Fast, geometry-generalizing surrogates for unsteady flow remain challenging. We present a time-dependent, geometry-aware Deep Operator Network that predicts velocity fields for moderate-Re flows around parametric and non-parametric shapes. The model encodes geometry via a signed distance field (SDF) trunk and flow history via a CNN branch, trained on 841 high-fidelity simulations. On held-out shapes, it attains sim 5% relative L2 single-step error and up to 1000X speedups over CFD. We provide physics-centric rollout diagnostics, including phase error at probes and divergence norms, to quantify long-horizon fidelity. These reveal accurate near-term transients but error accumulation in fine-scale wakes, most pronounced for sharp-cornered geometries. We analyze failure modes and outline practical mitigations. Code, splits, and scripts are openly released at: https://github.com/baskargroup/TimeDependent-DeepONet to support reproducibility and benchmarking.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/665755ba4d33c039d11bc73d/fL_VnMUmjVJ3Kxqxrm9iW.gif"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04434.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "665755ba4d33c039d11bc73d",
            "avatarUrl": "/avatars/e48205e66a7ab81328eabe6ece93c7ff.svg",
            "fullname": "Ali Rabeh",
            "name": "arabeh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "667c6818b2b8235b6e55a9d7",
            "name": "iowastate",
            "fullname": "iowa state university"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.08923",
            "authors": [
                {
                    "_id": "69392639dfc35938ba12a179",
                    "user": {
                        "_id": "661797a78d7b07669c896092",
                        "avatarUrl": "/avatars/971845551815d7826f97100f98df713b.svg",
                        "isPro": false,
                        "fullname": "Angela van Sprang",
                        "user": "Angela0603",
                        "type": "user"
                    },
                    "name": "Angela van Sprang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-10T15:49:24.707Z",
                    "hidden": false
                },
                {
                    "_id": "69392639dfc35938ba12a17a",
                    "name": "Laurens Samson",
                    "hidden": false
                },
                {
                    "_id": "69392639dfc35938ba12a17b",
                    "name": "Ana Lucic",
                    "hidden": false
                },
                {
                    "_id": "69392639dfc35938ba12a17c",
                    "user": {
                        "_id": "67adeeeff7d84e72910cafd2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/X9vk10KOuOrWuaEMBCoWz.png",
                        "isPro": false,
                        "fullname": "Erman Acar",
                        "user": "ermancr",
                        "type": "user"
                    },
                    "name": "Erman Acar",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-10T15:49:55.886Z",
                    "hidden": false
                },
                {
                    "_id": "69392639dfc35938ba12a17d",
                    "name": "Sennay Ghebreab",
                    "hidden": false
                },
                {
                    "_id": "69392639dfc35938ba12a17e",
                    "user": {
                        "_id": "637d21239a5217b88b7549c3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637d21239a5217b88b7549c3/LrIGPiva5VGVZG87rTAJz.jpeg",
                        "isPro": false,
                        "fullname": "Yuki Asano",
                        "user": "yukimasano",
                        "type": "user"
                    },
                    "name": "Yuki M. Asano",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-10T15:49:46.462Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-09T18:57:07.000Z",
            "submittedOnDailyAt": "2025-12-10T05:21:55.669Z",
            "title": "Same Content, Different Answers: Cross-Modal Inconsistency in MLLMs",
            "submittedOnDailyBy": {
                "_id": "637d21239a5217b88b7549c3",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637d21239a5217b88b7549c3/LrIGPiva5VGVZG87rTAJz.jpeg",
                "isPro": false,
                "fullname": "Yuki Asano",
                "user": "yukimasano",
                "type": "user"
            },
            "summary": "We introduce two new benchmarks REST and REST+(Render-Equivalence Stress Tests) to enable systematic evaluation of cross-modal inconsistency in multimodal large language models (MLLMs). MLLMs are trained to represent vision and language in the same embedding space, yet they cannot perform the same tasks in both modalities. Our benchmarks contain samples with the same semantic information in three modalities (image, text, mixed) and we show that state-of-the-art MLLMs cannot consistently reason over these different modalities. We evaluate 15 MLLMs and find that the degree of modality inconsistency varies substantially, even when accounting for problems with text recognition (OCR). Neither rendering text as image nor rendering an image as text solves the inconsistency. Even if OCR is correct, we find that visual characteristics (text colour and resolution, but not font) and the number of vision tokens have an impact on model performance. Finally, we find that our consistency score correlates with the modality gap between text and images, highlighting a mechanistic interpretation of cross-modal inconsistent MLLMs.",
            "upvotes": 0,
            "discussionId": "69392639dfc35938ba12a17f",
            "ai_summary": "Two new benchmarks assess cross-modal inconsistency in multimodal large language models, showing significant variability and impact of visual characteristics on performance.",
            "ai_keywords": [
                "multimodal large language models",
                "embedding space",
                "cross-modal inconsistency",
                "REST",
                "REST+",
                "Render-Equivalence Stress Tests",
                "OCR",
                "vision tokens",
                "modality gap"
            ],
            "organization": {
                "_id": "670635814d8c8b01343c35c5",
                "name": "FunAILab",
                "fullname": "Fundamental AI Lab at UTN",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/637d21239a5217b88b7549c3/5rDBCg9OOamKK_UqCof_z.png"
            }
        },
        "publishedAt": "2025-12-09T13:57:07.000Z",
        "title": "Same Content, Different Answers: Cross-Modal Inconsistency in MLLMs",
        "summary": "We introduce two new benchmarks REST and REST+(Render-Equivalence Stress Tests) to enable systematic evaluation of cross-modal inconsistency in multimodal large language models (MLLMs). MLLMs are trained to represent vision and language in the same embedding space, yet they cannot perform the same tasks in both modalities. Our benchmarks contain samples with the same semantic information in three modalities (image, text, mixed) and we show that state-of-the-art MLLMs cannot consistently reason over these different modalities. We evaluate 15 MLLMs and find that the degree of modality inconsistency varies substantially, even when accounting for problems with text recognition (OCR). Neither rendering text as image nor rendering an image as text solves the inconsistency. Even if OCR is correct, we find that visual characteristics (text colour and resolution, but not font) and the number of vision tokens have an impact on model performance. Finally, we find that our consistency score correlates with the modality gap between text and images, highlighting a mechanistic interpretation of cross-modal inconsistent MLLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.08923.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "637d21239a5217b88b7549c3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637d21239a5217b88b7549c3/LrIGPiva5VGVZG87rTAJz.jpeg",
            "fullname": "Yuki Asano",
            "name": "yukimasano",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "organization": {
            "_id": "670635814d8c8b01343c35c5",
            "name": "FunAILab",
            "fullname": "Fundamental AI Lab at UTN",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/637d21239a5217b88b7549c3/5rDBCg9OOamKK_UqCof_z.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.08309",
            "authors": [
                {
                    "_id": "69391526dfc35938ba12a148",
                    "user": {
                        "_id": "6910ddac0ea668b79a45338f",
                        "avatarUrl": "/avatars/c94f91892070f08eb1d1b54df978cafa.svg",
                        "isPro": true,
                        "fullname": "Alexander Goslin",
                        "user": "xandergos",
                        "type": "user"
                    },
                    "name": "Alexander Goslin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-10T08:39:28.515Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-09T07:10:35.000Z",
            "submittedOnDailyAt": "2025-12-10T15:07:05.673Z",
            "title": "Terrain Diffusion: A Diffusion-Based Successor to Perlin Noise in Infinite, Real-Time Terrain Generation",
            "submittedOnDailyBy": {
                "_id": "6910ddac0ea668b79a45338f",
                "avatarUrl": "/avatars/c94f91892070f08eb1d1b54df978cafa.svg",
                "isPro": true,
                "fullname": "Alexander Goslin",
                "user": "xandergos",
                "type": "user"
            },
            "summary": "For decades, procedural worlds have been built on procedural noise functions such as Perlin noise, which are fast and infinite, yet fundamentally limited in realism and large-scale coherence. We introduce Terrain Diffusion, an AI-era successor to Perlin noise that bridges the fidelity of diffusion models with the properties that made procedural noise indispensable: seamless infinite extent, seed-consistency, and constant-time random access. At its core is InfiniteDiffusion, a novel algorithm for infinite generation, enabling seamless, real-time synthesis of boundless landscapes. A hierarchical stack of diffusion models couples planetary context with local detail, while a compact Laplacian encoding stabilizes outputs across Earth-scale dynamic ranges. An open-source infinite-tensor framework supports constant-memory manipulation of unbounded tensors, and few-step consistency distillation enables efficient generation. Together, these components establish diffusion models as a practical foundation for procedural world generation, capable of synthesizing entire planets coherently, controllably, and without limits.",
            "upvotes": 0,
            "discussionId": "69391526dfc35938ba12a149",
            "projectPage": "https://xandergos.github.io/terrain-diffusion/",
            "githubRepo": "https://github.com/xandergos/terrain-diffusion",
            "githubRepoAddedBy": "user",
            "ai_summary": "Terrain Diffusion uses diffusion models and a novel algorithm called InfiniteDiffusion to generate realistic, seamless, and boundless procedural worlds with constant-time random access.",
            "ai_keywords": [
                "Terrain Diffusion",
                "InfiniteDiffusion",
                "diffusion models",
                "procedural noise",
                "Perlin noise",
                "seamless infinite extent",
                "seed-consistency",
                "constant-time random access",
                "hierarchical stack",
                "planetary context",
                "local detail",
                "compact Laplacian encoding",
                "Earth-scale dynamic ranges",
                "infinite-tensor framework",
                "constant-memory manipulation",
                "few-step consistency distillation"
            ],
            "githubStars": 1
        },
        "publishedAt": "2025-12-09T02:10:35.000Z",
        "title": "Terrain Diffusion: A Diffusion-Based Successor to Perlin Noise in Infinite, Real-Time Terrain Generation",
        "summary": "For decades, procedural worlds have been built on procedural noise functions such as Perlin noise, which are fast and infinite, yet fundamentally limited in realism and large-scale coherence. We introduce Terrain Diffusion, an AI-era successor to Perlin noise that bridges the fidelity of diffusion models with the properties that made procedural noise indispensable: seamless infinite extent, seed-consistency, and constant-time random access. At its core is InfiniteDiffusion, a novel algorithm for infinite generation, enabling seamless, real-time synthesis of boundless landscapes. A hierarchical stack of diffusion models couples planetary context with local detail, while a compact Laplacian encoding stabilizes outputs across Earth-scale dynamic ranges. An open-source infinite-tensor framework supports constant-memory manipulation of unbounded tensors, and few-step consistency distillation enables efficient generation. Together, these components establish diffusion models as a practical foundation for procedural world generation, capable of synthesizing entire planets coherently, controllably, and without limits.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.08309.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6910ddac0ea668b79a45338f",
            "avatarUrl": "/avatars/c94f91892070f08eb1d1b54df978cafa.svg",
            "fullname": "Alexander Goslin",
            "name": "xandergos",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    }
]