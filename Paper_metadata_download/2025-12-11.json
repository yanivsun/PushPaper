[
    {
        "paper": {
            "id": "2512.09363",
            "authors": [
                {
                    "_id": "693a379e74fced5bf9c32412",
                    "user": {
                        "_id": "6486ff6561053da6442fef1a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6486ff6561053da6442fef1a/72sdWErAwWtWNJIV5VZsy.jpeg",
                        "isPro": false,
                        "fullname": "KeXing",
                        "user": "KXingLab",
                        "type": "user"
                    },
                    "name": "Ke Xing",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-11T10:13:26.656Z",
                    "hidden": false
                },
                {
                    "_id": "693a379e74fced5bf9c32413",
                    "name": "Longfei Li",
                    "hidden": false
                },
                {
                    "_id": "693a379e74fced5bf9c32414",
                    "user": {
                        "_id": "64b7ab4c037d6452a31910eb",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b7ab4c037d6452a31910eb/0UaBtwyQTysBMndFWZdKu.png",
                        "isPro": false,
                        "fullname": "yuyangyin",
                        "user": "yuyangyin",
                        "type": "user"
                    },
                    "name": "Yuyang Yin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-11T10:43:30.256Z",
                    "hidden": false
                },
                {
                    "_id": "693a379e74fced5bf9c32415",
                    "name": "Hanwen Liang",
                    "hidden": false
                },
                {
                    "_id": "693a379e74fced5bf9c32416",
                    "name": "Guixun Luo",
                    "hidden": false
                },
                {
                    "_id": "693a379e74fced5bf9c32417",
                    "name": "Chen Fang",
                    "hidden": false
                },
                {
                    "_id": "693a379e74fced5bf9c32418",
                    "name": "Jue Wang",
                    "hidden": false
                },
                {
                    "_id": "693a379e74fced5bf9c32419",
                    "name": "Konstantinos N. Plataniotis",
                    "hidden": false
                },
                {
                    "_id": "693a379e74fced5bf9c3241a",
                    "name": "Xiaojie Jin",
                    "hidden": false
                },
                {
                    "_id": "693a379e74fced5bf9c3241b",
                    "name": "Yao Zhao",
                    "hidden": false
                },
                {
                    "_id": "693a379e74fced5bf9c3241c",
                    "name": "Yunchao Wei",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/dFNy8Tf5Ts5qNeWXCvkcB.mp4"
            ],
            "publishedAt": "2025-12-10T06:50:16.000Z",
            "submittedOnDailyAt": "2025-12-11T00:46:52.612Z",
            "title": "StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "The growing adoption of XR devices has fueled strong demand for high-quality stereo video, yet its production remains costly and artifact-prone. To address this challenge, we present StereoWorld, an end-to-end framework that repurposes a pretrained video generator for high-fidelity monocular-to-stereo video generation. Our framework jointly conditions the model on the monocular video input while explicitly supervising the generation with a geometry-aware regularization to ensure 3D structural fidelity. A spatio-temporal tiling scheme is further integrated to enable efficient, high-resolution synthesis. To enable large-scale training and evaluation, we curate a high-definition stereo video dataset containing over 11M frames aligned to natural human interpupillary distance (IPD). Extensive experiments demonstrate that StereoWorld substantially outperforms prior methods, generating stereo videos with superior visual fidelity and geometric consistency. The project webpage is available at https://ke-xing.github.io/StereoWorld/.",
            "upvotes": 47,
            "discussionId": "693a379e74fced5bf9c3241d",
            "projectPage": "https://ke-xing.github.io/StereoWorld/",
            "ai_summary": "StereoWorld generates high-quality stereo video from monocular input using a pretrained video generator with geometry-aware regularization and spatio-temporal tiling.",
            "ai_keywords": [
                "stereo video",
                "monocular-to-stereo",
                "pretrained video generator",
                "geometry-aware regularization",
                "spatio-temporal tiling",
                "high-definition stereo video dataset",
                "natural human interpupillary distance (IPD)",
                "visual fidelity",
                "geometric consistency"
            ]
        },
        "publishedAt": "2025-12-10T01:50:16.000Z",
        "title": "StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation",
        "summary": "The growing adoption of XR devices has fueled strong demand for high-quality stereo video, yet its production remains costly and artifact-prone. To address this challenge, we present StereoWorld, an end-to-end framework that repurposes a pretrained video generator for high-fidelity monocular-to-stereo video generation. Our framework jointly conditions the model on the monocular video input while explicitly supervising the generation with a geometry-aware regularization to ensure 3D structural fidelity. A spatio-temporal tiling scheme is further integrated to enable efficient, high-resolution synthesis. To enable large-scale training and evaluation, we curate a high-definition stereo video dataset containing over 11M frames aligned to natural human interpupillary distance (IPD). Extensive experiments demonstrate that StereoWorld substantially outperforms prior methods, generating stereo videos with superior visual fidelity and geometric consistency. The project webpage is available at https://ke-xing.github.io/StereoWorld/.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/dFNy8Tf5Ts5qNeWXCvkcB.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.09363.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 181
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.08560",
            "authors": [
                {
                    "_id": "693a7def74fced5bf9c32537",
                    "user": {
                        "_id": "6492c419702103104f9450c4",
                        "avatarUrl": "/avatars/d6700e4bb1b9f172096ea31ba15a83b2.svg",
                        "isPro": false,
                        "fullname": "navve wasserman",
                        "user": "navvew",
                        "type": "user"
                    },
                    "name": "Navve Wasserman",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-11T10:07:40.033Z",
                    "hidden": false
                },
                {
                    "_id": "693a7def74fced5bf9c32538",
                    "user": {
                        "_id": "6735088b22d14a01ae17501f",
                        "avatarUrl": "/avatars/23d2eb2bb833dcf7a05434b499fedd5e.svg",
                        "isPro": false,
                        "fullname": "Matias Cosarinsky",
                        "user": "mcosarinsky",
                        "type": "user"
                    },
                    "name": "Matias Cosarinsky",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-11T10:45:36.526Z",
                    "hidden": false
                },
                {
                    "_id": "693a7def74fced5bf9c32539",
                    "user": {
                        "_id": "687f44e793eb81b0684b4eee",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/uDef9M8sQxTweYvwcVs07.png",
                        "isPro": false,
                        "fullname": "Yuval Golbari",
                        "user": "yuvalgolbari",
                        "type": "user"
                    },
                    "name": "Yuval Golbari",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-11T10:45:41.956Z",
                    "hidden": false
                },
                {
                    "_id": "693a7def74fced5bf9c3253a",
                    "name": "Aude Oliva",
                    "hidden": false
                },
                {
                    "_id": "693a7def74fced5bf9c3253b",
                    "user": {
                        "_id": "6744c6ec6ec99a37d4ba9235",
                        "avatarUrl": "/avatars/a5384b63bb615192f6fa157c6ea89e92.svg",
                        "isPro": false,
                        "fullname": "Antonio",
                        "user": "Antoniotorralbaborruel",
                        "type": "user"
                    },
                    "name": "Antonio Torralba",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-11T10:45:57.490Z",
                    "hidden": false
                },
                {
                    "_id": "693a7def74fced5bf9c3253c",
                    "user": {
                        "_id": "63914afeb6b839bb6143a6be",
                        "avatarUrl": "/avatars/da25b555d105f4755c8187479469ca77.svg",
                        "isPro": false,
                        "fullname": "Tamar Rott Shaham",
                        "user": "tamarott",
                        "type": "user"
                    },
                    "name": "Tamar Rott Shaham",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-11T10:46:03.740Z",
                    "hidden": false
                },
                {
                    "_id": "693a7def74fced5bf9c3253d",
                    "name": "Michal Irani",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-09T13:01:17.000Z",
            "submittedOnDailyAt": "2025-12-11T05:54:20.652Z",
            "title": "BrainExplore: Large-Scale Discovery of Interpretable Visual Representations in the Human Brain",
            "submittedOnDailyBy": {
                "_id": "6492c419702103104f9450c4",
                "avatarUrl": "/avatars/d6700e4bb1b9f172096ea31ba15a83b2.svg",
                "isPro": false,
                "fullname": "navve wasserman",
                "user": "navvew",
                "type": "user"
            },
            "summary": "Understanding how the human brain represents visual concepts, and in which brain regions these representations are encoded, remains a long-standing challenge. Decades of work have advanced our understanding of visual representations, yet brain signals remain large and complex, and the space of possible visual concepts is vast. As a result, most studies remain small-scale, rely on manual inspection, focus on specific regions and properties, and rarely include systematic validation. We present a large-scale, automated framework for discovering and explaining visual representations across the human cortex. Our method comprises two main stages. First, we discover candidate interpretable patterns in fMRI activity through unsupervised, data-driven decomposition methods. Next, we explain each pattern by identifying the set of natural images that most strongly elicit it and generating a natural-language description of their shared visual meaning. To scale this process, we introduce an automated pipeline that tests multiple candidate explanations, assigns quantitative reliability scores, and selects the most consistent description for each voxel pattern. Our framework reveals thousands of interpretable patterns spanning many distinct visual concepts, including fine-grained representations previously unreported.",
            "upvotes": 32,
            "discussionId": "693a7def74fced5bf9c3253e",
            "projectPage": "https://navvewas.github.io/BrainExplore/",
            "ai_summary": "An automated framework uses unsupervised decomposition and natural language descriptions to identify and explain visual representations in human brain fMRI data.",
            "ai_keywords": [
                "fMRI",
                "unsupervised decomposition",
                "natural images",
                "natural-language descriptions",
                "voxel patterns"
            ]
        },
        "publishedAt": "2025-12-09T08:01:17.000Z",
        "title": "BrainExplore: Large-Scale Discovery of Interpretable Visual Representations in the Human Brain",
        "summary": "Understanding how the human brain represents visual concepts, and in which brain regions these representations are encoded, remains a long-standing challenge. Decades of work have advanced our understanding of visual representations, yet brain signals remain large and complex, and the space of possible visual concepts is vast. As a result, most studies remain small-scale, rely on manual inspection, focus on specific regions and properties, and rarely include systematic validation. We present a large-scale, automated framework for discovering and explaining visual representations across the human cortex. Our method comprises two main stages. First, we discover candidate interpretable patterns in fMRI activity through unsupervised, data-driven decomposition methods. Next, we explain each pattern by identifying the set of natural images that most strongly elicit it and generating a natural-language description of their shared visual meaning. To scale this process, we introduce an automated pipeline that tests multiple candidate explanations, assigns quantitative reliability scores, and selects the most consistent description for each voxel pattern. Our framework reveals thousands of interpretable patterns spanning many distinct visual concepts, including fine-grained representations previously unreported.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.08560.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6492c419702103104f9450c4",
            "avatarUrl": "/avatars/d6700e4bb1b9f172096ea31ba15a83b2.svg",
            "fullname": "navve wasserman",
            "name": "navvew",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.09247",
            "authors": [
                {
                    "_id": "693a372474fced5bf9c323fd",
                    "name": "Cheng Liu",
                    "hidden": false
                },
                {
                    "_id": "693a372474fced5bf9c323fe",
                    "user": {
                        "_id": "64311a95034ecbefddd141ef",
                        "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
                        "isPro": false,
                        "fullname": "Yiren Song",
                        "user": "yiren98",
                        "type": "user"
                    },
                    "name": "Yiren Song",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-11T10:48:03.757Z",
                    "hidden": false
                },
                {
                    "_id": "693a372474fced5bf9c323ff",
                    "user": {
                        "_id": "637745113a63a2983ffbde13",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669187672174-637745113a63a2983ffbde13.jpeg",
                        "isPro": false,
                        "fullname": "Haofan Wang",
                        "user": "wanghaofan",
                        "type": "user"
                    },
                    "name": "Haofan Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-11T10:47:57.012Z",
                    "hidden": false
                },
                {
                    "_id": "693a372474fced5bf9c32400",
                    "user": {
                        "_id": "661ab3da2b14565c7acccf5c",
                        "avatarUrl": "/avatars/fa4fc03664803e02aede4d4c3d50b393.svg",
                        "isPro": false,
                        "fullname": "Mike Zheng Shou",
                        "user": "AnalMom",
                        "type": "user"
                    },
                    "name": "Mike Zheng Shou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-11T10:47:38.881Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/GAFA0NvcooaUISwA3XiJq.mp4"
            ],
            "publishedAt": "2025-12-10T02:09:59.000Z",
            "submittedOnDailyAt": "2025-12-11T00:45:04.130Z",
            "title": "OmniPSD: Layered PSD Generation with Diffusion Transformer",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Recent advances in diffusion models have greatly improved image generation and editing, yet generating or reconstructing layered PSD files with transparent alpha channels remains highly challenging. We propose OmniPSD, a unified diffusion framework built upon the Flux ecosystem that enables both text-to-PSD generation and image-to-PSD decomposition through in-context learning. For text-to-PSD generation, OmniPSD arranges multiple target layers spatially into a single canvas and learns their compositional relationships through spatial attention, producing semantically coherent and hierarchically structured layers. For image-to-PSD decomposition, it performs iterative in-context editing, progressively extracting and erasing textual and foreground components to reconstruct editable PSD layers from a single flattened image. An RGBA-VAE is employed as an auxiliary representation module to preserve transparency without affecting structure learning. Extensive experiments on our new RGBA-layered dataset demonstrate that OmniPSD achieves high-fidelity generation, structural consistency, and transparency awareness, offering a new paradigm for layered design generation and decomposition with diffusion transformers.",
            "upvotes": 26,
            "discussionId": "693a372474fced5bf9c32401",
            "projectPage": "https://showlab.github.io/OmniPSD/",
            "ai_summary": "OmniPSD, a diffusion framework within the Flux ecosystem, enables text-to-PSD generation and image-to-PSD decomposition, achieving high-fidelity results with transparency awareness.",
            "ai_keywords": [
                "diffusion models",
                "OmniPSD",
                "Flux ecosystem",
                "in-context learning",
                "spatial attention",
                "iterative in-context editing",
                "RGBA-VAE",
                "RGBA-layered dataset",
                "diffusion transformers"
            ]
        },
        "publishedAt": "2025-12-09T21:09:59.000Z",
        "title": "OmniPSD: Layered PSD Generation with Diffusion Transformer",
        "summary": "Recent advances in diffusion models have greatly improved image generation and editing, yet generating or reconstructing layered PSD files with transparent alpha channels remains highly challenging. We propose OmniPSD, a unified diffusion framework built upon the Flux ecosystem that enables both text-to-PSD generation and image-to-PSD decomposition through in-context learning. For text-to-PSD generation, OmniPSD arranges multiple target layers spatially into a single canvas and learns their compositional relationships through spatial attention, producing semantically coherent and hierarchically structured layers. For image-to-PSD decomposition, it performs iterative in-context editing, progressively extracting and erasing textual and foreground components to reconstruct editable PSD layers from a single flattened image. An RGBA-VAE is employed as an auxiliary representation module to preserve transparency without affecting structure learning. Extensive experiments on our new RGBA-layered dataset demonstrate that OmniPSD achieves high-fidelity generation, structural consistency, and transparency awareness, offering a new paradigm for layered design generation and decomposition with diffusion transformers.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/GAFA0NvcooaUISwA3XiJq.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.09247.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 181
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.09824",
            "authors": [
                {
                    "_id": "693a2f6d74fced5bf9c323c2",
                    "user": {
                        "_id": "6428fd124fe87caede856311",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5OrNPwZkxu3Dm1IInCxML.jpeg",
                        "isPro": false,
                        "fullname": "Xianghao Kong",
                        "user": "refkxh",
                        "type": "user"
                    },
                    "name": "Xianghao Kong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-11T10:13:32.571Z",
                    "hidden": false
                },
                {
                    "_id": "693a2f6d74fced5bf9c323c3",
                    "user": {
                        "_id": "65b00730403a23a2fd765110",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b00730403a23a2fd765110/Uw-obs-VymyU9iMVKLURZ.jpeg",
                        "isPro": false,
                        "fullname": "Zeyu Zhang",
                        "user": "ZeyuZhang",
                        "type": "user"
                    },
                    "name": "Zeyu Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-11T10:45:03.580Z",
                    "hidden": false
                },
                {
                    "_id": "693a2f6d74fced5bf9c323c4",
                    "name": "Yuwei Guo",
                    "hidden": false
                },
                {
                    "_id": "693a2f6d74fced5bf9c323c5",
                    "user": {
                        "_id": "67f87bc19d597ac661a75b68",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/CZiu75y2QIR53MZkEjgJd.png",
                        "isPro": false,
                        "fullname": "Zhuoran Zhao",
                        "user": "Alicezrzhao",
                        "type": "user"
                    },
                    "name": "Zhuoran Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-11T10:13:29.933Z",
                    "hidden": false
                },
                {
                    "_id": "693a2f6d74fced5bf9c323c6",
                    "name": "Songchun Zhang",
                    "hidden": false
                },
                {
                    "_id": "693a2f6d74fced5bf9c323c7",
                    "user": {
                        "_id": "63f8130749569335b679af62",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f8130749569335b679af62/vgTu23-y0UKocwAGqNMwT.jpeg",
                        "isPro": false,
                        "fullname": "Anyi Rao",
                        "user": "anyirao",
                        "type": "user"
                    },
                    "name": "Anyi Rao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-11T10:45:24.045Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/67f87bc19d597ac661a75b68/zeD4CrSjVY6WpEKMkjlat.gif"
            ],
            "publishedAt": "2025-12-10T16:57:31.000Z",
            "submittedOnDailyAt": "2025-12-11T04:44:05.510Z",
            "title": "Composing Concepts from Images and Videos via Concept-prompt Binding",
            "submittedOnDailyBy": {
                "_id": "67f87bc19d597ac661a75b68",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/CZiu75y2QIR53MZkEjgJd.png",
                "isPro": false,
                "fullname": "Zhuoran Zhao",
                "user": "Alicezrzhao",
                "type": "user"
            },
            "summary": "Visual concept composition, which aims to integrate different elements from images and videos into a single, coherent visual output, still falls short in accurately extracting complex concepts from visual inputs and flexibly combining concepts from both images and videos. We introduce Bind & Compose, a one-shot method that enables flexible visual concept composition by binding visual concepts with corresponding prompt tokens and composing the target prompt with bound tokens from various sources. It adopts a hierarchical binder structure for cross-attention conditioning in Diffusion Transformers to encode visual concepts into corresponding prompt tokens for accurate decomposition of complex visual concepts. To improve concept-token binding accuracy, we design a Diversify-and-Absorb Mechanism that uses an extra absorbent token to eliminate the impact of concept-irrelevant details when training with diversified prompts. To enhance the compatibility between image and video concepts, we present a Temporal Disentanglement Strategy that decouples the training process of video concepts into two stages with a dual-branch binder structure for temporal modeling. Evaluations demonstrate that our method achieves superior concept consistency, prompt fidelity, and motion quality over existing approaches, opening up new possibilities for visual creativity.",
            "upvotes": 24,
            "discussionId": "693a2f6d74fced5bf9c323c8",
            "projectPage": "https://refkxh.github.io/BiCo_Webpage/",
            "githubRepo": "https://github.com/refkxh/bico",
            "githubRepoAddedBy": "user",
            "ai_summary": "Bind & Compose uses Diffusion Transformers with hierarchical binders and temporal strategies to accurately compose complex visual concepts from images and videos.",
            "ai_keywords": [
                "visual concept composition",
                "prompt tokens",
                "cross-attention conditioning",
                "Diffusion Transformers",
                "hierarchical binder structure",
                "Diversify-and-Absorb Mechanism",
                "absorbent token",
                "Temporal Disentanglement Strategy",
                "dual-branch binder structure"
            ],
            "githubStars": 45,
            "organization": {
                "_id": "693a3d43dd71dc07fc3a7cfe",
                "name": "mmlab-hkust",
                "fullname": "MMLab@HKUST",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6428fd124fe87caede856311/u4kVky_MS1iH4OYTzMF0H.jpeg"
            }
        },
        "publishedAt": "2025-12-10T11:57:31.000Z",
        "title": "Composing Concepts from Images and Videos via Concept-prompt Binding",
        "summary": "Visual concept composition, which aims to integrate different elements from images and videos into a single, coherent visual output, still falls short in accurately extracting complex concepts from visual inputs and flexibly combining concepts from both images and videos. We introduce Bind & Compose, a one-shot method that enables flexible visual concept composition by binding visual concepts with corresponding prompt tokens and composing the target prompt with bound tokens from various sources. It adopts a hierarchical binder structure for cross-attention conditioning in Diffusion Transformers to encode visual concepts into corresponding prompt tokens for accurate decomposition of complex visual concepts. To improve concept-token binding accuracy, we design a Diversify-and-Absorb Mechanism that uses an extra absorbent token to eliminate the impact of concept-irrelevant details when training with diversified prompts. To enhance the compatibility between image and video concepts, we present a Temporal Disentanglement Strategy that decouples the training process of video concepts into two stages with a dual-branch binder structure for temporal modeling. Evaluations demonstrate that our method achieves superior concept consistency, prompt fidelity, and motion quality over existing approaches, opening up new possibilities for visual creativity.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/67f87bc19d597ac661a75b68/zeD4CrSjVY6WpEKMkjlat.gif"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.09824.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67f87bc19d597ac661a75b68",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/CZiu75y2QIR53MZkEjgJd.png",
            "fullname": "Zhuoran Zhao",
            "name": "Alicezrzhao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "693a3d43dd71dc07fc3a7cfe",
            "name": "mmlab-hkust",
            "fullname": "MMLab@HKUST",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6428fd124fe87caede856311/u4kVky_MS1iH4OYTzMF0H.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.08829",
            "authors": [
                {
                    "_id": "693a1f0274fced5bf9c32399",
                    "user": {
                        "_id": "66a105bb456284adf458d656",
                        "avatarUrl": "/avatars/b543a324f7e159d6e84bc68915e93d24.svg",
                        "isPro": false,
                        "fullname": "Tao Hongyuan",
                        "user": "HongyuanTao",
                        "type": "user"
                    },
                    "name": "Hongyuan Tao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-11T10:46:24.642Z",
                    "hidden": false
                },
                {
                    "_id": "693a1f0274fced5bf9c3239a",
                    "user": {
                        "_id": "6577073fc2bf55b1f6bafb49",
                        "avatarUrl": "/avatars/58803398b1a918b7570db17893e65122.svg",
                        "isPro": false,
                        "fullname": "Bencheng liao",
                        "user": "LegendBC",
                        "type": "user"
                    },
                    "name": "Bencheng Liao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-11T10:46:30.879Z",
                    "hidden": false
                },
                {
                    "_id": "693a1f0274fced5bf9c3239b",
                    "name": "Shaoyu Chen",
                    "hidden": false
                },
                {
                    "_id": "693a1f0274fced5bf9c3239c",
                    "name": "Haoran Yin",
                    "hidden": false
                },
                {
                    "_id": "693a1f0274fced5bf9c3239d",
                    "name": "Qian Zhang",
                    "hidden": false
                },
                {
                    "_id": "693a1f0274fced5bf9c3239e",
                    "user": {
                        "_id": "66c2e7fc934e2f07753542ac",
                        "avatarUrl": "/avatars/f6fa3f94435cf1c1d06daa6c925d07d0.svg",
                        "isPro": false,
                        "fullname": "LWY",
                        "user": "wenyuliu",
                        "type": "user"
                    },
                    "name": "Wenyu Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-11T10:46:45.682Z",
                    "hidden": false
                },
                {
                    "_id": "693a1f0274fced5bf9c3239f",
                    "user": {
                        "_id": "62600de6d47e3dbae32ce1ce",
                        "avatarUrl": "/avatars/a536417cfec6e10ac415091bd1829426.svg",
                        "isPro": false,
                        "fullname": "Xinggang Wang",
                        "user": "xinggangw",
                        "type": "user"
                    },
                    "name": "Xinggang Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-11T10:46:51.670Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-09T17:18:32.000Z",
            "submittedOnDailyAt": "2025-12-11T03:25:34.652Z",
            "title": "InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models",
            "submittedOnDailyBy": {
                "_id": "66a105bb456284adf458d656",
                "avatarUrl": "/avatars/b543a324f7e159d6e84bc68915e93d24.svg",
                "isPro": false,
                "fullname": "Tao Hongyuan",
                "user": "HongyuanTao",
                "type": "user"
            },
            "summary": "Window attention and linear attention represent two principal strategies for mitigating the quadratic complexity and ever-growing KV cache in Vision-Language Models (VLMs). However, we observe that window-based VLMs suffer performance degradation when sequence length exceeds the window size, while linear attention underperforms on information-intensive tasks such as OCR and document understanding. To overcome these limitations, we propose InfiniteVL, a linear-complexity VLM architecture that synergizes sliding window attention (SWA) with Gated DeltaNet. For achieving competitive multimodal performance under constrained resources, we design a three-stage training strategy comprising distillation pretraining, instruction tuning, and long-sequence SFT. Remarkably, using less than 2\\% of the training data required by leading VLMs, InfiniteVL not only substantially outperforms previous linear-complexity VLMs but also matches the performance of leading Transformer-based VLMs, while demonstrating effective long-term memory retention. Compared to similar-sized Transformer-based VLMs accelerated by FlashAttention-2, InfiniteVL achieves over 3.6\\times inference speedup while maintaining constant latency and memory footprint. In streaming video understanding scenarios, it sustains a stable 24 FPS real-time prefill speed while preserving long-term memory cache. Code and models are available at https://github.com/hustvl/InfiniteVL.",
            "upvotes": 12,
            "discussionId": "693a1f0274fced5bf9c323a0",
            "projectPage": "https://github.com/hustvl/InfiniteVL",
            "githubRepo": "https://github.com/hustvl/InfiniteVL",
            "githubRepoAddedBy": "user",
            "ai_summary": "InfiniteVL, a linear-complexity VLM architecture combining sliding window attention and Gated DeltaNet, achieves competitive performance with less data and faster inference than leading Transformer-based models.",
            "ai_keywords": [
                "window attention",
                "linear attention",
                "Vision-Language Models",
                "VLMs",
                "sequence length",
                "window size",
                "sliding window attention",
                "Gated DeltaNet",
                "distillation pretraining",
                "instruction tuning",
                "long-sequence SFT",
                "multimodal performance",
                "long-term memory retention",
                "FlashAttention-2",
                "inference speedup",
                "real-time prefill speed"
            ],
            "githubStars": 30,
            "organization": {
                "_id": "62600e67ffe8827cb1d6180b",
                "name": "hustvl",
                "fullname": "HUST Vision Lab"
            }
        },
        "publishedAt": "2025-12-09T12:18:32.000Z",
        "title": "InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models",
        "summary": "Window attention and linear attention represent two principal strategies for mitigating the quadratic complexity and ever-growing KV cache in Vision-Language Models (VLMs). However, we observe that window-based VLMs suffer performance degradation when sequence length exceeds the window size, while linear attention underperforms on information-intensive tasks such as OCR and document understanding. To overcome these limitations, we propose InfiniteVL, a linear-complexity VLM architecture that synergizes sliding window attention (SWA) with Gated DeltaNet. For achieving competitive multimodal performance under constrained resources, we design a three-stage training strategy comprising distillation pretraining, instruction tuning, and long-sequence SFT. Remarkably, using less than 2\\% of the training data required by leading VLMs, InfiniteVL not only substantially outperforms previous linear-complexity VLMs but also matches the performance of leading Transformer-based VLMs, while demonstrating effective long-term memory retention. Compared to similar-sized Transformer-based VLMs accelerated by FlashAttention-2, InfiniteVL achieves over 3.6\\times inference speedup while maintaining constant latency and memory footprint. In streaming video understanding scenarios, it sustains a stable 24 FPS real-time prefill speed while preserving long-term memory cache. Code and models are available at https://github.com/hustvl/InfiniteVL.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.08829.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66a105bb456284adf458d656",
            "avatarUrl": "/avatars/b543a324f7e159d6e84bc68915e93d24.svg",
            "fullname": "Tao Hongyuan",
            "name": "HongyuanTao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "organization": {
            "_id": "62600e67ffe8827cb1d6180b",
            "name": "hustvl",
            "fullname": "HUST Vision Lab"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.09928",
            "authors": [
                {
                    "_id": "693a481d74fced5bf9c3247d",
                    "name": "Minghui Lin",
                    "hidden": false
                },
                {
                    "_id": "693a481d74fced5bf9c3247e",
                    "name": "Pengxiang Ding",
                    "hidden": false
                },
                {
                    "_id": "693a481d74fced5bf9c3247f",
                    "name": "Shu Wang",
                    "hidden": false
                },
                {
                    "_id": "693a481d74fced5bf9c32480",
                    "name": "Zifeng Zhuang",
                    "hidden": false
                },
                {
                    "_id": "693a481d74fced5bf9c32481",
                    "name": "Yang Liu",
                    "hidden": false
                },
                {
                    "_id": "693a481d74fced5bf9c32482",
                    "name": "Xinyang Tong",
                    "hidden": false
                },
                {
                    "_id": "693a481d74fced5bf9c32483",
                    "name": "Wenxuan Song",
                    "hidden": false
                },
                {
                    "_id": "693a481d74fced5bf9c32484",
                    "name": "Shangke Lyu",
                    "hidden": false
                },
                {
                    "_id": "693a481d74fced5bf9c32485",
                    "user": {
                        "_id": "65fd82762bf2cd20ddaa193f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png",
                        "isPro": false,
                        "fullname": "Siteng Huang",
                        "user": "huangsiteng",
                        "type": "user"
                    },
                    "name": "Siteng Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-11T10:50:45.555Z",
                    "hidden": false
                },
                {
                    "_id": "693a481d74fced5bf9c32486",
                    "name": "Donglin Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-10T18:59:32.000Z",
            "submittedOnDailyAt": "2025-12-11T01:59:46.905Z",
            "title": "HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models",
            "submittedOnDailyBy": {
                "_id": "65fd82762bf2cd20ddaa193f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png",
                "isPro": false,
                "fullname": "Siteng Huang",
                "user": "huangsiteng",
                "type": "user"
            },
            "summary": "Vision-Language-Action (VLA) models have recently enabled robotic manipulation by grounding visual and linguistic cues into actions. However, most VLAs assume the Markov property, relying only on the current observation and thus suffering from temporal myopia that degrades long-horizon coherence. In this work, we view motion as a more compact and informative representation of temporal context and world dynamics, capturing inter-state changes while filtering static pixel-level noise. Building on this idea, we propose HiF-VLA (Hindsight, Insight, and Foresight for VLAs), a unified framework that leverages motion for bidirectional temporal reasoning. HiF-VLA encodes past dynamics through hindsight priors, anticipates future motion via foresight reasoning, and integrates both through a hindsight-modulated joint expert to enable a ''think-while-acting'' paradigm for long-horizon manipulation. As a result, HiF-VLA surpasses strong baselines on LIBERO-Long and CALVIN ABC-D benchmarks, while incurring negligible additional inference latency. Furthermore, HiF-VLA achieves substantial improvements in real-world long-horizon manipulation tasks, demonstrating its broad effectiveness in practical robotic settings.",
            "upvotes": 10,
            "discussionId": "693a481e74fced5bf9c32487",
            "projectPage": "https://github.com/OpenHelix-Team/HiF-VLA",
            "githubRepo": "https://github.com/OpenHelix-Team/HiF-VLA",
            "githubRepoAddedBy": "user",
            "ai_summary": "HiF-VLA integrates motion for bidirectional temporal reasoning in VLA models, improving long-horizon manipulation performance with minimal additional latency.",
            "ai_keywords": [
                "Motion",
                "bidirectional temporal reasoning",
                "hindsight priors",
                "foresight reasoning",
                "hindsight-modulated joint expert",
                "think-while-acting",
                "LIBERO-Long",
                "CALVIN ABC-D benchmarks",
                "real-world long-horizon manipulation tasks"
            ],
            "githubStars": 17,
            "organization": {
                "_id": "6757b280cd7b1c0076deca02",
                "name": "westlakerobotics",
                "fullname": "Westlake Robotics Technology (Hangzhou) Co., Ltd.",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64d0a450228324a28bef9816/TG6rVKYnq5xD0AtzBIlEz.jpeg"
            }
        },
        "publishedAt": "2025-12-10T13:59:32.000Z",
        "title": "HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models",
        "summary": "Vision-Language-Action (VLA) models have recently enabled robotic manipulation by grounding visual and linguistic cues into actions. However, most VLAs assume the Markov property, relying only on the current observation and thus suffering from temporal myopia that degrades long-horizon coherence. In this work, we view motion as a more compact and informative representation of temporal context and world dynamics, capturing inter-state changes while filtering static pixel-level noise. Building on this idea, we propose HiF-VLA (Hindsight, Insight, and Foresight for VLAs), a unified framework that leverages motion for bidirectional temporal reasoning. HiF-VLA encodes past dynamics through hindsight priors, anticipates future motion via foresight reasoning, and integrates both through a hindsight-modulated joint expert to enable a ''think-while-acting'' paradigm for long-horizon manipulation. As a result, HiF-VLA surpasses strong baselines on LIBERO-Long and CALVIN ABC-D benchmarks, while incurring negligible additional inference latency. Furthermore, HiF-VLA achieves substantial improvements in real-world long-horizon manipulation tasks, demonstrating its broad effectiveness in practical robotic settings.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.09928.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65fd82762bf2cd20ddaa193f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png",
            "fullname": "Siteng Huang",
            "name": "huangsiteng",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 11
        },
        "organization": {
            "_id": "6757b280cd7b1c0076deca02",
            "name": "westlakerobotics",
            "fullname": "Westlake Robotics Technology (Hangzhou) Co., Ltd.",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64d0a450228324a28bef9816/TG6rVKYnq5xD0AtzBIlEz.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.09616",
            "authors": [
                {
                    "_id": "693ae233e1612ff4bc2a497e",
                    "name": "Yiwu Zhong",
                    "hidden": false
                },
                {
                    "_id": "693ae233e1612ff4bc2a497f",
                    "name": "Zi-Yuan Hu",
                    "hidden": false
                },
                {
                    "_id": "693ae233e1612ff4bc2a4980",
                    "name": "Yin Li",
                    "hidden": false
                },
                {
                    "_id": "693ae233e1612ff4bc2a4981",
                    "name": "Liwei Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-10T13:05:55.000Z",
            "submittedOnDailyAt": "2025-12-11T12:55:11.736Z",
            "title": "Rethinking Chain-of-Thought Reasoning for Videos",
            "submittedOnDailyBy": {
                "_id": "62b4fe7ab25cb80fcf2ffd66",
                "avatarUrl": "/avatars/22f6a05cdbf4224d29ec9259c9fdd7a4.svg",
                "isPro": false,
                "fullname": "Yiwu Zhong",
                "user": "YiwuZhong",
                "type": "user"
            },
            "summary": "Chain-of-thought (CoT) reasoning has been highly successful in solving complex tasks in natural language processing, and recent multimodal large language models (MLLMs) have extended this paradigm to video reasoning. However, these models typically build on lengthy reasoning chains and large numbers of input visual tokens. Motivated by empirical observations from our benchmark study, we hypothesize that concise reasoning combined with a reduced set of visual tokens can be sufficient for effective video reasoning. To evaluate this hypothesis, we design and validate an efficient post-training and inference framework that enhances a video MLLM's reasoning capability. Our framework enables models to operate on compressed visual tokens and generate brief reasoning traces prior to answering. The resulting models achieve substantially improved inference efficiency, deliver competitive performance across diverse benchmarks, and avoid reliance on manual CoT annotations or supervised fine-tuning. Collectively, our results suggest that long, human-like CoT reasoning may not be necessary for general video reasoning, and that concise reasoning can be both effective and efficient. Our code will be released at https://github.com/LaVi-Lab/Rethink_CoT_Video.",
            "upvotes": 9,
            "discussionId": "693ae234e1612ff4bc2a4982",
            "githubRepo": "https://github.com/LaVi-Lab/Rethink_CoT_Video",
            "githubRepoAddedBy": "user",
            "ai_summary": "Efficient video reasoning can be achieved using concise chains of thought and reduced visual tokens without manual annotations or supervised fine-tuning.",
            "ai_keywords": [
                "chain-of-thought",
                "multimodal large language models",
                "video reasoning",
                "visual tokens",
                "reasoning traces",
                "post-training",
                "inference framework",
                "inference efficiency",
                "benchmarks",
                "human-like CoT reasoning"
            ],
            "githubStars": 4
        },
        "publishedAt": "2025-12-10T08:05:55.000Z",
        "title": "Rethinking Chain-of-Thought Reasoning for Videos",
        "summary": "Chain-of-thought (CoT) reasoning has been highly successful in solving complex tasks in natural language processing, and recent multimodal large language models (MLLMs) have extended this paradigm to video reasoning. However, these models typically build on lengthy reasoning chains and large numbers of input visual tokens. Motivated by empirical observations from our benchmark study, we hypothesize that concise reasoning combined with a reduced set of visual tokens can be sufficient for effective video reasoning. To evaluate this hypothesis, we design and validate an efficient post-training and inference framework that enhances a video MLLM's reasoning capability. Our framework enables models to operate on compressed visual tokens and generate brief reasoning traces prior to answering. The resulting models achieve substantially improved inference efficiency, deliver competitive performance across diverse benchmarks, and avoid reliance on manual CoT annotations or supervised fine-tuning. Collectively, our results suggest that long, human-like CoT reasoning may not be necessary for general video reasoning, and that concise reasoning can be both effective and efficient. Our code will be released at https://github.com/LaVi-Lab/Rethink_CoT_Video.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.09616.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62b4fe7ab25cb80fcf2ffd66",
            "avatarUrl": "/avatars/22f6a05cdbf4224d29ec9259c9fdd7a4.svg",
            "fullname": "Yiwu Zhong",
            "name": "YiwuZhong",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.02892",
            "authors": [
                {
                    "_id": "693a4c4d74fced5bf9c32493",
                    "user": {
                        "_id": "655efd24afee0e00788bb589",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655efd24afee0e00788bb589/22guLxIWNybbJR3jI-c4w.jpeg",
                        "isPro": false,
                        "fullname": "Amr Mohamed",
                        "user": "amr-mohamed",
                        "type": "user"
                    },
                    "name": "Amr Mohamed",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-11T10:12:35.890Z",
                    "hidden": false
                },
                {
                    "_id": "693a4c4d74fced5bf9c32494",
                    "name": "Yang Zhang",
                    "hidden": false
                },
                {
                    "_id": "693a4c4d74fced5bf9c32495",
                    "user": {
                        "_id": "6839c2d132331eaf76bea940",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U2daTtJiD-9JYYnbr4Xbu.png",
                        "isPro": false,
                        "fullname": "Michalis Vazirgiannis",
                        "user": "mvazirg",
                        "type": "user"
                    },
                    "name": "Michalis Vazirgiannis",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-11T10:48:25.471Z",
                    "hidden": false
                },
                {
                    "_id": "693a4c4d74fced5bf9c32496",
                    "user": {
                        "_id": "6087e598e2b7cc3a117b0dc5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6087e598e2b7cc3a117b0dc5/Ctz_W-uo1gOQRBHXalD1P.png",
                        "isPro": false,
                        "fullname": "Guokan Shang",
                        "user": "guokan-shang",
                        "type": "user"
                    },
                    "name": "Guokan Shang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-11T10:48:31.176Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-02T16:01:08.000Z",
            "submittedOnDailyAt": "2025-12-11T02:21:01.009Z",
            "title": "Fast-Decoding Diffusion Language Models via Progress-Aware Confidence Schedules",
            "submittedOnDailyBy": {
                "_id": "655efd24afee0e00788bb589",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655efd24afee0e00788bb589/22guLxIWNybbJR3jI-c4w.jpeg",
                "isPro": false,
                "fullname": "Amr Mohamed",
                "user": "amr-mohamed",
                "type": "user"
            },
            "summary": "Diffusion large language models (dLLMs) offer a promising alternative to autoregressive models, but their practical utility is severely hampered by slow, iterative sampling. We present SchED, a training-free, model-agnostic early-exit algorithm that aggregates full-span logit margins and halts decoding once a smooth, progress-dependent confidence threshold is met. We evaluated SchED on two dLLM families (Dream and LLaDA), in base and instruction-tuned variants across ten benchmarks spanning downstream tasks including multiple-choice question answering (MCQ), math, long-form QA/summarization, and translation. SchED delivers large, stable accelerations: on instruction-tuned models, it achieves 3.8-4.0times speedups while retaining 99.8-100% of the baseline score on average. On base models, SchED yields consistent speedup gains with 99.1-100% performance retention, with up to 2.34times under more aggressive settings. Using a conservative speed metric that heavily penalizes quality loss (QPS, {=}4), we show that SchED is robust and clearly outperforms prior confidence-based early-exit methods, which break down on long-form generation. An entropy analysis of the model's token predictions reveals that instruction tuning speeds up the decay of predictive entropy. By turning genuine confidence stabilization into computational savings, SchED makes dLLM decoding substantially more efficient.",
            "upvotes": 8,
            "discussionId": "693a4c4d74fced5bf9c32497",
            "ai_summary": "SchED, a training-free early-exit algorithm, accelerates diffusion large language model decoding with minimal performance loss across various tasks.",
            "ai_keywords": [
                "diffusion large language models",
                "dLLMs",
                "autoregressive models",
                "SchED",
                "early-exit algorithm",
                "full-span logit margins",
                "confidence threshold",
                "multiple-choice question answering",
                "math",
                "long-form QA/summarization",
                "translation",
                "instruction-tuned models",
                "base models",
                "QPS",
                "entropy analysis",
                "predictive entropy"
            ]
        },
        "publishedAt": "2025-12-02T11:01:08.000Z",
        "title": "Fast-Decoding Diffusion Language Models via Progress-Aware Confidence Schedules",
        "summary": "Diffusion large language models (dLLMs) offer a promising alternative to autoregressive models, but their practical utility is severely hampered by slow, iterative sampling. We present SchED, a training-free, model-agnostic early-exit algorithm that aggregates full-span logit margins and halts decoding once a smooth, progress-dependent confidence threshold is met. We evaluated SchED on two dLLM families (Dream and LLaDA), in base and instruction-tuned variants across ten benchmarks spanning downstream tasks including multiple-choice question answering (MCQ), math, long-form QA/summarization, and translation. SchED delivers large, stable accelerations: on instruction-tuned models, it achieves 3.8-4.0times speedups while retaining 99.8-100% of the baseline score on average. On base models, SchED yields consistent speedup gains with 99.1-100% performance retention, with up to 2.34times under more aggressive settings. Using a conservative speed metric that heavily penalizes quality loss (QPS, {=}4), we show that SchED is robust and clearly outperforms prior confidence-based early-exit methods, which break down on long-form generation. An entropy analysis of the model's token predictions reveals that instruction tuning speeds up the decay of predictive entropy. By turning genuine confidence stabilization into computational savings, SchED makes dLLM decoding substantially more efficient.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02892.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "655efd24afee0e00788bb589",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655efd24afee0e00788bb589/22guLxIWNybbJR3jI-c4w.jpeg",
            "fullname": "Amr Mohamed",
            "name": "amr-mohamed",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 10
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.04753",
            "authors": [
                {
                    "_id": "693a66ff74fced5bf9c324db",
                    "name": "Ruilin Li",
                    "hidden": false
                },
                {
                    "_id": "693a66ff74fced5bf9c324dc",
                    "user": {
                        "_id": "654c6845bac6e6e49895a5b5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/KXQaAxulqr8jNBSpEaYM4.png",
                        "isPro": false,
                        "fullname": "SII-Yibin Wang",
                        "user": "CodeGoat24",
                        "type": "user"
                    },
                    "name": "Yibin Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-11T10:49:48.784Z",
                    "hidden": false
                },
                {
                    "_id": "693a66ff74fced5bf9c324dd",
                    "name": "Wenhong Zhu",
                    "hidden": false
                },
                {
                    "_id": "693a66ff74fced5bf9c324de",
                    "name": "Chenglin Li",
                    "hidden": false
                },
                {
                    "_id": "693a66ff74fced5bf9c324df",
                    "name": "Jinghao Zhang",
                    "hidden": false
                },
                {
                    "_id": "693a66ff74fced5bf9c324e0",
                    "name": "Chenliang Li",
                    "hidden": false
                },
                {
                    "_id": "693a66ff74fced5bf9c324e1",
                    "user": {
                        "_id": "667289f903c802764985d8c6",
                        "avatarUrl": "/avatars/916befcbf0e52ce56be49617f31c7bb2.svg",
                        "isPro": false,
                        "fullname": "Junchi Yan",
                        "user": "Rethinker",
                        "type": "user"
                    },
                    "name": "Junchi Yan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-11T10:50:22.116Z",
                    "hidden": false
                },
                {
                    "_id": "693a66ff74fced5bf9c324e2",
                    "name": "Jiaqi Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-04T12:43:50.000Z",
            "submittedOnDailyAt": "2025-12-11T04:11:11.838Z",
            "title": "EtCon: Edit-then-Consolidate for Reliable Knowledge Editing",
            "submittedOnDailyBy": {
                "_id": "654c6845bac6e6e49895a5b5",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/KXQaAxulqr8jNBSpEaYM4.png",
                "isPro": false,
                "fullname": "SII-Yibin Wang",
                "user": "CodeGoat24",
                "type": "user"
            },
            "summary": "Knowledge editing aims to update specific facts in large language models (LLMs) without full retraining. Prior efforts sought to tune the knowledge layers of LLMs, proving effective for making selective edits. However, a significant gap exists between their performance in controlled, teacher-forcing evaluations and their real-world effectiveness in lifelong learning scenarios, which greatly limits their practical applicability. This work's empirical analysis reveals two recurring issues associated with this gap: (1) Most traditional methods lead the edited model to overfit to the new fact, thereby degrading pre-trained capabilities; (2) There is a critical absence of a knowledge consolidation stage, leaving new facts insufficiently integrated into LLMs' inference-time behavior under autoregressive generation, thereby leading to a mismatch between parametric knowledge and actual generation behavior. To this end, we propose Edit-then-Consolidate, a novel knowledge editing paradigm that aims to bridge the gap between theoretical knowledge editing methods and their real-world applicability. Specifically, (1) our framework mitigates overfitting via Targeted Proximal Supervised Fine-Tuning (TPSFT) that localizes the edit via a trust-region objective to limit policy drift; (2) Then, a consolidation stage using Group Relative Policy Optimization (GRPO) aligns the edited knowledge with CoT-based inference policy by optimizing trajectory-level behavior under comprehensive reward signals. Extensive experiments demonstrate our framework consistently improves editing reliability and generalization under real-world evaluations, while better preserving locality and pre-trained capabilities.",
            "upvotes": 7,
            "discussionId": "693a670074fced5bf9c324e3",
            "githubRepo": "https://github.com/RlinL/EtCon",
            "githubRepoAddedBy": "user",
            "ai_summary": "A novel knowledge editing framework, Edit-then-Consolidate, addresses overfitting and lack of knowledge integration in large language models through targeted fine-tuning and policy optimization, enhancing reliability and generalization.",
            "ai_keywords": [
                "knowledge editing",
                "large language models",
                "teacher-forcing evaluations",
                "lifelong learning",
                "overfitting",
                "knowledge consolidation",
                "Targeted Proximal Supervised Fine-Tuning",
                "trust-region objective",
                "policy drift",
                "Group Relative Policy Optimization",
                "trajectory-level behavior",
                "comprehensive reward signals",
                "CoT-based inference policy"
            ],
            "githubStars": 5
        },
        "publishedAt": "2025-12-04T07:43:50.000Z",
        "title": "EtCon: Edit-then-Consolidate for Reliable Knowledge Editing",
        "summary": "Knowledge editing aims to update specific facts in large language models (LLMs) without full retraining. Prior efforts sought to tune the knowledge layers of LLMs, proving effective for making selective edits. However, a significant gap exists between their performance in controlled, teacher-forcing evaluations and their real-world effectiveness in lifelong learning scenarios, which greatly limits their practical applicability. This work's empirical analysis reveals two recurring issues associated with this gap: (1) Most traditional methods lead the edited model to overfit to the new fact, thereby degrading pre-trained capabilities; (2) There is a critical absence of a knowledge consolidation stage, leaving new facts insufficiently integrated into LLMs' inference-time behavior under autoregressive generation, thereby leading to a mismatch between parametric knowledge and actual generation behavior. To this end, we propose Edit-then-Consolidate, a novel knowledge editing paradigm that aims to bridge the gap between theoretical knowledge editing methods and their real-world applicability. Specifically, (1) our framework mitigates overfitting via Targeted Proximal Supervised Fine-Tuning (TPSFT) that localizes the edit via a trust-region objective to limit policy drift; (2) Then, a consolidation stage using Group Relative Policy Optimization (GRPO) aligns the edited knowledge with CoT-based inference policy by optimizing trajectory-level behavior under comprehensive reward signals. Extensive experiments demonstrate our framework consistently improves editing reliability and generalization under real-world evaluations, while better preserving locality and pre-trained capabilities.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04753.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "654c6845bac6e6e49895a5b5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/KXQaAxulqr8jNBSpEaYM4.png",
            "fullname": "SII-Yibin Wang",
            "name": "CodeGoat24",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 19
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.09864",
            "authors": [
                {
                    "_id": "693a387b74fced5bf9c3241f",
                    "name": "Hao Lu",
                    "hidden": false
                },
                {
                    "_id": "693a387b74fced5bf9c32420",
                    "name": "Ziyang Liu",
                    "hidden": false
                },
                {
                    "_id": "693a387b74fced5bf9c32421",
                    "user": {
                        "_id": "660146b2bcf0790b8d73e447",
                        "avatarUrl": "/avatars/726be122297a03585c107f44f7aa0ed8.svg",
                        "isPro": false,
                        "fullname": "jiang guangfeng",
                        "user": "jiangxb24",
                        "type": "user"
                    },
                    "name": "Guangfeng Jiang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-11T10:49:22.527Z",
                    "hidden": false
                },
                {
                    "_id": "693a387b74fced5bf9c32422",
                    "name": "Yuanfei Luo",
                    "hidden": false
                },
                {
                    "_id": "693a387b74fced5bf9c32423",
                    "name": "Sheng Chen",
                    "hidden": false
                },
                {
                    "_id": "693a387b74fced5bf9c32424",
                    "name": "Yangang Zhang",
                    "hidden": false
                },
                {
                    "_id": "693a387b74fced5bf9c32425",
                    "user": {
                        "_id": "655cba1d87b67834000590e8",
                        "avatarUrl": "/avatars/3bd43b7c9351f65b8f38f4c8237a0146.svg",
                        "isPro": false,
                        "fullname": "Yingcong Chen",
                        "user": "yingcongchen",
                        "type": "user"
                    },
                    "name": "Ying-Cong Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-11T10:48:52.786Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/t3b2dEDsShvqIai2Iit1B.mp4"
            ],
            "publishedAt": "2025-12-10T17:50:29.000Z",
            "submittedOnDailyAt": "2025-12-11T00:51:07.730Z",
            "title": "UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Autonomous driving (AD) systems struggle in long-tail scenarios due to limited world knowledge and weak visual dynamic modeling. Existing vision-language-action (VLA)-based methods cannot leverage unlabeled videos for visual causal learning, while world model-based methods lack reasoning capabilities from large language models. In this paper, we construct multiple specialized datasets providing reasoning and planning annotations for complex scenarios. Then, a unified Understanding-Generation-Planning framework, named UniUGP, is proposed to synergize scene reasoning, future video generation, and trajectory planning through a hybrid expert architecture. By integrating pre-trained VLMs and video generation models, UniUGP leverages visual dynamics and semantic reasoning to enhance planning performance. Taking multi-frame observations and language instructions as input, it produces interpretable chain-of-thought reasoning, physically consistent trajectories, and coherent future videos. We introduce a four-stage training strategy that progressively builds these capabilities across multiple existing AD datasets, along with the proposed specialized datasets. Experiments demonstrate state-of-the-art performance in perception, reasoning, and decision-making, with superior generalization to challenging long-tail situations.",
            "upvotes": 6,
            "discussionId": "693a387b74fced5bf9c32426",
            "ai_summary": "A unified framework combines vision-language models and video generation to improve autonomous driving in complex scenarios by enhancing reasoning, trajectory planning, and video generation.",
            "ai_keywords": [
                "vision-language-action",
                "world model",
                "Understanding-Generation-Planning",
                "VLMs",
                "video generation models",
                "chain-of-thought reasoning",
                "physically consistent trajectories",
                "coherent future videos"
            ],
            "organization": {
                "_id": "67d1140985ea0644e2f14b99",
                "name": "ByteDance-Seed",
                "fullname": "ByteDance Seed",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
            }
        },
        "publishedAt": "2025-12-10T12:50:29.000Z",
        "title": "UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving",
        "summary": "Autonomous driving (AD) systems struggle in long-tail scenarios due to limited world knowledge and weak visual dynamic modeling. Existing vision-language-action (VLA)-based methods cannot leverage unlabeled videos for visual causal learning, while world model-based methods lack reasoning capabilities from large language models. In this paper, we construct multiple specialized datasets providing reasoning and planning annotations for complex scenarios. Then, a unified Understanding-Generation-Planning framework, named UniUGP, is proposed to synergize scene reasoning, future video generation, and trajectory planning through a hybrid expert architecture. By integrating pre-trained VLMs and video generation models, UniUGP leverages visual dynamics and semantic reasoning to enhance planning performance. Taking multi-frame observations and language instructions as input, it produces interpretable chain-of-thought reasoning, physically consistent trajectories, and coherent future videos. We introduce a four-stage training strategy that progressively builds these capabilities across multiple existing AD datasets, along with the proposed specialized datasets. Experiments demonstrate state-of-the-art performance in perception, reasoning, and decision-making, with superior generalization to challenging long-tail situations.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/t3b2dEDsShvqIai2Iit1B.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.09864.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 181
        },
        "organization": {
            "_id": "67d1140985ea0644e2f14b99",
            "name": "ByteDance-Seed",
            "fullname": "ByteDance Seed",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.09164",
            "authors": [
                {
                    "_id": "693a392174fced5bf9c32428",
                    "name": "Jin Cao",
                    "hidden": false
                },
                {
                    "_id": "693a392174fced5bf9c32429",
                    "name": "Hong-Xing Yu",
                    "hidden": false
                },
                {
                    "_id": "693a392174fced5bf9c3242a",
                    "name": "Jiajun Wu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/ikvAYAxXPrZFYikFpzXIN.mp4"
            ],
            "publishedAt": "2025-12-09T22:21:07.000Z",
            "submittedOnDailyAt": "2025-12-11T00:53:27.429Z",
            "title": "WonderZoom: Multi-Scale 3D World Generation",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We present WonderZoom, a novel approach to generating 3D scenes with contents across multiple spatial scales from a single image. Existing 3D world generation models remain limited to single-scale synthesis and cannot produce coherent scene contents at varying granularities. The fundamental challenge is the lack of a scale-aware 3D representation capable of generating and rendering content with largely different spatial sizes. WonderZoom addresses this through two key innovations: (1) scale-adaptive Gaussian surfels for generating and real-time rendering of multi-scale 3D scenes, and (2) a progressive detail synthesizer that iteratively generates finer-scale 3D contents. Our approach enables users to \"zoom into\" a 3D region and auto-regressively synthesize previously non-existent fine details from landscapes to microscopic features. Experiments demonstrate that WonderZoom significantly outperforms state-of-the-art video and 3D models in both quality and alignment, enabling multi-scale 3D world creation from a single image. We show video results and an interactive viewer of generated multi-scale 3D worlds in https://wonderzoom.github.io/",
            "upvotes": 6,
            "discussionId": "693a392174fced5bf9c3242b",
            "ai_summary": "WonderZoom generates multi-scale 3D scenes from a single image using scale-adaptive Gaussian surfels and a progressive detail synthesizer, outperforming existing models in quality and alignment.",
            "ai_keywords": [
                "scale-adaptive Gaussian surfels",
                "progressive detail synthesizer",
                "multi-scale 3D scenes",
                "single image",
                "3D world generation",
                "real-time rendering",
                "fine details",
                "video results",
                "interactive viewer"
            ]
        },
        "publishedAt": "2025-12-09T17:21:07.000Z",
        "title": "WonderZoom: Multi-Scale 3D World Generation",
        "summary": "We present WonderZoom, a novel approach to generating 3D scenes with contents across multiple spatial scales from a single image. Existing 3D world generation models remain limited to single-scale synthesis and cannot produce coherent scene contents at varying granularities. The fundamental challenge is the lack of a scale-aware 3D representation capable of generating and rendering content with largely different spatial sizes. WonderZoom addresses this through two key innovations: (1) scale-adaptive Gaussian surfels for generating and real-time rendering of multi-scale 3D scenes, and (2) a progressive detail synthesizer that iteratively generates finer-scale 3D contents. Our approach enables users to \"zoom into\" a 3D region and auto-regressively synthesize previously non-existent fine details from landscapes to microscopic features. Experiments demonstrate that WonderZoom significantly outperforms state-of-the-art video and 3D models in both quality and alignment, enabling multi-scale 3D world creation from a single image. We show video results and an interactive viewer of generated multi-scale 3D worlds in https://wonderzoom.github.io/",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/ikvAYAxXPrZFYikFpzXIN.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.09164.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 181
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.09106",
            "authors": [
                {
                    "_id": "693a36d474fced5bf9c323f2",
                    "user": {
                        "_id": "6662f75e338c960a6048728c",
                        "avatarUrl": "/avatars/9d01f63fa58fab25d4476f77064f457f.svg",
                        "isPro": false,
                        "fullname": "Metod Jazbec",
                        "user": "mjazbec",
                        "type": "user"
                    },
                    "name": "Metod Jazbec",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-11T10:51:43.631Z",
                    "hidden": false
                },
                {
                    "_id": "693a36d474fced5bf9c323f3",
                    "user": {
                        "_id": "6489e543b9e9258ba06d73de",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Xjs0N338heFNkRkvHg9hL.jpeg",
                        "isPro": false,
                        "fullname": "Theo X. Olausson",
                        "user": "theoxo",
                        "type": "user"
                    },
                    "name": "Theo X. Olausson",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-11T10:51:49.683Z",
                    "hidden": false
                },
                {
                    "_id": "693a36d474fced5bf9c323f4",
                    "user": {
                        "_id": "662a8d7faa7781f8ea521f16",
                        "avatarUrl": "/avatars/c05f9a7885b89c8818d52b98b52ecc7b.svg",
                        "isPro": false,
                        "fullname": "Louis Bethune",
                        "user": "AlgueRythme",
                        "type": "user"
                    },
                    "name": "Louis Bthune",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-11T10:51:55.984Z",
                    "hidden": false
                },
                {
                    "_id": "693a36d474fced5bf9c323f5",
                    "user": {
                        "_id": "68779eeb7ef0f82c9b75b1a8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/peQ2TU6FiNPJgNU30UQop.png",
                        "isPro": false,
                        "fullname": "Pierre Ablin",
                        "user": "pierreablin",
                        "type": "user"
                    },
                    "name": "Pierre Ablin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-11T10:52:01.637Z",
                    "hidden": false
                },
                {
                    "_id": "693a36d474fced5bf9c323f6",
                    "user": {
                        "_id": "6698ff8866813fa1b885f052",
                        "avatarUrl": "/avatars/49f3d379fa6ee134bb97f3fcd9447def.svg",
                        "isPro": false,
                        "fullname": "Michael Kirchhof",
                        "user": "michaelkir",
                        "type": "user"
                    },
                    "name": "Michael Kirchhof",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-11T10:52:07.133Z",
                    "hidden": false
                },
                {
                    "_id": "693a36d474fced5bf9c323f7",
                    "name": "Joao Monterio",
                    "hidden": false
                },
                {
                    "_id": "693a36d474fced5bf9c323f8",
                    "name": "Victor Turrisi",
                    "hidden": false
                },
                {
                    "_id": "693a36d474fced5bf9c323f9",
                    "name": "Jason Ramapuram",
                    "hidden": false
                },
                {
                    "_id": "693a36d474fced5bf9c323fa",
                    "user": {
                        "_id": "65d88dfda5269668b735a39b",
                        "avatarUrl": "/avatars/cf46727be075a51ab894efbe906ee25a.svg",
                        "isPro": false,
                        "fullname": "marco cuturi",
                        "user": "marcocuturi",
                        "type": "user"
                    },
                    "name": "Marco Cuturi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-11T10:52:24.596Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-09T20:44:33.000Z",
            "submittedOnDailyAt": "2025-12-11T00:43:32.812Z",
            "title": "Learning Unmasking Policies for Diffusion Language Models",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Diffusion (Large) Language Models (dLLMs) now match the downstream performance of their autoregressive counterparts on many tasks, while holding the promise of being more efficient during inference. One particularly successful variant is masked discrete diffusion, in which a buffer filled with special mask tokens is progressively replaced with tokens sampled from the model's vocabulary. Efficiency can be gained by unmasking several tokens in parallel, but doing too many at once risks degrading the generation quality. Thus, one critical design aspect of dLLMs is the sampling procedure that selects, at each step of the diffusion process, which tokens to replace. Indeed, recent work has found that heuristic strategies such as confidence thresholding lead to both higher quality and token throughput compared to random unmasking. However, such heuristics have downsides: they require manual tuning, and we observe that their performance degrades with larger buffer sizes. In this work, we instead propose to train sampling procedures using reinforcement learning. Specifically, we formalize masked diffusion sampling as a Markov decision process in which the dLLM serves as the environment, and propose a lightweight policy architecture based on a single-layer transformer that maps dLLM token confidences to unmasking decisions. Our experiments show that these trained policies match the performance of state-of-the-art heuristics when combined with semi-autoregressive generation, while outperforming them in the full diffusion setting. We also examine the transferability of these policies, finding that they can generalize to new underlying dLLMs and longer sequence lengths. However, we also observe that their performance degrades when applied to out-of-domain data, and that fine-grained tuning of the accuracy-efficiency trade-off can be challenging with our approach.",
            "upvotes": 4,
            "discussionId": "693a36d474fced5bf9c323fb",
            "ai_summary": "Reinforcement learning is used to train sampling procedures for masked discrete diffusion language models, improving token throughput and quality compared to heuristic strategies.",
            "ai_keywords": [
                "diffusion (large) language models",
                "masked discrete diffusion",
                "buffer",
                "mask tokens",
                "sampling procedure",
                "confidence thresholding",
                "reinforcement learning",
                "Markov decision process",
                "single-layer transformer",
                "semi-autoregressive generation",
                "full diffusion setting",
                "transferability",
                "out-of-domain data",
                "accuracy-efficiency trade-off"
            ],
            "organization": {
                "_id": "628cbd99ef14f971b69948ab",
                "name": "apple",
                "fullname": "Apple",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
            }
        },
        "publishedAt": "2025-12-09T15:44:33.000Z",
        "title": "Learning Unmasking Policies for Diffusion Language Models",
        "summary": "Diffusion (Large) Language Models (dLLMs) now match the downstream performance of their autoregressive counterparts on many tasks, while holding the promise of being more efficient during inference. One particularly successful variant is masked discrete diffusion, in which a buffer filled with special mask tokens is progressively replaced with tokens sampled from the model's vocabulary. Efficiency can be gained by unmasking several tokens in parallel, but doing too many at once risks degrading the generation quality. Thus, one critical design aspect of dLLMs is the sampling procedure that selects, at each step of the diffusion process, which tokens to replace. Indeed, recent work has found that heuristic strategies such as confidence thresholding lead to both higher quality and token throughput compared to random unmasking. However, such heuristics have downsides: they require manual tuning, and we observe that their performance degrades with larger buffer sizes. In this work, we instead propose to train sampling procedures using reinforcement learning. Specifically, we formalize masked diffusion sampling as a Markov decision process in which the dLLM serves as the environment, and propose a lightweight policy architecture based on a single-layer transformer that maps dLLM token confidences to unmasking decisions. Our experiments show that these trained policies match the performance of state-of-the-art heuristics when combined with semi-autoregressive generation, while outperforming them in the full diffusion setting. We also examine the transferability of these policies, finding that they can generalize to new underlying dLLMs and longer sequence lengths. However, we also observe that their performance degrades when applied to out-of-domain data, and that fine-grained tuning of the accuracy-efficiency trade-off can be challenging with our approach.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.09106.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 181
        },
        "organization": {
            "_id": "628cbd99ef14f971b69948ab",
            "name": "apple",
            "fullname": "Apple",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.08296",
            "authors": [
                {
                    "_id": "693aad7ee1612ff4bc2a4946",
                    "name": "Yubin Kim",
                    "hidden": false
                },
                {
                    "_id": "693aad7ee1612ff4bc2a4947",
                    "name": "Ken Gu",
                    "hidden": false
                },
                {
                    "_id": "693aad7ee1612ff4bc2a4948",
                    "name": "Chanwoo Park",
                    "hidden": false
                },
                {
                    "_id": "693aad7ee1612ff4bc2a4949",
                    "name": "Chunjong Park",
                    "hidden": false
                },
                {
                    "_id": "693aad7ee1612ff4bc2a494a",
                    "name": "Samuel Schmidgall",
                    "hidden": false
                },
                {
                    "_id": "693aad7ee1612ff4bc2a494b",
                    "name": "A. Ali Heydari",
                    "hidden": false
                },
                {
                    "_id": "693aad7ee1612ff4bc2a494c",
                    "name": "Yao Yan",
                    "hidden": false
                },
                {
                    "_id": "693aad7ee1612ff4bc2a494d",
                    "name": "Zhihan Zhang",
                    "hidden": false
                },
                {
                    "_id": "693aad7ee1612ff4bc2a494e",
                    "name": "Yuchen Zhuang",
                    "hidden": false
                },
                {
                    "_id": "693aad7ee1612ff4bc2a494f",
                    "name": "Mark Malhotra",
                    "hidden": false
                },
                {
                    "_id": "693aad7ee1612ff4bc2a4950",
                    "name": "Paul Pu Liang",
                    "hidden": false
                },
                {
                    "_id": "693aad7ee1612ff4bc2a4951",
                    "name": "Hae Won Park",
                    "hidden": false
                },
                {
                    "_id": "693aad7ee1612ff4bc2a4952",
                    "name": "Yuzhe Yang",
                    "hidden": false
                },
                {
                    "_id": "693aad7ee1612ff4bc2a4953",
                    "name": "Xuhai Xu",
                    "hidden": false
                },
                {
                    "_id": "693aad7ee1612ff4bc2a4954",
                    "name": "Yilun Du",
                    "hidden": false
                },
                {
                    "_id": "693aad7ee1612ff4bc2a4955",
                    "name": "Shwetak Patel",
                    "hidden": false
                },
                {
                    "_id": "693aad7ee1612ff4bc2a4956",
                    "name": "Tim Althoff",
                    "hidden": false
                },
                {
                    "_id": "693aad7ee1612ff4bc2a4957",
                    "name": "Daniel McDuff",
                    "hidden": false
                },
                {
                    "_id": "693aad7ee1612ff4bc2a4958",
                    "name": "Xin Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-09T06:52:21.000Z",
            "submittedOnDailyAt": "2025-12-11T15:43:52.249Z",
            "title": "Towards a Science of Scaling Agent Systems",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": true,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "Agents, language model (LM)-based systems that are capable of reasoning, planning, and acting are becoming the dominant paradigm for real-world AI applications. Despite this widespread adoption, the principles that determine their performance remain underexplored, leaving practitioners to rely on heuristics rather than principled design choices. We address this gap by deriving quantitative scaling principles for agent systems. We evaluate this across four diverse benchmarks: Finance-Agent, BrowseComp-Plus, PlanCraft, and Workbench. Using five canonical architectures (Single, Independent, Centralized, Decentralized, Hybrid) instantiated across three LLM families, we perform a controlled evaluation spanning 180 configurations with standardized tools and token budgets. We derive a predictive model using empirical coordination metrics, including efficiency, overhead, error amplification, and redundancy, that achieves cross-validated R^2=0.513. We identify three dominant effects: (1) a tool-coordination trade-off: under fixed computational budgets, tool-heavy tasks suffer disproportionately from multi-agent overhead. (2) a capability saturation: coordination yields diminishing or negative returns (beta=-0.408, p<0.001) once single-agent baselines exceed ~45%. (3) topology-dependent error amplification: independent agents amplify errors 17.2x through unchecked propagation, while centralized coordination contains this to 4.4x. Centralized coordination improves performance by 80.9% on parallelizable tasks like financial reasoning, while decentralized coordination excels on dynamic web navigation (+9.2% vs. +0.2%). Yet for sequential reasoning tasks, all multi-agent variants degraded performance by 39-70%. The framework predicts the optimal coordination strategy for 87% of held-out configurations, providing a predictive principle of agentic scaling based on measurable task properties.",
            "upvotes": 4,
            "discussionId": "693aad7fe1612ff4bc2a4959",
            "ai_summary": "A quantitative framework for agent system scaling using empirical coordination metrics identifies optimal multi-agent strategies based on task properties.",
            "ai_keywords": [
                "agent systems",
                "language model (LM)",
                "reasoning",
                "planning",
                "acting",
                "quantitative scaling principles",
                "Finance-Agent",
                "BrowseComp-Plus",
                "PlanCraft",
                "Workbench",
                "Single",
                "Independent",
                "Centralized",
                "Decentralized",
                "Hybrid",
                "LLM families",
                "coordination metrics",
                "efficiency",
                "overhead",
                "error amplification",
                "redundancy",
                "predictive model",
                "tool-coordination trade-off",
                "capability saturation",
                "topology-dependent error amplification",
                "centralized coordination",
                "decentralized coordination",
                "sequential reasoning tasks",
                "agentic scaling"
            ]
        },
        "publishedAt": "2025-12-09T01:52:21.000Z",
        "title": "Towards a Science of Scaling Agent Systems",
        "summary": "Agents, language model (LM)-based systems that are capable of reasoning, planning, and acting are becoming the dominant paradigm for real-world AI applications. Despite this widespread adoption, the principles that determine their performance remain underexplored, leaving practitioners to rely on heuristics rather than principled design choices. We address this gap by deriving quantitative scaling principles for agent systems. We evaluate this across four diverse benchmarks: Finance-Agent, BrowseComp-Plus, PlanCraft, and Workbench. Using five canonical architectures (Single, Independent, Centralized, Decentralized, Hybrid) instantiated across three LLM families, we perform a controlled evaluation spanning 180 configurations with standardized tools and token budgets. We derive a predictive model using empirical coordination metrics, including efficiency, overhead, error amplification, and redundancy, that achieves cross-validated R^2=0.513. We identify three dominant effects: (1) a tool-coordination trade-off: under fixed computational budgets, tool-heavy tasks suffer disproportionately from multi-agent overhead. (2) a capability saturation: coordination yields diminishing or negative returns (beta=-0.408, p<0.001) once single-agent baselines exceed ~45%. (3) topology-dependent error amplification: independent agents amplify errors 17.2x through unchecked propagation, while centralized coordination contains this to 4.4x. Centralized coordination improves performance by 80.9% on parallelizable tasks like financial reasoning, while decentralized coordination excels on dynamic web navigation (+9.2% vs. +0.2%). Yet for sequential reasoning tasks, all multi-agent variants degraded performance by 39-70%. The framework predicts the optimal coordination strategy for 87% of held-out configurations, providing a predictive principle of agentic scaling based on measurable task properties.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.08296.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": true,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8892
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.09663",
            "authors": [
                {
                    "_id": "693a4a8b74fced5bf9c32489",
                    "name": "Tao Zhang",
                    "hidden": false
                },
                {
                    "_id": "693a4a8b74fced5bf9c3248a",
                    "user": {
                        "_id": "66e03d7123e5f162e7e5c5ac",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66e03d7123e5f162e7e5c5ac/PHHGlQKnZhr_BNpciB9Zz.jpeg",
                        "isPro": false,
                        "fullname": "hongyuyang",
                        "user": "hongyuyang23casia",
                        "type": "user"
                    },
                    "name": "Yuyang Hong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-11T10:12:37.559Z",
                    "hidden": false
                },
                {
                    "_id": "693a4a8b74fced5bf9c3248b",
                    "name": "Yang Xia",
                    "hidden": false
                },
                {
                    "_id": "693a4a8b74fced5bf9c3248c",
                    "name": "Kun Ding",
                    "hidden": false
                },
                {
                    "_id": "693a4a8b74fced5bf9c3248d",
                    "name": "Zeyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "693a4a8b74fced5bf9c3248e",
                    "name": "Ying Wang",
                    "hidden": false
                },
                {
                    "_id": "693a4a8b74fced5bf9c3248f",
                    "name": "Shiming Xiang",
                    "hidden": false
                },
                {
                    "_id": "693a4a8b74fced5bf9c32490",
                    "name": "Chunhong Pan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-10T14:01:02.000Z",
            "submittedOnDailyAt": "2025-12-11T08:16:55.074Z",
            "title": "IF-Bench: Benchmarking and Enhancing MLLMs for Infrared Images with Generative Visual Prompting",
            "submittedOnDailyBy": {
                "_id": "66e03d7123e5f162e7e5c5ac",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66e03d7123e5f162e7e5c5ac/PHHGlQKnZhr_BNpciB9Zz.jpeg",
                "isPro": false,
                "fullname": "hongyuyang",
                "user": "hongyuyang23casia",
                "type": "user"
            },
            "summary": "Recent advances in multimodal large language models (MLLMs) have led to impressive progress across various benchmarks. However, their capability in understanding infrared images remains unexplored. To address this gap, we introduce IF-Bench, the first high-quality benchmark designed for evaluating multimodal understanding of infrared images. IF-Bench consists of 499 images sourced from 23 infrared datasets and 680 carefully curated visual question-answer pairs, covering 10 essential dimensions of image understanding. Based on this benchmark, we systematically evaluate over 40 open-source and closed-source MLLMs, employing cyclic evaluation, bilingual assessment, and hybrid judgment strategies to enhance the reliability of the results. Our analysis reveals how model scale, architecture, and inference paradigms affect infrared image comprehension, providing valuable insights for this area. Furthermore, we propose a training-free generative visual prompting (GenViP) method, which leverages advanced image editing models to translate infrared images into semantically and spatially aligned RGB counterparts, thereby mitigating domain distribution shifts. Extensive experiments demonstrate that our method consistently yields significant performance improvements across a wide range of MLLMs. The benchmark and code are available at https://github.com/casiatao/IF-Bench.",
            "upvotes": 3,
            "discussionId": "693a4a8c74fced5bf9c32491",
            "ai_summary": "The introduction of IF-Bench evaluates multimodal large language models' performance on infrared images using diverse assessment strategies and proposes a training-free method to improve comprehension.",
            "ai_keywords": [
                "multimodal large language models",
                "MLLMs",
                "IF-Bench",
                "infrared images",
                "visual question-answer pairs",
                "cyclic evaluation",
                "bilingual assessment",
                "hybrid judgment strategies",
                "model scale",
                "architecture",
                "inference paradigms",
                "generative visual prompting",
                "GenViP",
                "domain distribution shifts"
            ],
            "organization": {
                "_id": "640a887796aae649741a586f",
                "name": "CASIA",
                "fullname": "Chinese Academic of Science Institute of Automation",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1678411888885-6388984e8a5dbe2f3dc5afee.jpeg"
            }
        },
        "publishedAt": "2025-12-10T09:01:02.000Z",
        "title": "IF-Bench: Benchmarking and Enhancing MLLMs for Infrared Images with Generative Visual Prompting",
        "summary": "Recent advances in multimodal large language models (MLLMs) have led to impressive progress across various benchmarks. However, their capability in understanding infrared images remains unexplored. To address this gap, we introduce IF-Bench, the first high-quality benchmark designed for evaluating multimodal understanding of infrared images. IF-Bench consists of 499 images sourced from 23 infrared datasets and 680 carefully curated visual question-answer pairs, covering 10 essential dimensions of image understanding. Based on this benchmark, we systematically evaluate over 40 open-source and closed-source MLLMs, employing cyclic evaluation, bilingual assessment, and hybrid judgment strategies to enhance the reliability of the results. Our analysis reveals how model scale, architecture, and inference paradigms affect infrared image comprehension, providing valuable insights for this area. Furthermore, we propose a training-free generative visual prompting (GenViP) method, which leverages advanced image editing models to translate infrared images into semantically and spatially aligned RGB counterparts, thereby mitigating domain distribution shifts. Extensive experiments demonstrate that our method consistently yields significant performance improvements across a wide range of MLLMs. The benchmark and code are available at https://github.com/casiatao/IF-Bench.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.09663.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66e03d7123e5f162e7e5c5ac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66e03d7123e5f162e7e5c5ac/PHHGlQKnZhr_BNpciB9Zz.jpeg",
            "fullname": "hongyuyang",
            "name": "hongyuyang23casia",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "organization": {
            "_id": "640a887796aae649741a586f",
            "name": "CASIA",
            "fullname": "Chinese Academic of Science Institute of Automation",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1678411888885-6388984e8a5dbe2f3dc5afee.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.05446",
            "authors": [
                {
                    "_id": "693a78c774fced5bf9c32529",
                    "name": "Cheng-Yuan Ho",
                    "hidden": false
                },
                {
                    "_id": "693a78c774fced5bf9c3252a",
                    "name": "He-Bi Yang",
                    "hidden": false
                },
                {
                    "_id": "693a78c774fced5bf9c3252b",
                    "name": "Jui-Chiu Chiang",
                    "hidden": false
                },
                {
                    "_id": "693a78c774fced5bf9c3252c",
                    "user": {
                        "_id": "6459d5da3b6fafd9664807ab",
                        "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
                        "isPro": false,
                        "fullname": "Yu-Lun Liu",
                        "user": "yulunliu",
                        "type": "user"
                    },
                    "name": "Yu-Lun Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-11T10:53:41.993Z",
                    "hidden": false
                },
                {
                    "_id": "693a78c774fced5bf9c3252d",
                    "name": "Wen-Hsiao Peng",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/ufOCRvkV9oBzxsWmLIbP3.png"
            ],
            "publishedAt": "2025-12-05T05:46:35.000Z",
            "submittedOnDailyAt": "2025-12-11T05:25:55.176Z",
            "title": "TED-4DGS: Temporally Activated and Embedding-based Deformation for 4DGS Compression",
            "submittedOnDailyBy": {
                "_id": "6459d5da3b6fafd9664807ab",
                "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
                "isPro": false,
                "fullname": "Yu-Lun Liu",
                "user": "yulunliu",
                "type": "user"
            },
            "summary": "Building on the success of 3D Gaussian Splatting (3DGS) in static 3D scene representation, its extension to dynamic scenes, commonly referred to as 4DGS or dynamic 3DGS, has attracted increasing attention. However, designing more compact and efficient deformation schemes together with rate-distortion-optimized compression strategies for dynamic 3DGS representations remains an underexplored area. Prior methods either rely on space-time 4DGS with overspecified, short-lived Gaussian primitives or on canonical 3DGS with deformation that lacks explicit temporal control. To address this, we present TED-4DGS, a temporally activated and embedding-based deformation scheme for rate-distortion-optimized 4DGS compression that unifies the strengths of both families. TED-4DGS is built on a sparse anchor-based 3DGS representation. Each canonical anchor is assigned learnable temporal-activation parameters to specify its appearance and disappearance transitions over time, while a lightweight per-anchor temporal embedding queries a shared deformation bank to produce anchor-specific deformation. For rate-distortion compression, we incorporate an implicit neural representation (INR)-based hyperprior to model anchor attribute distributions, along with a channel-wise autoregressive model to capture intra-anchor correlations. With these novel elements, our scheme achieves state-of-the-art rate-distortion performance on several real-world datasets. To the best of our knowledge, this work represents one of the first attempts to pursue a rate-distortion-optimized compression framework for dynamic 3DGS representations.",
            "upvotes": 3,
            "discussionId": "693a78c774fced5bf9c3252e",
            "ai_summary": "TED-4DGS combines sparse anchor-based 3D Gaussian Splatting with temporal activation and embedding for efficient and high-quality compression of dynamic 3D scenes.",
            "ai_keywords": [
                "Gaussian Splatting",
                "4DGS",
                "dynamic 3DGS",
                "temporally activated",
                "embedding-based deformation",
                "sparse anchor-based",
                "temporal-activation parameters",
                "temporal embedding",
                "deformation bank",
                "implicit neural representation",
                "hyperprior",
                "channel-wise autoregressive model",
                "rate-distortion performance"
            ],
            "organization": {
                "_id": "63e39e6499a032b1c950403d",
                "name": "NYCU",
                "fullname": "National Yang Ming Chiao Tung University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63e39df6c65f975b436bb6b8/WLWf1bSpvrXBYYKEdXbgU.png"
            }
        },
        "publishedAt": "2025-12-05T00:46:35.000Z",
        "title": "TED-4DGS: Temporally Activated and Embedding-based Deformation for 4DGS Compression",
        "summary": "Building on the success of 3D Gaussian Splatting (3DGS) in static 3D scene representation, its extension to dynamic scenes, commonly referred to as 4DGS or dynamic 3DGS, has attracted increasing attention. However, designing more compact and efficient deformation schemes together with rate-distortion-optimized compression strategies for dynamic 3DGS representations remains an underexplored area. Prior methods either rely on space-time 4DGS with overspecified, short-lived Gaussian primitives or on canonical 3DGS with deformation that lacks explicit temporal control. To address this, we present TED-4DGS, a temporally activated and embedding-based deformation scheme for rate-distortion-optimized 4DGS compression that unifies the strengths of both families. TED-4DGS is built on a sparse anchor-based 3DGS representation. Each canonical anchor is assigned learnable temporal-activation parameters to specify its appearance and disappearance transitions over time, while a lightweight per-anchor temporal embedding queries a shared deformation bank to produce anchor-specific deformation. For rate-distortion compression, we incorporate an implicit neural representation (INR)-based hyperprior to model anchor attribute distributions, along with a channel-wise autoregressive model to capture intra-anchor correlations. With these novel elements, our scheme achieves state-of-the-art rate-distortion performance on several real-world datasets. To the best of our knowledge, this work represents one of the first attempts to pursue a rate-distortion-optimized compression framework for dynamic 3DGS representations.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/ufOCRvkV9oBzxsWmLIbP3.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.05446.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6459d5da3b6fafd9664807ab",
            "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
            "fullname": "Yu-Lun Liu",
            "name": "yulunliu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "organization": {
            "_id": "63e39e6499a032b1c950403d",
            "name": "NYCU",
            "fullname": "National Yang Ming Chiao Tung University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63e39df6c65f975b436bb6b8/WLWf1bSpvrXBYYKEdXbgU.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.08006",
            "authors": [
                {
                    "_id": "693a67ea74fced5bf9c324ef",
                    "user": {
                        "_id": "657c45f6429a20edb520d7a8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657c45f6429a20edb520d7a8/jXVRbbaUzodZLupJiub84.png",
                        "isPro": false,
                        "fullname": "Mahta Fetrat",
                        "user": "MahtaFetrat",
                        "type": "user"
                    },
                    "name": "Mahta Fetrat",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-11T10:07:53.073Z",
                    "hidden": false
                },
                {
                    "_id": "693a67ea74fced5bf9c324f0",
                    "user": {
                        "_id": "6868e2cc118df64ef614984c",
                        "avatarUrl": "/avatars/10751b92746044c2556ec02f2778b3ea.svg",
                        "isPro": false,
                        "fullname": "Donya Navabi",
                        "user": "dninvb",
                        "type": "user"
                    },
                    "name": "Donya Navabi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-11T10:54:32.567Z",
                    "hidden": false
                },
                {
                    "_id": "693a67ea74fced5bf9c324f1",
                    "user": {
                        "_id": "675814b757e4bcad32c68d8b",
                        "avatarUrl": "/avatars/8f251e4a56b5c046218b8398a1c4bcfc.svg",
                        "isPro": false,
                        "fullname": "Dehghanian",
                        "user": "ZahraDehghanian97",
                        "type": "user"
                    },
                    "name": "Zahra Dehghanian",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-11T10:54:38.768Z",
                    "hidden": false
                },
                {
                    "_id": "693a67ea74fced5bf9c324f2",
                    "name": "Morteza Abolghasemi",
                    "hidden": false
                },
                {
                    "_id": "693a67ea74fced5bf9c324f3",
                    "user": {
                        "_id": "6927d6bfa61c770460c4c3c5",
                        "avatarUrl": "/avatars/01158a151cf5c6d254ffd34984e647e0.svg",
                        "isPro": false,
                        "fullname": "Hamid R. Rabiee",
                        "user": "hrrabiee",
                        "type": "user"
                    },
                    "name": "Hamid R. Rabiee",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-11T10:54:49.039Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-08T19:49:33.000Z",
            "submittedOnDailyAt": "2025-12-11T04:13:40.186Z",
            "title": "Beyond Unified Models: A Service-Oriented Approach to Low Latency, Context Aware Phonemization for Real Time TTS",
            "submittedOnDailyBy": {
                "_id": "657c45f6429a20edb520d7a8",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657c45f6429a20edb520d7a8/jXVRbbaUzodZLupJiub84.png",
                "isPro": false,
                "fullname": "Mahta Fetrat",
                "user": "MahtaFetrat",
                "type": "user"
            },
            "summary": "Lightweight, real-time text-to-speech systems are crucial for accessibility. However, the most efficient TTS models often rely on lightweight phonemizers that struggle with context-dependent challenges. In contrast, more advanced phonemizers with a deeper linguistic understanding typically incur high computational costs, which prevents real-time performance.\n  This paper examines the trade-off between phonemization quality and inference speed in G2P-aided TTS systems, introducing a practical framework to bridge this gap. We propose lightweight strategies for context-aware phonemization and a service-oriented TTS architecture that executes these modules as independent services. This design decouples heavy context-aware components from the core TTS engine, effectively breaking the latency barrier and enabling real-time use of high-quality phonemization models. Experimental results confirm that the proposed system improves pronunciation soundness and linguistic accuracy while maintaining real-time responsiveness, making it well-suited for offline and end-device TTS applications.",
            "upvotes": 2,
            "discussionId": "693a67ea74fced5bf9c324f4",
            "githubRepo": "https://github.com/MahtaFetrat/Piper-with-LCA-Phonemizer",
            "githubRepoAddedBy": "user",
            "ai_summary": "A framework is proposed to improve phonemization quality in TTS systems without sacrificing real-time performance through lightweight context-aware phonemization and a service-oriented architecture.",
            "ai_keywords": [
                "G2P-aided TTS",
                "phonemization quality",
                "inference speed",
                "context-aware phonemization",
                "service-oriented architecture",
                "real-time performance",
                "TTS systems",
                "pronunciation soundness",
                "linguistic accuracy"
            ],
            "githubStars": 7
        },
        "publishedAt": "2025-12-08T14:49:33.000Z",
        "title": "Beyond Unified Models: A Service-Oriented Approach to Low Latency, Context Aware Phonemization for Real Time TTS",
        "summary": "Lightweight, real-time text-to-speech systems are crucial for accessibility. However, the most efficient TTS models often rely on lightweight phonemizers that struggle with context-dependent challenges. In contrast, more advanced phonemizers with a deeper linguistic understanding typically incur high computational costs, which prevents real-time performance.\n  This paper examines the trade-off between phonemization quality and inference speed in G2P-aided TTS systems, introducing a practical framework to bridge this gap. We propose lightweight strategies for context-aware phonemization and a service-oriented TTS architecture that executes these modules as independent services. This design decouples heavy context-aware components from the core TTS engine, effectively breaking the latency barrier and enabling real-time use of high-quality phonemization models. Experimental results confirm that the proposed system improves pronunciation soundness and linguistic accuracy while maintaining real-time responsiveness, making it well-suited for offline and end-device TTS applications.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.08006.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "657c45f6429a20edb520d7a8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657c45f6429a20edb520d7a8/jXVRbbaUzodZLupJiub84.png",
            "fullname": "Mahta Fetrat",
            "name": "MahtaFetrat",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 30
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.04519",
            "authors": [
                {
                    "_id": "69395a75dfc35938ba12a23e",
                    "user": {
                        "_id": "63e9ae22dd2c4effdd6c2256",
                        "avatarUrl": "/avatars/2a1da995a1baab503c0277587791a171.svg",
                        "isPro": false,
                        "fullname": "Yifei Yu",
                        "user": "yuyifei",
                        "type": "user"
                    },
                    "name": "Yifei Yu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-11T10:13:37.633Z",
                    "hidden": false
                },
                {
                    "_id": "69395a75dfc35938ba12a23f",
                    "name": "Xiaoshan Wu",
                    "hidden": false
                },
                {
                    "_id": "69395a75dfc35938ba12a240",
                    "name": "Xinting Hu",
                    "hidden": false
                },
                {
                    "_id": "69395a75dfc35938ba12a241",
                    "name": "Tao Hu",
                    "hidden": false
                },
                {
                    "_id": "69395a75dfc35938ba12a242",
                    "name": "Yangtian Sun",
                    "hidden": false
                },
                {
                    "_id": "69395a75dfc35938ba12a243",
                    "name": "Xiaoyang Lyu",
                    "hidden": false
                },
                {
                    "_id": "69395a75dfc35938ba12a244",
                    "name": "Bo Wang",
                    "hidden": false
                },
                {
                    "_id": "69395a75dfc35938ba12a245",
                    "name": "Lin Ma",
                    "hidden": false
                },
                {
                    "_id": "69395a75dfc35938ba12a246",
                    "name": "Yuewen Ma",
                    "hidden": false
                },
                {
                    "_id": "69395a75dfc35938ba12a247",
                    "name": "Zhongrui Wang",
                    "hidden": false
                },
                {
                    "_id": "69395a75dfc35938ba12a248",
                    "name": "Xiaojuan Qi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-04T07:06:02.000Z",
            "submittedOnDailyAt": "2025-12-11T04:50:35.118Z",
            "title": "VideoSSM: Autoregressive Long Video Generation with Hybrid State-Space Memory",
            "submittedOnDailyBy": {
                "_id": "63e9ae22dd2c4effdd6c2256",
                "avatarUrl": "/avatars/2a1da995a1baab503c0277587791a171.svg",
                "isPro": false,
                "fullname": "Yifei Yu",
                "user": "yuyifei",
                "type": "user"
            },
            "summary": "Autoregressive (AR) diffusion enables streaming, interactive long-video generation by producing frames causally, yet maintaining coherence over minute-scale horizons remains challenging due to accumulated errors, motion drift, and content repetition. We approach this problem from a memory perspective, treating video synthesis as a recurrent dynamical process that requires coordinated short- and long-term context. We propose VideoSSM, a Long Video Model that unifies AR diffusion with a hybrid state-space memory. The state-space model (SSM) serves as an evolving global memory of scene dynamics across the entire sequence, while a context window provides local memory for motion cues and fine details. This hybrid design preserves global consistency without frozen, repetitive patterns, supports prompt-adaptive interaction, and scales in linear time with sequence length. Experiments on short- and long-range benchmarks demonstrate state-of-the-art temporal consistency and motion stability among autoregressive video generator especially at minute-scale horizons, enabling content diversity and interactive prompt-based control, thereby establishing a scalable, memory-aware framework for long video generation.",
            "upvotes": 2,
            "discussionId": "69395a75dfc35938ba12a249",
            "ai_summary": "VideoSSM, a hybrid state-space memory model combining AR diffusion, achieves state-of-the-art temporal consistency and motion stability in long-video generation by coordinating short- and long-term context.",
            "ai_keywords": [
                "autoregressive diffusion",
                "VideoSSM",
                "state-space model",
                "global memory",
                "local memory",
                "scene dynamics",
                "context window",
                "motion cues",
                "fine details",
                "temporal consistency",
                "motion stability",
                "content diversity",
                "prompt-based control"
            ],
            "organization": {
                "_id": "67ea9ecfc234715db8dbf339",
                "name": "hkuhk",
                "fullname": "The University of Hong Kong",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67ea9e8d2d95c10a0da11b0c/FNnR4M7YqKRuG43N5771B.png"
            }
        },
        "publishedAt": "2025-12-04T02:06:02.000Z",
        "title": "VideoSSM: Autoregressive Long Video Generation with Hybrid State-Space Memory",
        "summary": "Autoregressive (AR) diffusion enables streaming, interactive long-video generation by producing frames causally, yet maintaining coherence over minute-scale horizons remains challenging due to accumulated errors, motion drift, and content repetition. We approach this problem from a memory perspective, treating video synthesis as a recurrent dynamical process that requires coordinated short- and long-term context. We propose VideoSSM, a Long Video Model that unifies AR diffusion with a hybrid state-space memory. The state-space model (SSM) serves as an evolving global memory of scene dynamics across the entire sequence, while a context window provides local memory for motion cues and fine details. This hybrid design preserves global consistency without frozen, repetitive patterns, supports prompt-adaptive interaction, and scales in linear time with sequence length. Experiments on short- and long-range benchmarks demonstrate state-of-the-art temporal consistency and motion stability among autoregressive video generator especially at minute-scale horizons, enabling content diversity and interactive prompt-based control, thereby establishing a scalable, memory-aware framework for long video generation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04519.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63e9ae22dd2c4effdd6c2256",
            "avatarUrl": "/avatars/2a1da995a1baab503c0277587791a171.svg",
            "fullname": "Yifei Yu",
            "name": "yuyifei",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "67ea9ecfc234715db8dbf339",
            "name": "hkuhk",
            "fullname": "The University of Hong Kong",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67ea9e8d2d95c10a0da11b0c/FNnR4M7YqKRuG43N5771B.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.09112",
            "authors": [
                {
                    "_id": "693b0202804e33f037cf8e65",
                    "name": "Frdric Fortier-Chouinard",
                    "hidden": false
                },
                {
                    "_id": "693b0202804e33f037cf8e66",
                    "name": "Yannick Hold-Geoffroy",
                    "hidden": false
                },
                {
                    "_id": "693b0202804e33f037cf8e67",
                    "name": "Valentin Deschaintre",
                    "hidden": false
                },
                {
                    "_id": "693b0202804e33f037cf8e68",
                    "name": "Matheus Gadelha",
                    "hidden": false
                },
                {
                    "_id": "693b0202804e33f037cf8e69",
                    "name": "Jean-Franois Lalonde",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-09T20:54:35.000Z",
            "submittedOnDailyAt": "2025-12-11T15:11:47.014Z",
            "title": "GimbalDiffusion: Gravity-Aware Camera Control for Video Generation",
            "submittedOnDailyBy": {
                "_id": "64763956fb22e3b77f3e13d6",
                "avatarUrl": "/avatars/1e6acf5d733280931ba2ce099481b2be.svg",
                "isPro": false,
                "fullname": "Frdric Fortier-Chouinard",
                "user": "lefreud",
                "type": "user"
            },
            "summary": "Recent progress in text-to-video generation has achieved remarkable realism, yet fine-grained control over camera motion and orientation remains elusive. Existing approaches typically encode camera trajectories through relative or ambiguous representations, limiting explicit geometric control. We introduce GimbalDiffusion, a framework that enables camera control grounded in physical-world coordinates, using gravity as a global reference. Instead of describing motion relative to previous frames, our method defines camera trajectories in an absolute coordinate system, allowing precise and interpretable control over camera parameters without requiring an initial reference frame. We leverage panoramic 360-degree videos to construct a wide variety of camera trajectories, well beyond the predominantly straight, forward-facing trajectories seen in conventional video data. To further enhance camera guidance, we introduce null-pitch conditioning, an annotation strategy that reduces the model's reliance on text content when conflicting with camera specifications (e.g., generating grass while the camera points towards the sky). Finally, we establish a benchmark for camera-aware video generation by rebalancing SpatialVID-HQ for comprehensive evaluation under wide camera pitch variation. Together, these contributions advance the controllability and robustness of text-to-video models, enabling precise, gravity-aligned camera manipulation within generative frameworks.",
            "upvotes": 1,
            "discussionId": "693b0202804e33f037cf8e6a",
            "projectPage": "https://lvsn.github.io/GimbalDiffusion/",
            "ai_summary": "GimbalDiffusion framework enables precise camera control in text-to-video generation using absolute coordinates and gravity as a reference, enhancing controllability and robustness.",
            "ai_keywords": [
                "GimbalDiffusion",
                "camera trajectories",
                "physical-world coordinates",
                "gravity",
                "absolute coordinate system",
                "panoramic 360-degree videos",
                "null-pitch conditioning",
                "SpatialVID-HQ",
                "camera pitch variation"
            ],
            "organization": {
                "_id": "61e5d14f77496de0a6d95c6b",
                "name": "adobe",
                "fullname": "Adobe",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1645217431826-61e35e517ac6b6d06cfa8081.png"
            }
        },
        "publishedAt": "2025-12-09T15:54:35.000Z",
        "title": "GimbalDiffusion: Gravity-Aware Camera Control for Video Generation",
        "summary": "Recent progress in text-to-video generation has achieved remarkable realism, yet fine-grained control over camera motion and orientation remains elusive. Existing approaches typically encode camera trajectories through relative or ambiguous representations, limiting explicit geometric control. We introduce GimbalDiffusion, a framework that enables camera control grounded in physical-world coordinates, using gravity as a global reference. Instead of describing motion relative to previous frames, our method defines camera trajectories in an absolute coordinate system, allowing precise and interpretable control over camera parameters without requiring an initial reference frame. We leverage panoramic 360-degree videos to construct a wide variety of camera trajectories, well beyond the predominantly straight, forward-facing trajectories seen in conventional video data. To further enhance camera guidance, we introduce null-pitch conditioning, an annotation strategy that reduces the model's reliance on text content when conflicting with camera specifications (e.g., generating grass while the camera points towards the sky). Finally, we establish a benchmark for camera-aware video generation by rebalancing SpatialVID-HQ for comprehensive evaluation under wide camera pitch variation. Together, these contributions advance the controllability and robustness of text-to-video models, enabling precise, gravity-aligned camera manipulation within generative frameworks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.09112.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "64763956fb22e3b77f3e13d6",
            "avatarUrl": "/avatars/1e6acf5d733280931ba2ce099481b2be.svg",
            "fullname": "Frdric Fortier-Chouinard",
            "name": "lefreud",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "61e5d14f77496de0a6d95c6b",
            "name": "adobe",
            "fullname": "Adobe",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1645217431826-61e35e517ac6b6d06cfa8081.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.07222",
            "authors": [
                {
                    "_id": "693a41e374fced5bf9c32477",
                    "user": {
                        "_id": "689b16601133f93a6f4afdca",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/MWlw4vtHV6ptOpTT6-U57.png",
                        "isPro": false,
                        "fullname": "Qiwei Tian",
                        "user": "michaeltqw108",
                        "type": "user"
                    },
                    "name": "Qiwei Tian",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-11T10:54:08.007Z",
                    "hidden": false
                },
                {
                    "_id": "693a41e374fced5bf9c32478",
                    "user": {
                        "_id": "669389688db712bad7c70e44",
                        "avatarUrl": "/avatars/1938192a68bd231a39c662e379cfbb7f.svg",
                        "isPro": false,
                        "fullname": "CHEN HAO LIN",
                        "user": "chl810903",
                        "type": "user"
                    },
                    "name": "Chenhao Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-11T10:54:14.957Z",
                    "hidden": false
                },
                {
                    "_id": "693a41e374fced5bf9c32479",
                    "user": {
                        "_id": "634cc244a5d4e109c6af9375",
                        "avatarUrl": "/avatars/ebc7e2c966b7434f26def3f05da45cd9.svg",
                        "isPro": false,
                        "fullname": "zhengyuzhao",
                        "user": "zhaozhengyu",
                        "type": "user"
                    },
                    "name": "Zhengyu Zhao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-11T10:54:20.650Z",
                    "hidden": false
                },
                {
                    "_id": "693a41e374fced5bf9c3247a",
                    "name": "Chao Shen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-08T07:05:18.000Z",
            "submittedOnDailyAt": "2025-12-11T01:32:57.516Z",
            "title": "Pay Less Attention to Function Words for Free Robustness of Vision-Language Models",
            "submittedOnDailyBy": {
                "_id": "689b16601133f93a6f4afdca",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/MWlw4vtHV6ptOpTT6-U57.png",
                "isPro": false,
                "fullname": "Qiwei Tian",
                "user": "michaeltqw108",
                "type": "user"
            },
            "summary": "To address the trade-off between robustness and performance for robust VLM, we observe that function words could incur vulnerability of VLMs against cross-modal adversarial attacks, and propose Function-word De-Attention (FDA) accordingly to mitigate the impact of function words. Similar to differential amplifiers, our FDA calculates the original and the function-word cross-attention within attention heads, and differentially subtracts the latter from the former for more aligned and robust VLMs. Comprehensive experiments include 2 SOTA baselines under 6 different attacks on 2 downstream tasks, 3 datasets, and 3 models. Overall, our FDA yields an average 18/13/53% ASR drop with only 0.2/0.3/0.6% performance drops on the 3 tested models on retrieval, and a 90% ASR drop with a 0.3% performance gain on visual grounding. We demonstrate the scalability, generalization, and zero-shot performance of FDA experimentally, as well as in-depth ablation studies and analysis. Code will be made publicly at https://github.com/michaeltian108/FDA.",
            "upvotes": 1,
            "discussionId": "693a41e474fced5bf9c3247b",
            "githubRepo": "https://github.com/michaeltian108/FDA",
            "githubRepoAddedBy": "user",
            "ai_summary": "Function-word De-Attention (FDA) mitigates adversarial attacks on robust VLMs by differentially subtracting function-word cross-attention, improving robustness with minimal performance trade-offs.",
            "ai_keywords": [
                "function words",
                "cross-modal adversarial attacks",
                "Function-word De-Attention",
                "FDA",
                "differential amplifiers",
                "cross-attention",
                "VLMs",
                "ASR",
                "retrieval",
                "visual grounding"
            ],
            "githubStars": 0,
            "organization": {
                "_id": "624fea0b0953e1175c818274",
                "name": "xjtu",
                "fullname": "Xi'an Jiaotong University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/dASJ0jUAN1oUX2VXniwtB.png"
            }
        },
        "publishedAt": "2025-12-08T02:05:18.000Z",
        "title": "Pay Less Attention to Function Words for Free Robustness of Vision-Language Models",
        "summary": "To address the trade-off between robustness and performance for robust VLM, we observe that function words could incur vulnerability of VLMs against cross-modal adversarial attacks, and propose Function-word De-Attention (FDA) accordingly to mitigate the impact of function words. Similar to differential amplifiers, our FDA calculates the original and the function-word cross-attention within attention heads, and differentially subtracts the latter from the former for more aligned and robust VLMs. Comprehensive experiments include 2 SOTA baselines under 6 different attacks on 2 downstream tasks, 3 datasets, and 3 models. Overall, our FDA yields an average 18/13/53% ASR drop with only 0.2/0.3/0.6% performance drops on the 3 tested models on retrieval, and a 90% ASR drop with a 0.3% performance gain on visual grounding. We demonstrate the scalability, generalization, and zero-shot performance of FDA experimentally, as well as in-depth ablation studies and analysis. Code will be made publicly at https://github.com/michaeltian108/FDA.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.07222.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "689b16601133f93a6f4afdca",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/MWlw4vtHV6ptOpTT6-U57.png",
            "fullname": "Qiwei Tian",
            "name": "michaeltqw108",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "624fea0b0953e1175c818274",
            "name": "xjtu",
            "fullname": "Xi'an Jiaotong University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/dASJ0jUAN1oUX2VXniwtB.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.05402",
            "authors": [
                {
                    "_id": "693932fddfc35938ba12a1ad",
                    "user": {
                        "_id": "678d43fdb1b6f536ba96d75c",
                        "avatarUrl": "/avatars/1b4407293505c0ae89bccf363bf3b7fc.svg",
                        "isPro": false,
                        "fullname": "Sithumi",
                        "user": "sithuWiki",
                        "type": "user"
                    },
                    "name": "Sithumi Wickramasinghe",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-10T13:07:48.310Z",
                    "hidden": false
                },
                {
                    "_id": "693932fddfc35938ba12a1ae",
                    "name": "Bikramjit Das",
                    "hidden": false
                },
                {
                    "_id": "693932fddfc35938ba12a1af",
                    "user": {
                        "_id": "655431b2997379e9b0999d23",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655431b2997379e9b0999d23/OhHCPCXfOS61Z7CSewUCU.png",
                        "isPro": false,
                        "fullname": "Dorien Herremans",
                        "user": "dorienh",
                        "type": "user"
                    },
                    "name": "Dorien Herremans",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-10T13:07:46.215Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-05T03:47:13.000Z",
            "submittedOnDailyAt": "2025-12-11T13:35:47.467Z",
            "title": "Smart Timing for Mining: A Deep Learning Framework for Bitcoin Hardware ROI Prediction",
            "submittedOnDailyBy": {
                "_id": "678d43fdb1b6f536ba96d75c",
                "avatarUrl": "/avatars/1b4407293505c0ae89bccf363bf3b7fc.svg",
                "isPro": false,
                "fullname": "Sithumi",
                "user": "sithuWiki",
                "type": "user"
            },
            "summary": "Bitcoin mining hardware acquisition requires strategic timing due to volatile markets, rapid technological obsolescence, and protocol-driven revenue cycles. Despite mining's evolution into a capital-intensive industry, there is little guidance on when to purchase new Application-Specific Integrated Circuit (ASIC) hardware, and no prior computational frameworks address this decision problem. We address this gap by formulating hardware acquisition as a time series classification task, predicting whether purchasing ASIC machines yields profitable (Return on Investment (ROI) >= 1), marginal (0 < ROI < 1), or unprofitable (ROI <= 0) returns within one year. We propose MineROI-Net, an open source Transformer-based architecture designed to capture multi-scale temporal patterns in mining profitability. Evaluated on data from 20 ASIC miners released between 2015 and 2024 across diverse market regimes, MineROI-Net outperforms LSTM-based and TSLANet baselines, achieving 83.7% accuracy and 83.1% macro F1-score. The model demonstrates strong economic relevance, achieving 93.6% precision in detecting unprofitable periods and 98.5% precision for profitable ones, while avoiding misclassification of profitable scenarios as unprofitable and vice versa. These results indicate that MineROI-Net offers a practical, data-driven tool for timing mining hardware acquisitions, potentially reducing financial risk in capital-intensive mining operations. The model is available through: https://github.com/AMAAI-Lab/MineROI-Net.",
            "upvotes": 1,
            "discussionId": "693932fddfc35938ba12a1b0",
            "ai_summary": "MineROI-Net, a Transformer-based model, predicts profitability for ASIC mining hardware acquisitions, enhancing decision-making in the volatile mining industry.",
            "ai_keywords": [
                "Transformer-based architecture",
                "time series classification",
                "LSTM-based",
                "TSLANet",
                "Return on Investment (ROI)",
                "multi-scale temporal patterns"
            ],
            "organization": {
                "_id": "65125b2313f72a4e096f5abe",
                "name": "SingaporeUniversityofTechnologyandDesign",
                "fullname": "Singapore University of Technology and Design",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/rh8WOk1QbeY0RPbf1_qKs.jpeg"
            }
        },
        "publishedAt": "2025-12-04T22:47:13.000Z",
        "title": "Smart Timing for Mining: A Deep Learning Framework for Bitcoin Hardware ROI Prediction",
        "summary": "Bitcoin mining hardware acquisition requires strategic timing due to volatile markets, rapid technological obsolescence, and protocol-driven revenue cycles. Despite mining's evolution into a capital-intensive industry, there is little guidance on when to purchase new Application-Specific Integrated Circuit (ASIC) hardware, and no prior computational frameworks address this decision problem. We address this gap by formulating hardware acquisition as a time series classification task, predicting whether purchasing ASIC machines yields profitable (Return on Investment (ROI) >= 1), marginal (0 < ROI < 1), or unprofitable (ROI <= 0) returns within one year. We propose MineROI-Net, an open source Transformer-based architecture designed to capture multi-scale temporal patterns in mining profitability. Evaluated on data from 20 ASIC miners released between 2015 and 2024 across diverse market regimes, MineROI-Net outperforms LSTM-based and TSLANet baselines, achieving 83.7% accuracy and 83.1% macro F1-score. The model demonstrates strong economic relevance, achieving 93.6% precision in detecting unprofitable periods and 98.5% precision for profitable ones, while avoiding misclassification of profitable scenarios as unprofitable and vice versa. These results indicate that MineROI-Net offers a practical, data-driven tool for timing mining hardware acquisitions, potentially reducing financial risk in capital-intensive mining operations. The model is available through: https://github.com/AMAAI-Lab/MineROI-Net.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.05402.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "678d43fdb1b6f536ba96d75c",
            "avatarUrl": "/avatars/1b4407293505c0ae89bccf363bf3b7fc.svg",
            "fullname": "Sithumi",
            "name": "sithuWiki",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "65125b2313f72a4e096f5abe",
            "name": "SingaporeUniversityofTechnologyandDesign",
            "fullname": "Singapore University of Technology and Design",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/rh8WOk1QbeY0RPbf1_qKs.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.01453",
            "authors": [
                {
                    "_id": "693a377e74fced5bf9c3240b",
                    "user": {
                        "_id": "6849a876cd8bfad9ca455d4e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/VWKXiGwcPiCQ48FZ9p5jG.png",
                        "isPro": false,
                        "fullname": "Xiaoquan Zhi",
                        "user": "zxq1942461723",
                        "type": "user"
                    },
                    "name": "Xiaoquan Zhi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-11T10:53:11.108Z",
                    "hidden": false
                },
                {
                    "_id": "693a377e74fced5bf9c3240c",
                    "name": "Hongke Zhao",
                    "hidden": false
                },
                {
                    "_id": "693a377e74fced5bf9c3240d",
                    "user": {
                        "_id": "6913533a2384a4b39dc39ff0",
                        "avatarUrl": "/avatars/cd56a32cc716641ea50ad9ab9c92eaa2.svg",
                        "isPro": false,
                        "fullname": "likang wu",
                        "user": "likang03",
                        "type": "user"
                    },
                    "name": "Likang Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-11T10:53:20.434Z",
                    "hidden": false
                },
                {
                    "_id": "693a377e74fced5bf9c3240e",
                    "user": {
                        "_id": "655f131eb9673a4249be8808",
                        "avatarUrl": "/avatars/5e3577b11f920e7ec3f2f0a645f7f6cd.svg",
                        "isPro": false,
                        "fullname": "ChuangZhao",
                        "user": "ChuangZhao",
                        "type": "user"
                    },
                    "name": "Chuang Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-11T10:13:28.222Z",
                    "hidden": false
                },
                {
                    "_id": "693a377e74fced5bf9c3240f",
                    "name": "Hengshu Zhu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-01T09:39:39.000Z",
            "submittedOnDailyAt": "2025-12-11T00:47:14.542Z",
            "title": "Reinventing Clinical Dialogue: Agentic Paradigms for LLM Enabled Healthcare Communication",
            "submittedOnDailyBy": {
                "_id": "660162a9eee53450ba93c34b",
                "avatarUrl": "/avatars/d6128f630e041e29d1cdc178e112f23f.svg",
                "isPro": false,
                "fullname": "guanzhong",
                "user": "guanzhong2",
                "type": "user"
            },
            "summary": "Clinical dialogue represents a complex duality requiring both the empathetic fluency of natural conversation and the rigorous precision of evidence-based medicine. While Large Language Models possess unprecedented linguistic capabilities, their architectural reliance on reactive and stateless processing often favors probabilistic plausibility over factual veracity. This structural limitation has catalyzed a paradigm shift in medical AI from generative text prediction to agentic autonomy, where the model functions as a central reasoning engine capable of deliberate planning and persistent memory. Moving beyond existing reviews that primarily catalog downstream applications, this survey provides a first-principles analysis of the cognitive architecture underpinning this shift. We introduce a novel taxonomy structured along the orthogonal axes of knowledge source and agency objective to delineate the provenance of clinical knowledge against the system's operational scope. This framework facilitates a systematic analysis of the intrinsic trade-offs between creativity and reliability by categorizing methods into four archetypes: Latent Space Clinicians, Emergent Planners, Grounded Synthesizers, and Verifiable Workflow Automators. For each paradigm, we deconstruct the technical realization across the entire cognitive pipeline, encompassing strategic planning, memory management, action execution, collaboration, and evolution to reveal how distinct architectural choices balance the tension between autonomy and safety.",
            "upvotes": 1,
            "discussionId": "693a377e74fced5bf9c32410",
            "githubRepo": "https://github.com/xqz614/Awesome-Agentic-Clinical-Dialogue",
            "githubRepoAddedBy": "user",
            "ai_summary": "The survey analyzes the cognitive architecture of medical AI systems, focusing on the shift from generative text prediction to agentic autonomy, and categorizes methods into four archetypes based on knowledge source and agency objective.",
            "ai_keywords": [
                "Large Language Models",
                "reactive processing",
                "stateless processing",
                "agentic autonomy",
                "cognitive architecture",
                "Latent Space Clinicians",
                "Emergent Planners",
                "Grounded Synthesizers",
                "Verifiable Workflow Automators",
                "strategic planning",
                "memory management",
                "action execution",
                "collaboration",
                "evolution"
            ]
        },
        "publishedAt": "2025-12-01T04:39:39.000Z",
        "title": "Reinventing Clinical Dialogue: Agentic Paradigms for LLM Enabled Healthcare Communication",
        "summary": "Clinical dialogue represents a complex duality requiring both the empathetic fluency of natural conversation and the rigorous precision of evidence-based medicine. While Large Language Models possess unprecedented linguistic capabilities, their architectural reliance on reactive and stateless processing often favors probabilistic plausibility over factual veracity. This structural limitation has catalyzed a paradigm shift in medical AI from generative text prediction to agentic autonomy, where the model functions as a central reasoning engine capable of deliberate planning and persistent memory. Moving beyond existing reviews that primarily catalog downstream applications, this survey provides a first-principles analysis of the cognitive architecture underpinning this shift. We introduce a novel taxonomy structured along the orthogonal axes of knowledge source and agency objective to delineate the provenance of clinical knowledge against the system's operational scope. This framework facilitates a systematic analysis of the intrinsic trade-offs between creativity and reliability by categorizing methods into four archetypes: Latent Space Clinicians, Emergent Planners, Grounded Synthesizers, and Verifiable Workflow Automators. For each paradigm, we deconstruct the technical realization across the entire cognitive pipeline, encompassing strategic planning, memory management, action execution, collaboration, and evolution to reveal how distinct architectural choices balance the tension between autonomy and safety.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.01453.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "660162a9eee53450ba93c34b",
            "avatarUrl": "/avatars/d6128f630e041e29d1cdc178e112f23f.svg",
            "fullname": "guanzhong",
            "name": "guanzhong2",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    }
]