[
    {
        "paper": {
            "id": "2512.10430",
            "authors": [
                {
                    "_id": "693bba8d9874a2a5e4ffb3ab",
                    "user": {
                        "_id": "64f4c8739ee58d48e8507e0e",
                        "avatarUrl": "/avatars/4be540dfb4a949f37cba2d3c3729fbde.svg",
                        "isPro": false,
                        "fullname": "Dmitrii Stoianov",
                        "user": "heylimon",
                        "type": "user"
                    },
                    "name": "Dmitrii Stoianov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-12T09:14:40.198Z",
                    "hidden": false
                },
                {
                    "_id": "693bba8d9874a2a5e4ffb3ac",
                    "user": {
                        "_id": "64fb054ebb362cbf2fe53159",
                        "avatarUrl": "/avatars/936c37a77d46d0ea579d2f8a9aea9284.svg",
                        "isPro": false,
                        "fullname": "Danil Taranets",
                        "user": "taranetsdan",
                        "type": "user"
                    },
                    "name": "Danil Taranets",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-12T09:14:29.281Z",
                    "hidden": false
                },
                {
                    "_id": "693bba8d9874a2a5e4ffb3ad",
                    "user": {
                        "_id": "6612fe63da0c53de48c7ce3b",
                        "avatarUrl": "/avatars/207c80b5da078239371a31b17f63ccfd.svg",
                        "isPro": false,
                        "fullname": "Olga Tsymboi",
                        "user": "oltsy",
                        "type": "user"
                    },
                    "name": "Olga Tsymboi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-12T09:14:38.180Z",
                    "hidden": false
                },
                {
                    "_id": "693bba8d9874a2a5e4ffb3ae",
                    "user": {
                        "_id": "6780dcd6acf8d824c03864da",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/6PeN6OXbSq0M-L4OxFTrn.png",
                        "isPro": false,
                        "fullname": "Ramil Latypov",
                        "user": "kylecr4ne",
                        "type": "user"
                    },
                    "name": "Ramil Latypov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-12T09:14:23.437Z",
                    "hidden": false
                },
                {
                    "_id": "693bba8d9874a2a5e4ffb3af",
                    "user": {
                        "_id": "6513f03e86d74f32ed65e3b8",
                        "avatarUrl": "/avatars/c327966623f775a2d1f3d984ca162ef6.svg",
                        "isPro": false,
                        "fullname": "Almaz Dautov",
                        "user": "the-hir0",
                        "type": "user"
                    },
                    "name": "Almaz Dautov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-12T09:14:30.990Z",
                    "hidden": false
                },
                {
                    "_id": "693bba8d9874a2a5e4ffb3b0",
                    "user": {
                        "_id": "621a8daf325b927e60fcef08",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/621a8daf325b927e60fcef08/bM8W-of2u0yvL8FHeY2ra.jpeg",
                        "isPro": false,
                        "fullname": "Vladislav Kruglikov",
                        "user": "vladislavkruglikov",
                        "type": "user"
                    },
                    "name": "Vladislav Kruglikov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-12T09:14:21.170Z",
                    "hidden": false
                },
                {
                    "_id": "693bba8d9874a2a5e4ffb3b1",
                    "name": "Nikita Surkov",
                    "hidden": false
                },
                {
                    "_id": "693bba8d9874a2a5e4ffb3b2",
                    "user": {
                        "_id": "63188c428d698d8c1642a0d8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63188c428d698d8c1642a0d8/MlcBnU7CmnKdRF053JcY4.jpeg",
                        "isPro": false,
                        "fullname": "German Abramov",
                        "user": "germanjke",
                        "type": "user"
                    },
                    "name": "German Abramov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-12T12:59:28.579Z",
                    "hidden": false
                },
                {
                    "_id": "693bba8d9874a2a5e4ffb3b3",
                    "name": "Pavel Gein",
                    "hidden": false
                },
                {
                    "_id": "693bba8d9874a2a5e4ffb3b4",
                    "user": {
                        "_id": "636a9a07e3ad78bc68b1a5a2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1668020490988-636a9a07e3ad78bc68b1a5a2.jpeg",
                        "isPro": false,
                        "fullname": "Dmitry Abulkhanov",
                        "user": "mponty",
                        "type": "user"
                    },
                    "name": "Dmitry Abulkhanov",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-12T13:02:42.107Z",
                    "hidden": true
                },
                {
                    "_id": "693bba8d9874a2a5e4ffb3b5",
                    "user": {
                        "_id": "658bc20cfdf2279d4721f218",
                        "avatarUrl": "/avatars/5f1cb94373fbbbcfed9b848c5ebdd1ad.svg",
                        "isPro": false,
                        "fullname": "Mikhail Gashkov",
                        "user": "MikeGashkov",
                        "type": "user"
                    },
                    "name": "Mikhail Gashkov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-12T09:14:32.809Z",
                    "hidden": false
                },
                {
                    "_id": "693bba8d9874a2a5e4ffb3b6",
                    "name": "Viktor Zelenkovskiy",
                    "hidden": false
                },
                {
                    "_id": "693bba8d9874a2a5e4ffb3b7",
                    "user": {
                        "_id": "644f64bc17b6189cda54cae8",
                        "avatarUrl": "/avatars/f684a65b35a9be06cbb16fb8f44a4782.svg",
                        "isPro": false,
                        "fullname": "Artem Batalov",
                        "user": "batalovme",
                        "type": "user"
                    },
                    "name": "Artem Batalov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-12T09:14:27.497Z",
                    "hidden": false
                },
                {
                    "_id": "693bba8d9874a2a5e4ffb3b8",
                    "user": {
                        "_id": "62609d224e6e4b84475eb8d9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62609d224e6e4b84475eb8d9/PKQvuLm40PGg91VKRVSIb.jpeg",
                        "isPro": false,
                        "fullname": "Alex Medvedev",
                        "user": "kenkaneki",
                        "type": "user"
                    },
                    "name": "Aleksandr Medvedev",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-12T09:14:36.035Z",
                    "hidden": false
                },
                {
                    "_id": "693bba8d9874a2a5e4ffb3b9",
                    "user": {
                        "_id": "63f26358be95ed4c9a9b0583",
                        "avatarUrl": "/avatars/133f2d28c5e5139d61048dfef5e9f4ff.svg",
                        "isPro": false,
                        "fullname": "Anatoly Potapov",
                        "user": "AnatoliiPotapov",
                        "type": "user"
                    },
                    "name": "Anatolii Potapov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-12T09:14:25.666Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-11T08:40:10.000Z",
            "submittedOnDailyAt": "2025-12-12T08:33:32.798Z",
            "title": "T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground",
            "submittedOnDailyBy": {
                "_id": "6612fe63da0c53de48c7ce3b",
                "avatarUrl": "/avatars/207c80b5da078239371a31b17f63ccfd.svg",
                "isPro": false,
                "fullname": "Olga Tsymboi",
                "user": "oltsy",
                "type": "user"
            },
            "summary": "We introduce T-pro 2.0, an open-weight Russian LLM for hybrid reasoning and efficient inference. The model supports direct answering and reasoning-trace generation, using a Cyrillic-dense tokenizer and an adapted EAGLE speculative-decoding pipeline to reduce latency. To enable reproducible and extensible research, we release the model weights, the T-Wix 500k instruction corpus, the T-Math reasoning benchmark, and the EAGLE weights on Hugging Face. These resources allow users to study Russian-language reasoning and to extend or adapt both the model and the inference pipeline. A public web demo exposes reasoning and non-reasoning modes and illustrates the speedups achieved by our inference stack across domains. T-pro 2.0 thus serves as an accessible open system for building and evaluating efficient, practical Russian LLM applications.",
            "upvotes": 60,
            "discussionId": "693bba8e9874a2a5e4ffb3ba",
            "ai_summary": "T-pro 2.0 is an open-weight Russian LLM for hybrid reasoning and efficient inference, using a Cyrillic-dense tokenizer and EAGLE speculative-decoding pipeline.",
            "ai_keywords": [
                "Cyrillic-dense tokenizer",
                "EAGLE speculative-decoding pipeline",
                "hybrid reasoning",
                "efficient inference",
                "reasoning-trace generation"
            ],
            "organization": {
                "_id": "675861e944dbb69c2673c71c",
                "name": "t-tech",
                "fullname": "T-Tech",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/674ea07d320a043daeb2d98b/IwSCMolFY4Otk7sFXzWhi.jpeg"
            }
        },
        "publishedAt": "2025-12-11T03:40:10.000Z",
        "title": "T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground",
        "summary": "We introduce T-pro 2.0, an open-weight Russian LLM for hybrid reasoning and efficient inference. The model supports direct answering and reasoning-trace generation, using a Cyrillic-dense tokenizer and an adapted EAGLE speculative-decoding pipeline to reduce latency. To enable reproducible and extensible research, we release the model weights, the T-Wix 500k instruction corpus, the T-Math reasoning benchmark, and the EAGLE weights on Hugging Face. These resources allow users to study Russian-language reasoning and to extend or adapt both the model and the inference pipeline. A public web demo exposes reasoning and non-reasoning modes and illustrates the speedups achieved by our inference stack across domains. T-pro 2.0 thus serves as an accessible open system for building and evaluating efficient, practical Russian LLM applications.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.10430.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6612fe63da0c53de48c7ce3b",
            "avatarUrl": "/avatars/207c80b5da078239371a31b17f63ccfd.svg",
            "fullname": "Olga Tsymboi",
            "name": "oltsy",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "organization": {
            "_id": "675861e944dbb69c2673c71c",
            "name": "t-tech",
            "fullname": "T-Tech",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/674ea07d320a043daeb2d98b/IwSCMolFY4Otk7sFXzWhi.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.10739",
            "authors": [
                {
                    "_id": "693b91d89874a2a5e4ffb329",
                    "name": "Songyang Gao",
                    "hidden": false
                },
                {
                    "_id": "693b91d89874a2a5e4ffb32a",
                    "user": {
                        "_id": "6601196cc91ba4c08ad6e270",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/venywO3WPi2fNi5WUJTH0.jpeg",
                        "isPro": false,
                        "fullname": "Yuzhe Gu",
                        "user": "vanilla1116",
                        "type": "user"
                    },
                    "name": "Yuzhe Gu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-12T09:14:56.632Z",
                    "hidden": false
                },
                {
                    "_id": "693b91d89874a2a5e4ffb32b",
                    "name": "Zijian Wu",
                    "hidden": false
                },
                {
                    "_id": "693b91d89874a2a5e4ffb32c",
                    "name": "Lingkai Kong",
                    "hidden": false
                },
                {
                    "_id": "693b91d89874a2a5e4ffb32d",
                    "user": {
                        "_id": "64e8505321540e1da3226b54",
                        "avatarUrl": "/avatars/18958b8406d1ce492b54c1c839f18c54.svg",
                        "isPro": false,
                        "fullname": "Wenwei Zhang",
                        "user": "ZwwWayne",
                        "type": "user"
                    },
                    "name": "Wenwei Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-12T13:03:21.987Z",
                    "hidden": false
                },
                {
                    "_id": "693b91d89874a2a5e4ffb32e",
                    "name": "Zhongrui Cai",
                    "hidden": false
                },
                {
                    "_id": "693b91d89874a2a5e4ffb32f",
                    "name": "Fan Zheng",
                    "hidden": false
                },
                {
                    "_id": "693b91d89874a2a5e4ffb330",
                    "user": {
                        "_id": "670f8df2005a358fdc6c2fb6",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/qw2yocepl2vhC5T2ae49b.png",
                        "isPro": false,
                        "fullname": "tianyou",
                        "user": "matianyou",
                        "type": "user"
                    },
                    "name": "Tianyou Ma",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-12T13:03:57.205Z",
                    "hidden": false
                },
                {
                    "_id": "693b91d89874a2a5e4ffb331",
                    "user": {
                        "_id": "687f853bb39262ba84f3eeff",
                        "avatarUrl": "/avatars/cdfc44fde8237f08f10192553fe5a075.svg",
                        "isPro": false,
                        "fullname": "Junhao Shen",
                        "user": "shenjunhao",
                        "type": "user"
                    },
                    "name": "Junhao Shen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-12T09:15:00.496Z",
                    "hidden": false
                },
                {
                    "_id": "693b91d89874a2a5e4ffb332",
                    "name": "Haiteng Zhao",
                    "hidden": false
                },
                {
                    "_id": "693b91d89874a2a5e4ffb333",
                    "user": {
                        "_id": "6454b1073aaeff9f3d330ef6",
                        "avatarUrl": "/avatars/331fcbbf18a72b0dc8419ca3a77299bb.svg",
                        "isPro": false,
                        "fullname": "Duanyang Zhang",
                        "user": "KKKDaniel",
                        "type": "user"
                    },
                    "name": "Duanyang Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-12T13:04:12.911Z",
                    "hidden": false
                },
                {
                    "_id": "693b91d89874a2a5e4ffb334",
                    "name": "Huilun Zhang",
                    "hidden": false
                },
                {
                    "_id": "693b91d89874a2a5e4ffb335",
                    "user": {
                        "_id": "63fd691794cc8f815d50c112",
                        "avatarUrl": "/avatars/87305d1cbfcc717e910ccdfaf0568f80.svg",
                        "isPro": false,
                        "fullname": "liu",
                        "user": "Harold-lkk",
                        "type": "user"
                    },
                    "name": "Kuikun Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-12T09:14:58.650Z",
                    "hidden": false
                },
                {
                    "_id": "693b91d89874a2a5e4ffb336",
                    "name": "Chengqi Lyu",
                    "hidden": false
                },
                {
                    "_id": "693b91d89874a2a5e4ffb337",
                    "name": "Yanhui Duan",
                    "hidden": false
                },
                {
                    "_id": "693b91d89874a2a5e4ffb338",
                    "name": "Chiyu Chen",
                    "hidden": false
                },
                {
                    "_id": "693b91d89874a2a5e4ffb339",
                    "name": "Ningsheng Ma",
                    "hidden": false
                },
                {
                    "_id": "693b91d89874a2a5e4ffb33a",
                    "name": "Jianfei Gao",
                    "hidden": false
                },
                {
                    "_id": "693b91d89874a2a5e4ffb33b",
                    "name": "Han Lyu",
                    "hidden": false
                },
                {
                    "_id": "693b91d89874a2a5e4ffb33c",
                    "user": {
                        "_id": "636317ed80c1a705a6eff396",
                        "avatarUrl": "/avatars/3db090e101b916d9256d0d3e043db71d.svg",
                        "isPro": false,
                        "fullname": "Dahua Lin",
                        "user": "lindahua",
                        "type": "user"
                    },
                    "name": "Dahua Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-12T13:04:20.346Z",
                    "hidden": false
                },
                {
                    "_id": "693b91d89874a2a5e4ffb33d",
                    "name": "Kai Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-11T15:26:28.000Z",
            "submittedOnDailyAt": "2025-12-12T01:27:55.307Z",
            "title": "Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving",
            "submittedOnDailyBy": {
                "_id": "6601196cc91ba4c08ad6e270",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/venywO3WPi2fNi5WUJTH0.jpeg",
                "isPro": false,
                "fullname": "Yuzhe Gu",
                "user": "vanilla1116",
                "type": "user"
            },
            "summary": "Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out \\thisbench, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2\\% to 73.3\\% on AIME2025 as the compute budget scales.",
            "upvotes": 37,
            "discussionId": "693b91d99874a2a5e4ffb33e",
            "ai_summary": "OPV, an iterative active learning framework with Rejection Fine-Tuning, enhances verification of long reasoning chains in large language models, achieving state-of-the-art results and improving accuracy in collaborative tasks.",
            "ai_keywords": [
                "Reinforcement Learning with Verifiable Rewards (RLVR)",
                "outcome-based verifiers (OVs)",
                "process-based verifiers (PVs)",
                "long reasoning chains of thought (CoTs)",
                "iterative active learning",
                "Rejection Fine-Tuning (RFT)",
                "F1 score",
                "accuracy",
                "DeepSeek-R1-Distill-Qwen-32B",
                "AIME2025"
            ],
            "organization": {
                "_id": "6747ee5decec679eafb90450",
                "name": "ShanghaiAiLab",
                "fullname": "shanghai ailab "
            }
        },
        "publishedAt": "2025-12-11T10:26:28.000Z",
        "title": "Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving",
        "summary": "Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out \\thisbench, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2\\% to 73.3\\% on AIME2025 as the compute budget scales.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.10739.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6601196cc91ba4c08ad6e270",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/venywO3WPi2fNi5WUJTH0.jpeg",
            "fullname": "Yuzhe Gu",
            "name": "vanilla1116",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "organization": {
            "_id": "6747ee5decec679eafb90450",
            "name": "ShanghaiAiLab",
            "fullname": "shanghai ailab "
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.10949",
            "authors": [
                {
                    "_id": "693b895d9874a2a5e4ffb302",
                    "user": {
                        "_id": "6552f1ad5d55ccb20e9142a0",
                        "avatarUrl": "/avatars/0e3e80cba64b5ae0bc5638694ac33dbf.svg",
                        "isPro": false,
                        "fullname": "Ivan Tang",
                        "user": "IvanTang",
                        "type": "user"
                    },
                    "name": "Yiwen Tang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-12T13:04:55.504Z",
                    "hidden": false
                },
                {
                    "_id": "693b895d9874a2a5e4ffb303",
                    "user": {
                        "_id": "642a8302d651bae3c11b72b1",
                        "avatarUrl": "/avatars/4d2d422613e274d80482fed9a7d3f785.svg",
                        "isPro": false,
                        "fullname": "Zoey Guo",
                        "user": "Purple1288",
                        "type": "user"
                    },
                    "name": "Zoey Guo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-12T13:05:01.705Z",
                    "hidden": false
                },
                {
                    "_id": "693b895d9874a2a5e4ffb304",
                    "user": {
                        "_id": "6708920aeae29d1cd41a703b",
                        "avatarUrl": "/avatars/922427a86523b0aa810412fd2d75f88e.svg",
                        "isPro": false,
                        "fullname": "kaixin zhu",
                        "user": "czkk566",
                        "type": "user"
                    },
                    "name": "Kaixin Zhu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-12T09:16:10.275Z",
                    "hidden": false
                },
                {
                    "_id": "693b895d9874a2a5e4ffb305",
                    "name": "Ray Zhang",
                    "hidden": false
                },
                {
                    "_id": "693b895d9874a2a5e4ffb306",
                    "user": {
                        "_id": "6535045a910b844786a6642f",
                        "avatarUrl": "/avatars/37a94864a7a348151837b421ea6d77e3.svg",
                        "isPro": false,
                        "fullname": "Qizhi Chen",
                        "user": "Tavish9",
                        "type": "user"
                    },
                    "name": "Qizhi Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-12T13:05:11.060Z",
                    "hidden": false
                },
                {
                    "_id": "693b895d9874a2a5e4ffb307",
                    "user": {
                        "_id": "6349214f8146350b3a4c5cdf",
                        "avatarUrl": "/avatars/cfd24caac9a87efb528d0f4c375932bc.svg",
                        "isPro": false,
                        "fullname": "Dongzhi Jiang",
                        "user": "CaraJ",
                        "type": "user"
                    },
                    "name": "Dongzhi Jiang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-12T13:06:02.910Z",
                    "hidden": false
                },
                {
                    "_id": "693b895d9874a2a5e4ffb308",
                    "name": "Junli Liu",
                    "hidden": false
                },
                {
                    "_id": "693b895d9874a2a5e4ffb309",
                    "user": {
                        "_id": "6671214c92412fd4640714eb",
                        "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg",
                        "isPro": false,
                        "fullname": "bohan zeng",
                        "user": "zbhpku",
                        "type": "user"
                    },
                    "name": "Bohan Zeng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-12T09:15:08.733Z",
                    "hidden": false
                },
                {
                    "_id": "693b895d9874a2a5e4ffb30a",
                    "user": {
                        "_id": "6662d2c9de4c4e1f04bd29c7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QnnO_KOyZjFd-iXuPnHqR.png",
                        "isPro": false,
                        "fullname": "HaomingSong",
                        "user": "HaomingSong",
                        "type": "user"
                    },
                    "name": "Haoming Song",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-12T13:05:30.062Z",
                    "hidden": false
                },
                {
                    "_id": "693b895d9874a2a5e4ffb30b",
                    "user": {
                        "_id": "64daecec888b7e9c400f59b5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64daecec888b7e9c400f59b5/f4pfOfWk6jYJX-Nf2-qHn.png",
                        "isPro": false,
                        "fullname": "Delin Qu",
                        "user": "delinqu",
                        "type": "user"
                    },
                    "name": "Delin Qu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-12T09:16:34.085Z",
                    "hidden": false
                },
                {
                    "_id": "693b895d9874a2a5e4ffb30c",
                    "name": "Tianyi Bai",
                    "hidden": false
                },
                {
                    "_id": "693b895d9874a2a5e4ffb30d",
                    "name": "Dan Xu",
                    "hidden": false
                },
                {
                    "_id": "693b895d9874a2a5e4ffb30e",
                    "name": "Wentao Zhang",
                    "hidden": false
                },
                {
                    "_id": "693b895d9874a2a5e4ffb30f",
                    "name": "Bin Zhao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-11T18:59:52.000Z",
            "submittedOnDailyAt": "2025-12-12T00:49:43.850Z",
            "title": "Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation",
            "submittedOnDailyBy": {
                "_id": "6552f1ad5d55ccb20e9142a0",
                "avatarUrl": "/avatars/0e3e80cba64b5ae0bc5638694ac33dbf.svg",
                "isPro": false,
                "fullname": "Ivan Tang",
                "user": "IvanTang",
                "type": "user"
            },
            "summary": "Reinforcement learning (RL), earlier proven to be effective in large language and multi-modal models, has been successfully extended to enhance 2D image generation recently. However, applying RL to 3D generation remains largely unexplored due to the higher spatial complexity of 3D objects, which require globally consistent geometry and fine-grained local textures. This makes 3D generation significantly sensitive to reward designs and RL algorithms. To address these challenges, we conduct the first systematic study of RL for text-to-3D autoregressive generation across several dimensions. (1) Reward designs: We evaluate reward dimensions and model choices, showing that alignment with human preference is crucial, and that general multi-modal models provide robust signal for 3D attributes. (2) RL algorithms: We study GRPO variants, highlighting the effectiveness of token-level optimization, and further investigate the scaling of training data and iterations. (3) Text-to-3D Benchmarks: Since existing benchmarks fail to measure implicit reasoning abilities in 3D generation models, we introduce MME-3DR. (4) Advanced RL paradigms: Motivated by the natural hierarchy of 3D generation, we propose Hi-GRPO, which optimizes the global-to-local hierarchical 3D generation through dedicated reward ensembles. Based on these insights, we develop AR3D-R1, the first RL-enhanced text-to-3D model, expert from coarse shape to texture refinement. We hope this study provides insights into RL-driven reasoning for 3D generation. Code is released at https://github.com/Ivan-Tang-3D/3DGen-R1.",
            "upvotes": 36,
            "discussionId": "693b895d9874a2a5e4ffb310",
            "githubRepo": "https://github.com/Ivan-Tang-3D/3DGen-R1",
            "githubRepoAddedBy": "user",
            "ai_summary": "This study investigates reinforcement learning for text-to-3D generation, focusing on reward designs, RL algorithms, benchmarking, and hierarchical optimization, introducing AR3D-R1 as the first RL-enhanced model for 3D generation.",
            "ai_keywords": [
                "reinforcement learning",
                "text-to-3D generation",
                "reward designs",
                "GRPO variants",
                "token-level optimization",
                "MME-3DR",
                "Hi-GRPO",
                "hierarchical 3D generation",
                "AR3D-R1"
            ],
            "githubStars": 40,
            "organization": {
                "_id": "6747ee5decec679eafb90450",
                "name": "ShanghaiAiLab",
                "fullname": "shanghai ailab "
            }
        },
        "publishedAt": "2025-12-11T13:59:52.000Z",
        "title": "Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation",
        "summary": "Reinforcement learning (RL), earlier proven to be effective in large language and multi-modal models, has been successfully extended to enhance 2D image generation recently. However, applying RL to 3D generation remains largely unexplored due to the higher spatial complexity of 3D objects, which require globally consistent geometry and fine-grained local textures. This makes 3D generation significantly sensitive to reward designs and RL algorithms. To address these challenges, we conduct the first systematic study of RL for text-to-3D autoregressive generation across several dimensions. (1) Reward designs: We evaluate reward dimensions and model choices, showing that alignment with human preference is crucial, and that general multi-modal models provide robust signal for 3D attributes. (2) RL algorithms: We study GRPO variants, highlighting the effectiveness of token-level optimization, and further investigate the scaling of training data and iterations. (3) Text-to-3D Benchmarks: Since existing benchmarks fail to measure implicit reasoning abilities in 3D generation models, we introduce MME-3DR. (4) Advanced RL paradigms: Motivated by the natural hierarchy of 3D generation, we propose Hi-GRPO, which optimizes the global-to-local hierarchical 3D generation through dedicated reward ensembles. Based on these insights, we develop AR3D-R1, the first RL-enhanced text-to-3D model, expert from coarse shape to texture refinement. We hope this study provides insights into RL-driven reasoning for 3D generation. Code is released at https://github.com/Ivan-Tang-3D/3DGen-R1.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.10949.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6552f1ad5d55ccb20e9142a0",
            "avatarUrl": "/avatars/0e3e80cba64b5ae0bc5638694ac33dbf.svg",
            "fullname": "Ivan Tang",
            "name": "IvanTang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "6747ee5decec679eafb90450",
            "name": "ShanghaiAiLab",
            "fullname": "shanghai ailab "
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.10756",
            "authors": [
                {
                    "_id": "693b91539874a2a5e4ffb318",
                    "name": "Zijian Wu",
                    "hidden": false
                },
                {
                    "_id": "693b91539874a2a5e4ffb319",
                    "name": "Lingkai Kong",
                    "hidden": false
                },
                {
                    "_id": "693b91539874a2a5e4ffb31a",
                    "name": "Wenwei Zhang",
                    "hidden": false
                },
                {
                    "_id": "693b91539874a2a5e4ffb31b",
                    "name": "Songyang Gao",
                    "hidden": false
                },
                {
                    "_id": "693b91539874a2a5e4ffb31c",
                    "user": {
                        "_id": "6601196cc91ba4c08ad6e270",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/venywO3WPi2fNi5WUJTH0.jpeg",
                        "isPro": false,
                        "fullname": "Yuzhe Gu",
                        "user": "vanilla1116",
                        "type": "user"
                    },
                    "name": "Yuzhe Gu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-12T09:15:02.946Z",
                    "hidden": false
                },
                {
                    "_id": "693b91539874a2a5e4ffb31d",
                    "name": "Zhongrui Cai",
                    "hidden": false
                },
                {
                    "_id": "693b91539874a2a5e4ffb31e",
                    "name": "Tianyou Ma",
                    "hidden": false
                },
                {
                    "_id": "693b91539874a2a5e4ffb31f",
                    "name": "Yuhong Liu",
                    "hidden": false
                },
                {
                    "_id": "693b91539874a2a5e4ffb320",
                    "name": "Zhi Wang",
                    "hidden": false
                },
                {
                    "_id": "693b91539874a2a5e4ffb321",
                    "name": "Runyuan Ma",
                    "hidden": false
                },
                {
                    "_id": "693b91539874a2a5e4ffb322",
                    "name": "Guangyu Wang",
                    "hidden": false
                },
                {
                    "_id": "693b91539874a2a5e4ffb323",
                    "name": "Wei Li",
                    "hidden": false
                },
                {
                    "_id": "693b91539874a2a5e4ffb324",
                    "name": "Conghui He",
                    "hidden": false
                },
                {
                    "_id": "693b91539874a2a5e4ffb325",
                    "name": "Dahua Lin",
                    "hidden": false
                },
                {
                    "_id": "693b91539874a2a5e4ffb326",
                    "name": "Kai Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-11T15:47:38.000Z",
            "submittedOnDailyAt": "2025-12-12T01:23:10.732Z",
            "title": "OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification",
            "submittedOnDailyBy": {
                "_id": "6601196cc91ba4c08ad6e270",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/venywO3WPi2fNi5WUJTH0.jpeg",
                "isPro": false,
                "fullname": "Yuzhe Gu",
                "user": "vanilla1116",
                "type": "user"
            },
            "summary": "Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out OPV-Bench, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2% to 73.3% on AIME2025 as the compute budget scales.",
            "upvotes": 30,
            "discussionId": "693b91539874a2a5e4ffb327",
            "ai_summary": "The Outcome-based Process Verifier (OPV) improves the verification of complex reasoning chains in large language models by combining outcome-based and process-based verification with iterative active learning and Rejection Fine-Tuning, achieving state-of-the-art performance on various benchmarks.",
            "ai_keywords": [
                "Reinforcement Learning with Verifiable Rewards (RLVR)",
                "verifiers",
                "outcome-based verifiers (OVs)",
                "process-based verifiers (PVs)",
                "CoTs",
                "iterative active learning",
                "Rejection Fine-Tuning (RFT)",
                "OPV-Bench",
                "AIME2025",
                "DeepSeek-R1-Distill-Qwen-32B"
            ],
            "organization": {
                "_id": "6747ee5decec679eafb90450",
                "name": "ShanghaiAiLab",
                "fullname": "shanghai ailab "
            }
        },
        "publishedAt": "2025-12-11T10:47:38.000Z",
        "title": "OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification",
        "summary": "Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out OPV-Bench, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2% to 73.3% on AIME2025 as the compute budget scales.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.10756.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6601196cc91ba4c08ad6e270",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/venywO3WPi2fNi5WUJTH0.jpeg",
            "fullname": "Yuzhe Gu",
            "name": "vanilla1116",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "organization": {
            "_id": "6747ee5decec679eafb90450",
            "name": "ShanghaiAiLab",
            "fullname": "shanghai ailab "
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.10534",
            "authors": [
                {
                    "_id": "693b94359874a2a5e4ffb340",
                    "name": "Haiteng Zhao",
                    "hidden": false
                },
                {
                    "_id": "693b94359874a2a5e4ffb341",
                    "user": {
                        "_id": "687f853bb39262ba84f3eeff",
                        "avatarUrl": "/avatars/cdfc44fde8237f08f10192553fe5a075.svg",
                        "isPro": false,
                        "fullname": "Junhao Shen",
                        "user": "shenjunhao",
                        "type": "user"
                    },
                    "name": "Junhao Shen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-12T09:14:51.355Z",
                    "hidden": false
                },
                {
                    "_id": "693b94359874a2a5e4ffb342",
                    "user": {
                        "_id": "64ccaa4687ec96aa4752e754",
                        "avatarUrl": "/avatars/d2dd2040a521de4f55c7335cb7771c75.svg",
                        "isPro": false,
                        "fullname": "Yiming Zhang",
                        "user": "ymzhang319",
                        "type": "user"
                    },
                    "name": "Yiming Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-12T12:59:27.118Z",
                    "hidden": false
                },
                {
                    "_id": "693b94359874a2a5e4ffb343",
                    "name": "Songyang Gao",
                    "hidden": false
                },
                {
                    "_id": "693b94359874a2a5e4ffb344",
                    "user": {
                        "_id": "63fd691794cc8f815d50c112",
                        "avatarUrl": "/avatars/87305d1cbfcc717e910ccdfaf0568f80.svg",
                        "isPro": false,
                        "fullname": "liu",
                        "user": "Harold-lkk",
                        "type": "user"
                    },
                    "name": "Kuikun Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-12T09:14:54.577Z",
                    "hidden": false
                },
                {
                    "_id": "693b94359874a2a5e4ffb345",
                    "name": "Tianyou Ma",
                    "hidden": false
                },
                {
                    "_id": "693b94359874a2a5e4ffb346",
                    "name": "Fan Zheng",
                    "hidden": false
                },
                {
                    "_id": "693b94359874a2a5e4ffb347",
                    "name": "Dahua Lin",
                    "hidden": false
                },
                {
                    "_id": "693b94359874a2a5e4ffb348",
                    "name": "Wenwei Zhang",
                    "hidden": false
                },
                {
                    "_id": "693b94359874a2a5e4ffb349",
                    "name": "Kai Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-11T11:05:04.000Z",
            "submittedOnDailyAt": "2025-12-12T03:29:52.050Z",
            "title": "Achieving Olympia-Level Geometry Large Language Model Agent via Complexity Boosting Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "6601196cc91ba4c08ad6e270",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/venywO3WPi2fNi5WUJTH0.jpeg",
                "isPro": false,
                "fullname": "Yuzhe Gu",
                "user": "vanilla1116",
                "type": "user"
            },
            "summary": "Large language model (LLM) agents exhibit strong mathematical problem-solving abilities and can even solve International Mathematical Olympiad (IMO) level problems with the assistance of formal proof systems. However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation. In this work, we make the first attempt to build a medalist-level LLM agent for geometry and present InternGeometry. InternGeometry overcomes the heuristic limitations in geometry by iteratively proposing propositions and auxiliary constructions, verifying them with a symbolic engine, and reflecting on the engine's feedback to guide subsequent proposals. A dynamic memory mechanism enables InternGeometry to conduct more than two hundred interactions with the symbolic engine per problem. To further accelerate learning, we introduce Complexity-Boosting Reinforcement Learning (CBRL), which gradually increases the complexity of synthesized problems across training stages. Built on InternThinker-32B, InternGeometry solves 44 of 50 IMO geometry problems (2000-2024), exceeding the average gold medalist score (40.9), using only 13K training examples, just 0.004% of the data used by AlphaGeometry 2, demonstrating the potential of LLM agents on expert-level geometry tasks. InternGeometry can also propose novel auxiliary constructions for IMO problems that do not appear in human solutions. We will release the model, data, and symbolic engine to support future research.",
            "upvotes": 25,
            "discussionId": "693b94359874a2a5e4ffb34a",
            "ai_summary": "InternGeometry, an LLM agent, surpasses human performance on IMO geometry problems using a heuristic-driven approach with iterative proposition verification and a dynamic memory mechanism, significantly outperforming AlphaGeometry 2 with limited training data.",
            "ai_keywords": [
                "LLM agents",
                "mathematical problem-solving",
                "International Mathematical Olympiad",
                "AI for geometry",
                "AlphaGeometry 2",
                "large-scale data synthesis",
                "search",
                "heuristic limitations",
                "propositions",
                "auxiliary constructions",
                "symbolic engine",
                "dynamic memory mechanism",
                "Complexity-Boosting Reinforcement Learning",
                "CBRL",
                "training examples",
                "InternThinker-32B",
                "expert-level geometry tasks"
            ],
            "organization": {
                "_id": "6747ee5decec679eafb90450",
                "name": "ShanghaiAiLab",
                "fullname": "shanghai ailab "
            }
        },
        "publishedAt": "2025-12-11T06:05:04.000Z",
        "title": "Achieving Olympia-Level Geometry Large Language Model Agent via Complexity Boosting Reinforcement Learning",
        "summary": "Large language model (LLM) agents exhibit strong mathematical problem-solving abilities and can even solve International Mathematical Olympiad (IMO) level problems with the assistance of formal proof systems. However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation. In this work, we make the first attempt to build a medalist-level LLM agent for geometry and present InternGeometry. InternGeometry overcomes the heuristic limitations in geometry by iteratively proposing propositions and auxiliary constructions, verifying them with a symbolic engine, and reflecting on the engine's feedback to guide subsequent proposals. A dynamic memory mechanism enables InternGeometry to conduct more than two hundred interactions with the symbolic engine per problem. To further accelerate learning, we introduce Complexity-Boosting Reinforcement Learning (CBRL), which gradually increases the complexity of synthesized problems across training stages. Built on InternThinker-32B, InternGeometry solves 44 of 50 IMO geometry problems (2000-2024), exceeding the average gold medalist score (40.9), using only 13K training examples, just 0.004% of the data used by AlphaGeometry 2, demonstrating the potential of LLM agents on expert-level geometry tasks. InternGeometry can also propose novel auxiliary constructions for IMO problems that do not appear in human solutions. We will release the model, data, and symbolic engine to support future research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.10534.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6601196cc91ba4c08ad6e270",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/venywO3WPi2fNi5WUJTH0.jpeg",
            "fullname": "Yuzhe Gu",
            "name": "vanilla1116",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "organization": {
            "_id": "6747ee5decec679eafb90450",
            "name": "ShanghaiAiLab",
            "fullname": "shanghai ailab "
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.10881",
            "authors": [
                {
                    "_id": "693b87039874a2a5e4ffb2f5",
                    "user": {
                        "_id": "6412d08e027aea38bc90c802",
                        "avatarUrl": "/avatars/86e3fa33193305af591d7d3cc79feb5c.svg",
                        "isPro": false,
                        "fullname": "Gongkehong",
                        "user": "kehong",
                        "type": "user"
                    },
                    "name": "Kehong Gong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-12T13:07:11.844Z",
                    "hidden": false
                },
                {
                    "_id": "693b87039874a2a5e4ffb2f6",
                    "user": {
                        "_id": "62f4b9edd6d189cbd25fc7f8",
                        "avatarUrl": "/avatars/12428e2ca213bf64c86dc7ed26b4dbcc.svg",
                        "isPro": false,
                        "fullname": "Zhengyu Wen",
                        "user": "wzy27",
                        "type": "user"
                    },
                    "name": "Zhengyu Wen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-12T13:07:21.767Z",
                    "hidden": false
                },
                {
                    "_id": "693b87039874a2a5e4ffb2f7",
                    "user": {
                        "_id": "68faf7fcc8107da320682a57",
                        "avatarUrl": "/avatars/324fb883e23acc65ce8a14af87a37c65.svg",
                        "isPro": false,
                        "fullname": "He Weixia",
                        "user": "weixia111111",
                        "type": "user"
                    },
                    "name": "Weixia He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-12T13:07:32.488Z",
                    "hidden": false
                },
                {
                    "_id": "693b87039874a2a5e4ffb2f8",
                    "name": "Mingxi Xu",
                    "hidden": false
                },
                {
                    "_id": "693b87039874a2a5e4ffb2f9",
                    "name": "Qi Wang",
                    "hidden": false
                },
                {
                    "_id": "693b87039874a2a5e4ffb2fa",
                    "name": "Ning Zhang",
                    "hidden": false
                },
                {
                    "_id": "693b87039874a2a5e4ffb2fb",
                    "name": "Zhengyu Li",
                    "hidden": false
                },
                {
                    "_id": "693b87039874a2a5e4ffb2fc",
                    "user": {
                        "_id": "64ac10d2c08741fc97bda2e9",
                        "avatarUrl": "/avatars/ffe028f28e2284a44b0a0617f4abe387.svg",
                        "isPro": false,
                        "fullname": "Dongze Lian",
                        "user": "DonaldLian",
                        "type": "user"
                    },
                    "name": "Dongze Lian",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-12T13:07:42.284Z",
                    "hidden": false
                },
                {
                    "_id": "693b87039874a2a5e4ffb2fd",
                    "name": "Wei Zhao",
                    "hidden": false
                },
                {
                    "_id": "693b87039874a2a5e4ffb2fe",
                    "name": "Xiaoyu He",
                    "hidden": false
                },
                {
                    "_id": "693b87039874a2a5e4ffb2ff",
                    "name": "Mingyuan Zhang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/xNfWwMFlhegi3RqwZxqPG.png",
                "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/xdSriPIZ-k8QLE8m0A8c0.qt"
            ],
            "publishedAt": "2025-12-11T18:09:48.000Z",
            "submittedOnDailyAt": "2025-12-12T00:41:38.576Z",
            "title": "MoCapAnything: Unified 3D Motion Capture for Arbitrary Skeletons from Monocular Videos",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Motion capture now underpins content creation far beyond digital humans, yet most existing pipelines remain species- or template-specific. We formalize this gap as Category-Agnostic Motion Capture (CAMoCap): given a monocular video and an arbitrary rigged 3D asset as a prompt, the goal is to reconstruct a rotation-based animation such as BVH that directly drives the specific asset. We present MoCapAnything, a reference-guided, factorized framework that first predicts 3D joint trajectories and then recovers asset-specific rotations via constraint-aware inverse kinematics. The system contains three learnable modules and a lightweight IK stage: (1) a Reference Prompt Encoder that extracts per-joint queries from the asset's skeleton, mesh, and rendered images; (2) a Video Feature Extractor that computes dense visual descriptors and reconstructs a coarse 4D deforming mesh to bridge the gap between video and joint space; and (3) a Unified Motion Decoder that fuses these cues to produce temporally coherent trajectories. We also curate Truebones Zoo with 1038 motion clips, each providing a standardized skeleton-mesh-render triad. Experiments on both in-domain benchmarks and in-the-wild videos show that MoCapAnything delivers high-quality skeletal animations and exhibits meaningful cross-species retargeting across heterogeneous rigs, enabling scalable, prompt-driven 3D motion capture for arbitrary assets. Project page: https://animotionlab.github.io/MoCapAnything/",
            "upvotes": 20,
            "discussionId": "693b87049874a2a5e4ffb300",
            "ai_summary": "MoCapAnything is a reference-guided framework that reconstructs rotation-based animations from monocular video for arbitrary rigged 3D assets, enabling cross-species retargeting and scalable 3D motion capture.",
            "ai_keywords": [
                "Category-Agnostic Motion Capture",
                "CAMoCap",
                "MoCapAnything",
                "Reference Prompt Encoder",
                "Video Feature Extractor",
                "Unified Motion Decoder",
                "3D joint trajectories",
                "inverse kinematics",
                "BVH",
                "Truebones Zoo",
                "skeletal animations",
                "cross-species retargeting"
            ]
        },
        "publishedAt": "2025-12-11T13:09:48.000Z",
        "title": "MoCapAnything: Unified 3D Motion Capture for Arbitrary Skeletons from Monocular Videos",
        "summary": "Motion capture now underpins content creation far beyond digital humans, yet most existing pipelines remain species- or template-specific. We formalize this gap as Category-Agnostic Motion Capture (CAMoCap): given a monocular video and an arbitrary rigged 3D asset as a prompt, the goal is to reconstruct a rotation-based animation such as BVH that directly drives the specific asset. We present MoCapAnything, a reference-guided, factorized framework that first predicts 3D joint trajectories and then recovers asset-specific rotations via constraint-aware inverse kinematics. The system contains three learnable modules and a lightweight IK stage: (1) a Reference Prompt Encoder that extracts per-joint queries from the asset's skeleton, mesh, and rendered images; (2) a Video Feature Extractor that computes dense visual descriptors and reconstructs a coarse 4D deforming mesh to bridge the gap between video and joint space; and (3) a Unified Motion Decoder that fuses these cues to produce temporally coherent trajectories. We also curate Truebones Zoo with 1038 motion clips, each providing a standardized skeleton-mesh-render triad. Experiments on both in-domain benchmarks and in-the-wild videos show that MoCapAnything delivers high-quality skeletal animations and exhibits meaningful cross-species retargeting across heterogeneous rigs, enabling scalable, prompt-driven 3D motion capture for arbitrary assets. Project page: https://animotionlab.github.io/MoCapAnything/",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/xNfWwMFlhegi3RqwZxqPG.png",
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/xdSriPIZ-k8QLE8m0A8c0.qt"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.10881.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 182
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.05439",
            "authors": [
                {
                    "_id": "693c5619f516c69324680c76",
                    "name": "Tarun Suresh",
                    "hidden": false
                },
                {
                    "_id": "693c5619f516c69324680c77",
                    "name": "Nalin Wadhwa",
                    "hidden": false
                },
                {
                    "_id": "693c5619f516c69324680c78",
                    "name": "Debangshu Banerjee",
                    "hidden": false
                },
                {
                    "_id": "693c5619f516c69324680c79",
                    "name": "Gagandeep Singh",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-05T05:34:06.000Z",
            "submittedOnDailyAt": "2025-12-12T15:32:48.549Z",
            "title": "BEAVER: An Efficient Deterministic LLM Verifier",
            "submittedOnDailyBy": {
                "_id": "65e7bb35e5e78134ab049942",
                "avatarUrl": "/avatars/3c0972f0d59e51ebb5c218ee736d4458.svg",
                "isPro": false,
                "fullname": "Tarun Suresh",
                "user": "tarsur909",
                "type": "user"
            },
            "summary": "As large language models (LLMs) transition from research prototypes to production systems, practitioners often need reliable methods to verify that model outputs satisfy required constraints. While sampling-based estimates provide an intuition of model behavior, they offer no sound guarantees. We present BEAVER, the first practical framework for computing deterministic, sound probability bounds on LLM constraint satisfaction. Given any prefix-closed semantic constraint, BEAVER systematically explores the generation space using novel token trie and frontier data structures, maintaining provably sound bounds at every iteration. We formalize the verification problem, prove soundness of our approach, and evaluate BEAVER on correctness verification, privacy verification and secure code generation tasks across multiple state of the art LLMs. BEAVER achieves 6 to 8 times tighter probability bounds and identifies 3 to 4 times more high risk instances compared to baseline methods under identical computational budgets, enabling precise characterization and risk assessment that loose bounds or empirical evaluation cannot provide.",
            "upvotes": 14,
            "discussionId": "693c561af516c69324680c7a",
            "ai_summary": "BEAVER is a framework that provides deterministic and sound probability bounds for verifying constraints in large language models, achieving tighter bounds and identifying more high-risk instances than baseline methods.",
            "ai_keywords": [
                "large language models",
                "LLMs",
                "BEAVER",
                "prefix-closed semantic constraint",
                "token trie",
                "frontier data structures",
                "sound probability bounds",
                "correctness verification",
                "privacy verification",
                "secure code generation",
                "high-risk instances"
            ],
            "organization": {
                "_id": "65448bef5b5d9185ba3202b9",
                "name": "UIUC-CS",
                "fullname": "University of Illinois at Urbana-Champaign",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"
            }
        },
        "publishedAt": "2025-12-05T00:34:06.000Z",
        "title": "BEAVER: An Efficient Deterministic LLM Verifier",
        "summary": "As large language models (LLMs) transition from research prototypes to production systems, practitioners often need reliable methods to verify that model outputs satisfy required constraints. While sampling-based estimates provide an intuition of model behavior, they offer no sound guarantees. We present BEAVER, the first practical framework for computing deterministic, sound probability bounds on LLM constraint satisfaction. Given any prefix-closed semantic constraint, BEAVER systematically explores the generation space using novel token trie and frontier data structures, maintaining provably sound bounds at every iteration. We formalize the verification problem, prove soundness of our approach, and evaluate BEAVER on correctness verification, privacy verification and secure code generation tasks across multiple state of the art LLMs. BEAVER achieves 6 to 8 times tighter probability bounds and identifies 3 to 4 times more high risk instances compared to baseline methods under identical computational budgets, enabling precise characterization and risk assessment that loose bounds or empirical evaluation cannot provide.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.05439.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65e7bb35e5e78134ab049942",
            "avatarUrl": "/avatars/3c0972f0d59e51ebb5c218ee736d4458.svg",
            "fullname": "Tarun Suresh",
            "name": "tarsur909",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "organization": {
            "_id": "65448bef5b5d9185ba3202b9",
            "name": "UIUC-CS",
            "fullname": "University of Illinois at Urbana-Champaign",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.10867",
            "authors": [
                {
                    "_id": "693bb6089874a2a5e4ffb39e",
                    "user": {
                        "_id": "670f86e4d75f114352916a35",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/g3zZ6TTbgV-Xu789lPaYN.png",
                        "isPro": false,
                        "fullname": "Li",
                        "user": "zongzhao",
                        "type": "user"
                    },
                    "name": "Zongzhao Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-12T09:14:42.257Z",
                    "hidden": false
                },
                {
                    "_id": "693bb6089874a2a5e4ffb39f",
                    "name": "Xiangzhe Kong",
                    "hidden": false
                },
                {
                    "_id": "693bb6089874a2a5e4ffb3a0",
                    "name": "Jiahui Su",
                    "hidden": false
                },
                {
                    "_id": "693bb6089874a2a5e4ffb3a1",
                    "name": "Zongyang Ma",
                    "hidden": false
                },
                {
                    "_id": "693bb6089874a2a5e4ffb3a2",
                    "name": "Mingze Li",
                    "hidden": false
                },
                {
                    "_id": "693bb6089874a2a5e4ffb3a3",
                    "name": "Songyou Li",
                    "hidden": false
                },
                {
                    "_id": "693bb6089874a2a5e4ffb3a4",
                    "name": "Yuelin Zhang",
                    "hidden": false
                },
                {
                    "_id": "693bb6089874a2a5e4ffb3a5",
                    "name": "Yu Rong",
                    "hidden": false
                },
                {
                    "_id": "693bb6089874a2a5e4ffb3a6",
                    "name": "Tingyang Xu",
                    "hidden": false
                },
                {
                    "_id": "693bb6089874a2a5e4ffb3a7",
                    "name": "Deli Zhao",
                    "hidden": false
                },
                {
                    "_id": "693bb6089874a2a5e4ffb3a8",
                    "name": "Wenbing Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-11T18:00:21.000Z",
            "submittedOnDailyAt": "2025-12-12T04:05:11.878Z",
            "title": "From Macro to Micro: Benchmarking Microscopic Spatial Intelligence on Molecules via Vision-Language Models",
            "submittedOnDailyBy": {
                "_id": "61bb00f6c4ac95d207b25f1b",
                "avatarUrl": "/avatars/3b6eba701d64518d6f694942f5b2e9a9.svg",
                "isPro": false,
                "fullname": "Zongyang Ma",
                "user": "zyma",
                "type": "user"
            },
            "summary": "This paper introduces the concept of Microscopic Spatial Intelligence (MiSI), the capability to perceive and reason about the spatial relationships of invisible microscopic entities, which is fundamental to scientific discovery. To assess the potential of Vision-Language Models (VLMs) in this domain, we propose a systematic benchmark framework MiSI-Bench. This framework features over 163,000 question-answer pairs and 587,000 images derived from approximately 4,000 molecular structures, covering nine complementary tasks that evaluate abilities ranging from elementary spatial transformations to complex relational identifications. Experimental results reveal that current state-of-the-art VLMs perform significantly below human level on this benchmark. However, a fine-tuned 7B model demonstrates substantial potential, even surpassing humans in spatial transformation tasks, while its poor performance in scientifically-grounded tasks like hydrogen bond recognition underscores the necessity of integrating explicit domain knowledge for progress toward scientific AGI. The datasets are available at https://huggingface.co/datasets/zongzhao/MiSI-bench.",
            "upvotes": 11,
            "discussionId": "693bb6099874a2a5e4ffb3a9",
            "ai_summary": "A benchmark framework evaluates Vision-Language Models in understanding microscopic spatial relationships, showing potential but highlighting the need for domain-specific knowledge integration.",
            "ai_keywords": [
                "Vision-Language Models",
                "VLMs",
                "Microscopic Spatial Intelligence",
                "MiSI",
                "MiSI-Bench",
                "spatial transformations",
                "relational identifications",
                "hydrogen bond recognition",
                "scientific AGI"
            ],
            "organization": {
                "_id": "622177ac43826d6f261f8208",
                "name": "RUC",
                "fullname": "Renmin University of China",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/670IAX9A2-BflqA5MiSBW.jpeg"
            }
        },
        "publishedAt": "2025-12-11T13:00:21.000Z",
        "title": "From Macro to Micro: Benchmarking Microscopic Spatial Intelligence on Molecules via Vision-Language Models",
        "summary": "This paper introduces the concept of Microscopic Spatial Intelligence (MiSI), the capability to perceive and reason about the spatial relationships of invisible microscopic entities, which is fundamental to scientific discovery. To assess the potential of Vision-Language Models (VLMs) in this domain, we propose a systematic benchmark framework MiSI-Bench. This framework features over 163,000 question-answer pairs and 587,000 images derived from approximately 4,000 molecular structures, covering nine complementary tasks that evaluate abilities ranging from elementary spatial transformations to complex relational identifications. Experimental results reveal that current state-of-the-art VLMs perform significantly below human level on this benchmark. However, a fine-tuned 7B model demonstrates substantial potential, even surpassing humans in spatial transformation tasks, while its poor performance in scientifically-grounded tasks like hydrogen bond recognition underscores the necessity of integrating explicit domain knowledge for progress toward scientific AGI. The datasets are available at https://huggingface.co/datasets/zongzhao/MiSI-bench.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.10867.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "61bb00f6c4ac95d207b25f1b",
            "avatarUrl": "/avatars/3b6eba701d64518d6f694942f5b2e9a9.svg",
            "fullname": "Zongyang Ma",
            "name": "zyma",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "622177ac43826d6f261f8208",
            "name": "RUC",
            "fullname": "Renmin University of China",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/670IAX9A2-BflqA5MiSBW.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2511.23386",
            "authors": [
                {
                    "_id": "693ba6879874a2a5e4ffb362",
                    "name": "Sinan Du",
                    "hidden": false
                },
                {
                    "_id": "693ba6879874a2a5e4ffb363",
                    "name": "Jiahao Guo",
                    "hidden": false
                },
                {
                    "_id": "693ba6879874a2a5e4ffb364",
                    "user": {
                        "_id": "68fed13597cae9c0f484ebed",
                        "avatarUrl": "/avatars/8df380d81daab4fae8a52d0816bf8099.svg",
                        "isPro": false,
                        "fullname": "libo",
                        "user": "libo31",
                        "type": "user"
                    },
                    "name": "Bo Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-12T09:14:49.405Z",
                    "hidden": false
                },
                {
                    "_id": "693ba6879874a2a5e4ffb365",
                    "name": "Shuhao Cui",
                    "hidden": false
                },
                {
                    "_id": "693ba6879874a2a5e4ffb366",
                    "name": "Zhengzhuo Xu",
                    "hidden": false
                },
                {
                    "_id": "693ba6879874a2a5e4ffb367",
                    "name": "Yifu Luo",
                    "hidden": false
                },
                {
                    "_id": "693ba6879874a2a5e4ffb368",
                    "name": "Yongxian Wei",
                    "hidden": false
                },
                {
                    "_id": "693ba6879874a2a5e4ffb369",
                    "name": "Kun Gai",
                    "hidden": false
                },
                {
                    "_id": "693ba6879874a2a5e4ffb36a",
                    "name": "Xinggang Wang",
                    "hidden": false
                },
                {
                    "_id": "693ba6879874a2a5e4ffb36b",
                    "user": {
                        "_id": "68e741ea3edb0ff47e20084e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68e741ea3edb0ff47e20084e/OyBgFqcU4QWyPF_K58Gt5.jpeg",
                        "isPro": false,
                        "fullname": "Wu Kai",
                        "user": "KaiiWuu1993",
                        "type": "user"
                    },
                    "name": "Kai Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-12T09:14:47.442Z",
                    "hidden": false
                },
                {
                    "_id": "693ba6879874a2a5e4ffb36c",
                    "name": "Chun Yuan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-11-28T17:26:34.000Z",
            "submittedOnDailyAt": "2025-12-12T02:54:20.848Z",
            "title": "VQRAE: Representation Quantization Autoencoders for Multimodal Understanding, Generation and Reconstruction",
            "submittedOnDailyBy": {
                "_id": "68e741ea3edb0ff47e20084e",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68e741ea3edb0ff47e20084e/OyBgFqcU4QWyPF_K58Gt5.jpeg",
                "isPro": false,
                "fullname": "Wu Kai",
                "user": "KaiiWuu1993",
                "type": "user"
            },
            "summary": "Unifying multimodal understanding, generation and reconstruction representation in a single tokenizer remains a key challenge in building unified models. Previous research predominantly attempts to address this in a dual encoder paradigm, e.g., utilizing the separate encoders for understanding and generation respectively or balancing semantic representations and low-level features with contrastive loss. In this paper, we propose VQRAE, a Vector Quantization version of Representation AutoEncoders, which pioneers the first exploration in unified representation to produce Continuous semantic features for image understanding and Discrete tokens for visual generation within a unified tokenizer. Specifically, we build upon pretrained vision foundation models with a symmetric ViT decoder and adopt a two-stage training strategy: first, it freezes the encoder and learns a high-dimensional semantic VQ codebook with pixel reconstruction objective; then jointly optimizes the encoder with self-distillation constraints. This design enables negligible semantic information for maintaining the ability of multimodal understanding, discrete tokens that are compatible for generation and fine-grained reconstruction. Besides, we identify the intriguing property in quantizing semantic encoders that rely on high-dimensional codebook in contrast to the previous common practice of low-dimensional codebook in image reconstruction. The semantic VQ codebook can achieve a 100% utilization ratio at a dimension of 1536. VQRAE presents competitive performance on several benchmarks of visual understanding, generation and reconstruction with promising scaling property in the autoregressive paradigm for its discrete merits.",
            "upvotes": 10,
            "discussionId": "693ba6879874a2a5e4ffb36d",
            "ai_summary": "VQRAE, a Vector Quantization Representation AutoEncoder, unifies multimodal understanding, generation, and reconstruction using a unified tokenizer with continuous semantic features and discrete tokens.",
            "ai_keywords": [
                "multimodal understanding",
                "generation",
                "reconstruction",
                "tokenizer",
                "dual encoder paradigm",
                "VQRAE",
                "Representation AutoEncoders",
                "Vector Quantization",
                "symmetric ViT decoder",
                "two-stage training strategy",
                "encoder",
                "self-distillation constraints",
                "semantic VQ codebook",
                "pixel reconstruction objective",
                "multimodal understanding",
                "discrete tokens",
                "fine-grained reconstruction",
                "autoregressive paradigm"
            ],
            "organization": {
                "_id": "665f02ce9f9e5b38d0a256a8",
                "name": "Kwai-Kolors",
                "fullname": "Kolors Team, Kuaishou Technology",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62f0babaef9cc6810cec02ff/sVnELkcfVo5kxg5308rkr.png"
            }
        },
        "publishedAt": "2025-11-28T12:26:34.000Z",
        "title": "VQRAE: Representation Quantization Autoencoders for Multimodal Understanding, Generation and Reconstruction",
        "summary": "Unifying multimodal understanding, generation and reconstruction representation in a single tokenizer remains a key challenge in building unified models. Previous research predominantly attempts to address this in a dual encoder paradigm, e.g., utilizing the separate encoders for understanding and generation respectively or balancing semantic representations and low-level features with contrastive loss. In this paper, we propose VQRAE, a Vector Quantization version of Representation AutoEncoders, which pioneers the first exploration in unified representation to produce Continuous semantic features for image understanding and Discrete tokens for visual generation within a unified tokenizer. Specifically, we build upon pretrained vision foundation models with a symmetric ViT decoder and adopt a two-stage training strategy: first, it freezes the encoder and learns a high-dimensional semantic VQ codebook with pixel reconstruction objective; then jointly optimizes the encoder with self-distillation constraints. This design enables negligible semantic information for maintaining the ability of multimodal understanding, discrete tokens that are compatible for generation and fine-grained reconstruction. Besides, we identify the intriguing property in quantizing semantic encoders that rely on high-dimensional codebook in contrast to the previous common practice of low-dimensional codebook in image reconstruction. The semantic VQ codebook can achieve a 100% utilization ratio at a dimension of 1536. VQRAE presents competitive performance on several benchmarks of visual understanding, generation and reconstruction with promising scaling property in the autoregressive paradigm for its discrete merits.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.23386.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "68e741ea3edb0ff47e20084e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68e741ea3edb0ff47e20084e/OyBgFqcU4QWyPF_K58Gt5.jpeg",
            "fullname": "Wu Kai",
            "name": "KaiiWuu1993",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "665f02ce9f9e5b38d0a256a8",
            "name": "Kwai-Kolors",
            "fullname": "Kolors Team, Kuaishou Technology",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62f0babaef9cc6810cec02ff/sVnELkcfVo5kxg5308rkr.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.10675",
            "authors": [
                {
                    "_id": "693b83939874a2a5e4ffb2a4",
                    "name": "Gemini Robotics Team",
                    "hidden": false
                },
                {
                    "_id": "693b83939874a2a5e4ffb2a5",
                    "name": "Coline Devin",
                    "hidden": false
                },
                {
                    "_id": "693b83939874a2a5e4ffb2a6",
                    "user": {
                        "_id": "63c9bd445fdc575773c732fe",
                        "avatarUrl": "/avatars/def472d1ab3fbf751225357c0932ae7e.svg",
                        "isPro": false,
                        "fullname": "Yilun Du",
                        "user": "yilundu",
                        "type": "user"
                    },
                    "name": "Yilun Du",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-12T13:14:15.439Z",
                    "hidden": false
                },
                {
                    "_id": "693b83939874a2a5e4ffb2a7",
                    "name": "Debidatta Dwibedi",
                    "hidden": false
                },
                {
                    "_id": "693b83939874a2a5e4ffb2a8",
                    "name": "Ruiqi Gao",
                    "hidden": false
                },
                {
                    "_id": "693b83939874a2a5e4ffb2a9",
                    "name": "Abhishek Jindal",
                    "hidden": false
                },
                {
                    "_id": "693b83939874a2a5e4ffb2aa",
                    "user": {
                        "_id": "6413572b6cd62eb3ba2024e9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6413572b6cd62eb3ba2024e9/UCtMdeV8_N6w-SBnTDXBy.jpeg",
                        "isPro": false,
                        "fullname": "Thomas Kipf",
                        "user": "tkipf",
                        "type": "user"
                    },
                    "name": "Thomas Kipf",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-12T13:14:33.451Z",
                    "hidden": false
                },
                {
                    "_id": "693b83939874a2a5e4ffb2ab",
                    "user": {
                        "_id": "648c85f33ffc11989bddcce1",
                        "avatarUrl": "/avatars/bdde73b95a36e32cb2975656cca46022.svg",
                        "isPro": false,
                        "fullname": "Sean Kirmani",
                        "user": "skirmani",
                        "type": "user"
                    },
                    "name": "Sean Kirmani",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-12T13:14:39.448Z",
                    "hidden": false
                },
                {
                    "_id": "693b83939874a2a5e4ffb2ac",
                    "user": {
                        "_id": "63609ef63605bd411c1c7509",
                        "avatarUrl": "/avatars/3fadc3c4117cdf17d25f3efc545388cd.svg",
                        "isPro": false,
                        "fullname": "Fangchen Liu",
                        "user": "fangchenliu",
                        "type": "user"
                    },
                    "name": "Fangchen Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-12T13:14:44.842Z",
                    "hidden": false
                },
                {
                    "_id": "693b83939874a2a5e4ffb2ad",
                    "name": "Anirudha Majumdar",
                    "hidden": false
                },
                {
                    "_id": "693b83939874a2a5e4ffb2ae",
                    "user": {
                        "_id": "60ac2ec408805574fcf076f4",
                        "avatarUrl": "/avatars/a8cd60f39ba89939c5841252dc9d742b.svg",
                        "isPro": false,
                        "fullname": "Andrew Marmon",
                        "user": "acoadmarmon",
                        "type": "user"
                    },
                    "name": "Andrew Marmon",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-12T13:14:54.023Z",
                    "hidden": false
                },
                {
                    "_id": "693b83939874a2a5e4ffb2af",
                    "user": {
                        "_id": "65b7c3e87817e067a5b66cf1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/KysLXVoFmkPM2pQMgDlsb.jpeg",
                        "isPro": false,
                        "fullname": "Carolina Parada",
                        "user": "CarolinaParada",
                        "type": "user"
                    },
                    "name": "Carolina Parada",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-12T13:14:59.800Z",
                    "hidden": false
                },
                {
                    "_id": "693b83939874a2a5e4ffb2b0",
                    "name": "Yulia Rubanova",
                    "hidden": false
                },
                {
                    "_id": "693b83939874a2a5e4ffb2b1",
                    "name": "Dhruv Shah",
                    "hidden": false
                },
                {
                    "_id": "693b83939874a2a5e4ffb2b2",
                    "name": "Vikas Sindhwani",
                    "hidden": false
                },
                {
                    "_id": "693b83939874a2a5e4ffb2b3",
                    "name": "Jie Tan",
                    "hidden": false
                },
                {
                    "_id": "693b83939874a2a5e4ffb2b4",
                    "name": "Fei Xia",
                    "hidden": false
                },
                {
                    "_id": "693b83939874a2a5e4ffb2b5",
                    "name": "Ted Xiao",
                    "hidden": false
                },
                {
                    "_id": "693b83939874a2a5e4ffb2b6",
                    "name": "Sherry Yang",
                    "hidden": false
                },
                {
                    "_id": "693b83939874a2a5e4ffb2b7",
                    "name": "Wenhao Yu",
                    "hidden": false
                },
                {
                    "_id": "693b83939874a2a5e4ffb2b8",
                    "name": "Allan Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-11T14:22:14.000Z",
            "submittedOnDailyAt": "2025-12-12T00:23:17.104Z",
            "title": "Evaluating Gemini Robotics Policies in a Veo World Simulator",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Generative world models hold significant potential for simulating interactions with visuomotor policies in varied environments. Frontier video models can enable generation of realistic observations and environment interactions in a scalable and general manner. However, the use of video models in robotics has been limited primarily to in-distribution evaluations, i.e., scenarios that are similar to ones used to train the policy or fine-tune the base video model. In this report, we demonstrate that video models can be used for the entire spectrum of policy evaluation use cases in robotics: from assessing nominal performance to out-of-distribution (OOD) generalization, and probing physical and semantic safety. We introduce a generative evaluation system built upon a frontier video foundation model (Veo). The system is optimized to support robot action conditioning and multi-view consistency, while integrating generative image-editing and multi-view completion to synthesize realistic variations of real-world scenes along multiple axes of generalization. We demonstrate that the system preserves the base capabilities of the video model to enable accurate simulation of scenes that have been edited to include novel interaction objects, novel visual backgrounds, and novel distractor objects. This fidelity enables accurately predicting the relative performance of different policies in both nominal and OOD conditions, determining the relative impact of different axes of generalization on policy performance, and performing red teaming of policies to expose behaviors that violate physical or semantic safety constraints. We validate these capabilities through 1600+ real-world evaluations of eight Gemini Robotics policy checkpoints and five tasks for a bimanual manipulator.",
            "upvotes": 8,
            "discussionId": "693b83939874a2a5e4ffb2b9",
            "ai_summary": "A generative evaluation system using a frontier video model (Veo) enables comprehensive policy evaluation in robotics, including nominal performance, out-of-distribution generalization, and safety checks.",
            "ai_keywords": [
                "generative world models",
                "video models",
                "policy evaluation",
                "out-of-distribution (OOD) generalization",
                "generative evaluation system",
                "frontier video foundation model",
                "Veo",
                "robot action conditioning",
                "multi-view consistency",
                "generative image-editing",
                "multi-view completion",
                "bimanual manipulator"
            ],
            "organization": {
                "_id": "60f6cbb2852126bac698c89e",
                "name": "deepmind",
                "fullname": "Deepmind",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1638956859875-5f1158120c833276f61f1a84.jpeg"
            }
        },
        "publishedAt": "2025-12-11T09:22:14.000Z",
        "title": "Evaluating Gemini Robotics Policies in a Veo World Simulator",
        "summary": "Generative world models hold significant potential for simulating interactions with visuomotor policies in varied environments. Frontier video models can enable generation of realistic observations and environment interactions in a scalable and general manner. However, the use of video models in robotics has been limited primarily to in-distribution evaluations, i.e., scenarios that are similar to ones used to train the policy or fine-tune the base video model. In this report, we demonstrate that video models can be used for the entire spectrum of policy evaluation use cases in robotics: from assessing nominal performance to out-of-distribution (OOD) generalization, and probing physical and semantic safety. We introduce a generative evaluation system built upon a frontier video foundation model (Veo). The system is optimized to support robot action conditioning and multi-view consistency, while integrating generative image-editing and multi-view completion to synthesize realistic variations of real-world scenes along multiple axes of generalization. We demonstrate that the system preserves the base capabilities of the video model to enable accurate simulation of scenes that have been edited to include novel interaction objects, novel visual backgrounds, and novel distractor objects. This fidelity enables accurately predicting the relative performance of different policies in both nominal and OOD conditions, determining the relative impact of different axes of generalization on policy performance, and performing red teaming of policies to expose behaviors that violate physical or semantic safety constraints. We validate these capabilities through 1600+ real-world evaluations of eight Gemini Robotics policy checkpoints and five tasks for a bimanual manipulator.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.10675.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 182
        },
        "organization": {
            "_id": "60f6cbb2852126bac698c89e",
            "name": "deepmind",
            "fullname": "Deepmind",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1638956859875-5f1158120c833276f61f1a84.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.08511",
            "authors": [
                {
                    "_id": "693bcd629874a2a5e4ffb41a",
                    "user": {
                        "_id": "662f1aa0e4a1f7302a7f134a",
                        "avatarUrl": "/avatars/2fef164e2ccd24c7f3c46942e4b12ab5.svg",
                        "isPro": false,
                        "fullname": "Wenxi Yang",
                        "user": "ywenxi",
                        "type": "user"
                    },
                    "name": "Wenxi Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-12T09:14:15.266Z",
                    "hidden": false
                },
                {
                    "_id": "693bcd629874a2a5e4ffb41b",
                    "user": {
                        "_id": "635b6b3590204c509f9755bd",
                        "avatarUrl": "/avatars/94398dd17ca1a2e74dc8d775abf636a7.svg",
                        "isPro": false,
                        "fullname": "zhaoyuzhong",
                        "user": "callsys",
                        "type": "user"
                    },
                    "name": "Yuzhong Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-12T09:14:13.057Z",
                    "hidden": false
                },
                {
                    "_id": "693bcd629874a2a5e4ffb41c",
                    "name": "Fang Wan",
                    "hidden": false
                },
                {
                    "_id": "693bcd629874a2a5e4ffb41d",
                    "name": "Qixiang Ye",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-09T11:53:21.000Z",
            "submittedOnDailyAt": "2025-12-12T07:03:32.098Z",
            "title": "Thinking with Images via Self-Calling Agent",
            "submittedOnDailyBy": {
                "_id": "662f1aa0e4a1f7302a7f134a",
                "avatarUrl": "/avatars/2fef164e2ccd24c7f3c46942e4b12ab5.svg",
                "isPro": false,
                "fullname": "Wenxi Yang",
                "user": "ywenxi",
                "type": "user"
            },
            "summary": "Thinking-with-images paradigms have showcased remarkable visual reasoning capability by integrating visual information as dynamic elements into the Chain-of-Thought (CoT). However, optimizing interleaved multimodal CoT (iMCoT) through reinforcement learning remains challenging, as it relies on scarce high-quality reasoning data. In this study, we propose Self-Calling Chain-of-Thought (sCoT), a novel visual reasoning paradigm that reformulates iMCoT as a language-only CoT with self-calling. Specifically, a main agent decomposes the complex visual reasoning task to atomic subtasks and invokes its virtual replicas, i.e. parameter-sharing subagents, to solve them in isolated context. sCoT enjoys substantial training effectiveness and efficiency, as it requires no explicit interleaving between modalities. sCoT employs group-relative policy optimization to reinforce effective reasoning behavior to enhance optimization. Experiments on HR-Bench 4K show that sCoT improves the overall reasoning performance by up to 1.9% with sim 75% fewer GPU hours compared to strong baseline approaches. Code is available at https://github.com/YWenxi/think-with-images-through-self-calling.",
            "upvotes": 8,
            "discussionId": "693bcd629874a2a5e4ffb41e",
            "githubRepo": "https://github.com/YWenxi/think-with-images-through-self-calling",
            "githubRepoAddedBy": "user",
            "ai_summary": "sCoT, a language-only CoT paradigm with self-calling subagents, enhances visual reasoning performance and efficiency through group-relative policy optimization.",
            "ai_keywords": [
                "Chain-of-Thought",
                "CoT",
                "interleaved multimodal CoT",
                "iMCoT",
                "reinforcement learning",
                "visual reasoning",
                "parameter-sharing subagents",
                "group-relative policy optimization",
                "HR-Bench 4K"
            ],
            "githubStars": 11
        },
        "publishedAt": "2025-12-09T06:53:21.000Z",
        "title": "Thinking with Images via Self-Calling Agent",
        "summary": "Thinking-with-images paradigms have showcased remarkable visual reasoning capability by integrating visual information as dynamic elements into the Chain-of-Thought (CoT). However, optimizing interleaved multimodal CoT (iMCoT) through reinforcement learning remains challenging, as it relies on scarce high-quality reasoning data. In this study, we propose Self-Calling Chain-of-Thought (sCoT), a novel visual reasoning paradigm that reformulates iMCoT as a language-only CoT with self-calling. Specifically, a main agent decomposes the complex visual reasoning task to atomic subtasks and invokes its virtual replicas, i.e. parameter-sharing subagents, to solve them in isolated context. sCoT enjoys substantial training effectiveness and efficiency, as it requires no explicit interleaving between modalities. sCoT employs group-relative policy optimization to reinforce effective reasoning behavior to enhance optimization. Experiments on HR-Bench 4K show that sCoT improves the overall reasoning performance by up to 1.9% with sim 75% fewer GPU hours compared to strong baseline approaches. Code is available at https://github.com/YWenxi/think-with-images-through-self-calling.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.08511.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "662f1aa0e4a1f7302a7f134a",
            "avatarUrl": "/avatars/2fef164e2ccd24c7f3c46942e4b12ab5.svg",
            "fullname": "Wenxi Yang",
            "name": "ywenxi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.10959",
            "authors": [
                {
                    "_id": "693be7329874a2a5e4ffb446",
                    "name": "Tjark Behrens",
                    "hidden": false
                },
                {
                    "_id": "693be7329874a2a5e4ffb447",
                    "name": "Anton Obukhov",
                    "hidden": false
                },
                {
                    "_id": "693be7329874a2a5e4ffb448",
                    "name": "Bingxin Ke",
                    "hidden": false
                },
                {
                    "_id": "693be7329874a2a5e4ffb449",
                    "name": "Fabio Tosi",
                    "hidden": false
                },
                {
                    "_id": "693be7329874a2a5e4ffb44a",
                    "name": "Matteo Poggi",
                    "hidden": false
                },
                {
                    "_id": "693be7329874a2a5e4ffb44b",
                    "name": "Konrad Schindler",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/62f93abbc4817cfc0756b6f8/PrKqcy64r8Rm1L4APLHye.jpeg"
            ],
            "publishedAt": "2025-12-11T18:59:59.000Z",
            "submittedOnDailyAt": "2025-12-12T10:54:09.635Z",
            "title": "StereoSpace: Depth-Free Synthesis of Stereo Geometry via End-to-End Diffusion in a Canonical Space",
            "submittedOnDailyBy": {
                "_id": "62f93abbc4817cfc0756b6f8",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f93abbc4817cfc0756b6f8/rGYLaq-rmoJJYotkC1VXk.jpeg",
                "isPro": true,
                "fullname": "Anton Obukhov",
                "user": "toshas",
                "type": "user"
            },
            "summary": "We introduce StereoSpace, a diffusion-based framework for monocular-to-stereo synthesis that models geometry purely through viewpoint conditioning, without explicit depth or warping. A canonical rectified space and the conditioning guide the generator to infer correspondences and fill disocclusions end-to-end. To ensure fair and leakage-free evaluation, we introduce an end-to-end protocol that excludes any ground truth or proxy geometry estimates at test time. The protocol emphasizes metrics reflecting downstream relevance: iSQoE for perceptual comfort and MEt3R for geometric consistency. StereoSpace surpasses other methods from the warp & inpaint, latent-warping, and warped-conditioning categories, achieving sharp parallax and strong robustness on layered and non-Lambertian scenes. This establishes viewpoint-conditioned diffusion as a scalable, depth-free solution for stereo generation.",
            "upvotes": 7,
            "discussionId": "693be7329874a2a5e4ffb44c",
            "projectPage": "https://huggingface.co/spaces/prs-eth/stereospace_web",
            "githubRepo": "https://github.com/prs-eth/stereospace",
            "githubRepoAddedBy": "user",
            "ai_summary": "StereoSpace uses viewpoint-conditioned diffusion to generate stereo images without explicit depth or warping, achieving superior performance on various scenes.",
            "ai_keywords": [
                "diffusion-based framework",
                "monocular-to-stereo synthesis",
                "viewpoint conditioning",
                "canonical rectified space",
                "generator",
                "correspondences",
                "disocclusions",
                "end-to-end protocol",
                "iSQoE",
                "MEt3R",
                "perceptual comfort",
                "geometric consistency",
                "warp & inpaint",
                "latent-warping",
                "warped-conditioning",
                "sharp parallax",
                "robustness",
                "layered scenes",
                "non-Lambertian scenes",
                "viewpoint-conditioned diffusion"
            ],
            "githubStars": 12,
            "organization": {
                "_id": "65f7727efac77fd37dc14086",
                "name": "prs-eth",
                "fullname": "Photogrammetry and Remote Sensing Lab of ETH Zurich",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62f93abbc4817cfc0756b6f8/9PMWSIbL-mFS5oFJ1bZFx.png"
            }
        },
        "publishedAt": "2025-12-11T13:59:59.000Z",
        "title": "StereoSpace: Depth-Free Synthesis of Stereo Geometry via End-to-End Diffusion in a Canonical Space",
        "summary": "We introduce StereoSpace, a diffusion-based framework for monocular-to-stereo synthesis that models geometry purely through viewpoint conditioning, without explicit depth or warping. A canonical rectified space and the conditioning guide the generator to infer correspondences and fill disocclusions end-to-end. To ensure fair and leakage-free evaluation, we introduce an end-to-end protocol that excludes any ground truth or proxy geometry estimates at test time. The protocol emphasizes metrics reflecting downstream relevance: iSQoE for perceptual comfort and MEt3R for geometric consistency. StereoSpace surpasses other methods from the warp & inpaint, latent-warping, and warped-conditioning categories, achieving sharp parallax and strong robustness on layered and non-Lambertian scenes. This establishes viewpoint-conditioned diffusion as a scalable, depth-free solution for stereo generation.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/62f93abbc4817cfc0756b6f8/PrKqcy64r8Rm1L4APLHye.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.10959.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62f93abbc4817cfc0756b6f8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f93abbc4817cfc0756b6f8/rGYLaq-rmoJJYotkC1VXk.jpeg",
            "fullname": "Anton Obukhov",
            "name": "toshas",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 85
        },
        "organization": {
            "_id": "65f7727efac77fd37dc14086",
            "name": "prs-eth",
            "fullname": "Photogrammetry and Remote Sensing Lab of ETH Zurich",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62f93abbc4817cfc0756b6f8/9PMWSIbL-mFS5oFJ1bZFx.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.10938",
            "authors": [
                {
                    "_id": "693b85ec9874a2a5e4ffb2ee",
                    "user": {
                        "_id": "64bfbbf06fc8c03398804a0c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Ij2peTEyoGnx-J1wN8P-p.png",
                        "isPro": false,
                        "fullname": "Mingzhi Chen",
                        "user": "Fishloong",
                        "type": "user"
                    },
                    "name": "Mingzhi Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-12T13:13:32.371Z",
                    "hidden": false
                },
                {
                    "_id": "693b85ec9874a2a5e4ffb2ef",
                    "user": {
                        "_id": "656a9b9f9496f21be8271f1b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656a9b9f9496f21be8271f1b/e7p5kx2YNs_A1AJYSOlio.png",
                        "isPro": true,
                        "fullname": "TaiMing",
                        "user": "TaiMingLu",
                        "type": "user"
                    },
                    "name": "Taiming Lu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-12T13:13:39.570Z",
                    "hidden": false
                },
                {
                    "_id": "693b85ec9874a2a5e4ffb2f0",
                    "name": "Jiachen Zhu",
                    "hidden": false
                },
                {
                    "_id": "693b85ec9874a2a5e4ffb2f1",
                    "name": "Mingjie Sun",
                    "hidden": false
                },
                {
                    "_id": "693b85ec9874a2a5e4ffb2f2",
                    "name": "Zhuang Liu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/4IefkF8m-mTTbJfyO2VfS.png"
            ],
            "publishedAt": "2025-12-11T18:58:49.000Z",
            "submittedOnDailyAt": "2025-12-12T00:33:57.887Z",
            "title": "Stronger Normalization-Free Transformers",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Although normalization layers have long been viewed as indispensable components of deep learning architectures, the recent introduction of Dynamic Tanh (DyT) has demonstrated that alternatives are possible. The point-wise function DyT constrains extreme values for stable convergence and reaches normalization-level performance; this work seeks further for function designs that can surpass it. We first study how the intrinsic properties of point-wise functions influence training and performance. Building on these findings, we conduct a large-scale search for a more effective function design. Through this exploration, we introduce Derf(x) = erf(x + s), where erf(x) is the rescaled Gaussian cumulative distribution function, and identify it as the most performant design. Derf outperforms LayerNorm, RMSNorm, and DyT across a wide range of domains, including vision (image recognition and generation), speech representation, and DNA sequence modeling. Our findings suggest that the performance gains of Derf largely stem from its improved generalization rather than stronger fitting capacity. Its simplicity and stronger performance make Derf a practical choice for normalization-free Transformer architectures.",
            "upvotes": 7,
            "discussionId": "693b85ec9874a2a5e4ffb2f3",
            "ai_summary": "Derf, a novel point-wise normalization function, outperforms existing alternatives across various domains, enhancing generalization without increased fitting capacity.",
            "ai_keywords": [
                "Dynamic Tanh",
                "point-wise functions",
                "intrinsic properties",
                "large-scale search",
                "Derf",
                "rescaled Gaussian cumulative distribution function",
                "LayerNorm",
                "RMSNorm",
                "vision",
                "image recognition",
                "image generation",
                "speech representation",
                "DNA sequence modeling",
                "normalization-free Transformer architectures"
            ]
        },
        "publishedAt": "2025-12-11T13:58:49.000Z",
        "title": "Stronger Normalization-Free Transformers",
        "summary": "Although normalization layers have long been viewed as indispensable components of deep learning architectures, the recent introduction of Dynamic Tanh (DyT) has demonstrated that alternatives are possible. The point-wise function DyT constrains extreme values for stable convergence and reaches normalization-level performance; this work seeks further for function designs that can surpass it. We first study how the intrinsic properties of point-wise functions influence training and performance. Building on these findings, we conduct a large-scale search for a more effective function design. Through this exploration, we introduce Derf(x) = erf(x + s), where erf(x) is the rescaled Gaussian cumulative distribution function, and identify it as the most performant design. Derf outperforms LayerNorm, RMSNorm, and DyT across a wide range of domains, including vision (image recognition and generation), speech representation, and DNA sequence modeling. Our findings suggest that the performance gains of Derf largely stem from its improved generalization rather than stronger fitting capacity. Its simplicity and stronger performance make Derf a practical choice for normalization-free Transformer architectures.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/4IefkF8m-mTTbJfyO2VfS.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.10938.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 182
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.10791",
            "authors": [
                {
                    "_id": "693b82729874a2a5e4ffb261",
                    "name": "Aileen Cheng",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb262",
                    "name": "Alon Jacovi",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb263",
                    "name": "Amir Globerson",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb264",
                    "name": "Ben Golan",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb265",
                    "name": "Charles Kwong",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb266",
                    "name": "Chris Alberti",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb267",
                    "name": "Connie Tao",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb268",
                    "name": "Eyal Ben-David",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb269",
                    "name": "Gaurav Singh Tomar",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb26a",
                    "name": "Lukas Haas",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb26b",
                    "name": "Yonatan Bitton",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb26c",
                    "name": "Adam Bloniarz",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb26d",
                    "name": "Aijun Bai",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb26e",
                    "name": "Andrew Wang",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb26f",
                    "name": "Anfal Siddiqui",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb270",
                    "name": "Arturo Bajuelos Castillo",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb271",
                    "name": "Aviel Atias",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb272",
                    "name": "Chang Liu",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb273",
                    "name": "Corey Fry",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb274",
                    "name": "Daniel Balle",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb275",
                    "name": "Deepanway Ghosal",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb276",
                    "name": "Doron Kukliansky",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb277",
                    "name": "Dror Marcus",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb278",
                    "name": "Elena Gribovskaya",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb279",
                    "name": "Eran Ofek",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb27a",
                    "name": "Honglei Zhuang",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb27b",
                    "name": "Itay Laish",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb27c",
                    "name": "Jan Ackermann",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb27d",
                    "name": "Lily Wang",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb27e",
                    "name": "Meg Risdal",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb27f",
                    "name": "Megan Barnes",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb280",
                    "name": "Michael Fink",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb281",
                    "name": "Mohamed Amin",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb282",
                    "name": "Moran Ambar",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb283",
                    "name": "Natan Potikha",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb284",
                    "name": "Nikita Gupta",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb285",
                    "name": "Nitzan Katz",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb286",
                    "name": "Noam Velan",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb287",
                    "name": "Ofir Roval",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb288",
                    "name": "Ori Ram",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb289",
                    "name": "Polina Zablotskaia",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb28a",
                    "name": "Prathamesh Bang",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb28b",
                    "name": "Priyanka Agrawal",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb28c",
                    "name": "Rakesh Ghiya",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb28d",
                    "name": "Sanjay Ganapathy",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb28e",
                    "name": "Simon Baumgartner",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb28f",
                    "name": "Sofia Erell",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb290",
                    "name": "Sushant Prakash",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb291",
                    "name": "Thibault Sellam",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb292",
                    "name": "Vikram Rao",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb293",
                    "name": "Xuanhui Wang",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb294",
                    "name": "Yaroslav Akulov",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb295",
                    "name": "Yulong Yang",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb296",
                    "name": "Zhen Yang",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb297",
                    "name": "Zhixin Lai",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb298",
                    "name": "Zhongru Wu",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb299",
                    "name": "Anca Dragan",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb29a",
                    "name": "Avinatan Hassidim",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb29b",
                    "name": "Fernando Pereira",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb29c",
                    "name": "Slav Petrov",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb29d",
                    "name": "Srinivasan Venkatachary",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb29e",
                    "name": "Tulsee Doshi",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb29f",
                    "name": "Yossi Matias",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb2a0",
                    "name": "Sasha Goldshtein",
                    "hidden": false
                },
                {
                    "_id": "693b82729874a2a5e4ffb2a1",
                    "name": "Dipanjan Das",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-11T16:35:14.000Z",
            "submittedOnDailyAt": "2025-12-12T00:18:31.591Z",
            "title": "The FACTS Leaderboard: A Comprehensive Benchmark for Large Language Model Factuality",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We introduce The FACTS Leaderboard, an online leaderboard suite and associated set of benchmarks that comprehensively evaluates the ability of language models to generate factually accurate text across diverse scenarios. The suite provides a holistic measure of factuality by aggregating the performance of models on four distinct sub-leaderboards: (1) FACTS Multimodal, which measures the factuality of responses to image-based questions; (2) FACTS Parametric, which assesses models' world knowledge by answering closed-book factoid questions from internal parameters; (3) FACTS Search, which evaluates factuality in information-seeking scenarios, where the model must use a search API; and (4) FACTS Grounding (v2), which evaluates whether long-form responses are grounded in provided documents, featuring significantly improved judge models. Each sub-leaderboard employs automated judge models to score model responses, and the final suite score is an average of the four components, designed to provide a robust and balanced assessment of a model's overall factuality. The FACTS Leaderboard Suite will be actively maintained, containing both public and private splits to allow for external participation while guarding its integrity. It can be found at https://www.kaggle.com/benchmarks/google/facts .",
            "upvotes": 3,
            "discussionId": "693b82739874a2a5e4ffb2a2",
            "projectPage": "https://www.kaggle.com/benchmarks/google/facts",
            "ai_summary": "The FACTS Leaderboard evaluates language models' factual accuracy across different scenarios using four sub-leaderboards: image-based questions, closed-book factoid questions, information-seeking with search API, and document-grounded long-form responses.",
            "ai_keywords": [
                "FACTS Leaderboard",
                "FACTS Multimodal",
                "FACTS Parametric",
                "FACTS Search",
                "FACTS Grounding",
                "automated judge models"
            ],
            "organization": {
                "_id": "60f6cbb2852126bac698c89e",
                "name": "deepmind",
                "fullname": "Deepmind",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1638956859875-5f1158120c833276f61f1a84.jpeg"
            }
        },
        "publishedAt": "2025-12-11T11:35:14.000Z",
        "title": "The FACTS Leaderboard: A Comprehensive Benchmark for Large Language Model Factuality",
        "summary": "We introduce The FACTS Leaderboard, an online leaderboard suite and associated set of benchmarks that comprehensively evaluates the ability of language models to generate factually accurate text across diverse scenarios. The suite provides a holistic measure of factuality by aggregating the performance of models on four distinct sub-leaderboards: (1) FACTS Multimodal, which measures the factuality of responses to image-based questions; (2) FACTS Parametric, which assesses models' world knowledge by answering closed-book factoid questions from internal parameters; (3) FACTS Search, which evaluates factuality in information-seeking scenarios, where the model must use a search API; and (4) FACTS Grounding (v2), which evaluates whether long-form responses are grounded in provided documents, featuring significantly improved judge models. Each sub-leaderboard employs automated judge models to score model responses, and the final suite score is an average of the four components, designed to provide a robust and balanced assessment of a model's overall factuality. The FACTS Leaderboard Suite will be actively maintained, containing both public and private splits to allow for external participation while guarding its integrity. It can be found at https://www.kaggle.com/benchmarks/google/facts .",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.10791.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 182
        },
        "organization": {
            "_id": "60f6cbb2852126bac698c89e",
            "name": "deepmind",
            "fullname": "Deepmind",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1638956859875-5f1158120c833276f61f1a84.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.10359",
            "authors": [
                {
                    "_id": "693b8ec89874a2a5e4ffb312",
                    "user": {
                        "_id": "6434c9dc4b34368fdb07d421",
                        "avatarUrl": "/avatars/b1393b354f835b73958ae879decea01a.svg",
                        "isPro": false,
                        "fullname": "fansunqi",
                        "user": "fansunqi",
                        "type": "user"
                    },
                    "name": "Sunqi Fan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-12T09:15:05.304Z",
                    "hidden": false
                },
                {
                    "_id": "693b8ec89874a2a5e4ffb313",
                    "name": "Jiashuo Cui",
                    "hidden": false
                },
                {
                    "_id": "693b8ec89874a2a5e4ffb314",
                    "name": "Meng-Hao Guo",
                    "hidden": false
                },
                {
                    "_id": "693b8ec89874a2a5e4ffb315",
                    "name": "Shuojin Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-11T07:17:57.000Z",
            "submittedOnDailyAt": "2025-12-12T01:15:13.127Z",
            "title": "Tool-Augmented Spatiotemporal Reasoning for Streamlining Video Question Answering Task",
            "submittedOnDailyBy": {
                "_id": "6434c9dc4b34368fdb07d421",
                "avatarUrl": "/avatars/b1393b354f835b73958ae879decea01a.svg",
                "isPro": false,
                "fullname": "fansunqi",
                "user": "fansunqi",
                "type": "user"
            },
            "summary": "Video Question Answering (VideoQA) task serves as a critical playground for evaluating whether foundation models can effectively perceive, understand, and reason about dynamic real-world scenarios. However, existing Multimodal Large Language Models (MLLMs) struggle with simultaneously modeling spatial relationships within video frames and understanding the causal dynamics of temporal evolution on complex and reasoning-intensive VideoQA task. In this work, we equip MLLM with a comprehensive and extensible Video Toolkit, to enhance MLLM's spatiotemporal reasoning capabilities and ensure the harmony between the quantity and diversity of tools. To better control the tool invocation sequence and avoid toolchain shortcut issues, we propose a Spatiotemporal Reasoning Framework (STAR) that strategically schedules temporal and spatial tools, thereby progressively localizing the key area in the video. Our STAR framework enhances GPT-4o using lightweight tools, achieving an 8.2% gain on VideoMME and 4.6% on LongVideoBench. We believe that our proposed Video Toolkit and STAR framework make an important step towards building autonomous and intelligent video analysis assistants. The code is publicly available at https://github.com/fansunqi/VideoTool.",
            "upvotes": 3,
            "discussionId": "693b8ec89874a2a5e4ffb316",
            "githubRepo": "https://github.com/fansunqi/VideoTool",
            "githubRepoAddedBy": "user",
            "ai_summary": "A spatiotemporal reasoning framework enhances multimodal large language models for video question answering by strategically scheduling tools to improve spatial and temporal understanding.",
            "ai_keywords": [
                "Multimodal Large Language Models",
                "Video Toolkit",
                "Spatiotemporal Reasoning Framework",
                "STAR",
                "VideoMME",
                "LongVideoBench",
                "GPT-4o"
            ],
            "githubStars": 3,
            "organization": {
                "_id": "628735cbc83a2d6ab8d14a66",
                "name": "Tsinghua",
                "fullname": "Tsinghua University"
            }
        },
        "publishedAt": "2025-12-11T02:17:57.000Z",
        "title": "Tool-Augmented Spatiotemporal Reasoning for Streamlining Video Question Answering Task",
        "summary": "Video Question Answering (VideoQA) task serves as a critical playground for evaluating whether foundation models can effectively perceive, understand, and reason about dynamic real-world scenarios. However, existing Multimodal Large Language Models (MLLMs) struggle with simultaneously modeling spatial relationships within video frames and understanding the causal dynamics of temporal evolution on complex and reasoning-intensive VideoQA task. In this work, we equip MLLM with a comprehensive and extensible Video Toolkit, to enhance MLLM's spatiotemporal reasoning capabilities and ensure the harmony between the quantity and diversity of tools. To better control the tool invocation sequence and avoid toolchain shortcut issues, we propose a Spatiotemporal Reasoning Framework (STAR) that strategically schedules temporal and spatial tools, thereby progressively localizing the key area in the video. Our STAR framework enhances GPT-4o using lightweight tools, achieving an 8.2% gain on VideoMME and 4.6% on LongVideoBench. We believe that our proposed Video Toolkit and STAR framework make an important step towards building autonomous and intelligent video analysis assistants. The code is publicly available at https://github.com/fansunqi/VideoTool.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.10359.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6434c9dc4b34368fdb07d421",
            "avatarUrl": "/avatars/b1393b354f835b73958ae879decea01a.svg",
            "fullname": "fansunqi",
            "name": "fansunqi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "628735cbc83a2d6ab8d14a66",
            "name": "Tsinghua",
            "fullname": "Tsinghua University"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.09406",
            "authors": [
                {
                    "_id": "693b956d9874a2a5e4ffb34c",
                    "user": {
                        "_id": "647352516a972f252de1fd58",
                        "avatarUrl": "/avatars/8e93ca66e01f047c5fb28e1c4f737e8e.svg",
                        "isPro": false,
                        "fullname": "Hai Ci",
                        "user": "HaiCi",
                        "type": "user"
                    },
                    "name": "Hai Ci",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-12T13:09:23.967Z",
                    "hidden": false
                },
                {
                    "_id": "693b956d9874a2a5e4ffb34d",
                    "name": "Xiaokang Liu",
                    "hidden": false
                },
                {
                    "_id": "693b956d9874a2a5e4ffb34e",
                    "name": "Pei Yang",
                    "hidden": false
                },
                {
                    "_id": "693b956d9874a2a5e4ffb34f",
                    "user": {
                        "_id": "64311a95034ecbefddd141ef",
                        "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
                        "isPro": false,
                        "fullname": "Yiren Song",
                        "user": "yiren98",
                        "type": "user"
                    },
                    "name": "Yiren Song",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-12T13:09:39.096Z",
                    "hidden": false
                },
                {
                    "_id": "693b956d9874a2a5e4ffb350",
                    "user": {
                        "_id": "661ab3da2b14565c7acccf5c",
                        "avatarUrl": "/avatars/fa4fc03664803e02aede4d4c3d50b393.svg",
                        "isPro": false,
                        "fullname": "Mike Zheng Shou",
                        "user": "AnalMom",
                        "type": "user"
                    },
                    "name": "Mike Zheng Shou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-12T13:09:33.475Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/647352516a972f252de1fd58/Ex4VH4em0lNLgBgiOl-2M.mp4",
                "https://cdn-uploads.huggingface.co/production/uploads/647352516a972f252de1fd58/mUfLdCplKzDSoAJw7jCSk.mp4"
            ],
            "publishedAt": "2025-12-10T07:59:45.000Z",
            "submittedOnDailyAt": "2025-12-12T01:42:16.568Z",
            "title": "H2R-Grounder: A Paired-Data-Free Paradigm for Translating Human Interaction Videos into Physically Grounded Robot Videos",
            "submittedOnDailyBy": {
                "_id": "647352516a972f252de1fd58",
                "avatarUrl": "/avatars/8e93ca66e01f047c5fb28e1c4f737e8e.svg",
                "isPro": false,
                "fullname": "Hai Ci",
                "user": "HaiCi",
                "type": "user"
            },
            "summary": "Robots that learn manipulation skills from everyday human videos could acquire broad capabilities without tedious robot data collection. We propose a video-to-video translation framework that converts ordinary human-object interaction videos into motion-consistent robot manipulation videos with realistic, physically grounded interactions. Our approach does not require any paired human-robot videos for training only a set of unpaired robot videos, making the system easy to scale. We introduce a transferable representation that bridges the embodiment gap: by inpainting the robot arm in training videos to obtain a clean background and overlaying a simple visual cue (a marker and arrow indicating the gripper's position and orientation), we can condition a generative model to insert the robot arm back into the scene. At test time, we apply the same process to human videos (inpainting the person and overlaying human pose cues) and generate high-quality robot videos that mimic the human's actions. We fine-tune a SOTA video diffusion model (Wan 2.2) in an in-context learning manner to ensure temporal coherence and leveraging of its rich prior knowledge. Empirical results demonstrate that our approach achieves significantly more realistic and grounded robot motions compared to baselines, pointing to a promising direction for scaling up robot learning from unlabeled human videos. Project page: https://showlab.github.io/H2R-Grounder/",
            "upvotes": 3,
            "discussionId": "693b956e9874a2a5e4ffb351",
            "projectPage": "https://showlab.github.io/H2R-Grounder/",
            "githubRepo": "https://github.com/showlab/H2R-Grounder",
            "githubRepoAddedBy": "user",
            "ai_summary": "A video-to-video translation framework converts human-object interaction videos into realistic robot manipulation videos using unpaired training data and a generative model.",
            "ai_keywords": [
                "video-to-video translation",
                "motion-consistent",
                "physically grounded interactions",
                "unpaired robot videos",
                "transferable representation",
                "inpainting",
                "visual cue",
                "generative model",
                "in-context learning",
                "video diffusion model",
                "temporal coherence",
                "prior knowledge"
            ],
            "githubStars": 13
        },
        "publishedAt": "2025-12-10T02:59:45.000Z",
        "title": "H2R-Grounder: A Paired-Data-Free Paradigm for Translating Human Interaction Videos into Physically Grounded Robot Videos",
        "summary": "Robots that learn manipulation skills from everyday human videos could acquire broad capabilities without tedious robot data collection. We propose a video-to-video translation framework that converts ordinary human-object interaction videos into motion-consistent robot manipulation videos with realistic, physically grounded interactions. Our approach does not require any paired human-robot videos for training only a set of unpaired robot videos, making the system easy to scale. We introduce a transferable representation that bridges the embodiment gap: by inpainting the robot arm in training videos to obtain a clean background and overlaying a simple visual cue (a marker and arrow indicating the gripper's position and orientation), we can condition a generative model to insert the robot arm back into the scene. At test time, we apply the same process to human videos (inpainting the person and overlaying human pose cues) and generate high-quality robot videos that mimic the human's actions. We fine-tune a SOTA video diffusion model (Wan 2.2) in an in-context learning manner to ensure temporal coherence and leveraging of its rich prior knowledge. Empirical results demonstrate that our approach achieves significantly more realistic and grounded robot motions compared to baselines, pointing to a promising direction for scaling up robot learning from unlabeled human videos. Project page: https://showlab.github.io/H2R-Grounder/",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/647352516a972f252de1fd58/Ex4VH4em0lNLgBgiOl-2M.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/647352516a972f252de1fd58/mUfLdCplKzDSoAJw7jCSk.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.09406.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "647352516a972f252de1fd58",
            "avatarUrl": "/avatars/8e93ca66e01f047c5fb28e1c4f737e8e.svg",
            "fullname": "Hai Ci",
            "name": "HaiCi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.09270",
            "authors": [
                {
                    "_id": "693bc05c9874a2a5e4ffb3d4",
                    "user": {
                        "_id": "677e4ef9c27c05b2d178d775",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/xQoYxKiRmlnCOslOaxjCl.png",
                        "isPro": false,
                        "fullname": "Sangwoon Kwak",
                        "user": "sangwoonkwak",
                        "type": "user"
                    },
                    "name": "Sangwoon Kwak",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-12T13:13:17.500Z",
                    "hidden": false
                },
                {
                    "_id": "693bc05c9874a2a5e4ffb3d5",
                    "user": {
                        "_id": "6692506aab6879eceaafb083",
                        "avatarUrl": "/avatars/f7f735faf11f6b4036b164aca90345e6.svg",
                        "isPro": false,
                        "fullname": "WEEYOUNG KWON",
                        "user": "klavna",
                        "type": "user"
                    },
                    "name": "Weeyoung Kwon",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-12T13:12:48.580Z",
                    "hidden": false
                },
                {
                    "_id": "693bc05c9874a2a5e4ffb3d6",
                    "user": {
                        "_id": "677e68cc8a7269e1779e4def",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/PrMRHMrB4aEQPWV0pxNxX.png",
                        "isPro": false,
                        "fullname": "Jun Young Jeong",
                        "user": "shurek20",
                        "type": "user"
                    },
                    "name": "Jun Young Jeong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-12T13:12:54.353Z",
                    "hidden": false
                },
                {
                    "_id": "693bc05c9874a2a5e4ffb3d7",
                    "name": "Geonho Kim",
                    "hidden": false
                },
                {
                    "_id": "693bc05c9874a2a5e4ffb3d8",
                    "name": "Won-Sik Cheong",
                    "hidden": false
                },
                {
                    "_id": "693bc05c9874a2a5e4ffb3d9",
                    "name": "Jihyong Oh",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6692506aab6879eceaafb083/dX_BEJkylfrxbmL4KGG4i.png",
                "https://cdn-uploads.huggingface.co/production/uploads/6692506aab6879eceaafb083/kEK3Otp5vhwN8vmx3lavS.png",
                "https://cdn-uploads.huggingface.co/production/uploads/6692506aab6879eceaafb083/J3kPKj1dwPDkhcKbS_8uv.png"
            ],
            "publishedAt": "2025-12-10T02:49:09.000Z",
            "submittedOnDailyAt": "2025-12-12T04:55:44.764Z",
            "title": "MoRel: Long-Range Flicker-Free 4D Motion Modeling via Anchor Relay-based Bidirectional Blending with Hierarchical Densification",
            "submittedOnDailyBy": {
                "_id": "6692506aab6879eceaafb083",
                "avatarUrl": "/avatars/f7f735faf11f6b4036b164aca90345e6.svg",
                "isPro": false,
                "fullname": "WEEYOUNG KWON",
                "user": "klavna",
                "type": "user"
            },
            "summary": "Recent advances in 4D Gaussian Splatting (4DGS) have extended the high-speed rendering capability of 3D Gaussian Splatting (3DGS) into the temporal domain, enabling real-time rendering of dynamic scenes. However, one of the major remaining challenges lies in modeling long-range motion-contained dynamic videos, where a naive extension of existing methods leads to severe memory explosion, temporal flickering, and failure to handle appearing or disappearing occlusions over time. To address these challenges, we propose a novel 4DGS framework characterized by an Anchor Relay-based Bidirectional Blending (ARBB) mechanism, named MoRel, which enables temporally consistent and memory-efficient modeling of long-range dynamic scenes. Our method progressively constructs locally canonical anchor spaces at key-frame time index and models inter-frame deformations at the anchor level, enhancing temporal coherence. By learning bidirectional deformations between KfA and adaptively blending them through learnable opacity control, our approach mitigates temporal discontinuities and flickering artifacts. We further introduce a Feature-variance-guided Hierarchical Densification (FHD) scheme that effectively densifies KfA's while keeping rendering quality, based on an assigned level of feature-variance. To effectively evaluate our model's capability to handle real-world long-range 4D motion, we newly compose long-range 4D motion-contained dataset, called SelfCap_{LR}. It has larger average dynamic motion magnitude, captured at spatially wider spaces, compared to previous dynamic video datasets. Overall, our MoRel achieves temporally coherent and flicker-free long-range 4D reconstruction while maintaining bounded memory usage, demonstrating both scalability and efficiency in dynamic Gaussian-based representations.",
            "upvotes": 3,
            "discussionId": "693bc05d9874a2a5e4ffb3da",
            "projectPage": "https://cmlab-korea.github.io/MoRel/",
            "githubRepo": "https://github.com/CMLab-Korea/MoRel-arXiv",
            "githubRepoAddedBy": "user",
            "ai_summary": "MoRel, a novel 4D Gaussian Splatting framework, addresses long-range motion and memory efficiency in dynamic video rendering by using Anchor Relay-based Bidirectional Blending and Feature-variance-guided Hierarchical Densification.",
            "ai_keywords": [
                "Gaussian Splatting",
                "4D Gaussian Splatting",
                "Anchor Relay-based Bidirectional Blending",
                "Feature-variance-guided Hierarchical Densification",
                "temporal coherence",
                "memory-efficient",
                "long-range dynamic scenes",
                "key-frame time index",
                "inter-frame deformations",
                "bidirectional deformations",
                "learnable opacity control",
                "temporal discontinuities",
                "flickering artifacts",
                "real-world long-range 4D motion",
                "SelfCap$_{\\text{LR}}$ dataset"
            ],
            "githubStars": 5
        },
        "publishedAt": "2025-12-09T21:49:09.000Z",
        "title": "MoRel: Long-Range Flicker-Free 4D Motion Modeling via Anchor Relay-based Bidirectional Blending with Hierarchical Densification",
        "summary": "Recent advances in 4D Gaussian Splatting (4DGS) have extended the high-speed rendering capability of 3D Gaussian Splatting (3DGS) into the temporal domain, enabling real-time rendering of dynamic scenes. However, one of the major remaining challenges lies in modeling long-range motion-contained dynamic videos, where a naive extension of existing methods leads to severe memory explosion, temporal flickering, and failure to handle appearing or disappearing occlusions over time. To address these challenges, we propose a novel 4DGS framework characterized by an Anchor Relay-based Bidirectional Blending (ARBB) mechanism, named MoRel, which enables temporally consistent and memory-efficient modeling of long-range dynamic scenes. Our method progressively constructs locally canonical anchor spaces at key-frame time index and models inter-frame deformations at the anchor level, enhancing temporal coherence. By learning bidirectional deformations between KfA and adaptively blending them through learnable opacity control, our approach mitigates temporal discontinuities and flickering artifacts. We further introduce a Feature-variance-guided Hierarchical Densification (FHD) scheme that effectively densifies KfA's while keeping rendering quality, based on an assigned level of feature-variance. To effectively evaluate our model's capability to handle real-world long-range 4D motion, we newly compose long-range 4D motion-contained dataset, called SelfCap_{LR}. It has larger average dynamic motion magnitude, captured at spatially wider spaces, compared to previous dynamic video datasets. Overall, our MoRel achieves temporally coherent and flicker-free long-range 4D reconstruction while maintaining bounded memory usage, demonstrating both scalability and efficiency in dynamic Gaussian-based representations.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6692506aab6879eceaafb083/dX_BEJkylfrxbmL4KGG4i.png",
            "https://cdn-uploads.huggingface.co/production/uploads/6692506aab6879eceaafb083/kEK3Otp5vhwN8vmx3lavS.png",
            "https://cdn-uploads.huggingface.co/production/uploads/6692506aab6879eceaafb083/J3kPKj1dwPDkhcKbS_8uv.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.09270.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6692506aab6879eceaafb083",
            "avatarUrl": "/avatars/f7f735faf11f6b4036b164aca90345e6.svg",
            "fullname": "WEEYOUNG KWON",
            "name": "klavna",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.10955",
            "authors": [
                {
                    "_id": "693c66dff516c69324680c8e",
                    "name": "Tsai-Shien Chen",
                    "hidden": false
                },
                {
                    "_id": "693c66dff516c69324680c8f",
                    "name": "Aliaksandr Siarohin",
                    "hidden": false
                },
                {
                    "_id": "693c66dff516c69324680c90",
                    "name": "Guocheng Gordon Qian",
                    "hidden": false
                },
                {
                    "_id": "693c66dff516c69324680c91",
                    "name": "Kuan-Chieh Jackson Wang",
                    "hidden": false
                },
                {
                    "_id": "693c66dff516c69324680c92",
                    "name": "Egor Nemchinov",
                    "hidden": false
                },
                {
                    "_id": "693c66dff516c69324680c93",
                    "name": "Moayed Haji-Ali",
                    "hidden": false
                },
                {
                    "_id": "693c66dff516c69324680c94",
                    "name": "Riza Alp Guler",
                    "hidden": false
                },
                {
                    "_id": "693c66dff516c69324680c95",
                    "name": "Willi Menapace",
                    "hidden": false
                },
                {
                    "_id": "693c66dff516c69324680c96",
                    "name": "Ivan Skorokhodov",
                    "hidden": false
                },
                {
                    "_id": "693c66dff516c69324680c97",
                    "name": "Anil Kag",
                    "hidden": false
                },
                {
                    "_id": "693c66dff516c69324680c98",
                    "name": "Jun-Yan Zhu",
                    "hidden": false
                },
                {
                    "_id": "693c66dff516c69324680c99",
                    "name": "Sergey Tulyakov",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-11T18:59:56.000Z",
            "submittedOnDailyAt": "2025-12-12T16:33:50.216Z",
            "title": "Omni-Attribute: Open-vocabulary Attribute Encoder for Visual Concept Personalization",
            "submittedOnDailyBy": {
                "_id": "645fed74335c21d19f3bf76c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645fed74335c21d19f3bf76c/gwVsllRWtSHbg4a1erkdF.jpeg",
                "isPro": false,
                "fullname": "Guocheng Gordon Qian",
                "user": "guochengqian",
                "type": "user"
            },
            "summary": "Visual concept personalization aims to transfer only specific image attributes, such as identity, expression, lighting, and style, into unseen contexts. However, existing methods rely on holistic embeddings from general-purpose image encoders, which entangle multiple visual factors and make it difficult to isolate a single attribute. This often leads to information leakage and incoherent synthesis. To address this limitation, we introduce Omni-Attribute, the first open-vocabulary image attribute encoder designed to learn high-fidelity, attribute-specific representations. Our approach jointly designs the data and model: (i) we curate semantically linked image pairs annotated with positive and negative attributes to explicitly teach the encoder what to preserve or suppress; and (ii) we adopt a dual-objective training paradigm that balances generative fidelity with contrastive disentanglement. The resulting embeddings prove effective for open-vocabulary attribute retrieval, personalization, and compositional generation, achieving state-of-the-art performance across multiple benchmarks.",
            "upvotes": 2,
            "discussionId": "693c66e0f516c69324680c9a",
            "ai_summary": "Omni-Attribute, an open-vocabulary image attribute encoder, learns attribute-specific representations to enable precise visual concept personalization and compositional generation.",
            "ai_keywords": [
                "image attribute encoder",
                "high-fidelity",
                "attribute-specific representations",
                "semantically linked image pairs",
                "dual-objective training paradigm",
                "generative fidelity",
                "contrastive disentanglement",
                "open-vocabulary attribute retrieval",
                "visual concept personalization",
                "compositional generation"
            ],
            "organization": {
                "_id": "63c87c41cd6a490608ce31d1",
                "name": "snap-research",
                "fullname": "Snap Research",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1674083325534-61f19829233c91cbd2f79e70.png"
            }
        },
        "publishedAt": "2025-12-11T13:59:56.000Z",
        "title": "Omni-Attribute: Open-vocabulary Attribute Encoder for Visual Concept Personalization",
        "summary": "Visual concept personalization aims to transfer only specific image attributes, such as identity, expression, lighting, and style, into unseen contexts. However, existing methods rely on holistic embeddings from general-purpose image encoders, which entangle multiple visual factors and make it difficult to isolate a single attribute. This often leads to information leakage and incoherent synthesis. To address this limitation, we introduce Omni-Attribute, the first open-vocabulary image attribute encoder designed to learn high-fidelity, attribute-specific representations. Our approach jointly designs the data and model: (i) we curate semantically linked image pairs annotated with positive and negative attributes to explicitly teach the encoder what to preserve or suppress; and (ii) we adopt a dual-objective training paradigm that balances generative fidelity with contrastive disentanglement. The resulting embeddings prove effective for open-vocabulary attribute retrieval, personalization, and compositional generation, achieving state-of-the-art performance across multiple benchmarks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.10955.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "645fed74335c21d19f3bf76c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645fed74335c21d19f3bf76c/gwVsllRWtSHbg4a1erkdF.jpeg",
            "fullname": "Guocheng Gordon Qian",
            "name": "guochengqian",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "63c87c41cd6a490608ce31d1",
            "name": "snap-research",
            "fullname": "Snap Research",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1674083325534-61f19829233c91cbd2f79e70.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.10398",
            "authors": [
                {
                    "_id": "693b85a29874a2a5e4ffb2d0",
                    "user": {
                        "_id": "65e0bd07cb8d7502fa2ec6aa",
                        "avatarUrl": "/avatars/72d99ceb6071833f38ffb3232ea7b696.svg",
                        "isPro": false,
                        "fullname": "Zhaodong Wang",
                        "user": "northzw",
                        "type": "user"
                    },
                    "name": "Zhaodong Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-12T13:11:18.845Z",
                    "hidden": false
                },
                {
                    "_id": "693b85a29874a2a5e4ffb2d1",
                    "name": "Zhenting Qi",
                    "hidden": false
                },
                {
                    "_id": "693b85a29874a2a5e4ffb2d2",
                    "user": {
                        "_id": "65ae55de77ce425d18cb35f3",
                        "avatarUrl": "/avatars/762e166baf5596dcf4c18a38b27fc1d4.svg",
                        "isPro": false,
                        "fullname": "Sherman Wong",
                        "user": "PM45",
                        "type": "user"
                    },
                    "name": "Sherman Wong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-12T13:11:29.202Z",
                    "hidden": false
                },
                {
                    "_id": "693b85a29874a2a5e4ffb2d3",
                    "name": "Nathan Hu",
                    "hidden": false
                },
                {
                    "_id": "693b85a29874a2a5e4ffb2d4",
                    "name": "Samuel Lin",
                    "hidden": false
                },
                {
                    "_id": "693b85a29874a2a5e4ffb2d5",
                    "name": "Jun Ge",
                    "hidden": false
                },
                {
                    "_id": "693b85a29874a2a5e4ffb2d6",
                    "name": "Erwin Gao",
                    "hidden": false
                },
                {
                    "_id": "693b85a29874a2a5e4ffb2d7",
                    "name": "Yining Yang",
                    "hidden": false
                },
                {
                    "_id": "693b85a29874a2a5e4ffb2d8",
                    "name": "Ben Maurer",
                    "hidden": false
                },
                {
                    "_id": "693b85a29874a2a5e4ffb2d9",
                    "name": "Wenlin Chen",
                    "hidden": false
                },
                {
                    "_id": "693b85a29874a2a5e4ffb2da",
                    "user": {
                        "_id": "6536d37ee27a7c4df493db9a",
                        "avatarUrl": "/avatars/99b60c2e41618940c3d07038122d92ce.svg",
                        "isPro": false,
                        "fullname": "David Recordon",
                        "user": "davidrecordon",
                        "type": "user"
                    },
                    "name": "David Recordon",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-12T13:12:01.292Z",
                    "hidden": false
                },
                {
                    "_id": "693b85a29874a2a5e4ffb2db",
                    "user": {
                        "_id": "63c9bd445fdc575773c732fe",
                        "avatarUrl": "/avatars/def472d1ab3fbf751225357c0932ae7e.svg",
                        "isPro": false,
                        "fullname": "Yilun Du",
                        "user": "yilundu",
                        "type": "user"
                    },
                    "name": "Yilun Du",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-12T13:12:08.827Z",
                    "hidden": false
                },
                {
                    "_id": "693b85a29874a2a5e4ffb2dc",
                    "name": "Minlan Yu",
                    "hidden": false
                },
                {
                    "_id": "693b85a29874a2a5e4ffb2dd",
                    "name": "Ying Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-11T08:05:58.000Z",
            "submittedOnDailyAt": "2025-12-12T00:32:10.110Z",
            "title": "Confucius Code Agent: An Open-sourced AI Software Engineer at Industrial Scale",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Real-world AI software engineering demands coding agents that can reason over massive repositories, maintain durable memory across and within long sessions, and robustly coordinate complex toolchains at test time. Existing open-source coding agents provide transparency but frequently fall short when pushed to these industrial-scale workloads, while proprietary coding agents offer strong practical performance but limited extensibility, interpretability, and controllability. We present the Confucius Code Agent (CCA), an open-sourced AI software engineer that can operate at an industrial scale. CCA is built atop the Confucius SDK, an open-sourced agent development platform designed around three complementary perspectives: Agent Experience (AX), User Experience (UX), and Developer Experience (DX). The SDK introduces a unified orchestrator with hierarchical working memory for long-context reasoning, a persistent note-taking system for cross-session continual learning, and a modular extension module for robust tool use. Moreover, a meta-agent automates the synthesis, evaluation, and refinement of agent configurations through a build-test-improve loop, enabling rapid agent development on new tasks, environments, and tool stacks. Instantiated on Confucius SDK with these mechanisms, CCA delivers strong performance on real-world software engineering tasks. On SWE-Bench-Pro, CCA achieves a state-of-the-art Resolve@1 performance of 54.3%, substantially improving over prior coding agents. Together, the Confucius SDK and CCA provide a transparent, extensible, and reproducible foundation for AI agents, bridge gaps between research prototypes and production-grade systems, and support agent development and deployment at industrial scale.",
            "upvotes": 2,
            "discussionId": "693b85a39874a2a5e4ffb2de",
            "githubRepo": "https://github.com/facebook/confucius",
            "githubRepoAddedBy": "user",
            "githubStars": 13,
            "organization": {
                "_id": "66b54027408752ae16404b05",
                "name": "metaresearch",
                "fullname": "Meta Research",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66b25f3f58babfaeb76112dc/2GmiaF075AZ7BcE538oPk.png"
            }
        },
        "publishedAt": "2025-12-11T03:05:58.000Z",
        "title": "Confucius Code Agent: An Open-sourced AI Software Engineer at Industrial Scale",
        "summary": "Real-world AI software engineering demands coding agents that can reason over massive repositories, maintain durable memory across and within long sessions, and robustly coordinate complex toolchains at test time. Existing open-source coding agents provide transparency but frequently fall short when pushed to these industrial-scale workloads, while proprietary coding agents offer strong practical performance but limited extensibility, interpretability, and controllability. We present the Confucius Code Agent (CCA), an open-sourced AI software engineer that can operate at an industrial scale. CCA is built atop the Confucius SDK, an open-sourced agent development platform designed around three complementary perspectives: Agent Experience (AX), User Experience (UX), and Developer Experience (DX). The SDK introduces a unified orchestrator with hierarchical working memory for long-context reasoning, a persistent note-taking system for cross-session continual learning, and a modular extension module for robust tool use. Moreover, a meta-agent automates the synthesis, evaluation, and refinement of agent configurations through a build-test-improve loop, enabling rapid agent development on new tasks, environments, and tool stacks. Instantiated on Confucius SDK with these mechanisms, CCA delivers strong performance on real-world software engineering tasks. On SWE-Bench-Pro, CCA achieves a state-of-the-art Resolve@1 performance of 54.3%, substantially improving over prior coding agents. Together, the Confucius SDK and CCA provide a transparent, extensible, and reproducible foundation for AI agents, bridge gaps between research prototypes and production-grade systems, and support agent development and deployment at industrial scale.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.10398.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 182
        },
        "organization": {
            "_id": "66b54027408752ae16404b05",
            "name": "metaresearch",
            "fullname": "Meta Research",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66b25f3f58babfaeb76112dc/2GmiaF075AZ7BcE538oPk.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.09924",
            "authors": [
                {
                    "_id": "693c061d9874a2a5e4ffb46f",
                    "name": "Xinyu Liu",
                    "hidden": false
                },
                {
                    "_id": "693c061d9874a2a5e4ffb470",
                    "user": {
                        "_id": "649d54b314afbb10ce2a9eeb",
                        "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
                        "isPro": false,
                        "fullname": "Hangjie Yuan",
                        "user": "JacobYuan",
                        "type": "user"
                    },
                    "name": "Hangjie Yuan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-12T12:59:45.120Z",
                    "hidden": false
                },
                {
                    "_id": "693c061d9874a2a5e4ffb471",
                    "name": "Yujie Wei",
                    "hidden": false
                },
                {
                    "_id": "693c061d9874a2a5e4ffb472",
                    "name": "Jiazheng Xing",
                    "hidden": false
                },
                {
                    "_id": "693c061d9874a2a5e4ffb473",
                    "name": "Yujin Han",
                    "hidden": false
                },
                {
                    "_id": "693c061d9874a2a5e4ffb474",
                    "user": {
                        "_id": "64d08fde3c4a1b39a003774a",
                        "avatarUrl": "/avatars/df9d5e40223f80da5c45b2728ce4d3f9.svg",
                        "isPro": false,
                        "fullname": "Pan Jiahao",
                        "user": "LewisPan",
                        "type": "user"
                    },
                    "name": "Jiahao Pan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-12T13:10:29.261Z",
                    "hidden": false
                },
                {
                    "_id": "693c061d9874a2a5e4ffb475",
                    "user": {
                        "_id": "6604ec9ec86682af133865c3",
                        "avatarUrl": "/avatars/2ad04afce14fd3cbbdfa3a8699c69430.svg",
                        "isPro": false,
                        "fullname": "Yanbiao Ma",
                        "user": "Yanbiao",
                        "type": "user"
                    },
                    "name": "Yanbiao Ma",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-12T13:10:34.896Z",
                    "hidden": false
                },
                {
                    "_id": "693c061d9874a2a5e4ffb476",
                    "name": "Chi-Min Chan",
                    "hidden": false
                },
                {
                    "_id": "693c061d9874a2a5e4ffb477",
                    "name": "Kang Zhao",
                    "hidden": false
                },
                {
                    "_id": "693c061d9874a2a5e4ffb478",
                    "name": "Shiwei Zhang",
                    "hidden": false
                },
                {
                    "_id": "693c061d9874a2a5e4ffb479",
                    "user": {
                        "_id": "65f865cf806814aec87d45ce",
                        "avatarUrl": "/avatars/6b931c1efe20422a5985fce6a9e36aed.svg",
                        "isPro": false,
                        "fullname": "Wenhan Luo",
                        "user": "whluo",
                        "type": "user"
                    },
                    "name": "Wenhan Luo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-12T13:10:01.865Z",
                    "hidden": false
                },
                {
                    "_id": "693c061d9874a2a5e4ffb47a",
                    "user": {
                        "_id": "64762b325f70f9b2d0ade28e",
                        "avatarUrl": "/avatars/1f37382724b475ece805f943a8858acd.svg",
                        "isPro": false,
                        "fullname": "Yike Guo",
                        "user": "SuaLily",
                        "type": "user"
                    },
                    "name": "Yike Guo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-12T13:11:04.743Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-10T18:57:09.000Z",
            "submittedOnDailyAt": "2025-12-12T09:41:08.992Z",
            "title": "ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning",
            "submittedOnDailyBy": {
                "_id": "649d54b314afbb10ce2a9eeb",
                "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
                "isPro": false,
                "fullname": "Hangjie Yuan",
                "user": "JacobYuan",
                "type": "user"
            },
            "summary": "Video unified models exhibit strong capabilities in understanding and generation, yet they struggle with reason-informed visual editing even when equipped with powerful internal vision-language models (VLMs). We attribute this gap to two factors: 1) existing datasets are inadequate for training and evaluating reasoning-aware video editing, and 2) an inherent disconnect between the models' reasoning and editing capabilities, which prevents the rich understanding from effectively instructing the editing process. Bridging this gap requires an integrated framework that connects reasoning with visual transformation. To address this gap, we introduce the Reason-Informed Video Editing (RVE) task, which requires reasoning about physical plausibility and causal dynamics during editing. To support systematic evaluation, we construct RVE-Bench, a comprehensive benchmark with two complementary subsets: Reasoning-Informed Video Editing and In-Context Video Generation. These subsets cover diverse reasoning dimensions and real-world editing scenarios. Building upon this foundation, we propose the ReViSE, a Self-Reflective Reasoning (SRF) framework that unifies generation and evaluation within a single architecture. The model's internal VLM provides intrinsic feedback by assessing whether the edited video logically satisfies the given instruction. The differential feedback that refines the generator's reasoning behavior during training. Extensive experiments on RVE-Bench demonstrate that ReViSE significantly enhances editing accuracy and visual fidelity, achieving a 32% improvement of the Overall score in the reasoning-informed video editing subset over state-of-the-art methods.",
            "upvotes": 2,
            "discussionId": "693c061e9874a2a5e4ffb47b",
            "githubRepo": "https://github.com/Liuxinyv/ReViSE",
            "githubRepoAddedBy": "user",
            "ai_summary": "The ReViSE framework integrates reasoning and visual editing in video models using a self-reflective reasoning mechanism, enhancing editing accuracy and visual fidelity through intrinsic feedback from an internal vision-language model.",
            "ai_keywords": [
                "reason-informed video editing",
                "RVE task",
                "RVE-Bench",
                "self-reflective reasoning",
                "SRF framework",
                "reasoning-aware video editing",
                "vision-language models",
                "intrinsic feedback",
                "logical satisfaction",
                "editing accuracy",
                "visual fidelity"
            ],
            "githubStars": 3
        },
        "publishedAt": "2025-12-10T13:57:09.000Z",
        "title": "ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning",
        "summary": "Video unified models exhibit strong capabilities in understanding and generation, yet they struggle with reason-informed visual editing even when equipped with powerful internal vision-language models (VLMs). We attribute this gap to two factors: 1) existing datasets are inadequate for training and evaluating reasoning-aware video editing, and 2) an inherent disconnect between the models' reasoning and editing capabilities, which prevents the rich understanding from effectively instructing the editing process. Bridging this gap requires an integrated framework that connects reasoning with visual transformation. To address this gap, we introduce the Reason-Informed Video Editing (RVE) task, which requires reasoning about physical plausibility and causal dynamics during editing. To support systematic evaluation, we construct RVE-Bench, a comprehensive benchmark with two complementary subsets: Reasoning-Informed Video Editing and In-Context Video Generation. These subsets cover diverse reasoning dimensions and real-world editing scenarios. Building upon this foundation, we propose the ReViSE, a Self-Reflective Reasoning (SRF) framework that unifies generation and evaluation within a single architecture. The model's internal VLM provides intrinsic feedback by assessing whether the edited video logically satisfies the given instruction. The differential feedback that refines the generator's reasoning behavior during training. Extensive experiments on RVE-Bench demonstrate that ReViSE significantly enhances editing accuracy and visual fidelity, achieving a 32% improvement of the Overall score in the reasoning-informed video editing subset over state-of-the-art methods.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.09924.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "649d54b314afbb10ce2a9eeb",
            "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
            "fullname": "Hangjie Yuan",
            "name": "JacobYuan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.08870",
            "authors": [
                {
                    "_id": "693aa119e1612ff4bc2a492e",
                    "name": "Xiang Chen",
                    "hidden": false
                },
                {
                    "_id": "693aa119e1612ff4bc2a492f",
                    "user": {
                        "_id": "645b0c3ec35da9c7afd95421",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg",
                        "isPro": false,
                        "fullname": "Yuling",
                        "user": "YerbaPage",
                        "type": "user"
                    },
                    "name": "Yuling Shi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-11T10:52:27.356Z",
                    "hidden": false
                },
                {
                    "_id": "693aa119e1612ff4bc2a4930",
                    "user": {
                        "_id": "68f287f2faba6f123f8a3b3c",
                        "avatarUrl": "/avatars/9690464a99e2728bcd8bfccd3b872020.svg",
                        "isPro": false,
                        "fullname": "Qizhen Lan",
                        "user": "lanqz7766",
                        "type": "user"
                    },
                    "name": "Qizhen Lan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-12T13:09:08.869Z",
                    "hidden": false
                },
                {
                    "_id": "693aa119e1612ff4bc2a4931",
                    "name": "Yuchao Qiu",
                    "hidden": false
                },
                {
                    "_id": "693aa119e1612ff4bc2a4932",
                    "name": "Xiaodong Gu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-09T18:04:41.000Z",
            "submittedOnDailyAt": "2025-12-12T00:10:14.363Z",
            "title": "Fed-SE: Federated Self-Evolution for Privacy-Constrained Multi-Environment LLM Agents",
            "submittedOnDailyBy": {
                "_id": "645b0c3ec35da9c7afd95421",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg",
                "isPro": false,
                "fullname": "Yuling",
                "user": "YerbaPage",
                "type": "user"
            },
            "summary": "LLM agents are widely deployed in complex interactive tasks, yet privacy constraints often preclude centralized optimization and co-evolution across dynamic environments. While Federated Learning (FL) has proven effective on static datasets, its extension to the open-ended self-evolution of agents remains underexplored. Directly applying standard FL is challenging: heterogeneous tasks and sparse, trajectory-level rewards introduce severe gradient conflicts, destabilizing the global optimization process. To bridge this gap, we propose Fed-SE, a Federated Self-Evolution framework for LLM agents. Fed-SE establishes a local evolution-global aggregation paradigm. Locally, agents employ parameter-efficient fine-tuning on filtered, high-return trajectories to achieve stable gradient updates. Globally, Fed-SE aggregates updates within a low-rank subspace that disentangles environment-specific dynamics, effectively reducing negative transfer across clients. Experiments across five heterogeneous environments demonstrate that Fed-SE improves average task success rates by approximately 18% over federated baselines, validating its effectiveness in robust cross-environment knowledge transfer in privacy-constrained deployments.",
            "upvotes": 2,
            "discussionId": "693aa119e1612ff4bc2a4933",
            "ai_summary": "Fed-SE, a Federated Self-Evolution framework, enhances LLM agents in privacy-constrained environments by local parameter-efficient fine-tuning and global aggregation in a low-rank subspace.",
            "ai_keywords": [
                "Federated Learning (FL)",
                "parameter-efficient fine-tuning",
                "high-return trajectories",
                "low-rank subspace",
                "Fed-SE",
                "federated baselines",
                "cross-environment knowledge transfer"
            ]
        },
        "publishedAt": "2025-12-09T13:04:41.000Z",
        "title": "Fed-SE: Federated Self-Evolution for Privacy-Constrained Multi-Environment LLM Agents",
        "summary": "LLM agents are widely deployed in complex interactive tasks, yet privacy constraints often preclude centralized optimization and co-evolution across dynamic environments. While Federated Learning (FL) has proven effective on static datasets, its extension to the open-ended self-evolution of agents remains underexplored. Directly applying standard FL is challenging: heterogeneous tasks and sparse, trajectory-level rewards introduce severe gradient conflicts, destabilizing the global optimization process. To bridge this gap, we propose Fed-SE, a Federated Self-Evolution framework for LLM agents. Fed-SE establishes a local evolution-global aggregation paradigm. Locally, agents employ parameter-efficient fine-tuning on filtered, high-return trajectories to achieve stable gradient updates. Globally, Fed-SE aggregates updates within a low-rank subspace that disentangles environment-specific dynamics, effectively reducing negative transfer across clients. Experiments across five heterogeneous environments demonstrate that Fed-SE improves average task success rates by approximately 18% over federated baselines, validating its effectiveness in robust cross-environment knowledge transfer in privacy-constrained deployments.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.08870.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "645b0c3ec35da9c7afd95421",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg",
            "fullname": "Yuling",
            "name": "YerbaPage",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 191
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.09756",
            "authors": [
                {
                    "_id": "693bc0719874a2a5e4ffb3dc",
                    "user": {
                        "_id": "667fdaee20ee9ac417c7708c",
                        "avatarUrl": "/avatars/69dfba6ff392643af1dcfe8af0a42ae9.svg",
                        "isPro": false,
                        "fullname": "Chonghua Liao",
                        "user": "ChonghuaLiao",
                        "type": "user"
                    },
                    "name": "Chonghua Liao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-12T13:08:36.298Z",
                    "hidden": false
                },
                {
                    "_id": "693bc0719874a2a5e4ffb3dd",
                    "name": "Ke Wang",
                    "hidden": false
                },
                {
                    "_id": "693bc0719874a2a5e4ffb3de",
                    "user": {
                        "_id": "677f6b5883442ad1180a0087",
                        "avatarUrl": "/avatars/fe928cbed47b21439db90e75264e9a72.svg",
                        "isPro": false,
                        "fullname": "yuchuan wu",
                        "user": "yuchuan123",
                        "type": "user"
                    },
                    "name": "Yuchuan Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-12T13:08:15.799Z",
                    "hidden": false
                },
                {
                    "_id": "693bc0719874a2a5e4ffb3df",
                    "name": "Fei Huang",
                    "hidden": false
                },
                {
                    "_id": "693bc0719874a2a5e4ffb3e0",
                    "user": {
                        "_id": "66641b2fd8e1e34bc621e688",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66641b2fd8e1e34bc621e688/csPETwnx2zCIHSWi9uAi-.png",
                        "isPro": false,
                        "fullname": "Yongbin Li",
                        "user": "Yongbin-Li",
                        "type": "user"
                    },
                    "name": "Yongbin Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-12T13:08:42.655Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-10T15:35:11.000Z",
            "submittedOnDailyAt": "2025-12-12T04:43:11.678Z",
            "title": "MOA: Multi-Objective Alignment for Role-Playing Agents",
            "submittedOnDailyBy": {
                "_id": "62e0ef42edb0462c8d51818d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e0ef42edb0462c8d51818d/3YM7DUynIWiiRFM6_enpg.jpeg",
                "isPro": false,
                "fullname": "Ting-En Lin",
                "user": "tnlin",
                "type": "user"
            },
            "summary": "Role-playing agents (RPAs) must simultaneously master many conflicting skills -- following multi-turn instructions, exhibiting domain knowledge, and adopting a consistent linguistic style. Existing work either relies on supervised fine-tuning (SFT) that over-fits surface cues and yields low diversity, or applies reinforcement learning (RL) that fails to learn multiple dimensions for comprehensive RPA optimization. We present MOA (Multi-Objective Alignment), a reinforcement-learning framework that enables multi-dimensional, fine-grained rubric optimization for general RPAs. MOA introduces a novel multi-objective optimization strategy that trains simultaneously on multiple fine-grained rubrics to boost optimization performance. Besides, to address the issues of model output diversity and quality, we have also employed thought-augmented rollout with off-policy guidance. Extensive experiments on challenging benchmarks such as PersonaGym and RoleMRC show that MOA enables an 8B model to match or even outperform strong baselines such as GPT-4o and Claude across numerous dimensions. This demonstrates the great potential of MOA in building RPAs that can simultaneously meet the demands of role knowledge, persona style, diverse scenarios, and complex multi-turn conversations.",
            "upvotes": 1,
            "discussionId": "693bc0729874a2a5e4ffb3e1",
            "ai_summary": "MOA, a reinforcement-learning framework, optimizes multiple dimensions of role-playing agents using multi-objective alignment and thought-augmented rollout, outperforming baselines in diverse scenarios and complex conversations.",
            "ai_keywords": [
                "reinforcement learning",
                "multi-objective optimization",
                "fine-grained rubrics",
                "thought-augmented rollout",
                "off-policy guidance",
                "PersonaGym",
                "RoleMRC"
            ]
        },
        "publishedAt": "2025-12-10T10:35:11.000Z",
        "title": "MOA: Multi-Objective Alignment for Role-Playing Agents",
        "summary": "Role-playing agents (RPAs) must simultaneously master many conflicting skills -- following multi-turn instructions, exhibiting domain knowledge, and adopting a consistent linguistic style. Existing work either relies on supervised fine-tuning (SFT) that over-fits surface cues and yields low diversity, or applies reinforcement learning (RL) that fails to learn multiple dimensions for comprehensive RPA optimization. We present MOA (Multi-Objective Alignment), a reinforcement-learning framework that enables multi-dimensional, fine-grained rubric optimization for general RPAs. MOA introduces a novel multi-objective optimization strategy that trains simultaneously on multiple fine-grained rubrics to boost optimization performance. Besides, to address the issues of model output diversity and quality, we have also employed thought-augmented rollout with off-policy guidance. Extensive experiments on challenging benchmarks such as PersonaGym and RoleMRC show that MOA enables an 8B model to match or even outperform strong baselines such as GPT-4o and Claude across numerous dimensions. This demonstrates the great potential of MOA in building RPAs that can simultaneously meet the demands of role knowledge, persona style, diverse scenarios, and complex multi-turn conversations.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.09756.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62e0ef42edb0462c8d51818d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e0ef42edb0462c8d51818d/3YM7DUynIWiiRFM6_enpg.jpeg",
            "fullname": "Ting-En Lin",
            "name": "tnlin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 12
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.04537",
            "authors": [
                {
                    "_id": "693b9c1a9874a2a5e4ffb353",
                    "name": "Pei Yang",
                    "hidden": false
                },
                {
                    "_id": "693b9c1a9874a2a5e4ffb354",
                    "name": "Hai Ci",
                    "hidden": false
                },
                {
                    "_id": "693b9c1a9874a2a5e4ffb355",
                    "name": "Yiren Song",
                    "hidden": false
                },
                {
                    "_id": "693b9c1a9874a2a5e4ffb356",
                    "name": "Mike Zheng Shou",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64311a95034ecbefddd141ef/qEVgekoCxPSRmV304zHMr.qt"
            ],
            "publishedAt": "2025-12-04T07:34:08.000Z",
            "submittedOnDailyAt": "2025-12-12T14:24:53.808Z",
            "title": "X-Humanoid: Robotize Human Videos to Generate Humanoid Videos at Scale",
            "submittedOnDailyBy": {
                "_id": "64311a95034ecbefddd141ef",
                "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
                "isPro": false,
                "fullname": "Yiren Song",
                "user": "yiren98",
                "type": "user"
            },
            "summary": "The advancement of embodied AI has unlocked significant potential for intelligent humanoid robots. However, progress in both Vision-Language-Action (VLA) models and world models is severely hampered by the scarcity of large-scale, diverse training data. A promising solution is to \"robotize\" web-scale human videos, which has been proven effective for policy training. However, these solutions mainly \"overlay\" robot arms to egocentric videos, which cannot handle complex full-body motions and scene occlusions in third-person videos, making them unsuitable for robotizing humans. To bridge this gap, we introduce X-Humanoid, a generative video editing approach that adapts the powerful Wan 2.2 model into a video-to-video structure and finetunes it for the human-to-humanoid translation task. This finetuning requires paired human-humanoid videos, so we designed a scalable data creation pipeline, turning community assets into 17+ hours of paired synthetic videos using Unreal Engine. We then apply our trained model to 60 hours of the Ego-Exo4D videos, generating and releasing a new large-scale dataset of over 3.6 million \"robotized\" humanoid video frames. Quantitative analysis and user studies confirm our method's superiority over existing baselines: 69% of users rated it best for motion consistency, and 62.1% for embodiment correctness.",
            "upvotes": 1,
            "discussionId": "693b9c1b9874a2a5e4ffb357",
            "ai_summary": "X-Humanoid uses a generative video editing approach to translate human actions into humanoid robot actions, creating a large dataset for training embodied AI models.",
            "ai_keywords": [
                "embodied AI",
                "Vision-Language-Action (VLA) models",
                "world models",
                "generative video editing",
                "Wan 2.2 model",
                "video-to-video structure",
                "human-to-humanoid translation",
                "Unreal Engine",
                "Ego-Exo4D videos",
                "motion consistency",
                "embodiment correctness"
            ],
            "organization": {
                "_id": "63a553c4ce5763e06f78669c",
                "name": "showlab",
                "fullname": "Show Lab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1671779505215-63a55320ce5763e06f78519c.png"
            }
        },
        "publishedAt": "2025-12-04T02:34:08.000Z",
        "title": "X-Humanoid: Robotize Human Videos to Generate Humanoid Videos at Scale",
        "summary": "The advancement of embodied AI has unlocked significant potential for intelligent humanoid robots. However, progress in both Vision-Language-Action (VLA) models and world models is severely hampered by the scarcity of large-scale, diverse training data. A promising solution is to \"robotize\" web-scale human videos, which has been proven effective for policy training. However, these solutions mainly \"overlay\" robot arms to egocentric videos, which cannot handle complex full-body motions and scene occlusions in third-person videos, making them unsuitable for robotizing humans. To bridge this gap, we introduce X-Humanoid, a generative video editing approach that adapts the powerful Wan 2.2 model into a video-to-video structure and finetunes it for the human-to-humanoid translation task. This finetuning requires paired human-humanoid videos, so we designed a scalable data creation pipeline, turning community assets into 17+ hours of paired synthetic videos using Unreal Engine. We then apply our trained model to 60 hours of the Ego-Exo4D videos, generating and releasing a new large-scale dataset of over 3.6 million \"robotized\" humanoid video frames. Quantitative analysis and user studies confirm our method's superiority over existing baselines: 69% of users rated it best for motion consistency, and 62.1% for embodiment correctness.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64311a95034ecbefddd141ef/qEVgekoCxPSRmV304zHMr.qt"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04537.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64311a95034ecbefddd141ef",
            "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
            "fullname": "Yiren Song",
            "name": "yiren98",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 31
        },
        "organization": {
            "_id": "63a553c4ce5763e06f78669c",
            "name": "showlab",
            "fullname": "Show Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1671779505215-63a55320ce5763e06f78519c.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.10894",
            "authors": [
                {
                    "_id": "693ca49cf516c69324680ccb",
                    "name": "Peiying Zhang",
                    "hidden": false
                },
                {
                    "_id": "693ca49cf516c69324680ccc",
                    "name": "Nanxuan Zhao",
                    "hidden": false
                },
                {
                    "_id": "693ca49cf516c69324680ccd",
                    "name": "Matthew Fisher",
                    "hidden": false
                },
                {
                    "_id": "693ca49cf516c69324680cce",
                    "name": "Yiran Xu",
                    "hidden": false
                },
                {
                    "_id": "693ca49cf516c69324680ccf",
                    "name": "Jing Liao",
                    "hidden": false
                },
                {
                    "_id": "693ca49cf516c69324680cd0",
                    "name": "Difan Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-11T18:23:03.000Z",
            "submittedOnDailyAt": "2025-12-12T21:03:07.203Z",
            "title": "DuetSVG: Unified Multimodal SVG Generation with Internal Visual Guidance",
            "submittedOnDailyBy": {
                "_id": "6393a754d31aec207924611b",
                "avatarUrl": "/avatars/51afe808f8e8a032e34ae6d7a2d6e407.svg",
                "isPro": false,
                "fullname": "Difan Liu",
                "user": "smebliu",
                "type": "user"
            },
            "summary": "Recent vision-language model (VLM)-based approaches have achieved impressive results on SVG generation. However, because they generate only text and lack visual signals during decoding, they often struggle with complex semantics and fail to produce visually appealing or geometrically coherent SVGs. We introduce DuetSVG, a unified multimodal model that jointly generates image tokens and corresponding SVG tokens in an end-to-end manner. DuetSVG is trained on both image and SVG datasets. At inference, we apply a novel test-time scaling strategy that leverages the model's native visual predictions as guidance to improve SVG decoding quality. Extensive experiments show that our method outperforms existing methods, producing visually faithful, semantically aligned, and syntactically clean SVGs across a wide range of applications.",
            "upvotes": 0,
            "discussionId": "693ca49cf516c69324680cd1",
            "ai_summary": "DuetSVG generates both image and SVG tokens end-to-end, improving SVG quality with a test-time scaling strategy.",
            "ai_keywords": [
                "vision-language model",
                "VLM",
                "SVG generation",
                "image tokens",
                "SVG tokens",
                "multimodal model",
                "test-time scaling strategy",
                "visual predictions",
                "semantically aligned",
                "syntactically clean"
            ]
        },
        "publishedAt": "2025-12-11T13:23:03.000Z",
        "title": "DuetSVG: Unified Multimodal SVG Generation with Internal Visual Guidance",
        "summary": "Recent vision-language model (VLM)-based approaches have achieved impressive results on SVG generation. However, because they generate only text and lack visual signals during decoding, they often struggle with complex semantics and fail to produce visually appealing or geometrically coherent SVGs. We introduce DuetSVG, a unified multimodal model that jointly generates image tokens and corresponding SVG tokens in an end-to-end manner. DuetSVG is trained on both image and SVG datasets. At inference, we apply a novel test-time scaling strategy that leverages the model's native visual predictions as guidance to improve SVG decoding quality. Extensive experiments show that our method outperforms existing methods, producing visually faithful, semantically aligned, and syntactically clean SVGs across a wide range of applications.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.10894.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6393a754d31aec207924611b",
            "avatarUrl": "/avatars/51afe808f8e8a032e34ae6d7a2d6e407.svg",
            "fullname": "Difan Liu",
            "name": "smebliu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    }
]