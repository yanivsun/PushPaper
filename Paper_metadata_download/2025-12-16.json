[
    {
        "paper": {
            "id": "2512.13586",
            "authors": [
                {
                    "_id": "6940d86d65f1e24a117805cd",
                    "user": {
                        "_id": "67be0888267276f5a2f9ce71",
                        "avatarUrl": "/avatars/c27dc0003351d2e59d7361064d572e55.svg",
                        "isPro": false,
                        "fullname": "Jia-Nan Li",
                        "user": "JinaLeejnl",
                        "type": "user"
                    },
                    "name": "Jia-Nan Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-16T09:38:49.797Z",
                    "hidden": false
                },
                {
                    "_id": "6940d86d65f1e24a117805ce",
                    "name": "Jian Guan",
                    "hidden": false
                },
                {
                    "_id": "6940d86d65f1e24a117805cf",
                    "name": "Wei Wu",
                    "hidden": false
                },
                {
                    "_id": "6940d86d65f1e24a117805d0",
                    "name": "Chongxuan Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-15T17:41:19.000Z",
            "submittedOnDailyAt": "2025-12-16T01:39:12.101Z",
            "title": "ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding",
            "submittedOnDailyBy": {
                "_id": "67be0888267276f5a2f9ce71",
                "avatarUrl": "/avatars/c27dc0003351d2e59d7361064d572e55.svg",
                "isPro": false,
                "fullname": "Jia-Nan Li",
                "user": "JinaLeejnl",
                "type": "user"
            },
            "summary": "Autoregressive models (ARMs) are hindered by slow sequential inference. While masked diffusion models (MDMs) offer a parallel alternative, they suffer from critical drawbacks: high computational overhead from precluding Key-Value (KV) caching, and incoherent generation arising from learning dependencies over an intractable space of token combinations. To address these limitations, we introduce ReFusion, a novel masked diffusion model that achieves superior performance and efficiency by elevating parallel decoding from the token level to a higher slot level, where each slot is a fixed-length, contiguous sub-sequence. This is achieved through an iterative ``plan-and-infill'' decoding process: a diffusion-based planning step first identifies a set of weakly dependent slots, and an autoregressive infilling step then decodes these selected slots in parallel. The slot-based design simultaneously unlocks full KV cache reuse with a unified causal framework and reduces the learning complexity from the token combination space to a manageable slot-level permutation space. Extensive experiments on seven diverse benchmarks show that ReFusion not only overwhelmingly surpasses prior MDMs with 34% performance gains and an over 18times speedup on average, but also bridges the performance gap to strong ARMs while maintaining a 2.33times average speedup.",
            "upvotes": 75,
            "discussionId": "6940d86d65f1e24a117805d1",
            "githubRepo": "https://github.com/ML-GSAI/ReFusion",
            "githubRepoAddedBy": "user",
            "ai_summary": "ReFusion, a novel masked diffusion model, improves performance and efficiency by using slot-based parallel decoding, achieving superior results compared to autoregressive models and traditional masked diffusion models.",
            "ai_keywords": [
                "autoregressive models",
                "masked diffusion models",
                "Key-Value caching",
                "parallel decoding",
                "token level",
                "slot level",
                "diffusion-based planning",
                "autoregressive infilling",
                "slot-based design",
                "slot-level permutation space"
            ],
            "githubStars": 18
        },
        "publishedAt": "2025-12-15T12:41:19.000Z",
        "title": "ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding",
        "summary": "Autoregressive models (ARMs) are hindered by slow sequential inference. While masked diffusion models (MDMs) offer a parallel alternative, they suffer from critical drawbacks: high computational overhead from precluding Key-Value (KV) caching, and incoherent generation arising from learning dependencies over an intractable space of token combinations. To address these limitations, we introduce ReFusion, a novel masked diffusion model that achieves superior performance and efficiency by elevating parallel decoding from the token level to a higher slot level, where each slot is a fixed-length, contiguous sub-sequence. This is achieved through an iterative ``plan-and-infill'' decoding process: a diffusion-based planning step first identifies a set of weakly dependent slots, and an autoregressive infilling step then decodes these selected slots in parallel. The slot-based design simultaneously unlocks full KV cache reuse with a unified causal framework and reduces the learning complexity from the token combination space to a manageable slot-level permutation space. Extensive experiments on seven diverse benchmarks show that ReFusion not only overwhelmingly surpasses prior MDMs with 34% performance gains and an over 18times speedup on average, but also bridges the performance gap to strong ARMs while maintaining a 2.33times average speedup.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13586.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "67be0888267276f5a2f9ce71",
            "avatarUrl": "/avatars/c27dc0003351d2e59d7361064d572e55.svg",
            "fullname": "Jia-Nan Li",
            "name": "JinaLeejnl",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.13687",
            "authors": [
                {
                    "_id": "6940ee0665f1e24a1178066d",
                    "user": {
                        "_id": "67756c9c846a267749304255",
                        "avatarUrl": "/avatars/01f09805b561887c55d1b9ad4e96b461.svg",
                        "isPro": false,
                        "fullname": "Jingfeng Yao",
                        "user": "MapleF9",
                        "type": "user"
                    },
                    "name": "Jingfeng Yao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-16T10:34:03.365Z",
                    "hidden": false
                },
                {
                    "_id": "6940ee0665f1e24a1178066e",
                    "user": {
                        "_id": "6264bf5a1ed8d81e47ae3a62",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1650769741842-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Yuda Song",
                        "user": "IDKiro",
                        "type": "user"
                    },
                    "name": "Yuda Song",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-16T14:10:38.071Z",
                    "hidden": false
                },
                {
                    "_id": "6940ee0665f1e24a1178066f",
                    "user": {
                        "_id": "64192280d459c9e7fbb03aa1",
                        "avatarUrl": "/avatars/89935de92f3f9107d7b768b82fb27e70.svg",
                        "isPro": false,
                        "fullname": "Zhou",
                        "user": "Yucong",
                        "type": "user"
                    },
                    "name": "Yucong Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-16T10:34:50.881Z",
                    "hidden": false
                },
                {
                    "_id": "6940ee0665f1e24a11780670",
                    "user": {
                        "_id": "62600de6d47e3dbae32ce1ce",
                        "avatarUrl": "/avatars/a536417cfec6e10ac415091bd1829426.svg",
                        "isPro": false,
                        "fullname": "Xinggang Wang",
                        "user": "xinggangw",
                        "type": "user"
                    },
                    "name": "Xinggang Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-16T10:34:40.199Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-15T18:59:54.000Z",
            "submittedOnDailyAt": "2025-12-16T07:11:50.997Z",
            "title": "Towards Scalable Pre-training of Visual Tokenizers for Generation",
            "submittedOnDailyBy": {
                "_id": "67756c9c846a267749304255",
                "avatarUrl": "/avatars/01f09805b561887c55d1b9ad4e96b461.svg",
                "isPro": false,
                "fullname": "Jingfeng Yao",
                "user": "MapleF9",
                "type": "user"
            },
            "summary": "The quality of the latent space in visual tokenizers (e.g., VAEs) is crucial for modern generative models. However, the standard reconstruction-based training paradigm produces a latent space that is biased towards low-level information, leading to a foundation flaw: better pixel-level accuracy does not lead to higher-quality generation. This implies that pouring extensive compute into visual tokenizer pre-training translates poorly to improved performance in generation. We identify this as the ``pre-training scaling problem`` and suggest a necessary shift: to be effective for generation, a latent space must concisely represent high-level semantics. We present VTP, a unified visual tokenizer pre-training framework, pioneering the joint optimization of image-text contrastive, self-supervised, and reconstruction losses. Our large-scale study reveals two principal findings: (1) understanding is a key driver of generation, and (2) much better scaling properties, where generative performance scales effectively with compute, parameters, and data allocated to the pretraining of the visual tokenizer. After large-scale pre-training, our tokenizer delivers a competitive profile (78.2 zero-shot accuracy and 0.36 rFID on ImageNet) and 4.1 times faster convergence on generation compared to advanced distillation methods. More importantly, it scales effectively: without modifying standard DiT training specs, solely investing more FLOPS in pretraining VTP achieves 65.8\\% FID improvement in downstream generation, while conventional autoencoder stagnates very early at 1/10 FLOPS. Our pre-trained models are available at https://github.com/MiniMax-AI/VTP.",
            "upvotes": 69,
            "discussionId": "6940ee0665f1e24a11780671",
            "githubRepo": "https://github.com/MiniMax-AI/VTP",
            "githubRepoAddedBy": "user",
            "ai_summary": "A unified visual tokenizer pre-training framework (VTP) improves generative performance by optimizing image-text contrastive, self-supervised, and reconstruction losses, leading to better scaling properties and higher zero-shot accuracy and faster convergence.",
            "ai_keywords": [
                "latent space",
                "visual tokenizers",
                "VAEs",
                "reconstruction-based training",
                "low-level information",
                "pre-training scaling problem",
                "high-level semantics",
                "VTP",
                "image-text contrastive",
                "self-supervised",
                "reconstruction losses",
                "generative performance",
                "ImageNet",
                "zero-shot accuracy",
                "rFID",
                "FLOPS",
                "DiT",
                "advanced distillation methods"
            ],
            "githubStars": 92,
            "organization": {
                "_id": "6778fc29920093dbc0c24917",
                "name": "MiniMaxAI",
                "fullname": "MiniMax",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/676e38ad04af5bec20bc9faf/dUd-LsZEX0H_d4qefO_g6.jpeg"
            }
        },
        "publishedAt": "2025-12-15T13:59:54.000Z",
        "title": "Towards Scalable Pre-training of Visual Tokenizers for Generation",
        "summary": "The quality of the latent space in visual tokenizers (e.g., VAEs) is crucial for modern generative models. However, the standard reconstruction-based training paradigm produces a latent space that is biased towards low-level information, leading to a foundation flaw: better pixel-level accuracy does not lead to higher-quality generation. This implies that pouring extensive compute into visual tokenizer pre-training translates poorly to improved performance in generation. We identify this as the ``pre-training scaling problem`` and suggest a necessary shift: to be effective for generation, a latent space must concisely represent high-level semantics. We present VTP, a unified visual tokenizer pre-training framework, pioneering the joint optimization of image-text contrastive, self-supervised, and reconstruction losses. Our large-scale study reveals two principal findings: (1) understanding is a key driver of generation, and (2) much better scaling properties, where generative performance scales effectively with compute, parameters, and data allocated to the pretraining of the visual tokenizer. After large-scale pre-training, our tokenizer delivers a competitive profile (78.2 zero-shot accuracy and 0.36 rFID on ImageNet) and 4.1 times faster convergence on generation compared to advanced distillation methods. More importantly, it scales effectively: without modifying standard DiT training specs, solely investing more FLOPS in pretraining VTP achieves 65.8\\% FID improvement in downstream generation, while conventional autoencoder stagnates very early at 1/10 FLOPS. Our pre-trained models are available at https://github.com/MiniMax-AI/VTP.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13687.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67756c9c846a267749304255",
            "avatarUrl": "/avatars/01f09805b561887c55d1b9ad4e96b461.svg",
            "fullname": "Jingfeng Yao",
            "name": "MapleF9",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "organization": {
            "_id": "6778fc29920093dbc0c24917",
            "name": "MiniMaxAI",
            "fullname": "MiniMax",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/676e38ad04af5bec20bc9faf/dUd-LsZEX0H_d4qefO_g6.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.13564",
            "authors": [
                {
                    "_id": "6940d68565f1e24a1178056c",
                    "user": {
                        "_id": "6544b9b646dbdeca34ee5f52",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6544b9b646dbdeca34ee5f52/nRx6m1C4wfZ_xSWoBUNJf.png",
                        "isPro": false,
                        "fullname": "Yuyang Hu",
                        "user": "namespace-ERI",
                        "type": "user"
                    },
                    "name": "Yuyang Hu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-16T14:10:40.040Z",
                    "hidden": false
                },
                {
                    "_id": "6940d68565f1e24a1178056d",
                    "user": {
                        "_id": "65435cad429b80b14922ab8d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/N8oWq4ZZn3dRxmXi18FrA.jpeg",
                        "isPro": false,
                        "fullname": "Shichun Liu",
                        "user": "Liusc2020",
                        "type": "user"
                    },
                    "name": "Shichun Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-16T09:39:40.844Z",
                    "hidden": false
                },
                {
                    "_id": "6940d68565f1e24a1178056e",
                    "name": "Yanwei Yue",
                    "hidden": false
                },
                {
                    "_id": "6940d68565f1e24a1178056f",
                    "name": "Guibin Zhang",
                    "hidden": false
                },
                {
                    "_id": "6940d68565f1e24a11780570",
                    "name": "Boyang Liu",
                    "hidden": false
                },
                {
                    "_id": "6940d68565f1e24a11780571",
                    "name": "Fangyi Zhu",
                    "hidden": false
                },
                {
                    "_id": "6940d68565f1e24a11780572",
                    "name": "Jiahang Lin",
                    "hidden": false
                },
                {
                    "_id": "6940d68565f1e24a11780573",
                    "user": {
                        "_id": "638ef0b0c67af472d31674a6",
                        "avatarUrl": "/avatars/02df97d15a0f46b47f9162221733b121.svg",
                        "isPro": false,
                        "fullname": "Honglin Guo",
                        "user": "KYLN24",
                        "type": "user"
                    },
                    "name": "Honglin Guo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-16T09:39:38.698Z",
                    "hidden": false
                },
                {
                    "_id": "6940d68565f1e24a11780574",
                    "name": "Shihan Dou",
                    "hidden": false
                },
                {
                    "_id": "6940d68565f1e24a11780575",
                    "name": "Zhiheng Xi",
                    "hidden": false
                },
                {
                    "_id": "6940d68565f1e24a11780576",
                    "name": "Senjie Jin",
                    "hidden": false
                },
                {
                    "_id": "6940d68565f1e24a11780577",
                    "user": {
                        "_id": "62e52483a944e2a56cd2c6ca",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e52483a944e2a56cd2c6ca/pG44O-1qD00q5CEJMMyFQ.jpeg",
                        "isPro": false,
                        "fullname": "Jiejun Tan",
                        "user": "zstanjj",
                        "type": "user"
                    },
                    "name": "Jiejun Tan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-16T09:39:42.726Z",
                    "hidden": false
                },
                {
                    "_id": "6940d68565f1e24a11780578",
                    "name": "Yanbin Yin",
                    "hidden": false
                },
                {
                    "_id": "6940d68565f1e24a11780579",
                    "name": "Jiongnan Liu",
                    "hidden": false
                },
                {
                    "_id": "6940d68565f1e24a1178057a",
                    "name": "Zeyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "6940d68565f1e24a1178057b",
                    "user": {
                        "_id": "6309bfdab8d7b3889319b588",
                        "avatarUrl": "/avatars/572acdad470f765ef2e058ead3741e24.svg",
                        "isPro": false,
                        "fullname": "SunZX",
                        "user": "Jeryi",
                        "type": "user"
                    },
                    "name": "Zhongxiang Sun",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-16T22:32:57.018Z",
                    "hidden": false
                },
                {
                    "_id": "6940d68565f1e24a1178057c",
                    "name": "Yutao Zhu",
                    "hidden": false
                },
                {
                    "_id": "6940d68565f1e24a1178057d",
                    "name": "Hao Sun",
                    "hidden": false
                },
                {
                    "_id": "6940d68565f1e24a1178057e",
                    "name": "Boci Peng",
                    "hidden": false
                },
                {
                    "_id": "6940d68565f1e24a1178057f",
                    "name": "Zhenrong Cheng",
                    "hidden": false
                },
                {
                    "_id": "6940d68565f1e24a11780580",
                    "name": "Xuanbo Fan",
                    "hidden": false
                },
                {
                    "_id": "6940d68565f1e24a11780581",
                    "name": "Jiaxin Guo",
                    "hidden": false
                },
                {
                    "_id": "6940d68565f1e24a11780582",
                    "name": "Xinlei Yu",
                    "hidden": false
                },
                {
                    "_id": "6940d68565f1e24a11780583",
                    "name": "Zhenhong Zhou",
                    "hidden": false
                },
                {
                    "_id": "6940d68565f1e24a11780584",
                    "name": "Zewen Hu",
                    "hidden": false
                },
                {
                    "_id": "6940d68565f1e24a11780585",
                    "name": "Jiahao Huo",
                    "hidden": false
                },
                {
                    "_id": "6940d68565f1e24a11780586",
                    "name": "Junhao Wang",
                    "hidden": false
                },
                {
                    "_id": "6940d68565f1e24a11780587",
                    "name": "Yuwei Niu",
                    "hidden": false
                },
                {
                    "_id": "6940d68565f1e24a11780588",
                    "name": "Yu Wang",
                    "hidden": false
                },
                {
                    "_id": "6940d68565f1e24a11780589",
                    "name": "Zhenfei Yin",
                    "hidden": false
                },
                {
                    "_id": "6940d68565f1e24a1178058a",
                    "name": "Xiaobin Hu",
                    "hidden": false
                },
                {
                    "_id": "6940d68565f1e24a1178058b",
                    "name": "Yue Liao",
                    "hidden": false
                },
                {
                    "_id": "6940d68565f1e24a1178058c",
                    "name": "Qiankun Li",
                    "hidden": false
                },
                {
                    "_id": "6940d68565f1e24a1178058d",
                    "name": "Kun Wang",
                    "hidden": false
                },
                {
                    "_id": "6940d68565f1e24a1178058e",
                    "name": "Wangchunshu Zhou",
                    "hidden": false
                },
                {
                    "_id": "6940d68565f1e24a1178058f",
                    "name": "Yixin Liu",
                    "hidden": false
                },
                {
                    "_id": "6940d68565f1e24a11780590",
                    "name": "Dawei Cheng",
                    "hidden": false
                },
                {
                    "_id": "6940d68565f1e24a11780591",
                    "name": "Qi Zhang",
                    "hidden": false
                },
                {
                    "_id": "6940d68565f1e24a11780592",
                    "name": "Tao Gui",
                    "hidden": false
                },
                {
                    "_id": "6940d68565f1e24a11780593",
                    "name": "Shirui Pan",
                    "hidden": false
                },
                {
                    "_id": "6940d68565f1e24a11780594",
                    "name": "Yan Zhang",
                    "hidden": false
                },
                {
                    "_id": "6940d68565f1e24a11780595",
                    "name": "Philip Torr",
                    "hidden": false
                },
                {
                    "_id": "6940d68565f1e24a11780596",
                    "name": "Zhicheng Dou",
                    "hidden": false
                },
                {
                    "_id": "6940d68565f1e24a11780597",
                    "name": "Ji-Rong Wen",
                    "hidden": false
                },
                {
                    "_id": "6940d68565f1e24a11780598",
                    "name": "Xuanjing Huang",
                    "hidden": false
                },
                {
                    "_id": "6940d68565f1e24a11780599",
                    "name": "Yu-Gang Jiang",
                    "hidden": false
                },
                {
                    "_id": "6940d68565f1e24a1178059a",
                    "name": "Shuicheng Yan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-15T17:22:34.000Z",
            "submittedOnDailyAt": "2025-12-16T01:18:34.363Z",
            "title": "Memory in the Age of AI Agents",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Memory has emerged, and will continue to remain, a core capability of foundation model-based agents. As research on agent memory rapidly expands and attracts unprecedented attention, the field has also become increasingly fragmented. Existing works that fall under the umbrella of agent memory often differ substantially in their motivations, implementations, and evaluation protocols, while the proliferation of loosely defined memory terminologies has further obscured conceptual clarity. Traditional taxonomies such as long/short-term memory have proven insufficient to capture the diversity of contemporary agent memory systems. This work aims to provide an up-to-date landscape of current agent memory research. We begin by clearly delineating the scope of agent memory and distinguishing it from related concepts such as LLM memory, retrieval augmented generation (RAG), and context engineering. We then examine agent memory through the unified lenses of forms, functions, and dynamics. From the perspective of forms, we identify three dominant realizations of agent memory, namely token-level, parametric, and latent memory. From the perspective of functions, we propose a finer-grained taxonomy that distinguishes factual, experiential, and working memory. From the perspective of dynamics, we analyze how memory is formed, evolved, and retrieved over time. To support practical development, we compile a comprehensive summary of memory benchmarks and open-source frameworks. Beyond consolidation, we articulate a forward-looking perspective on emerging research frontiers, including memory automation, reinforcement learning integration, multimodal memory, multi-agent memory, and trustworthiness issues. We hope this survey serves not only as a reference for existing work, but also as a conceptual foundation for rethinking memory as a first-class primitive in the design of future agentic intelligence.",
            "upvotes": 64,
            "discussionId": "6940d68565f1e24a1178059b",
            "githubRepo": "https://github.com/Shichun-Liu/Agent-Memory-Paper-List",
            "githubRepoAddedBy": "user",
            "ai_summary": "This survey provides an updated overview of agent memory research, distinguishing its forms, functions, and dynamics, and highlights emerging research directions.",
            "ai_keywords": [
                "agent memory",
                "LLM memory",
                "retrieval augmented generation (RAG)",
                "context engineering",
                "token-level memory",
                "parametric memory",
                "latent memory",
                "factual memory",
                "experiential memory",
                "working memory",
                "memory benchmarks",
                "open-source frameworks",
                "memory automation",
                "reinforcement learning integration",
                "multimodal memory",
                "multi-agent memory",
                "trustworthiness issues"
            ],
            "githubStars": 115
        },
        "publishedAt": "2025-12-15T12:22:34.000Z",
        "title": "Memory in the Age of AI Agents",
        "summary": "Memory has emerged, and will continue to remain, a core capability of foundation model-based agents. As research on agent memory rapidly expands and attracts unprecedented attention, the field has also become increasingly fragmented. Existing works that fall under the umbrella of agent memory often differ substantially in their motivations, implementations, and evaluation protocols, while the proliferation of loosely defined memory terminologies has further obscured conceptual clarity. Traditional taxonomies such as long/short-term memory have proven insufficient to capture the diversity of contemporary agent memory systems. This work aims to provide an up-to-date landscape of current agent memory research. We begin by clearly delineating the scope of agent memory and distinguishing it from related concepts such as LLM memory, retrieval augmented generation (RAG), and context engineering. We then examine agent memory through the unified lenses of forms, functions, and dynamics. From the perspective of forms, we identify three dominant realizations of agent memory, namely token-level, parametric, and latent memory. From the perspective of functions, we propose a finer-grained taxonomy that distinguishes factual, experiential, and working memory. From the perspective of dynamics, we analyze how memory is formed, evolved, and retrieved over time. To support practical development, we compile a comprehensive summary of memory benchmarks and open-source frameworks. Beyond consolidation, we articulate a forward-looking perspective on emerging research frontiers, including memory automation, reinforcement learning integration, multimodal memory, multi-agent memory, and trustworthiness issues. We hope this survey serves not only as a reference for existing work, but also as a conceptual foundation for rethinking memory as a first-class primitive in the design of future agentic intelligence.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13564.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 185
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.12967",
            "authors": [
                {
                    "_id": "6940d25d65f1e24a11780417",
                    "user": {
                        "_id": "64777a346e6c7ac608c1e9bf",
                        "avatarUrl": "/avatars/b0e65ba781c90c2560606eb5467101eb.svg",
                        "isPro": false,
                        "fullname": "Weizhou Shen",
                        "user": "shenwzh3",
                        "type": "user"
                    },
                    "name": "Weizhou Shen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-16T14:10:42.079Z",
                    "hidden": false
                },
                {
                    "_id": "6940d25d65f1e24a11780418",
                    "user": {
                        "_id": "64c9b0f28d2d187c24d1e6c1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/1CPnAaB3gsupdpiNWaoDc.png",
                        "isPro": false,
                        "fullname": "ZiYi Yang",
                        "user": "AALF",
                        "type": "user"
                    },
                    "name": "Ziyi Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-16T09:39:56.062Z",
                    "hidden": false
                },
                {
                    "_id": "6940d25d65f1e24a11780419",
                    "name": "Chenliang Li",
                    "hidden": false
                },
                {
                    "_id": "6940d25d65f1e24a1178041a",
                    "name": "Zhiyuan Lu",
                    "hidden": false
                },
                {
                    "_id": "6940d25d65f1e24a1178041b",
                    "name": "Miao Peng",
                    "hidden": false
                },
                {
                    "_id": "6940d25d65f1e24a1178041c",
                    "name": "Huashan Sun",
                    "hidden": false
                },
                {
                    "_id": "6940d25d65f1e24a1178041d",
                    "name": "Yingcheng Shi",
                    "hidden": false
                },
                {
                    "_id": "6940d25d65f1e24a1178041e",
                    "name": "Shengyi Liao",
                    "hidden": false
                },
                {
                    "_id": "6940d25d65f1e24a1178041f",
                    "name": "Shaopeng Lai",
                    "hidden": false
                },
                {
                    "_id": "6940d25d65f1e24a11780420",
                    "name": "Bo Zhang",
                    "hidden": false
                },
                {
                    "_id": "6940d25d65f1e24a11780421",
                    "name": "Dayiheng Liu",
                    "hidden": false
                },
                {
                    "_id": "6940d25d65f1e24a11780422",
                    "name": "Fei Huang",
                    "hidden": false
                },
                {
                    "_id": "6940d25d65f1e24a11780423",
                    "name": "Jingren Zhou",
                    "hidden": false
                },
                {
                    "_id": "6940d25d65f1e24a11780424",
                    "name": "Ming Yan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-15T04:11:11.000Z",
            "submittedOnDailyAt": "2025-12-16T01:29:41.007Z",
            "title": "QwenLong-L1.5: Post-Training Recipe for Long-Context Reasoning and Memory Management",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We introduce QwenLong-L1.5, a model that achieves superior long-context reasoning capabilities through systematic post-training innovations. The key technical breakthroughs of QwenLong-L1.5 are as follows: (1) Long-Context Data Synthesis Pipeline: We develop a systematic synthesis framework that generates challenging reasoning tasks requiring multi-hop grounding over globally distributed evidence. By deconstructing documents into atomic facts and their underlying relationships, and then programmatically composing verifiable reasoning questions, our approach creates high-quality training data at scale, moving substantially beyond simple retrieval tasks to enable genuine long-range reasoning capabilities. (2) Stabilized Reinforcement Learning for Long-Context Training: To overcome the critical instability in long-context RL, we introduce task-balanced sampling with task-specific advantage estimation to mitigate reward bias, and propose Adaptive Entropy-Controlled Policy Optimization (AEPO) that dynamically regulates exploration-exploitation trade-offs. (3) Memory-Augmented Architecture for Ultra-Long Contexts: Recognizing that even extended context windows cannot accommodate arbitrarily long sequences, we develop a memory management framework with multi-stage fusion RL training that seamlessly integrates single-pass reasoning with iterative memory-based processing for tasks exceeding 4M tokens. Based on Qwen3-30B-A3B-Thinking, QwenLong-L1.5 achieves performance comparable to GPT-5 and Gemini-2.5-Pro on long-context reasoning benchmarks, surpassing its baseline by 9.90 points on average. On ultra-long tasks (1M~4M tokens), QwenLong-L1.5's memory-agent framework yields a 9.48-point gain over the agent baseline. Additionally, the acquired long-context reasoning ability translates to enhanced performance in general domains like scientific reasoning, memory tool using, and extended dialogue.",
            "upvotes": 56,
            "discussionId": "6940d25d65f1e24a11780425",
            "githubRepo": "https://github.com/Tongyi-Zhiwen/Qwen-Doc",
            "githubRepoAddedBy": "user",
            "ai_summary": "QwenLong-L1.5 enhances long-context reasoning through data synthesis, stabilized reinforcement learning, and memory-augmented architecture, achieving superior performance on benchmarks and general domains.",
            "ai_keywords": [
                "Long-Context Data Synthesis Pipeline",
                "atomic facts",
                "verifiable reasoning questions",
                "Stabilized Reinforcement Learning",
                "task-balanced sampling",
                "task-specific advantage estimation",
                "Adaptive Entropy-Controlled Policy Optimization",
                "Memory-Augmented Architecture",
                "multi-stage fusion RL training",
                "single-pass reasoning",
                "iterative memory-based processing",
                "memory-agent framework"
            ],
            "githubStars": 312,
            "organization": {
                "_id": "67d15cca6e2cf0e062dbfb54",
                "name": "AlibabaTongyiLab",
                "fullname": "TongyiLab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
            }
        },
        "publishedAt": "2025-12-14T23:11:11.000Z",
        "title": "QwenLong-L1.5: Post-Training Recipe for Long-Context Reasoning and Memory Management",
        "summary": "We introduce QwenLong-L1.5, a model that achieves superior long-context reasoning capabilities through systematic post-training innovations. The key technical breakthroughs of QwenLong-L1.5 are as follows: (1) Long-Context Data Synthesis Pipeline: We develop a systematic synthesis framework that generates challenging reasoning tasks requiring multi-hop grounding over globally distributed evidence. By deconstructing documents into atomic facts and their underlying relationships, and then programmatically composing verifiable reasoning questions, our approach creates high-quality training data at scale, moving substantially beyond simple retrieval tasks to enable genuine long-range reasoning capabilities. (2) Stabilized Reinforcement Learning for Long-Context Training: To overcome the critical instability in long-context RL, we introduce task-balanced sampling with task-specific advantage estimation to mitigate reward bias, and propose Adaptive Entropy-Controlled Policy Optimization (AEPO) that dynamically regulates exploration-exploitation trade-offs. (3) Memory-Augmented Architecture for Ultra-Long Contexts: Recognizing that even extended context windows cannot accommodate arbitrarily long sequences, we develop a memory management framework with multi-stage fusion RL training that seamlessly integrates single-pass reasoning with iterative memory-based processing for tasks exceeding 4M tokens. Based on Qwen3-30B-A3B-Thinking, QwenLong-L1.5 achieves performance comparable to GPT-5 and Gemini-2.5-Pro on long-context reasoning benchmarks, surpassing its baseline by 9.90 points on average. On ultra-long tasks (1M~4M tokens), QwenLong-L1.5's memory-agent framework yields a 9.48-point gain over the agent baseline. Additionally, the acquired long-context reasoning ability translates to enhanced performance in general domains like scientific reasoning, memory tool using, and extended dialogue.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.12967.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 185
        },
        "organization": {
            "_id": "67d15cca6e2cf0e062dbfb54",
            "name": "AlibabaTongyiLab",
            "fullname": "TongyiLab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.13604",
            "authors": [
                {
                    "_id": "6940d44165f1e24a11780535",
                    "name": "Jianxiong Gao",
                    "hidden": false
                },
                {
                    "_id": "6940d44165f1e24a11780536",
                    "name": "Zhaoxi Chen",
                    "hidden": false
                },
                {
                    "_id": "6940d44165f1e24a11780537",
                    "name": "Xian Liu",
                    "hidden": false
                },
                {
                    "_id": "6940d44165f1e24a11780538",
                    "user": {
                        "_id": "64970d3d9c3b29dca8633f87",
                        "avatarUrl": "/avatars/11e3c9c66d28490d6d09925f9aa47cd1.svg",
                        "isPro": false,
                        "fullname": "JunhaoZhuang",
                        "user": "JunhaoZhuang",
                        "type": "user"
                    },
                    "name": "Junhao Zhuang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-16T09:39:51.620Z",
                    "hidden": false
                },
                {
                    "_id": "6940d44165f1e24a11780539",
                    "name": "Chengming Xu",
                    "hidden": false
                },
                {
                    "_id": "6940d44165f1e24a1178053a",
                    "name": "Jianfeng Feng",
                    "hidden": false
                },
                {
                    "_id": "6940d44165f1e24a1178053b",
                    "name": "Yu Qiao",
                    "hidden": false
                },
                {
                    "_id": "6940d44165f1e24a1178053c",
                    "name": "Yanwei Fu",
                    "hidden": false
                },
                {
                    "_id": "6940d44165f1e24a1178053d",
                    "user": {
                        "_id": "635f8ed47c05eb9f59963d3a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635f8ed47c05eb9f59963d3a/uQf4p9N9pSaFy87Wg9v4k.jpeg",
                        "isPro": false,
                        "fullname": "ChenyangSi",
                        "user": "ChenyangSi",
                        "type": "user"
                    },
                    "name": "Chenyang Si",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-16T09:39:49.025Z",
                    "hidden": false
                },
                {
                    "_id": "6940d44165f1e24a1178053e",
                    "name": "Ziwei Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-15T17:59:58.000Z",
            "submittedOnDailyAt": "2025-12-16T01:12:24.486Z",
            "title": "LongVie 2: Multimodal Controllable Ultra-Long Video World Model",
            "submittedOnDailyBy": {
                "_id": "643815c4961bb61e463c5896",
                "avatarUrl": "/avatars/3b44592472f16c56105bff8c314d9939.svg",
                "isPro": false,
                "fullname": "Jianxiong Gao",
                "user": "Jianxiong",
                "type": "user"
            },
            "summary": "Building video world models upon pretrained video generation systems represents an important yet challenging step toward general spatiotemporal intelligence. A world model should possess three essential properties: controllability, long-term visual quality, and temporal consistency. To this end, we take a progressive approach-first enhancing controllability and then extending toward long-term, high-quality generation. We present LongVie 2, an end-to-end autoregressive framework trained in three stages: (1) Multi-modal guidance, which integrates dense and sparse control signals to provide implicit world-level supervision and improve controllability; (2) Degradation-aware training on the input frame, bridging the gap between training and long-term inference to maintain high visual quality; and (3) History-context guidance, which aligns contextual information across adjacent clips to ensure temporal consistency. We further introduce LongVGenBench, a comprehensive benchmark comprising 100 high-resolution one-minute videos covering diverse real-world and synthetic environments. Extensive experiments demonstrate that LongVie 2 achieves state-of-the-art performance in long-range controllability, temporal coherence, and visual fidelity, and supports continuous video generation lasting up to five minutes, marking a significant step toward unified video world modeling.",
            "upvotes": 52,
            "discussionId": "6940d44165f1e24a1178053f",
            "projectPage": "https://vchitect.github.io/LongVie2-project/",
            "ai_summary": "LongVie 2, an end-to-end autoregressive framework, enhances controllability, visual quality, and temporal consistency in video world models through three progressive training stages.",
            "ai_keywords": [
                "autoregressive framework",
                "multi-modal guidance",
                "degradation-aware training",
                "history-context guidance",
                "video world models",
                "controllability",
                "visual quality",
                "temporal consistency",
                "LongVGenBench",
                "state-of-the-art performance",
                "temporal coherence",
                "visual fidelity"
            ]
        },
        "publishedAt": "2025-12-15T12:59:58.000Z",
        "title": "LongVie 2: Multimodal Controllable Ultra-Long Video World Model",
        "summary": "Building video world models upon pretrained video generation systems represents an important yet challenging step toward general spatiotemporal intelligence. A world model should possess three essential properties: controllability, long-term visual quality, and temporal consistency. To this end, we take a progressive approach-first enhancing controllability and then extending toward long-term, high-quality generation. We present LongVie 2, an end-to-end autoregressive framework trained in three stages: (1) Multi-modal guidance, which integrates dense and sparse control signals to provide implicit world-level supervision and improve controllability; (2) Degradation-aware training on the input frame, bridging the gap between training and long-term inference to maintain high visual quality; and (3) History-context guidance, which aligns contextual information across adjacent clips to ensure temporal consistency. We further introduce LongVGenBench, a comprehensive benchmark comprising 100 high-resolution one-minute videos covering diverse real-world and synthetic environments. Extensive experiments demonstrate that LongVie 2 achieves state-of-the-art performance in long-range controllability, temporal coherence, and visual fidelity, and supports continuous video generation lasting up to five minutes, marking a significant step toward unified video world modeling.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13604.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "643815c4961bb61e463c5896",
            "avatarUrl": "/avatars/3b44592472f16c56105bff8c314d9939.svg",
            "fullname": "Jianxiong Gao",
            "name": "Jianxiong",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.13168",
            "authors": [
                {
                    "_id": "6940eedd65f1e24a11780673",
                    "user": {
                        "_id": "637b08057ce76c3b834da15d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/PFmI1HNcZlHF0RoLjP3c2.png",
                        "isPro": false,
                        "fullname": "Haoyu Dong",
                        "user": "HaoyuDong",
                        "type": "user"
                    },
                    "name": "Haoyu Dong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-16T09:37:23.964Z",
                    "hidden": false
                },
                {
                    "_id": "6940eedd65f1e24a11780674",
                    "user": {
                        "_id": "684ae4a3a6d604475ef72f22",
                        "avatarUrl": "/avatars/70e61614ab115f07361c0baec351255a.svg",
                        "isPro": false,
                        "fullname": "Pengkun Zhang",
                        "user": "logicluo",
                        "type": "user"
                    },
                    "name": "Pengkun Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-16T15:09:52.673Z",
                    "hidden": false
                },
                {
                    "_id": "6940eedd65f1e24a11780675",
                    "user": {
                        "_id": "69369fc780cb6886b08dcf5b",
                        "avatarUrl": "/avatars/1159e73666a55706e551afd4518adf0b.svg",
                        "isPro": false,
                        "fullname": "Yan Gao",
                        "user": "gaoyansdyt",
                        "type": "user"
                    },
                    "name": "Yan Gao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-16T09:47:22.721Z",
                    "hidden": false
                },
                {
                    "_id": "6940eedd65f1e24a11780676",
                    "user": {
                        "_id": "69377b51e5af592ad600e1c5",
                        "avatarUrl": "/avatars/72612a4a69da2bd565ed4777287cff86.svg",
                        "isPro": false,
                        "fullname": "Xuanyu Dong",
                        "user": "KcNcooo",
                        "type": "user"
                    },
                    "name": "Xuanyu Dong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-16T14:13:14.674Z",
                    "hidden": false
                },
                {
                    "_id": "6940eedd65f1e24a11780677",
                    "user": {
                        "_id": "6777bb751ee6b20009098ab6",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6777bb751ee6b20009098ab6/OumKsg54DNDH8qkslg95x.jpeg",
                        "isPro": false,
                        "fullname": "Yilin Cheng",
                        "user": "Yilin-Cheng24",
                        "type": "user"
                    },
                    "name": "Yilin Cheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-16T10:18:24.145Z",
                    "hidden": false
                },
                {
                    "_id": "6940eedd65f1e24a11780678",
                    "user": {
                        "_id": "694135abb4ad5b6ca6c4c9fe",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/694135abb4ad5b6ca6c4c9fe/ZFl3u8k69DZpyErVLYRBk.jpeg",
                        "isPro": false,
                        "fullname": "Mingzhe Lu",
                        "user": "metaphor2ml",
                        "type": "user"
                    },
                    "name": "Mingzhe Lu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-16T14:10:36.238Z",
                    "hidden": false
                },
                {
                    "_id": "6940eedd65f1e24a11780679",
                    "user": {
                        "_id": "63a369d98c0c89dcae3b8329",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg",
                        "isPro": false,
                        "fullname": "Adina Yakefu",
                        "user": "AdinaY",
                        "type": "user"
                    },
                    "name": "Adina Yakefu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-12-16T09:47:16.061Z",
                    "hidden": false
                },
                {
                    "_id": "6940eedd65f1e24a1178067a",
                    "name": "Shuxin Zheng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-15T10:28:45.000Z",
            "submittedOnDailyAt": "2025-12-16T09:53:57.731Z",
            "title": "Finch: Benchmarking Finance & Accounting across Spreadsheet-Centric Enterprise Workflows",
            "submittedOnDailyBy": {
                "_id": "637b08057ce76c3b834da15d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/PFmI1HNcZlHF0RoLjP3c2.png",
                "isPro": false,
                "fullname": "Haoyu Dong",
                "user": "HaoyuDong",
                "type": "user"
            },
            "summary": "We introduce a finance & accounting benchmark (Finch) for evaluating AI agents on real-world, enterprise-grade professional workflows -- interleaving data entry, structuring, formatting, web search, cross-file retrieval, calculation, modeling, validation, translation, visualization, and reporting. Finch is sourced from authentic enterprise workspaces at Enron (15,000 spreadsheets and 500,000 emails from 150 employees) and other financial institutions, preserving in-the-wild messiness across multimodal artifacts (text, tables, formulas, charts, code, and images) and spanning diverse domains such as budgeting, trading, and asset management.\n  We propose a workflow construction process that combines LLM-assisted discovery with expert annotation: (1) LLM-assisted, expert-verified derivation of workflows from real-world email threads and version histories of spreadsheet files, and (2) meticulous expert annotation for workflows, requiring over 700 hours of domain-expert effort. This yields 172 composite workflows with 384 tasks, involving 1,710 spreadsheets with 27 million cells, along with PDFs and other artifacts, capturing the intrinsically messy, long-horizon, knowledge-intensive, and collaborative nature of real-world enterprise work.\n  We conduct both human and automated evaluations of frontier AI systems including GPT 5.1, Claude Sonnet 4.5, Gemini 3 Pro, Grok 4, and Qwen 3 Max, and GPT 5.1 Pro spends 48 hours in total yet passes only 38.4% of workflows, while Claude Sonnet 4.5 passes just 25.0%. Comprehensive case studies further surface the challenges that real-world enterprise workflows pose for AI agents.",
            "upvotes": 46,
            "discussionId": "6940eedd65f1e24a1178067b",
            "projectPage": "https://huggingface.co/datasets/FinWorkBench/Finch",
            "ai_summary": "Finch, a benchmark for AI agents in enterprise finance and accounting, evaluates performance across complex, real-world workflows using authentic data from Enron and other institutions.",
            "ai_keywords": [
                "LLM-assisted discovery",
                "expert annotation",
                "composite workflows",
                "spreadsheets",
                "PDFs",
                "AI systems",
                "GPT 5.1",
                "Claude Sonnet 4.5",
                "Gemini 3 Pro",
                "Grok 4",
                "Qwen 3 Max"
            ],
            "organization": {
                "_id": "690e8a61e75888a0f707c0ee",
                "name": "FinWorkBench",
                "fullname": "Finch: Finance and Accounting Workflow Benchmark",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/637b08057ce76c3b834da15d/0HrDjcjNQK-5-FCZfkLzp.png"
            }
        },
        "publishedAt": "2025-12-15T05:28:45.000Z",
        "title": "Finch: Benchmarking Finance & Accounting across Spreadsheet-Centric Enterprise Workflows",
        "summary": "We introduce a finance & accounting benchmark (Finch) for evaluating AI agents on real-world, enterprise-grade professional workflows -- interleaving data entry, structuring, formatting, web search, cross-file retrieval, calculation, modeling, validation, translation, visualization, and reporting. Finch is sourced from authentic enterprise workspaces at Enron (15,000 spreadsheets and 500,000 emails from 150 employees) and other financial institutions, preserving in-the-wild messiness across multimodal artifacts (text, tables, formulas, charts, code, and images) and spanning diverse domains such as budgeting, trading, and asset management.\n  We propose a workflow construction process that combines LLM-assisted discovery with expert annotation: (1) LLM-assisted, expert-verified derivation of workflows from real-world email threads and version histories of spreadsheet files, and (2) meticulous expert annotation for workflows, requiring over 700 hours of domain-expert effort. This yields 172 composite workflows with 384 tasks, involving 1,710 spreadsheets with 27 million cells, along with PDFs and other artifacts, capturing the intrinsically messy, long-horizon, knowledge-intensive, and collaborative nature of real-world enterprise work.\n  We conduct both human and automated evaluations of frontier AI systems including GPT 5.1, Claude Sonnet 4.5, Gemini 3 Pro, Grok 4, and Qwen 3 Max, and GPT 5.1 Pro spends 48 hours in total yet passes only 38.4% of workflows, while Claude Sonnet 4.5 passes just 25.0%. Comprehensive case studies further surface the challenges that real-world enterprise workflows pose for AI agents.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13168.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "637b08057ce76c3b834da15d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/PFmI1HNcZlHF0RoLjP3c2.png",
            "fullname": "Haoyu Dong",
            "name": "HaoyuDong",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "organization": {
            "_id": "690e8a61e75888a0f707c0ee",
            "name": "FinWorkBench",
            "fullname": "Finch: Finance and Accounting Workflow Benchmark",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/637b08057ce76c3b834da15d/0HrDjcjNQK-5-FCZfkLzp.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.12730",
            "authors": [
                {
                    "_id": "6940cff365f1e24a117803e5",
                    "user": {
                        "_id": "6737eb499ab5a7623aaf038a",
                        "avatarUrl": "/avatars/dfe21c7db767a34f2aff3e80cebcfa22.svg",
                        "isPro": false,
                        "fullname": "Jingzhe Ding",
                        "user": "JingzheDing",
                        "type": "user"
                    },
                    "name": "Jingzhe Ding",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-16T10:18:25.834Z",
                    "hidden": false
                },
                {
                    "_id": "6940cff365f1e24a117803e6",
                    "name": "Shengda Long",
                    "hidden": false
                },
                {
                    "_id": "6940cff365f1e24a117803e7",
                    "name": "Changxin Pu",
                    "hidden": false
                },
                {
                    "_id": "6940cff365f1e24a117803e8",
                    "name": "Huan Zhou",
                    "hidden": false
                },
                {
                    "_id": "6940cff365f1e24a117803e9",
                    "name": "Hongwan Gao",
                    "hidden": false
                },
                {
                    "_id": "6940cff365f1e24a117803ea",
                    "user": {
                        "_id": "671068b8243baf4c58fc29c2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/leMkq-kw1icPo2bCLivGv.png",
                        "isPro": false,
                        "fullname": "Xiang Gao",
                        "user": "coffiney",
                        "type": "user"
                    },
                    "name": "Xiang Gao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-16T09:47:27.017Z",
                    "hidden": false
                },
                {
                    "_id": "6940cff365f1e24a117803eb",
                    "name": "Chao He",
                    "hidden": false
                },
                {
                    "_id": "6940cff365f1e24a117803ec",
                    "user": {
                        "_id": "694100a19d8967e631cf94cf",
                        "avatarUrl": "/avatars/1c785c0fcd171a62e206ef4074061a81.svg",
                        "isPro": false,
                        "fullname": "Yue Hou",
                        "user": "YueHou",
                        "type": "user"
                    },
                    "name": "Yue Hou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-16T09:47:24.853Z",
                    "hidden": false
                },
                {
                    "_id": "6940cff365f1e24a117803ed",
                    "name": "Fei Hu",
                    "hidden": false
                },
                {
                    "_id": "6940cff365f1e24a117803ee",
                    "name": "Zhaojian Li",
                    "hidden": false
                },
                {
                    "_id": "6940cff365f1e24a117803ef",
                    "name": "Weiran Shi",
                    "hidden": false
                },
                {
                    "_id": "6940cff365f1e24a117803f0",
                    "name": "Zaiyuan Wang",
                    "hidden": false
                },
                {
                    "_id": "6940cff365f1e24a117803f1",
                    "name": "Daoguang Zan",
                    "hidden": false
                },
                {
                    "_id": "6940cff365f1e24a117803f2",
                    "name": "Chenchen Zhang",
                    "hidden": false
                },
                {
                    "_id": "6940cff365f1e24a117803f3",
                    "name": "Xiaoxu Zhang",
                    "hidden": false
                },
                {
                    "_id": "6940cff365f1e24a117803f4",
                    "name": "Qizhi Chen",
                    "hidden": false
                },
                {
                    "_id": "6940cff365f1e24a117803f5",
                    "name": "Xianfu Cheng",
                    "hidden": false
                },
                {
                    "_id": "6940cff365f1e24a117803f6",
                    "name": "Bo Deng",
                    "hidden": false
                },
                {
                    "_id": "6940cff365f1e24a117803f7",
                    "name": "Qingshui Gu",
                    "hidden": false
                },
                {
                    "_id": "6940cff365f1e24a117803f8",
                    "user": {
                        "_id": "64e851825ddcace745ba15bd",
                        "avatarUrl": "/avatars/7b6612c411222974d9ea181784eef915.svg",
                        "isPro": false,
                        "fullname": "Kai Hua",
                        "user": "kkish",
                        "type": "user"
                    },
                    "name": "Kai Hua",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-16T09:47:28.890Z",
                    "hidden": false
                },
                {
                    "_id": "6940cff365f1e24a117803f9",
                    "name": "Juntao Lin",
                    "hidden": false
                },
                {
                    "_id": "6940cff365f1e24a117803fa",
                    "name": "Pai Liu",
                    "hidden": false
                },
                {
                    "_id": "6940cff365f1e24a117803fb",
                    "name": "Mingchen Li",
                    "hidden": false
                },
                {
                    "_id": "6940cff365f1e24a117803fc",
                    "name": "Xuanguang Pan",
                    "hidden": false
                },
                {
                    "_id": "6940cff365f1e24a117803fd",
                    "name": "Zifan Peng",
                    "hidden": false
                },
                {
                    "_id": "6940cff365f1e24a117803fe",
                    "name": "Yujia Qin",
                    "hidden": false
                },
                {
                    "_id": "6940cff365f1e24a117803ff",
                    "user": {
                        "_id": "64b8cc7ebe76d2ff0703bfb3",
                        "avatarUrl": "/avatars/f1c7ff17fd923f1460d362333d9fbfe3.svg",
                        "isPro": false,
                        "fullname": "yong",
                        "user": "yo37",
                        "type": "user"
                    },
                    "name": "Yong Shan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-16T14:10:43.971Z",
                    "hidden": false
                },
                {
                    "_id": "6940cff365f1e24a11780400",
                    "name": "Zhewen Tan",
                    "hidden": false
                },
                {
                    "_id": "6940cff365f1e24a11780401",
                    "name": "Weihao Xie",
                    "hidden": false
                },
                {
                    "_id": "6940cff365f1e24a11780402",
                    "name": "Zihan Wang",
                    "hidden": false
                },
                {
                    "_id": "6940cff365f1e24a11780403",
                    "name": "Yishuo Yuan",
                    "hidden": false
                },
                {
                    "_id": "6940cff365f1e24a11780404",
                    "name": "Jiayu Zhang",
                    "hidden": false
                },
                {
                    "_id": "6940cff365f1e24a11780405",
                    "name": "Enduo Zhao",
                    "hidden": false
                },
                {
                    "_id": "6940cff365f1e24a11780406",
                    "name": "Yunfei Zhao",
                    "hidden": false
                },
                {
                    "_id": "6940cff365f1e24a11780407",
                    "name": "He Zhu",
                    "hidden": false
                },
                {
                    "_id": "6940cff365f1e24a11780408",
                    "name": "Chenyang Zou",
                    "hidden": false
                },
                {
                    "_id": "6940cff365f1e24a11780409",
                    "name": "Ming Ding",
                    "hidden": false
                },
                {
                    "_id": "6940cff365f1e24a1178040a",
                    "name": "Jianpeng Jiao",
                    "hidden": false
                },
                {
                    "_id": "6940cff365f1e24a1178040b",
                    "name": "Jiaheng Liu",
                    "hidden": false
                },
                {
                    "_id": "6940cff365f1e24a1178040c",
                    "name": "Minghao Liu",
                    "hidden": false
                },
                {
                    "_id": "6940cff365f1e24a1178040d",
                    "name": "Qian Liu",
                    "hidden": false
                },
                {
                    "_id": "6940cff365f1e24a1178040e",
                    "user": {
                        "_id": "648db8f92eb481bc8bc60469",
                        "avatarUrl": "/avatars/7a640980ed225e31117f750e09bcc18c.svg",
                        "isPro": false,
                        "fullname": "Chongyang Tao",
                        "user": "chongyang09",
                        "type": "user"
                    },
                    "name": "Chongyao Tao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-16T09:47:30.496Z",
                    "hidden": false
                },
                {
                    "_id": "6940cff365f1e24a1178040f",
                    "name": "Jian Yang",
                    "hidden": false
                },
                {
                    "_id": "6940cff365f1e24a11780410",
                    "name": "Tong Yang",
                    "hidden": false
                },
                {
                    "_id": "6940cff365f1e24a11780411",
                    "name": "Zhaoxiang Zhang",
                    "hidden": false
                },
                {
                    "_id": "6940cff365f1e24a11780412",
                    "name": "Xinjie Chen",
                    "hidden": false
                },
                {
                    "_id": "6940cff365f1e24a11780413",
                    "name": "Wenhao Huang",
                    "hidden": false
                },
                {
                    "_id": "6940cff365f1e24a11780414",
                    "name": "Ge Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-14T15:12:13.000Z",
            "submittedOnDailyAt": "2025-12-16T00:51:09.038Z",
            "title": "NL2Repo-Bench: Towards Long-Horizon Repository Generation Evaluation of Coding Agents",
            "submittedOnDailyBy": {
                "_id": "638efcf4c67af472d316d424",
                "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
                "isPro": false,
                "fullname": "Ge Zhang",
                "user": "zhangysk",
                "type": "user"
            },
            "summary": "Recent advances in coding agents suggest rapid progress toward autonomous software development, yet existing benchmarks fail to rigorously evaluate the long-horizon capabilities required to build complete software systems. Most prior evaluations focus on localized code generation, scaffolded completion, or short-term repair tasks, leaving open the question of whether agents can sustain coherent reasoning, planning, and execution over the extended horizons demanded by real-world repository construction. To address this gap, we present NL2Repo Bench, a benchmark explicitly designed to evaluate the long-horizon repository generation ability of coding agents. Given only a single natural-language requirements document and an empty workspace, agents must autonomously design the architecture, manage dependencies, implement multi-module logic, and produce a fully installable Python library. Our experiments across state-of-the-art open- and closed-source models reveal that long-horizon repository generation remains largely unsolved: even the strongest agents achieve below 40% average test pass rates and rarely complete an entire repository correctly. Detailed analysis uncovers fundamental long-horizon failure modes, including premature termination, loss of global coherence, fragile cross-file dependencies, and inadequate planning over hundreds of interaction steps. NL2Repo Bench establishes a rigorous, verifiable testbed for measuring sustained agentic competence and highlights long-horizon reasoning as a central bottleneck for the next generation of autonomous coding agents.",
            "upvotes": 39,
            "discussionId": "6940cff365f1e24a11780415",
            "githubRepo": "https://github.com/multimodal-art-projection/NL2RepoBench",
            "githubRepoAddedBy": "user",
            "ai_summary": "NL2Repo Bench evaluates long-horizon software development capabilities of coding agents by assessing their ability to generate complete Python libraries from natural-language requirements.",
            "ai_keywords": [
                "NL2Repo Bench",
                "long-horizon repository generation",
                "autonomous software development",
                "coding agents",
                "natural-language requirements",
                "architecture design",
                "dependency management",
                "multi-module logic",
                "installable Python library",
                "test pass rates",
                "global coherence",
                "cross-file dependencies",
                "long-horizon reasoning"
            ],
            "githubStars": 25
        },
        "publishedAt": "2025-12-14T10:12:13.000Z",
        "title": "NL2Repo-Bench: Towards Long-Horizon Repository Generation Evaluation of Coding Agents",
        "summary": "Recent advances in coding agents suggest rapid progress toward autonomous software development, yet existing benchmarks fail to rigorously evaluate the long-horizon capabilities required to build complete software systems. Most prior evaluations focus on localized code generation, scaffolded completion, or short-term repair tasks, leaving open the question of whether agents can sustain coherent reasoning, planning, and execution over the extended horizons demanded by real-world repository construction. To address this gap, we present NL2Repo Bench, a benchmark explicitly designed to evaluate the long-horizon repository generation ability of coding agents. Given only a single natural-language requirements document and an empty workspace, agents must autonomously design the architecture, manage dependencies, implement multi-module logic, and produce a fully installable Python library. Our experiments across state-of-the-art open- and closed-source models reveal that long-horizon repository generation remains largely unsolved: even the strongest agents achieve below 40% average test pass rates and rarely complete an entire repository correctly. Detailed analysis uncovers fundamental long-horizon failure modes, including premature termination, loss of global coherence, fragile cross-file dependencies, and inadequate planning over hundreds of interaction steps. NL2Repo Bench establishes a rigorous, verifiable testbed for measuring sustained agentic competence and highlights long-horizon reasoning as a central bottleneck for the next generation of autonomous coding agents.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.12730.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "638efcf4c67af472d316d424",
            "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
            "fullname": "Ge Zhang",
            "name": "zhangysk",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 74
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.12602",
            "authors": [
                {
                    "_id": "6940cc8365f1e24a1177ff48",
                    "user": {
                        "_id": "65756c5e1488186315c6696d",
                        "avatarUrl": "/avatars/2ec67e0d1921b6635c69669b591a3110.svg",
                        "isPro": true,
                        "fullname": "Jingdi Lei",
                        "user": "huaXiaKyrie",
                        "type": "user"
                    },
                    "name": "Jingdi Lei",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-16T10:18:27.530Z",
                    "hidden": false
                },
                {
                    "_id": "6940cc8365f1e24a1177ff49",
                    "user": {
                        "_id": "64bce15bafd1e46c5504ad38",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bce15bafd1e46c5504ad38/vkEjiu-mIagKlrXzDH75o.png",
                        "isPro": false,
                        "fullname": "Di Zhang",
                        "user": "di-zhang-fdu",
                        "type": "user"
                    },
                    "name": "Di Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-16T09:47:40.902Z",
                    "hidden": false
                },
                {
                    "_id": "6940cc8365f1e24a1177ff4a",
                    "name": "Soujanya Poria",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-14T08:51:02.000Z",
            "submittedOnDailyAt": "2025-12-16T00:36:26.596Z",
            "title": "Error-Free Linear Attention is a Free Lunch: Exact Solution from Continuous-Time Dynamics",
            "submittedOnDailyBy": {
                "_id": "626b626405fe1cb65725aca1",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/626b626405fe1cb65725aca1/ZVSbhynzpQhVGq9kGywW6.png",
                "isPro": false,
                "fullname": "Soujanya Poria",
                "user": "soujanyaporia",
                "type": "user"
            },
            "summary": "Linear-time attention and State Space Models (SSMs) promise to solve the quadratic cost bottleneck in long-context language models employing softmax attention. We introduce Error-Free Linear Attention (EFLA), a numerically stable, fully parallelism and generalized formulation of the delta rule. Specifically, we formulate the online learning update as a continuous-time dynamical system and prove that its exact solution is not only attainable but also computable in linear time with full parallelism. By leveraging the rank-1 structure of the dynamics matrix, we directly derive the exact closed-form solution effectively corresponding to the infinite-order Runge-Kutta method. This attention mechanism is theoretically free from error accumulation, perfectly capturing the continuous dynamics while preserving the linear-time complexity. Through an extensive suite of experiments, we show that EFLA enables robust performance in noisy environments, achieving lower language modeling perplexity and superior downstream benchmark performance than DeltaNet without introducing additional parameters. Our work provides a new theoretical foundation for building high-fidelity, scalable linear-time attention models.",
            "upvotes": 34,
            "discussionId": "6940cc8365f1e24a1177ff4b",
            "githubRepo": "https://github.com/declare-lab/EFLA",
            "githubRepoAddedBy": "user",
            "ai_summary": "Error-Free Linear Attention (EFLA) is a stable, parallelizable, and theoretically sound linear-time attention mechanism that outperforms DeltaNet in language modeling and downstream tasks.",
            "ai_keywords": [
                "linear-time attention",
                "state space models",
                "softmax attention",
                "error-free linear attention",
                "delta rule",
                "continuous-time dynamical system",
                "rank-1 structure",
                "infinite-order Runge-Kutta method",
                "language modeling perplexity",
                "DeltaNet"
            ],
            "githubStars": 27,
            "organization": {
                "_id": "626ab9dac804c432c1b27a48",
                "name": "declare-lab",
                "fullname": "Deep Cognition and Language Research (DeCLaRe) Lab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/626b626405fe1cb65725aca1/grq3rj2uj0WRjjPjAtR1I.png"
            }
        },
        "publishedAt": "2025-12-14T03:51:02.000Z",
        "title": "Error-Free Linear Attention is a Free Lunch: Exact Solution from Continuous-Time Dynamics",
        "summary": "Linear-time attention and State Space Models (SSMs) promise to solve the quadratic cost bottleneck in long-context language models employing softmax attention. We introduce Error-Free Linear Attention (EFLA), a numerically stable, fully parallelism and generalized formulation of the delta rule. Specifically, we formulate the online learning update as a continuous-time dynamical system and prove that its exact solution is not only attainable but also computable in linear time with full parallelism. By leveraging the rank-1 structure of the dynamics matrix, we directly derive the exact closed-form solution effectively corresponding to the infinite-order Runge-Kutta method. This attention mechanism is theoretically free from error accumulation, perfectly capturing the continuous dynamics while preserving the linear-time complexity. Through an extensive suite of experiments, we show that EFLA enables robust performance in noisy environments, achieving lower language modeling perplexity and superior downstream benchmark performance than DeltaNet without introducing additional parameters. Our work provides a new theoretical foundation for building high-fidelity, scalable linear-time attention models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.12602.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "626b626405fe1cb65725aca1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/626b626405fe1cb65725aca1/ZVSbhynzpQhVGq9kGywW6.png",
            "fullname": "Soujanya Poria",
            "name": "soujanyaporia",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 10
        },
        "organization": {
            "_id": "626ab9dac804c432c1b27a48",
            "name": "declare-lab",
            "fullname": "Deep Cognition and Language Research (DeCLaRe) Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/626b626405fe1cb65725aca1/grq3rj2uj0WRjjPjAtR1I.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.13313",
            "authors": [
                {
                    "_id": "6940d78b65f1e24a117805a4",
                    "name": "Kling Team",
                    "hidden": false
                },
                {
                    "_id": "6940d78b65f1e24a117805a5",
                    "name": "Jialu Chen",
                    "hidden": false
                },
                {
                    "_id": "6940d78b65f1e24a117805a6",
                    "name": "Yikang Ding",
                    "hidden": false
                },
                {
                    "_id": "6940d78b65f1e24a117805a7",
                    "name": "Zhixue Fang",
                    "hidden": false
                },
                {
                    "_id": "6940d78b65f1e24a117805a8",
                    "name": "Kun Gai",
                    "hidden": false
                },
                {
                    "_id": "6940d78b65f1e24a117805a9",
                    "name": "Yuan Gao",
                    "hidden": false
                },
                {
                    "_id": "6940d78b65f1e24a117805aa",
                    "name": "Kang He",
                    "hidden": false
                },
                {
                    "_id": "6940d78b65f1e24a117805ab",
                    "name": "Jingyun Hua",
                    "hidden": false
                },
                {
                    "_id": "6940d78b65f1e24a117805ac",
                    "name": "Boyuan Jiang",
                    "hidden": false
                },
                {
                    "_id": "6940d78b65f1e24a117805ad",
                    "name": "Mingming Lao",
                    "hidden": false
                },
                {
                    "_id": "6940d78b65f1e24a117805ae",
                    "name": "Xiaohan Li",
                    "hidden": false
                },
                {
                    "_id": "6940d78b65f1e24a117805af",
                    "name": "Hui Liu",
                    "hidden": false
                },
                {
                    "_id": "6940d78b65f1e24a117805b0",
                    "name": "Jiwen Liu",
                    "hidden": false
                },
                {
                    "_id": "6940d78b65f1e24a117805b1",
                    "name": "Xiaoqiang Liu",
                    "hidden": false
                },
                {
                    "_id": "6940d78b65f1e24a117805b2",
                    "name": "Yuan Liu",
                    "hidden": false
                },
                {
                    "_id": "6940d78b65f1e24a117805b3",
                    "name": "Shun Lu",
                    "hidden": false
                },
                {
                    "_id": "6940d78b65f1e24a117805b4",
                    "name": "Yongsen Mao",
                    "hidden": false
                },
                {
                    "_id": "6940d78b65f1e24a117805b5",
                    "name": "Yingchao Shao",
                    "hidden": false
                },
                {
                    "_id": "6940d78b65f1e24a117805b6",
                    "name": "Huafeng Shi",
                    "hidden": false
                },
                {
                    "_id": "6940d78b65f1e24a117805b7",
                    "name": "Xiaoyu Shi",
                    "hidden": false
                },
                {
                    "_id": "6940d78b65f1e24a117805b8",
                    "name": "Peiqin Sun",
                    "hidden": false
                },
                {
                    "_id": "6940d78b65f1e24a117805b9",
                    "name": "Songlin Tang",
                    "hidden": false
                },
                {
                    "_id": "6940d78b65f1e24a117805ba",
                    "name": "Pengfei Wan",
                    "hidden": false
                },
                {
                    "_id": "6940d78b65f1e24a117805bb",
                    "name": "Chao Wang",
                    "hidden": false
                },
                {
                    "_id": "6940d78b65f1e24a117805bc",
                    "name": "Xuebo Wang",
                    "hidden": false
                },
                {
                    "_id": "6940d78b65f1e24a117805bd",
                    "name": "Haoxian Zhang",
                    "hidden": false
                },
                {
                    "_id": "6940d78b65f1e24a117805be",
                    "name": "Yuanxing Zhang",
                    "hidden": false
                },
                {
                    "_id": "6940d78b65f1e24a117805bf",
                    "name": "Yan Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-15T13:30:51.000Z",
            "submittedOnDailyAt": "2025-12-16T01:23:37.055Z",
            "title": "KlingAvatar 2.0 Technical Report",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Avatar video generation models have achieved remarkable progress in recent years. However, prior work exhibits limited efficiency in generating long-duration high-resolution videos, suffering from temporal drifting, quality degradation, and weak prompt following as video length increases. To address these challenges, we propose KlingAvatar 2.0, a spatio-temporal cascade framework that performs upscaling in both spatial resolution and temporal dimension. The framework first generates low-resolution blueprint video keyframes that capture global semantics and motion, and then refines them into high-resolution, temporally coherent sub-clips using a first-last frame strategy, while retaining smooth temporal transitions in long-form videos. To enhance cross-modal instruction fusion and alignment in extended videos, we introduce a Co-Reasoning Director composed of three modality-specific large language model (LLM) experts. These experts reason about modality priorities and infer underlying user intent, converting inputs into detailed storylines through multi-turn dialogue. A Negative Director further refines negative prompts to improve instruction alignment. Building on these components, we extend the framework to support ID-specific multi-character control. Extensive experiments demonstrate that our model effectively addresses the challenges of efficient, multimodally aligned long-form high-resolution video generation, delivering enhanced visual clarity, realistic lip-teeth rendering with accurate lip synchronization, strong identity preservation, and coherent multimodal instruction following.",
            "upvotes": 31,
            "discussionId": "6940d78c65f1e24a117805c0",
            "projectPage": "https://app.klingai.com/global/ai-human/image/new/",
            "ai_summary": "KlingAvatar 2.0 addresses inefficiencies in generating long-duration, high-resolution videos by using a spatio-temporal cascade framework with a Co-Reasoning Director and Negative Director for improved multimodal instruction alignment.",
            "ai_keywords": [
                "spatio-temporal cascade",
                "blueprint video keyframes",
                "high-resolution",
                "temporally coherent sub-clips",
                "first-last frame strategy",
                "smooth temporal transitions",
                "Co-Reasoning Director",
                "modality-specific large language model",
                "multi-turn dialogue",
                "Negative Director",
                "multimodal instruction following",
                "identity preservation",
                "lip synchronization"
            ],
            "organization": {
                "_id": "662c559b322afcbae51b3c8b",
                "name": "KlingTeam",
                "fullname": "Kling Team",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"
            }
        },
        "publishedAt": "2025-12-15T08:30:51.000Z",
        "title": "KlingAvatar 2.0 Technical Report",
        "summary": "Avatar video generation models have achieved remarkable progress in recent years. However, prior work exhibits limited efficiency in generating long-duration high-resolution videos, suffering from temporal drifting, quality degradation, and weak prompt following as video length increases. To address these challenges, we propose KlingAvatar 2.0, a spatio-temporal cascade framework that performs upscaling in both spatial resolution and temporal dimension. The framework first generates low-resolution blueprint video keyframes that capture global semantics and motion, and then refines them into high-resolution, temporally coherent sub-clips using a first-last frame strategy, while retaining smooth temporal transitions in long-form videos. To enhance cross-modal instruction fusion and alignment in extended videos, we introduce a Co-Reasoning Director composed of three modality-specific large language model (LLM) experts. These experts reason about modality priorities and infer underlying user intent, converting inputs into detailed storylines through multi-turn dialogue. A Negative Director further refines negative prompts to improve instruction alignment. Building on these components, we extend the framework to support ID-specific multi-character control. Extensive experiments demonstrate that our model effectively addresses the challenges of efficient, multimodally aligned long-form high-resolution video generation, delivering enhanced visual clarity, realistic lip-teeth rendering with accurate lip synchronization, strong identity preservation, and coherent multimodal instruction following.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13313.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 185
        },
        "organization": {
            "_id": "662c559b322afcbae51b3c8b",
            "name": "KlingTeam",
            "fullname": "Kling Team",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.09636",
            "authors": [
                {
                    "_id": "6940c58565f1e24a1177fbb2",
                    "name": "Mengxi Xiao",
                    "hidden": false
                },
                {
                    "_id": "6940c58565f1e24a1177fbb3",
                    "user": {
                        "_id": "646fc402e9c03ba436d5e93e",
                        "avatarUrl": "/avatars/870c86dc99fb1cb6a348a7a0385b1a04.svg",
                        "isPro": false,
                        "fullname": "Kailai Yang",
                        "user": "klyang",
                        "type": "user"
                    },
                    "name": "Kailai Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-16T14:10:45.859Z",
                    "hidden": false
                },
                {
                    "_id": "6940c58565f1e24a1177fbb4",
                    "name": "Pengde Zhao",
                    "hidden": false
                },
                {
                    "_id": "6940c58565f1e24a1177fbb5",
                    "name": "Enze Zhang",
                    "hidden": false
                },
                {
                    "_id": "6940c58565f1e24a1177fbb6",
                    "user": {
                        "_id": "67fd030140e7539fcc5f6521",
                        "avatarUrl": "/avatars/d79c0eed928d964c3b014a0cf1f01d72.svg",
                        "isPro": false,
                        "fullname": "Ziyan Kuang",
                        "user": "plumjane",
                        "type": "user"
                    },
                    "name": "Ziyan Kuang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-16T09:47:51.153Z",
                    "hidden": false
                },
                {
                    "_id": "6940c58565f1e24a1177fbb7",
                    "name": "Zhiwei Liu",
                    "hidden": false
                },
                {
                    "_id": "6940c58565f1e24a1177fbb8",
                    "name": "Weiguang Han",
                    "hidden": false
                },
                {
                    "_id": "6940c58565f1e24a1177fbb9",
                    "name": "Shu Liao",
                    "hidden": false
                },
                {
                    "_id": "6940c58565f1e24a1177fbba",
                    "name": "Lianting Huang",
                    "hidden": false
                },
                {
                    "_id": "6940c58565f1e24a1177fbbb",
                    "name": "Jinpeng Hu",
                    "hidden": false
                },
                {
                    "_id": "6940c58565f1e24a1177fbbc",
                    "name": "Min Peng",
                    "hidden": false
                },
                {
                    "_id": "6940c58565f1e24a1177fbbd",
                    "user": {
                        "_id": "6479f4317c18dca75e9a9324",
                        "avatarUrl": "/avatars/9aa709230b057f57ee4415c04a622c63.svg",
                        "isPro": false,
                        "fullname": "Xie",
                        "user": "QianqianXie1994",
                        "type": "user"
                    },
                    "name": "Qianqian Xie",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-16T09:47:53.066Z",
                    "hidden": false
                },
                {
                    "_id": "6940c58565f1e24a1177fbbe",
                    "name": "Sophia Ananiadou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-10T13:26:22.000Z",
            "submittedOnDailyAt": "2025-12-16T00:08:57.103Z",
            "title": "MentraSuite: Post-Training Large Language Models for Mental Health Reasoning and Assessment",
            "submittedOnDailyBy": {
                "_id": "663adb42e14047f710dc1d29",
                "avatarUrl": "/avatars/7ca49d67a4a8b4cf0ee896e07646715f.svg",
                "isPro": false,
                "fullname": "Mengxi Xiao",
                "user": "ElsaShaw",
                "type": "user"
            },
            "summary": "Mental health disorders affect hundreds of millions globally, and the Web now serves as a primary medium for accessing support, information, and assessment. Large language models (LLMs) offer scalable and accessible assistance, yet their deployment in mental-health settings remains risky when their reasoning is incomplete, inconsistent, or ungrounded. Existing psychological LLMs emphasize emotional understanding or knowledge recall but overlook the step-wise, clinically aligned reasoning required for appraisal, diagnosis, intervention planning, abstraction, and verification. To address these issues, we introduce MentraSuite, a unified framework for advancing reliable mental-health reasoning. We propose MentraBench, a comprehensive benchmark spanning five core reasoning aspects, six tasks, and 13 datasets, evaluating both task performance and reasoning quality across five dimensions: conciseness, coherence, hallucination avoidance, task understanding, and internal consistency. We further present Mindora, a post-trained model optimized through a hybrid SFT-RL framework with an inconsistency-detection reward to enforce faithful and coherent reasoning. To support training, we construct high-quality trajectories using a novel reasoning trajectory generation strategy, that strategically filters difficult samples and applies a structured, consistency-oriented rewriting process to produce concise, readable, and well-balanced trajectories. Across 20 evaluated LLMs, Mindora achieves the highest average performance on MentraBench and shows remarkable performances in reasoning reliability, demonstrating its effectiveness for complex mental-health scenarios.",
            "upvotes": 22,
            "discussionId": "6940c58565f1e24a1177fbbf",
            "githubRepo": "https://github.com/elsa66666/MentraSuite",
            "githubRepoAddedBy": "user",
            "ai_summary": "MentraSuite, a unified framework, advances reliable mental health reasoning using Mindora, a post-trained model with hybrid SFT-RL, evaluated via MentraBench, a benchmark assessing task performance and reasoning quality.",
            "ai_keywords": [
                "Large language models (LLMs)",
                "mental-health reasoning",
                "MentraSuite",
                "MentraBench",
                "Mindora",
                "hybrid SFT-RL",
                "inconsistency-detection reward",
                "reasoning trajectory generation",
                "task performance",
                "reasoning quality",
                "conciseness",
                "coherence",
                "hallucination avoidance",
                "task understanding",
                "internal consistency"
            ],
            "githubStars": 7,
            "organization": {
                "_id": "6693770d0bf3f05db3017f31",
                "name": "NextGenWhu",
                "fullname": "CLAIN-WHU",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6479f4317c18dca75e9a9324/DydmQ18EutkpFraI3FGpy.png"
            }
        },
        "publishedAt": "2025-12-10T08:26:22.000Z",
        "title": "MentraSuite: Post-Training Large Language Models for Mental Health Reasoning and Assessment",
        "summary": "Mental health disorders affect hundreds of millions globally, and the Web now serves as a primary medium for accessing support, information, and assessment. Large language models (LLMs) offer scalable and accessible assistance, yet their deployment in mental-health settings remains risky when their reasoning is incomplete, inconsistent, or ungrounded. Existing psychological LLMs emphasize emotional understanding or knowledge recall but overlook the step-wise, clinically aligned reasoning required for appraisal, diagnosis, intervention planning, abstraction, and verification. To address these issues, we introduce MentraSuite, a unified framework for advancing reliable mental-health reasoning. We propose MentraBench, a comprehensive benchmark spanning five core reasoning aspects, six tasks, and 13 datasets, evaluating both task performance and reasoning quality across five dimensions: conciseness, coherence, hallucination avoidance, task understanding, and internal consistency. We further present Mindora, a post-trained model optimized through a hybrid SFT-RL framework with an inconsistency-detection reward to enforce faithful and coherent reasoning. To support training, we construct high-quality trajectories using a novel reasoning trajectory generation strategy, that strategically filters difficult samples and applies a structured, consistency-oriented rewriting process to produce concise, readable, and well-balanced trajectories. Across 20 evaluated LLMs, Mindora achieves the highest average performance on MentraBench and shows remarkable performances in reasoning reliability, demonstrating its effectiveness for complex mental-health scenarios.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.09636.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "663adb42e14047f710dc1d29",
            "avatarUrl": "/avatars/7ca49d67a4a8b4cf0ee896e07646715f.svg",
            "fullname": "Mengxi Xiao",
            "name": "ElsaShaw",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "6693770d0bf3f05db3017f31",
            "name": "NextGenWhu",
            "fullname": "CLAIN-WHU",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6479f4317c18dca75e9a9324/DydmQ18EutkpFraI3FGpy.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.10071",
            "authors": [
                {
                    "_id": "69404844b21c43ffbf4b75c3",
                    "name": "Junjie Bai",
                    "hidden": false
                },
                {
                    "_id": "69404844b21c43ffbf4b75c4",
                    "name": "Yu-Wei Chao",
                    "hidden": false
                },
                {
                    "_id": "69404844b21c43ffbf4b75c5",
                    "name": "Qizhi Chen",
                    "hidden": false
                },
                {
                    "_id": "69404844b21c43ffbf4b75c6",
                    "name": "Jinwei Gu",
                    "hidden": false
                },
                {
                    "_id": "69404844b21c43ffbf4b75c7",
                    "name": "Moo Jin Kim",
                    "hidden": false
                },
                {
                    "_id": "69404844b21c43ffbf4b75c8",
                    "name": "Zhaoshuo Li",
                    "hidden": false
                },
                {
                    "_id": "69404844b21c43ffbf4b75c9",
                    "name": "Xuan Li",
                    "hidden": false
                },
                {
                    "_id": "69404844b21c43ffbf4b75ca",
                    "name": "Tsung-Yi Lin",
                    "hidden": false
                },
                {
                    "_id": "69404844b21c43ffbf4b75cb",
                    "name": "Ming-Yu Liu",
                    "hidden": false
                },
                {
                    "_id": "69404844b21c43ffbf4b75cc",
                    "name": "Nic Ma",
                    "hidden": false
                },
                {
                    "_id": "69404844b21c43ffbf4b75cd",
                    "name": "Kaichun Mo",
                    "hidden": false
                },
                {
                    "_id": "69404844b21c43ffbf4b75ce",
                    "user": {
                        "_id": "64daecec888b7e9c400f59b5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64daecec888b7e9c400f59b5/f4pfOfWk6jYJX-Nf2-qHn.png",
                        "isPro": false,
                        "fullname": "Delin Qu",
                        "user": "delinqu",
                        "type": "user"
                    },
                    "name": "Delin Qu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-16T09:48:41.492Z",
                    "hidden": false
                },
                {
                    "_id": "69404844b21c43ffbf4b75cf",
                    "name": "Shangkun Sun",
                    "hidden": false
                },
                {
                    "_id": "69404844b21c43ffbf4b75d0",
                    "name": "Hongchi Xia",
                    "hidden": false
                },
                {
                    "_id": "69404844b21c43ffbf4b75d1",
                    "name": "Fangyin Wei",
                    "hidden": false
                },
                {
                    "_id": "69404844b21c43ffbf4b75d2",
                    "name": "Xiaohui Zeng",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64daecec888b7e9c400f59b5/bw1XqdpXA1rd0l5seu-Wr.mp4"
            ],
            "publishedAt": "2025-12-10T20:46:40.000Z",
            "submittedOnDailyAt": "2025-12-16T00:15:46.675Z",
            "title": "Openpi Comet: Competition Solution For 2025 BEHAVIOR Challenge",
            "submittedOnDailyBy": {
                "_id": "64daecec888b7e9c400f59b5",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64daecec888b7e9c400f59b5/f4pfOfWk6jYJX-Nf2-qHn.png",
                "isPro": false,
                "fullname": "Delin Qu",
                "user": "delinqu",
                "type": "user"
            },
            "summary": "The 2025 BEHAVIOR Challenge is designed to rigorously track progress toward solving long-horizon tasks by physical agents in simulated environments. BEHAVIOR-1K focuses on everyday household tasks that people most want robots to assist with and these tasks introduce long-horizon mobile manipulation challenges in realistic settings, bridging the gap between current research and real-world, human-centric applications. This report presents our solution to the 2025 BEHAVIOR Challenge in a very close 2nd place and substantially outperforms the rest of the submissions. Building on _{0.5}, we focus on systematically building our solution by studying the effects of training techniques and data. Through careful ablations, we show the scaling power in pre-training and post-training phases for competitive performance. We summarize our practical lessons and design recommendations that we hope will provide actionable insights for the broader embodied AI community when adapting powerful foundation models to complex embodied scenarios.",
            "upvotes": 16,
            "discussionId": "69404844b21c43ffbf4b75d3",
            "githubRepo": "https://github.com/mli0603/openpi-comet",
            "githubRepoAddedBy": "user",
            "ai_summary": "A solution for the 2025 BEHAVIOR Challenge in everyday household tasks using pre-training and post-training techniques substantially outperforms other submissions.",
            "ai_keywords": [
                "BEHAVIOR Challenge",
                "long-horizon tasks",
                "mobile manipulation",
                "simulated environments",
                "human-centric applications",
                "embodied AI",
                "pre-training",
                "post-training",
                "ablations",
                "foundation models"
            ],
            "githubStars": 148
        },
        "publishedAt": "2025-12-10T15:46:40.000Z",
        "title": "Openpi Comet: Competition Solution For 2025 BEHAVIOR Challenge",
        "summary": "The 2025 BEHAVIOR Challenge is designed to rigorously track progress toward solving long-horizon tasks by physical agents in simulated environments. BEHAVIOR-1K focuses on everyday household tasks that people most want robots to assist with and these tasks introduce long-horizon mobile manipulation challenges in realistic settings, bridging the gap between current research and real-world, human-centric applications. This report presents our solution to the 2025 BEHAVIOR Challenge in a very close 2nd place and substantially outperforms the rest of the submissions. Building on _{0.5}, we focus on systematically building our solution by studying the effects of training techniques and data. Through careful ablations, we show the scaling power in pre-training and post-training phases for competitive performance. We summarize our practical lessons and design recommendations that we hope will provide actionable insights for the broader embodied AI community when adapting powerful foundation models to complex embodied scenarios.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64daecec888b7e9c400f59b5/bw1XqdpXA1rd0l5seu-Wr.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.10071.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "64daecec888b7e9c400f59b5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64daecec888b7e9c400f59b5/f4pfOfWk6jYJX-Nf2-qHn.png",
            "fullname": "Delin Qu",
            "name": "delinqu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 12
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.13080",
            "authors": [
                {
                    "_id": "6940e59265f1e24a1178065c",
                    "name": "Yicheng Feng",
                    "hidden": false
                },
                {
                    "_id": "6940e59265f1e24a1178065d",
                    "user": {
                        "_id": "640dd700fdeaae139081f598",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640dd700fdeaae139081f598/L986zu4-iOPFs9Y3_T5Ue.jpeg",
                        "isPro": false,
                        "fullname": "Wanpeng Zhang",
                        "user": "zawnpn",
                        "type": "user"
                    },
                    "name": "Wanpeng Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-16T09:37:38.330Z",
                    "hidden": false
                },
                {
                    "_id": "6940e59265f1e24a1178065e",
                    "name": "Ye Wang",
                    "hidden": false
                },
                {
                    "_id": "6940e59265f1e24a1178065f",
                    "name": "Hao Luo",
                    "hidden": false
                },
                {
                    "_id": "6940e59265f1e24a11780660",
                    "name": "Haoqi Yuan",
                    "hidden": false
                },
                {
                    "_id": "6940e59265f1e24a11780661",
                    "name": "Sipeng Zheng",
                    "hidden": false
                },
                {
                    "_id": "6940e59265f1e24a11780662",
                    "name": "Zongqing Lu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-15T08:31:47.000Z",
            "submittedOnDailyAt": "2025-12-16T02:24:05.438Z",
            "title": "Spatial-Aware VLA Pretraining through Visual-Physical Alignment from Human Videos",
            "submittedOnDailyBy": {
                "_id": "640dd700fdeaae139081f598",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640dd700fdeaae139081f598/L986zu4-iOPFs9Y3_T5Ue.jpeg",
                "isPro": false,
                "fullname": "Wanpeng Zhang",
                "user": "zawnpn",
                "type": "user"
            },
            "summary": "Vision-Language-Action (VLA) models provide a promising paradigm for robot learning by integrating visual perception with language-guided policy learning. However, most existing approaches rely on 2D visual inputs to perform actions in 3D physical environments, creating a significant gap between perception and action grounding. To bridge this gap, we propose a Spatial-Aware VLA Pretraining paradigm that performs explicit alignment between visual space and physical space during pretraining, enabling models to acquire 3D spatial understanding before robot policy learning. Starting from pretrained vision-language models, we leverage large-scale human demonstration videos to extract 3D visual and 3D action annotations, forming a new source of supervision that aligns 2D visual observations with 3D spatial reasoning. We instantiate this paradigm with VIPA-VLA, a dual-encoder architecture that incorporates a 3D visual encoder to augment semantic visual representations with 3D-aware features. When adapted to downstream robot tasks, VIPA-VLA achieves significantly improved grounding between 2D vision and 3D action, resulting in more robust and generalizable robotic policies.",
            "upvotes": 13,
            "discussionId": "6940e59265f1e24a11780663",
            "projectPage": "https://beingbeyond.github.io/VIPA-VLA/",
            "githubRepo": "https://github.com/BeingBeyond/VIPA-VLA",
            "githubRepoAddedBy": "user",
            "ai_summary": "A Spatial-Aware VLA Pretraining paradigm improves 3D spatial understanding in robots by aligning 2D visual inputs with 3D actions using dual-encoder architecture with a 3D visual encoder.",
            "ai_keywords": [
                "Vision-Language-Action (VLA) models",
                "Spatial-Aware VLA Pretraining",
                "3D spatial understanding",
                "dual-encoder architecture",
                "3D visual encoder",
                "3D visual and 3D action annotations"
            ],
            "githubStars": 10,
            "organization": {
                "_id": "687a8ba5aedd77694bc94386",
                "name": "BeingBeyond",
                "fullname": "BeingBeyond",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/640dd700fdeaae139081f598/YBq5V88ndIv-QcHweJEUk.png"
            }
        },
        "publishedAt": "2025-12-15T03:31:47.000Z",
        "title": "Spatial-Aware VLA Pretraining through Visual-Physical Alignment from Human Videos",
        "summary": "Vision-Language-Action (VLA) models provide a promising paradigm for robot learning by integrating visual perception with language-guided policy learning. However, most existing approaches rely on 2D visual inputs to perform actions in 3D physical environments, creating a significant gap between perception and action grounding. To bridge this gap, we propose a Spatial-Aware VLA Pretraining paradigm that performs explicit alignment between visual space and physical space during pretraining, enabling models to acquire 3D spatial understanding before robot policy learning. Starting from pretrained vision-language models, we leverage large-scale human demonstration videos to extract 3D visual and 3D action annotations, forming a new source of supervision that aligns 2D visual observations with 3D spatial reasoning. We instantiate this paradigm with VIPA-VLA, a dual-encoder architecture that incorporates a 3D visual encoder to augment semantic visual representations with 3D-aware features. When adapted to downstream robot tasks, VIPA-VLA achieves significantly improved grounding between 2D vision and 3D action, resulting in more robust and generalizable robotic policies.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13080.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "640dd700fdeaae139081f598",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640dd700fdeaae139081f598/L986zu4-iOPFs9Y3_T5Ue.jpeg",
            "fullname": "Wanpeng Zhang",
            "name": "zawnpn",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "organization": {
            "_id": "687a8ba5aedd77694bc94386",
            "name": "BeingBeyond",
            "fullname": "BeingBeyond",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/640dd700fdeaae139081f598/YBq5V88ndIv-QcHweJEUk.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.12692",
            "authors": [
                {
                    "_id": "6941231865f1e24a11780720",
                    "user": {
                        "_id": "6565a60e0ff3292512bae26d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6565a60e0ff3292512bae26d/5P6wgEhCI0ndFVgX9KX8k.jpeg",
                        "isPro": false,
                        "fullname": "Mahir Labib Dihan",
                        "user": "mahirlabibdihan",
                        "type": "user"
                    },
                    "name": "Mahir Labib Dihan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-16T10:18:17.236Z",
                    "hidden": false
                },
                {
                    "_id": "6941231865f1e24a11780721",
                    "name": "Tanzima Hashem",
                    "hidden": false
                },
                {
                    "_id": "6941231865f1e24a11780722",
                    "name": "Mohammed Eunus Ali",
                    "hidden": false
                },
                {
                    "_id": "6941231865f1e24a11780723",
                    "name": "Md Rizwan Parvez",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-14T13:56:54.000Z",
            "submittedOnDailyAt": "2025-12-16T07:40:49.626Z",
            "title": "WebOperator: Action-Aware Tree Search for Autonomous Agents in Web Environment",
            "submittedOnDailyBy": {
                "_id": "6565a60e0ff3292512bae26d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6565a60e0ff3292512bae26d/5P6wgEhCI0ndFVgX9KX8k.jpeg",
                "isPro": false,
                "fullname": "Mahir Labib Dihan",
                "user": "mahirlabibdihan",
                "type": "user"
            },
            "summary": "LLM-based agents often operate in a greedy, step-by-step manner, selecting actions solely based on the current observation without considering long-term consequences or alternative paths. This lack of foresight is particularly problematic in web environments, which are only partially observable-limited to browser-visible content (e.g., DOM and UI elements)-where a single misstep often requires complex and brittle navigation to undo. Without an explicit backtracking mechanism, agents struggle to correct errors or systematically explore alternative paths. Tree-search methods provide a principled framework for such structured exploration, but existing approaches lack mechanisms for safe backtracking, making them prone to unintended side effects. They also assume that all actions are reversible, ignoring the presence of irreversible actions-limitations that reduce their effectiveness in realistic web tasks. To address these challenges, we introduce WebOperator, a tree-search framework that enables reliable backtracking and strategic exploration. Our method incorporates a best-first search strategy that ranks actions by both reward estimates and safety considerations, along with a robust backtracking mechanism that verifies the feasibility of previously visited paths before replaying them, preventing unintended side effects. To further guide exploration, WebOperator generates action candidates from multiple, varied reasoning contexts to ensure diverse and robust exploration, and subsequently curates a high-quality action set by filtering out invalid actions pre-execution and merging semantically equivalent ones. Experimental results on WebArena and WebVoyager demonstrate the effectiveness of WebOperator. On WebArena, WebOperator achieves a state-of-the-art 54.6% success rate with gpt-4o, underscoring the critical advantage of integrating strategic foresight with safe execution.",
            "upvotes": 11,
            "discussionId": "6941231865f1e24a11780724",
            "projectPage": "https://kagnlp.github.io/WebOperator",
            "githubRepo": "https://github.com/kagnlp/WebOperator",
            "githubRepoAddedBy": "user",
            "ai_summary": "WebOperator is a tree-search framework that enhances web-based agents by enabling reliable backtracking and strategic exploration, addressing the limitations of existing methods in handling irreversible actions and partial observability.",
            "ai_keywords": [
                "LLM-based agents",
                "greedy",
                "step-by-step manner",
                "partial observability",
                "DOM",
                "UI elements",
                "tree-search methods",
                "best-first search strategy",
                "safety considerations",
                "backtracking mechanism",
                "WebArena",
                "WebVoyager",
                "strategic foresight",
                "safe execution"
            ],
            "githubStars": 9
        },
        "publishedAt": "2025-12-14T08:56:54.000Z",
        "title": "WebOperator: Action-Aware Tree Search for Autonomous Agents in Web Environment",
        "summary": "LLM-based agents often operate in a greedy, step-by-step manner, selecting actions solely based on the current observation without considering long-term consequences or alternative paths. This lack of foresight is particularly problematic in web environments, which are only partially observable-limited to browser-visible content (e.g., DOM and UI elements)-where a single misstep often requires complex and brittle navigation to undo. Without an explicit backtracking mechanism, agents struggle to correct errors or systematically explore alternative paths. Tree-search methods provide a principled framework for such structured exploration, but existing approaches lack mechanisms for safe backtracking, making them prone to unintended side effects. They also assume that all actions are reversible, ignoring the presence of irreversible actions-limitations that reduce their effectiveness in realistic web tasks. To address these challenges, we introduce WebOperator, a tree-search framework that enables reliable backtracking and strategic exploration. Our method incorporates a best-first search strategy that ranks actions by both reward estimates and safety considerations, along with a robust backtracking mechanism that verifies the feasibility of previously visited paths before replaying them, preventing unintended side effects. To further guide exploration, WebOperator generates action candidates from multiple, varied reasoning contexts to ensure diverse and robust exploration, and subsequently curates a high-quality action set by filtering out invalid actions pre-execution and merging semantically equivalent ones. Experimental results on WebArena and WebVoyager demonstrate the effectiveness of WebOperator. On WebArena, WebOperator achieves a state-of-the-art 54.6% success rate with gpt-4o, underscoring the critical advantage of integrating strategic foresight with safe execution.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.12692.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6565a60e0ff3292512bae26d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6565a60e0ff3292512bae26d/5P6wgEhCI0ndFVgX9KX8k.jpeg",
            "fullname": "Mahir Labib Dihan",
            "name": "mahirlabibdihan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.12799",
            "authors": [
                {
                    "_id": "6940d45065f1e24a11780541",
                    "user": {
                        "_id": "675be14c40726aac79aa4b87",
                        "avatarUrl": "/avatars/0d15b1b46110750a2975b89d9a81e517.svg",
                        "isPro": false,
                        "fullname": "ZHE LIU",
                        "user": "happinessqq",
                        "type": "user"
                    },
                    "name": "Zhe Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-16T09:39:47.086Z",
                    "hidden": false
                },
                {
                    "_id": "6940d45065f1e24a11780542",
                    "name": "Runhui Huang",
                    "hidden": false
                },
                {
                    "_id": "6940d45065f1e24a11780543",
                    "name": "Rui Yang",
                    "hidden": false
                },
                {
                    "_id": "6940d45065f1e24a11780544",
                    "name": "Siming Yan",
                    "hidden": false
                },
                {
                    "_id": "6940d45065f1e24a11780545",
                    "name": "Zining Wang",
                    "hidden": false
                },
                {
                    "_id": "6940d45065f1e24a11780546",
                    "name": "Lu Hou",
                    "hidden": false
                },
                {
                    "_id": "6940d45065f1e24a11780547",
                    "name": "Di Lin",
                    "hidden": false
                },
                {
                    "_id": "6940d45065f1e24a11780548",
                    "name": "Xiang Bai",
                    "hidden": false
                },
                {
                    "_id": "6940d45065f1e24a11780549",
                    "name": "Hengshuang Zhao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-14T18:45:54.000Z",
            "submittedOnDailyAt": "2025-12-16T07:15:43.656Z",
            "title": "DrivePI: Spatial-aware 4D MLLM for Unified Autonomous Driving Understanding, Perception, Prediction and Planning",
            "submittedOnDailyBy": {
                "_id": "675be14c40726aac79aa4b87",
                "avatarUrl": "/avatars/0d15b1b46110750a2975b89d9a81e517.svg",
                "isPro": false,
                "fullname": "ZHE LIU",
                "user": "happinessqq",
                "type": "user"
            },
            "summary": "Although multi-modal large language models (MLLMs) have shown strong capabilities across diverse domains, their application in generating fine-grained 3D perception and prediction outputs in autonomous driving remains underexplored. In this paper, we propose DrivePI, a novel spatial-aware 4D MLLM that serves as a unified Vision-Language-Action (VLA) framework that is also compatible with vision-action (VA) models. Our method jointly performs spatial understanding, 3D perception (i.e., 3D occupancy), prediction (i.e., occupancy flow), and planning (i.e., action outputs) in parallel through end-to-end optimization. To obtain both precise geometric information and rich visual appearance, our approach integrates point clouds, multi-view images, and language instructions within a unified MLLM architecture. We further develop a data engine to generate text-occupancy and text-flow QA pairs for 4D spatial understanding. Remarkably, with only a 0.5B Qwen2.5 model as MLLM backbone, DrivePI as a single unified model matches or exceeds both existing VLA models and specialized VA models. Specifically, compared to VLA models, DrivePI outperforms OpenDriveVLA-7B by 2.5% mean accuracy on nuScenes-QA and reduces collision rate by 70% over ORION (from 0.37% to 0.11%) on nuScenes. Against specialized VA models, DrivePI surpasses FB-OCC by 10.3 RayIoU for 3D occupancy on OpenOcc, reduces the mAVE from 0.591 to 0.509 for occupancy flow on OpenOcc, and achieves 32% lower L2 error than VAD (from 0.72m to 0.49m) for planning on nuScenes. Code will be available at https://github.com/happinesslz/DrivePI",
            "upvotes": 8,
            "discussionId": "6940d45065f1e24a1178054a",
            "githubRepo": "https://github.com/happinesslz/DrivePI",
            "githubRepoAddedBy": "user",
            "ai_summary": "DrivePI, a spatial-aware 4D multi-modal large language model, achieves state-of-the-art performance in 3D perception, prediction, and planning for autonomous driving by integrating point clouds, images, and language instructions.",
            "ai_keywords": [
                "multi-modal large language models",
                "MLLM",
                "DrivePI",
                "Vision-Language-Action",
                "VLA",
                "vision-action",
                "VA",
                "spatial understanding",
                "3D perception",
                "3D occupancy",
                "prediction",
                "occupancy flow",
                "planning",
                "end-to-end optimization",
                "point clouds",
                "multi-view images",
                "language instructions",
                "unified MLLM architecture",
                "data engine",
                "text-occupancy",
                "text-flow QA pairs",
                "4D spatial understanding",
                "Qwen2.5",
                "OpenDriveVLA-7B",
                "nuScenes-QA",
                "ORION",
                "FB-OCC",
                "OpenOcc",
                "mAVE",
                "L2 error",
                "nuScenes"
            ],
            "githubStars": 22
        },
        "publishedAt": "2025-12-14T13:45:54.000Z",
        "title": "DrivePI: Spatial-aware 4D MLLM for Unified Autonomous Driving Understanding, Perception, Prediction and Planning",
        "summary": "Although multi-modal large language models (MLLMs) have shown strong capabilities across diverse domains, their application in generating fine-grained 3D perception and prediction outputs in autonomous driving remains underexplored. In this paper, we propose DrivePI, a novel spatial-aware 4D MLLM that serves as a unified Vision-Language-Action (VLA) framework that is also compatible with vision-action (VA) models. Our method jointly performs spatial understanding, 3D perception (i.e., 3D occupancy), prediction (i.e., occupancy flow), and planning (i.e., action outputs) in parallel through end-to-end optimization. To obtain both precise geometric information and rich visual appearance, our approach integrates point clouds, multi-view images, and language instructions within a unified MLLM architecture. We further develop a data engine to generate text-occupancy and text-flow QA pairs for 4D spatial understanding. Remarkably, with only a 0.5B Qwen2.5 model as MLLM backbone, DrivePI as a single unified model matches or exceeds both existing VLA models and specialized VA models. Specifically, compared to VLA models, DrivePI outperforms OpenDriveVLA-7B by 2.5% mean accuracy on nuScenes-QA and reduces collision rate by 70% over ORION (from 0.37% to 0.11%) on nuScenes. Against specialized VA models, DrivePI surpasses FB-OCC by 10.3 RayIoU for 3D occupancy on OpenOcc, reduces the mAVE from 0.591 to 0.509 for occupancy flow on OpenOcc, and achieves 32% lower L2 error than VAD (from 0.72m to 0.49m) for planning on nuScenes. Code will be available at https://github.com/happinesslz/DrivePI",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.12799.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "675be14c40726aac79aa4b87",
            "avatarUrl": "/avatars/0d15b1b46110750a2975b89d9a81e517.svg",
            "fullname": "ZHE LIU",
            "name": "happinessqq",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.11995",
            "authors": [
                {
                    "_id": "6940cee065f1e24a11780390",
                    "user": {
                        "_id": "64a8121e35fab7cd04c30ed0",
                        "avatarUrl": "/avatars/48849b84703158772f1022932331b143.svg",
                        "isPro": false,
                        "fullname": "Chenrui Fan",
                        "user": "Fcr09",
                        "type": "user"
                    },
                    "name": "Chenrui Fan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-16T09:47:34.480Z",
                    "hidden": false
                },
                {
                    "_id": "6940cee065f1e24a11780391",
                    "name": "Yijun Liang",
                    "hidden": false
                },
                {
                    "_id": "6940cee065f1e24a11780392",
                    "name": "Shweta Bhardwaj",
                    "hidden": false
                },
                {
                    "_id": "6940cee065f1e24a11780393",
                    "name": "Kwesi Cobbina",
                    "hidden": false
                },
                {
                    "_id": "6940cee065f1e24a11780394",
                    "name": "Ming Li",
                    "hidden": false
                },
                {
                    "_id": "6940cee065f1e24a11780395",
                    "user": {
                        "_id": "647f5af5b0e96764589f3b2a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
                        "isPro": false,
                        "fullname": "Tianyi Zhou",
                        "user": "zhoutianyi",
                        "type": "user"
                    },
                    "name": "Tianyi Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-16T09:47:32.272Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-12T19:18:41.000Z",
            "submittedOnDailyAt": "2025-12-16T01:14:05.549Z",
            "title": "V-REX: Benchmarking Exploratory Visual Reasoning via Chain-of-Questions",
            "submittedOnDailyBy": {
                "_id": "647f5af5b0e96764589f3b2a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
                "isPro": false,
                "fullname": "Tianyi Zhou",
                "user": "zhoutianyi",
                "type": "user"
            },
            "summary": "While many vision-language models (VLMs) are developed to answer well-defined, straightforward questions with highly specified targets, as in most benchmarks, they often struggle in practice with complex open-ended tasks, which usually require multiple rounds of exploration and reasoning in the visual space. Such visual thinking paths not only provide step-by-step exploration and verification as an AI detective but also produce better interpretations of the final answers. However, these paths are challenging to evaluate due to the large exploration space of intermediate steps. To bridge the gap, we develop an evaluation suite, ``Visual Reasoning with multi-step EXploration (V-REX)'', which is composed of a benchmark of challenging visual reasoning tasks requiring native multi-step exploration and an evaluation protocol. V-REX covers rich application scenarios across diverse domains. V-REX casts the multi-step exploratory reasoning into a Chain-of-Questions (CoQ) and disentangles VLMs' capability to (1) Planning: breaking down an open-ended task by selecting a chain of exploratory questions; and (2) Following: answering curated CoQ sequentially to collect information for deriving the final answer. By curating finite options of questions and answers per step, V-REX achieves a reliable quantitative and fine-grained analysis of the intermediate steps. By assessing SOTA proprietary and open-sourced VLMs, we reveal consistent scaling trends, significant differences between planning and following abilities, and substantial room for improvement in multi-step exploratory reasoning.",
            "upvotes": 8,
            "discussionId": "6940cee065f1e24a11780396",
            "githubRepo": "https://github.com/tianyi-lab/VREX",
            "githubRepoAddedBy": "user",
            "ai_summary": "The V-REX evaluation suite assesses vision-language models' multi-step reasoning and exploration capabilities through a Chain-of-Questions framework, revealing their strengths and weaknesses in planning and following.",
            "ai_keywords": [
                "vision-language models",
                "visual reasoning",
                "multi-step exploration",
                "Chain-of-Questions",
                "evaluation protocol",
                "planning",
                "following"
            ],
            "githubStars": 2
        },
        "publishedAt": "2025-12-12T14:18:41.000Z",
        "title": "V-REX: Benchmarking Exploratory Visual Reasoning via Chain-of-Questions",
        "summary": "While many vision-language models (VLMs) are developed to answer well-defined, straightforward questions with highly specified targets, as in most benchmarks, they often struggle in practice with complex open-ended tasks, which usually require multiple rounds of exploration and reasoning in the visual space. Such visual thinking paths not only provide step-by-step exploration and verification as an AI detective but also produce better interpretations of the final answers. However, these paths are challenging to evaluate due to the large exploration space of intermediate steps. To bridge the gap, we develop an evaluation suite, ``Visual Reasoning with multi-step EXploration (V-REX)'', which is composed of a benchmark of challenging visual reasoning tasks requiring native multi-step exploration and an evaluation protocol. V-REX covers rich application scenarios across diverse domains. V-REX casts the multi-step exploratory reasoning into a Chain-of-Questions (CoQ) and disentangles VLMs' capability to (1) Planning: breaking down an open-ended task by selecting a chain of exploratory questions; and (2) Following: answering curated CoQ sequentially to collect information for deriving the final answer. By curating finite options of questions and answers per step, V-REX achieves a reliable quantitative and fine-grained analysis of the intermediate steps. By assessing SOTA proprietary and open-sourced VLMs, we reveal consistent scaling trends, significant differences between planning and following abilities, and substantial room for improvement in multi-step exploratory reasoning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.11995.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "647f5af5b0e96764589f3b2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
            "fullname": "Tianyi Zhou",
            "name": "zhoutianyi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 18
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.13250",
            "authors": [
                {
                    "_id": "6940dba565f1e24a117805f7",
                    "name": "Juil Koo",
                    "hidden": false
                },
                {
                    "_id": "6940dba565f1e24a117805f8",
                    "user": {
                        "_id": "6616702547ea6347974667e5",
                        "avatarUrl": "/avatars/6bf95e50ba19df163ef89867ed63fecc.svg",
                        "isPro": false,
                        "fullname": "Daehyeon Choi",
                        "user": "daehyeonchoi",
                        "type": "user"
                    },
                    "name": "Daehyeon Choi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-16T09:38:47.490Z",
                    "hidden": false
                },
                {
                    "_id": "6940dba565f1e24a117805f9",
                    "user": {
                        "_id": "686269b24c9139a755a319dd",
                        "avatarUrl": "/avatars/9533d1c4ef685db3097a8cd65092f269.svg",
                        "isPro": false,
                        "fullname": "Sangwoo Youn",
                        "user": "sang-w00",
                        "type": "user"
                    },
                    "name": "Sangwoo Youn",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-16T09:37:42.523Z",
                    "hidden": false
                },
                {
                    "_id": "6940dba565f1e24a117805fa",
                    "user": {
                        "_id": "6342796a0875f2c99cfd313b",
                        "avatarUrl": "/avatars/98575092404c4197b20c929a6499a015.svg",
                        "isPro": false,
                        "fullname": "Yuseung \"Phillip\" Lee",
                        "user": "phillipinseoul",
                        "type": "user"
                    },
                    "name": "Phillip Y. Lee",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-16T09:37:44.580Z",
                    "hidden": false
                },
                {
                    "_id": "6940dba565f1e24a117805fb",
                    "name": "Minhyuk Sung",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6616702547ea6347974667e5/OwgiL8Yidmp-CeruaDQ51.png"
            ],
            "publishedAt": "2025-12-15T12:04:26.000Z",
            "submittedOnDailyAt": "2025-12-16T01:41:43.234Z",
            "title": "Toward Ambulatory Vision: Learning Visually-Grounded Active View Selection",
            "submittedOnDailyBy": {
                "_id": "6616702547ea6347974667e5",
                "avatarUrl": "/avatars/6bf95e50ba19df163ef89867ed63fecc.svg",
                "isPro": false,
                "fullname": "Daehyeon Choi",
                "user": "daehyeonchoi",
                "type": "user"
            },
            "summary": "Vision Language Models (VLMs) excel at visual question answering (VQA) but remain limited to snapshot vision, reasoning from static images. In contrast, embodied agents require ambulatory vision, actively moving to obtain more informative views. We introduce Visually Grounded Active View Selection (VG-AVS), a task that selects the most informative next viewpoint using only the visual information in the current image, without relying on scene memory or external knowledge. To support this task, we construct a synthetic dataset with automatically generated paired query-target views and question-answer prompts. We also propose a framework that fine-tunes pretrained VLMs through supervised fine-tuning (SFT) followed by RL-based policy optimization. Our approach achieves strong question answering performance based on viewpoint selection and generalizes robustly to unseen synthetic and real scenes. Furthermore, incorporating our learned VG-AVS framework into existing scene-exploration-based EQA systems improves downstream question-answering accuracy.",
            "upvotes": 7,
            "discussionId": "6940dba565f1e24a117805fc",
            "projectPage": "https://active-view-selection.github.io",
            "githubRepo": "https://github.com/KAIST-Visual-AI-Group/VG-AVS",
            "githubRepoAddedBy": "user",
            "ai_summary": "VG-AVS, a task and framework fine-tunes VLMs to select the most informative next viewpoint for visual question answering, enhancing performance and generalization.",
            "ai_keywords": [
                "Vision Language Models",
                "VLMs",
                "visual question answering",
                "VQA",
                "snapshot vision",
                "ambulatory vision",
                "Visually Grounded Active View Selection",
                "VG-AVS",
                "synthetic dataset",
                "supervised fine-tuning",
                "SFT",
                "RL-based policy optimization",
                "EQA systems"
            ],
            "githubStars": 8,
            "organization": {
                "_id": "64bcc40e140491ca9f45409c",
                "name": "KAIST-Visual-AI-Group",
                "fullname": "KAIST-Visual-AI-Lab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/632c17f9930d186432d56cc2/ODvlop9_DJc9jIlD6JAYX.png"
            }
        },
        "publishedAt": "2025-12-15T07:04:26.000Z",
        "title": "Toward Ambulatory Vision: Learning Visually-Grounded Active View Selection",
        "summary": "Vision Language Models (VLMs) excel at visual question answering (VQA) but remain limited to snapshot vision, reasoning from static images. In contrast, embodied agents require ambulatory vision, actively moving to obtain more informative views. We introduce Visually Grounded Active View Selection (VG-AVS), a task that selects the most informative next viewpoint using only the visual information in the current image, without relying on scene memory or external knowledge. To support this task, we construct a synthetic dataset with automatically generated paired query-target views and question-answer prompts. We also propose a framework that fine-tunes pretrained VLMs through supervised fine-tuning (SFT) followed by RL-based policy optimization. Our approach achieves strong question answering performance based on viewpoint selection and generalizes robustly to unseen synthetic and real scenes. Furthermore, incorporating our learned VG-AVS framework into existing scene-exploration-based EQA systems improves downstream question-answering accuracy.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6616702547ea6347974667e5/OwgiL8Yidmp-CeruaDQ51.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13250.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6616702547ea6347974667e5",
            "avatarUrl": "/avatars/6bf95e50ba19df163ef89867ed63fecc.svg",
            "fullname": "Daehyeon Choi",
            "name": "daehyeonchoi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "64bcc40e140491ca9f45409c",
            "name": "KAIST-Visual-AI-Group",
            "fullname": "KAIST-Visual-AI-Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/632c17f9930d186432d56cc2/ODvlop9_DJc9jIlD6JAYX.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.13592",
            "authors": [
                {
                    "_id": "6940ceae65f1e24a11780328",
                    "user": {
                        "_id": "63e9e92f20c109718713f5eb",
                        "avatarUrl": "/avatars/9ff312e854d803e1a2e9e685a21d12f8.svg",
                        "isPro": false,
                        "fullname": "Fu-Yun Wang",
                        "user": "wangfuyun",
                        "type": "user"
                    },
                    "name": "Fu-Yun Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-16T09:47:36.161Z",
                    "hidden": false
                },
                {
                    "_id": "6940ceae65f1e24a11780329",
                    "name": "Hao Zhou",
                    "hidden": false
                },
                {
                    "_id": "6940ceae65f1e24a1178032a",
                    "name": "Liangzhe Yuan",
                    "hidden": false
                },
                {
                    "_id": "6940ceae65f1e24a1178032b",
                    "name": "Sanghyun Woo",
                    "hidden": false
                },
                {
                    "_id": "6940ceae65f1e24a1178032c",
                    "name": "Boqing Gong",
                    "hidden": false
                },
                {
                    "_id": "6940ceae65f1e24a1178032d",
                    "name": "Bohyung Han",
                    "hidden": false
                },
                {
                    "_id": "6940ceae65f1e24a1178032e",
                    "name": "Ming-Hsuan Yang",
                    "hidden": false
                },
                {
                    "_id": "6940ceae65f1e24a1178032f",
                    "name": "Han Zhang",
                    "hidden": false
                },
                {
                    "_id": "6940ceae65f1e24a11780330",
                    "name": "Yukun Zhu",
                    "hidden": false
                },
                {
                    "_id": "6940ceae65f1e24a11780331",
                    "name": "Ting Liu",
                    "hidden": false
                },
                {
                    "_id": "6940ceae65f1e24a11780332",
                    "name": "Long Zhao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-15T17:47:49.000Z",
            "submittedOnDailyAt": "2025-12-16T00:50:04.323Z",
            "title": "Image Diffusion Preview with Consistency Solver",
            "submittedOnDailyBy": {
                "_id": "63e9e92f20c109718713f5eb",
                "avatarUrl": "/avatars/9ff312e854d803e1a2e9e685a21d12f8.svg",
                "isPro": false,
                "fullname": "Fu-Yun Wang",
                "user": "wangfuyun",
                "type": "user"
            },
            "summary": "The slow inference process of image diffusion models significantly degrades interactive user experiences. To address this, we introduce Diffusion Preview, a novel paradigm employing rapid, low-step sampling to generate preliminary outputs for user evaluation, deferring full-step refinement until the preview is deemed satisfactory. Existing acceleration methods, including training-free solvers and post-training distillation, struggle to deliver high-quality previews or ensure consistency between previews and final outputs. We propose ConsistencySolver derived from general linear multistep methods, a lightweight, trainable high-order solver optimized via Reinforcement Learning, that enhances preview quality and consistency. Experimental results demonstrate that ConsistencySolver significantly improves generation quality and consistency in low-step scenarios, making it ideal for efficient preview-and-refine workflows. Notably, it achieves FID scores on-par with Multistep DPM-Solver using 47% fewer steps, while outperforming distillation baselines. Furthermore, user studies indicate our approach reduces overall user interaction time by nearly 50% while maintaining generation quality. Code is available at https://github.com/G-U-N/consolver.",
            "upvotes": 6,
            "discussionId": "6940ceae65f1e24a11780333",
            "githubRepo": "https://github.com/G-U-N/consolver",
            "githubRepoAddedBy": "user",
            "ai_summary": "Diffusion Preview uses ConsistencySolver, a high-order trainable solver, to improve quality and consistency in low-step image generation, enhancing interactive user experiences.",
            "ai_keywords": [
                "image diffusion models",
                "Diffusion Preview",
                "rapid",
                "low-step sampling",
                "general linear multistep methods",
                "ConsistencySolver",
                "Reinforcement Learning",
                "FID scores",
                "Multistep DPM-Solver",
                "preview-and-refine workflows"
            ],
            "githubStars": 17,
            "organization": {
                "_id": "60f6cbb2852126bac698c89e",
                "name": "deepmind",
                "fullname": "Deepmind",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1638956859875-5f1158120c833276f61f1a84.jpeg"
            }
        },
        "publishedAt": "2025-12-15T12:47:49.000Z",
        "title": "Image Diffusion Preview with Consistency Solver",
        "summary": "The slow inference process of image diffusion models significantly degrades interactive user experiences. To address this, we introduce Diffusion Preview, a novel paradigm employing rapid, low-step sampling to generate preliminary outputs for user evaluation, deferring full-step refinement until the preview is deemed satisfactory. Existing acceleration methods, including training-free solvers and post-training distillation, struggle to deliver high-quality previews or ensure consistency between previews and final outputs. We propose ConsistencySolver derived from general linear multistep methods, a lightweight, trainable high-order solver optimized via Reinforcement Learning, that enhances preview quality and consistency. Experimental results demonstrate that ConsistencySolver significantly improves generation quality and consistency in low-step scenarios, making it ideal for efficient preview-and-refine workflows. Notably, it achieves FID scores on-par with Multistep DPM-Solver using 47% fewer steps, while outperforming distillation baselines. Furthermore, user studies indicate our approach reduces overall user interaction time by nearly 50% while maintaining generation quality. Code is available at https://github.com/G-U-N/consolver.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13592.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63e9e92f20c109718713f5eb",
            "avatarUrl": "/avatars/9ff312e854d803e1a2e9e685a21d12f8.svg",
            "fullname": "Fu-Yun Wang",
            "name": "wangfuyun",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2712
        },
        "organization": {
            "_id": "60f6cbb2852126bac698c89e",
            "name": "deepmind",
            "fullname": "Deepmind",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1638956859875-5f1158120c833276f61f1a84.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.11891",
            "authors": [
                {
                    "_id": "6940e5cf65f1e24a11780665",
                    "name": "Songqiao Hu",
                    "hidden": false
                },
                {
                    "_id": "6940e5cf65f1e24a11780666",
                    "name": "Zeyi Liu",
                    "hidden": false
                },
                {
                    "_id": "6940e5cf65f1e24a11780667",
                    "name": "Shuang Liu",
                    "hidden": false
                },
                {
                    "_id": "6940e5cf65f1e24a11780668",
                    "name": "Jun Cen",
                    "hidden": false
                },
                {
                    "_id": "6940e5cf65f1e24a11780669",
                    "name": "Zihan Meng",
                    "hidden": false
                },
                {
                    "_id": "6940e5cf65f1e24a1178066a",
                    "name": "Xiao He",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-09T16:53:44.000Z",
            "submittedOnDailyAt": "2025-12-16T02:24:16.733Z",
            "title": "VLSA: Vision-Language-Action Models with Plug-and-Play Safety Constraint Layer",
            "submittedOnDailyBy": {
                "_id": "644493bb9c1bd83bd1a09860",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VNb6fnFYraR5R4vSwMYZz.jpeg",
                "isPro": false,
                "fullname": "Jun CEN",
                "user": "jcenaa",
                "type": "user"
            },
            "summary": "Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in generalizing across diverse robotic manipulation tasks. However, deploying these models in unstructured environments remains challenging due to the critical need for simultaneous task compliance and safety assurance, particularly in preventing potential collisions during physical interactions. In this work, we introduce a Vision-Language-Safe Action (VLSA) architecture, named AEGIS, which contains a plug-and-play safety constraint (SC) layer formulated via control barrier functions. AEGIS integrates directly with existing VLA models to improve safety with theoretical guarantees, while maintaining their original instruction-following performance. To evaluate the efficacy of our architecture, we construct a comprehensive safety-critical benchmark SafeLIBERO, spanning distinct manipulation scenarios characterized by varying degrees of spatial complexity and obstacle intervention. Extensive experiments demonstrate the superiority of our method over state-of-the-art baselines. Notably, AEGIS achieves a 59.16% improvement in obstacle avoidance rate while substantially increasing the task execution success rate by 17.25%. To facilitate reproducibility and future research, we make our code, models, and the benchmark datasets publicly available at https://vlsa-aegis.github.io/.",
            "upvotes": 6,
            "discussionId": "6940e5d065f1e24a1178066b",
            "ai_summary": "AEGIS, a Vision-Language-Safe Action architecture with a plug-and-play safety constraint layer using control barrier functions, enhances safety and performance in robotic manipulation tasks.",
            "ai_keywords": [
                "VLSA",
                "AEGIS",
                "safety constraint layer",
                "control barrier functions",
                "SafeLIBERO",
                "obstacle avoidance rate",
                "task execution success rate"
            ]
        },
        "publishedAt": "2025-12-09T11:53:44.000Z",
        "title": "VLSA: Vision-Language-Action Models with Plug-and-Play Safety Constraint Layer",
        "summary": "Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in generalizing across diverse robotic manipulation tasks. However, deploying these models in unstructured environments remains challenging due to the critical need for simultaneous task compliance and safety assurance, particularly in preventing potential collisions during physical interactions. In this work, we introduce a Vision-Language-Safe Action (VLSA) architecture, named AEGIS, which contains a plug-and-play safety constraint (SC) layer formulated via control barrier functions. AEGIS integrates directly with existing VLA models to improve safety with theoretical guarantees, while maintaining their original instruction-following performance. To evaluate the efficacy of our architecture, we construct a comprehensive safety-critical benchmark SafeLIBERO, spanning distinct manipulation scenarios characterized by varying degrees of spatial complexity and obstacle intervention. Extensive experiments demonstrate the superiority of our method over state-of-the-art baselines. Notably, AEGIS achieves a 59.16% improvement in obstacle avoidance rate while substantially increasing the task execution success rate by 17.25%. To facilitate reproducibility and future research, we make our code, models, and the benchmark datasets publicly available at https://vlsa-aegis.github.io/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.11891.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "644493bb9c1bd83bd1a09860",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VNb6fnFYraR5R4vSwMYZz.jpeg",
            "fullname": "Jun CEN",
            "name": "jcenaa",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.12751",
            "authors": [
                {
                    "_id": "6940e1a065f1e24a11780651",
                    "user": {
                        "_id": "64e9c855233101ed99ca2315",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/ozwIxW0ofJMR6kj623p1T.png",
                        "isPro": false,
                        "fullname": "YANG, Zhenya",
                        "user": "ANIYA673",
                        "type": "user"
                    },
                    "name": "Zhenya Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-16T09:37:40.325Z",
                    "hidden": false
                },
                {
                    "_id": "6940e1a065f1e24a11780652",
                    "name": "Zhe Liu",
                    "hidden": false
                },
                {
                    "_id": "6940e1a065f1e24a11780653",
                    "name": "Yuxiang Lu",
                    "hidden": false
                },
                {
                    "_id": "6940e1a065f1e24a11780654",
                    "name": "Liping Hou",
                    "hidden": false
                },
                {
                    "_id": "6940e1a065f1e24a11780655",
                    "name": "Chenxuan Miao",
                    "hidden": false
                },
                {
                    "_id": "6940e1a065f1e24a11780656",
                    "name": "Siyi Peng",
                    "hidden": false
                },
                {
                    "_id": "6940e1a065f1e24a11780657",
                    "name": "Bailan Feng",
                    "hidden": false
                },
                {
                    "_id": "6940e1a065f1e24a11780658",
                    "name": "Xiang Bai",
                    "hidden": false
                },
                {
                    "_id": "6940e1a065f1e24a11780659",
                    "name": "Hengshuang Zhao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-14T16:23:51.000Z",
            "submittedOnDailyAt": "2025-12-16T10:46:18.823Z",
            "title": "GenieDrive: Towards Physics-Aware Driving World Model with 4D Occupancy Guided Video Generation",
            "submittedOnDailyBy": {
                "_id": "64e9c855233101ed99ca2315",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/ozwIxW0ofJMR6kj623p1T.png",
                "isPro": false,
                "fullname": "YANG, Zhenya",
                "user": "ANIYA673",
                "type": "user"
            },
            "summary": "Physics-aware driving world model is essential for drive planning, out-of-distribution data synthesis, and closed-loop evaluation. However, existing methods often rely on a single diffusion model to directly map driving actions to videos, which makes learning difficult and leads to physically inconsistent outputs. To overcome these challenges, we propose GenieDrive, a novel framework designed for physics-aware driving video generation. Our approach starts by generating 4D occupancy, which serves as a physics-informed foundation for subsequent video generation. 4D occupancy contains rich physical information, including high-resolution 3D structures and dynamics. To facilitate effective compression of such high-resolution occupancy, we propose a VAE that encodes occupancy into a latent tri-plane representation, reducing the latent size to only 58% of that used in previous methods. We further introduce Mutual Control Attention (MCA) to accurately model the influence of control on occupancy evolution, and we jointly train the VAE and the subsequent prediction module in an end-to-end manner to maximize forecasting accuracy. Together, these designs yield a 7.2% improvement in forecasting mIoU at an inference speed of 41 FPS, while using only 3.47 M parameters. Additionally, a Normalized Multi-View Attention is introduced in the video generation model to generate multi-view driving videos with guidance from our 4D occupancy, significantly improving video quality with a 20.7% reduction in FVD. Experiments demonstrate that GenieDrive enables highly controllable, multi-view consistent, and physics-aware driving video generation.",
            "upvotes": 5,
            "discussionId": "6940e1a065f1e24a1178065a",
            "ai_summary": "GenieDrive uses a 4D occupancy-based approach with a VAE and Mutual Control Attention for physics-aware driving video generation, improving forecasting accuracy and video quality.",
            "ai_keywords": [
                "diffusion model",
                "4D occupancy",
                "VAE",
                "latent tri-plane representation",
                "Mutual Control Attention",
                "forecasting mIoU",
                "inference speed",
                "Normalized Multi-View Attention",
                "FVD",
                "physics-aware driving video generation"
            ]
        },
        "publishedAt": "2025-12-14T11:23:51.000Z",
        "title": "GenieDrive: Towards Physics-Aware Driving World Model with 4D Occupancy Guided Video Generation",
        "summary": "Physics-aware driving world model is essential for drive planning, out-of-distribution data synthesis, and closed-loop evaluation. However, existing methods often rely on a single diffusion model to directly map driving actions to videos, which makes learning difficult and leads to physically inconsistent outputs. To overcome these challenges, we propose GenieDrive, a novel framework designed for physics-aware driving video generation. Our approach starts by generating 4D occupancy, which serves as a physics-informed foundation for subsequent video generation. 4D occupancy contains rich physical information, including high-resolution 3D structures and dynamics. To facilitate effective compression of such high-resolution occupancy, we propose a VAE that encodes occupancy into a latent tri-plane representation, reducing the latent size to only 58% of that used in previous methods. We further introduce Mutual Control Attention (MCA) to accurately model the influence of control on occupancy evolution, and we jointly train the VAE and the subsequent prediction module in an end-to-end manner to maximize forecasting accuracy. Together, these designs yield a 7.2% improvement in forecasting mIoU at an inference speed of 41 FPS, while using only 3.47 M parameters. Additionally, a Normalized Multi-View Attention is introduced in the video generation model to generate multi-view driving videos with guidance from our 4D occupancy, significantly improving video quality with a 20.7% reduction in FVD. Experiments demonstrate that GenieDrive enables highly controllable, multi-view consistent, and physics-aware driving video generation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.12751.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64e9c855233101ed99ca2315",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/ozwIxW0ofJMR6kj623p1T.png",
            "fullname": "YANG, Zhenya",
            "name": "ANIYA673",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.11883",
            "authors": [
                {
                    "_id": "6940cc9965f1e24a1177ff80",
                    "user": {
                        "_id": "65ebae78d767680a0cf5f833",
                        "avatarUrl": "/avatars/5e0cee3000c6c4166983c2892e27bc8f.svg",
                        "isPro": true,
                        "fullname": "Marshall Guo",
                        "user": "weathon",
                        "type": "user"
                    },
                    "name": "Wenqi Marshall Guo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-16T09:47:38.442Z",
                    "hidden": false
                },
                {
                    "_id": "6940cc9965f1e24a1177ff81",
                    "name": "Qingyun Qian",
                    "hidden": false
                },
                {
                    "_id": "6940cc9965f1e24a1177ff82",
                    "name": "Khalad Hasan",
                    "hidden": false
                },
                {
                    "_id": "6940cc9965f1e24a1177ff83",
                    "name": "Shan Du",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-09T00:24:29.000Z",
            "submittedOnDailyAt": "2025-12-16T00:36:24.272Z",
            "title": "Aesthetic Alignment Risks Assimilation: How Image Generation and Reward Models Reinforce Beauty Bias and Ideological \"Censorship\"",
            "submittedOnDailyBy": {
                "_id": "65ebae78d767680a0cf5f833",
                "avatarUrl": "/avatars/5e0cee3000c6c4166983c2892e27bc8f.svg",
                "isPro": true,
                "fullname": "Marshall Guo",
                "user": "weathon",
                "type": "user"
            },
            "summary": "Over-aligning image generation models to a generalized aesthetic preference conflicts with user intent, particularly when ``anti-aesthetic\" outputs are requested for artistic or critical purposes. This adherence prioritizes developer-centered values, compromising user autonomy and aesthetic pluralism. We test this bias by constructing a wide-spectrum aesthetics dataset and evaluating state-of-the-art generation and reward models. We find that aesthetic-aligned generation models frequently default to conventionally beautiful outputs, failing to respect instructions for low-quality or negative imagery. Crucially, reward models penalize anti-aesthetic images even when they perfectly match the explicit user prompt. We confirm this systemic bias through image-to-image editing and evaluation against real abstract artworks.",
            "upvotes": 5,
            "discussionId": "6940cc9965f1e24a1177ff84",
            "ai_summary": "State-of-the-art image generation and reward models exhibit bias towards conventional aesthetics, often failing to produce anti-aesthetic images as requested, thus compromising user autonomy and aesthetic diversity.",
            "ai_keywords": [
                "aesthetic preference",
                "anti-aesthetic outputs",
                "aesthetic-aligned generation models",
                "reward models",
                "image-to-image editing",
                "abstract artworks"
            ]
        },
        "publishedAt": "2025-12-08T19:24:29.000Z",
        "title": "Aesthetic Alignment Risks Assimilation: How Image Generation and Reward Models Reinforce Beauty Bias and Ideological \"Censorship\"",
        "summary": "Over-aligning image generation models to a generalized aesthetic preference conflicts with user intent, particularly when ``anti-aesthetic\" outputs are requested for artistic or critical purposes. This adherence prioritizes developer-centered values, compromising user autonomy and aesthetic pluralism. We test this bias by constructing a wide-spectrum aesthetics dataset and evaluating state-of-the-art generation and reward models. We find that aesthetic-aligned generation models frequently default to conventionally beautiful outputs, failing to respect instructions for low-quality or negative imagery. Crucially, reward models penalize anti-aesthetic images even when they perfectly match the explicit user prompt. We confirm this systemic bias through image-to-image editing and evaluation against real abstract artworks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.11883.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65ebae78d767680a0cf5f833",
            "avatarUrl": "/avatars/5e0cee3000c6c4166983c2892e27bc8f.svg",
            "fullname": "Marshall Guo",
            "name": "weathon",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.13674",
            "authors": [
                {
                    "_id": "6940db3765f1e24a117805e2",
                    "name": "Yiyi Cai",
                    "hidden": false
                },
                {
                    "_id": "6940db3765f1e24a117805e3",
                    "name": "Xuangeng Chu",
                    "hidden": false
                },
                {
                    "_id": "6940db3765f1e24a117805e4",
                    "name": "Xiwei Gao",
                    "hidden": false
                },
                {
                    "_id": "6940db3765f1e24a117805e5",
                    "name": "Sitong Gong",
                    "hidden": false
                },
                {
                    "_id": "6940db3765f1e24a117805e6",
                    "name": "Yifei Huang",
                    "hidden": false
                },
                {
                    "_id": "6940db3765f1e24a117805e7",
                    "name": "Caixin Kang",
                    "hidden": false
                },
                {
                    "_id": "6940db3765f1e24a117805e8",
                    "name": "Kunhang Li",
                    "hidden": false
                },
                {
                    "_id": "6940db3765f1e24a117805e9",
                    "name": "Haiyang Liu",
                    "hidden": false
                },
                {
                    "_id": "6940db3765f1e24a117805ea",
                    "name": "Ruicong Liu",
                    "hidden": false
                },
                {
                    "_id": "6940db3765f1e24a117805eb",
                    "name": "Yun Liu",
                    "hidden": false
                },
                {
                    "_id": "6940db3765f1e24a117805ec",
                    "name": "Dianwen Ng",
                    "hidden": false
                },
                {
                    "_id": "6940db3765f1e24a117805ed",
                    "name": "Zixiong Su",
                    "hidden": false
                },
                {
                    "_id": "6940db3765f1e24a117805ee",
                    "name": "Erwin Wu",
                    "hidden": false
                },
                {
                    "_id": "6940db3765f1e24a117805ef",
                    "name": "Yuhan Wu",
                    "hidden": false
                },
                {
                    "_id": "6940db3765f1e24a117805f0",
                    "name": "Dingkun Yan",
                    "hidden": false
                },
                {
                    "_id": "6940db3765f1e24a117805f1",
                    "name": "Tianyu Yan",
                    "hidden": false
                },
                {
                    "_id": "6940db3765f1e24a117805f2",
                    "name": "Chang Zeng",
                    "hidden": false
                },
                {
                    "_id": "6940db3765f1e24a117805f3",
                    "name": "Bo Zheng",
                    "hidden": false
                },
                {
                    "_id": "6940db3765f1e24a117805f4",
                    "name": "You Zhou",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/n9IVL_c9ziOiJJVkoFIDo.mp4"
            ],
            "publishedAt": "2025-12-15T18:57:35.000Z",
            "submittedOnDailyAt": "2025-12-16T01:38:30.812Z",
            "title": "Towards Interactive Intelligence for Digital Humans",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We introduce Interactive Intelligence, a novel paradigm of digital human that is capable of personality-aligned expression, adaptive interaction, and self-evolution. To realize this, we present Mio (Multimodal Interactive Omni-Avatar), an end-to-end framework composed of five specialized modules: Thinker, Talker, Face Animator, Body Animator, and Renderer. This unified architecture integrates cognitive reasoning with real-time multimodal embodiment to enable fluid, consistent interaction. Furthermore, we establish a new benchmark to rigorously evaluate the capabilities of interactive intelligence. Extensive experiments demonstrate that our framework achieves superior performance compared to state-of-the-art methods across all evaluated dimensions. Together, these contributions move digital humans beyond superficial imitation toward intelligent interaction.",
            "upvotes": 3,
            "discussionId": "6940db3865f1e24a117805f5",
            "projectPage": "https://shandaai.github.io/project_mio_page/",
            "ai_summary": "Interactive Intelligence, realized through Mio framework, enables advanced digital humans with personality, adaptive interactions, and self-evolution, surpassing current benchmarks.",
            "ai_keywords": [
                "Interactive Intelligence",
                "Mio",
                "Thinker",
                "Talker",
                "Face Animator",
                "Body Animator",
                "Renderer",
                "cognitive reasoning",
                "real-time multimodal embodiment",
                "fluid interaction",
                "benchmark",
                "intelligent interaction"
            ]
        },
        "publishedAt": "2025-12-15T13:57:35.000Z",
        "title": "Towards Interactive Intelligence for Digital Humans",
        "summary": "We introduce Interactive Intelligence, a novel paradigm of digital human that is capable of personality-aligned expression, adaptive interaction, and self-evolution. To realize this, we present Mio (Multimodal Interactive Omni-Avatar), an end-to-end framework composed of five specialized modules: Thinker, Talker, Face Animator, Body Animator, and Renderer. This unified architecture integrates cognitive reasoning with real-time multimodal embodiment to enable fluid, consistent interaction. Furthermore, we establish a new benchmark to rigorously evaluate the capabilities of interactive intelligence. Extensive experiments demonstrate that our framework achieves superior performance compared to state-of-the-art methods across all evaluated dimensions. Together, these contributions move digital humans beyond superficial imitation toward intelligent interaction.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/n9IVL_c9ziOiJJVkoFIDo.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13674.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 185
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.13421",
            "authors": [
                {
                    "_id": "6940fd0b65f1e24a117806c0",
                    "user": {
                        "_id": "656724074f6ec72017754d33",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/zLIgt4xvfbYPLZh0E3WWF.png",
                        "isPro": false,
                        "fullname": "QingyuShi",
                        "user": "QingyuShi",
                        "type": "user"
                    },
                    "name": "Qingyu Shi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-16T09:37:12.941Z",
                    "hidden": false
                },
                {
                    "_id": "6940fd0b65f1e24a117806c1",
                    "name": "Size Wu",
                    "hidden": false
                },
                {
                    "_id": "6940fd0b65f1e24a117806c2",
                    "user": {
                        "_id": "63fccdac93b993a4ebd7789a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fccdac93b993a4ebd7789a/KRx8vpdoDjsZBRw0j8Vg8.jpeg",
                        "isPro": false,
                        "fullname": "Jinbin Bai",
                        "user": "BryanW",
                        "type": "user"
                    },
                    "name": "Jinbin Bai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-16T09:37:10.480Z",
                    "hidden": false
                },
                {
                    "_id": "6940fd0b65f1e24a117806c3",
                    "name": "Kaidong Yu",
                    "hidden": false
                },
                {
                    "_id": "6940fd0b65f1e24a117806c4",
                    "name": "Yujing Wang",
                    "hidden": false
                },
                {
                    "_id": "6940fd0b65f1e24a117806c5",
                    "name": "Yunhai Tong",
                    "hidden": false
                },
                {
                    "_id": "6940fd0b65f1e24a117806c6",
                    "name": "Xiangtai Li",
                    "hidden": false
                },
                {
                    "_id": "6940fd0b65f1e24a117806c7",
                    "name": "Xuelong Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-15T15:14:20.000Z",
            "submittedOnDailyAt": "2025-12-16T04:05:38.211Z",
            "title": "RecTok: Reconstruction Distillation along Rectified Flow",
            "submittedOnDailyBy": {
                "_id": "656724074f6ec72017754d33",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/zLIgt4xvfbYPLZh0E3WWF.png",
                "isPro": false,
                "fullname": "QingyuShi",
                "user": "QingyuShi",
                "type": "user"
            },
            "summary": "Visual tokenizers play a crucial role in diffusion models. The dimensionality of latent space governs both reconstruction fidelity and the semantic expressiveness of the latent feature. However, a fundamental trade-off is inherent between dimensionality and generation quality, constraining existing methods to low-dimensional latent spaces. Although recent works have leveraged vision foundation models to enrich the semantics of visual tokenizers and accelerate convergence, high-dimensional tokenizers still underperform their low-dimensional counterparts. In this work, we propose RecTok, which overcomes the limitations of high-dimensional visual tokenizers through two key innovations: flow semantic distillation and reconstruction--alignment distillation. Our key insight is to make the forward flow in flow matching semantically rich, which serves as the training space of diffusion transformers, rather than focusing on the latent space as in previous works. Specifically, our method distills the semantic information in VFMs into the forward flow trajectories in flow matching. And we further enhance the semantics by introducing a masked feature reconstruction loss. Our RecTok achieves superior image reconstruction, generation quality, and discriminative performance. It achieves state-of-the-art results on the gFID-50K under both with and without classifier-free guidance settings, while maintaining a semantically rich latent space structure. Furthermore, as the latent dimensionality increases, we observe consistent improvements. Code and model are available at https://shi-qingyu.github.io/rectok.github.io.",
            "upvotes": 3,
            "discussionId": "6940fd0b65f1e24a117806c8",
            "projectPage": "https://shi-qingyu.github.io/rectok.github.io/",
            "githubRepo": "https://github.com/Shi-qingyu/RecTok",
            "githubRepoAddedBy": "user",
            "ai_summary": "RecTok improves diffusion models by enriching forward flow semantics and enhancing reconstruction, achieving state-of-the-art results with high-dimensional visual tokenizers.",
            "ai_keywords": [
                "visual tokenizers",
                "diffusion models",
                "latent space",
                "dimensionality",
                "generation quality",
                "vision foundation models",
                "RecTok",
                "flow semantic distillation",
                "reconstruction--alignment distillation",
                "flow matching",
                "diffusion transformers",
                "masked feature reconstruction loss",
                "gFID-50K",
                "classifier-free guidance"
            ],
            "githubStars": 6
        },
        "publishedAt": "2025-12-15T10:14:20.000Z",
        "title": "RecTok: Reconstruction Distillation along Rectified Flow",
        "summary": "Visual tokenizers play a crucial role in diffusion models. The dimensionality of latent space governs both reconstruction fidelity and the semantic expressiveness of the latent feature. However, a fundamental trade-off is inherent between dimensionality and generation quality, constraining existing methods to low-dimensional latent spaces. Although recent works have leveraged vision foundation models to enrich the semantics of visual tokenizers and accelerate convergence, high-dimensional tokenizers still underperform their low-dimensional counterparts. In this work, we propose RecTok, which overcomes the limitations of high-dimensional visual tokenizers through two key innovations: flow semantic distillation and reconstruction--alignment distillation. Our key insight is to make the forward flow in flow matching semantically rich, which serves as the training space of diffusion transformers, rather than focusing on the latent space as in previous works. Specifically, our method distills the semantic information in VFMs into the forward flow trajectories in flow matching. And we further enhance the semantics by introducing a masked feature reconstruction loss. Our RecTok achieves superior image reconstruction, generation quality, and discriminative performance. It achieves state-of-the-art results on the gFID-50K under both with and without classifier-free guidance settings, while maintaining a semantically rich latent space structure. Furthermore, as the latent dimensionality increases, we observe consistent improvements. Code and model are available at https://shi-qingyu.github.io/rectok.github.io.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13421.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "656724074f6ec72017754d33",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/zLIgt4xvfbYPLZh0E3WWF.png",
            "fullname": "QingyuShi",
            "name": "QingyuShi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.13006",
            "authors": [
                {
                    "_id": "6941639365f1e24a117808a1",
                    "name": "Yifan Pu",
                    "hidden": false
                },
                {
                    "_id": "6941639365f1e24a117808a2",
                    "name": "Yizeng Han",
                    "hidden": false
                },
                {
                    "_id": "6941639365f1e24a117808a3",
                    "name": "Zhiwei Tang",
                    "hidden": false
                },
                {
                    "_id": "6941639365f1e24a117808a4",
                    "name": "Jiasheng Tang",
                    "hidden": false
                },
                {
                    "_id": "6941639365f1e24a117808a5",
                    "name": "Fan Wang",
                    "hidden": false
                },
                {
                    "_id": "6941639365f1e24a117808a6",
                    "name": "Bohan Zhuang",
                    "hidden": false
                },
                {
                    "_id": "6941639365f1e24a117808a7",
                    "name": "Gao Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-15T05:58:36.000Z",
            "submittedOnDailyAt": "2025-12-16T11:23:27.742Z",
            "title": "Few-Step Distillation for Text-to-Image Generation: A Practical Guide",
            "submittedOnDailyBy": {
                "_id": "692815374e8f98aed81747d4",
                "avatarUrl": "/avatars/9a973e4b74369eba59b369af7a449566.svg",
                "isPro": false,
                "fullname": "Tang",
                "user": "Jiasheng1110",
                "type": "user"
            },
            "summary": "Diffusion distillation has dramatically accelerated class-conditional image synthesis, but its applicability to open-ended text-to-image (T2I) generation is still unclear. We present the first systematic study that adapts and compares state-of-the-art distillation techniques on a strong T2I teacher model, FLUX.1-lite. By casting existing methods into a unified framework, we identify the key obstacles that arise when moving from discrete class labels to free-form language prompts. Beyond a thorough methodological analysis, we offer practical guidelines on input scaling, network architecture, and hyperparameters, accompanied by an open-source implementation and pretrained student models. Our findings establish a solid foundation for deploying fast, high-fidelity, and resource-efficient diffusion generators in real-world T2I applications. Code is available on github.com/alibaba-damo-academy/T2I-Distill.",
            "upvotes": 3,
            "discussionId": "6941639365f1e24a117808a8",
            "githubRepo": "https://github.com/alibaba-damo-academy/T2I-Distill.git",
            "githubRepoAddedBy": "user",
            "ai_summary": "A systematic study adapts diffusion distillation techniques to text-to-image generation, providing guidelines for successful implementation and deployment.",
            "ai_keywords": [
                "diffusion distillation",
                "class-conditional image synthesis",
                "text-to-image generation",
                "FLUX.1-lite",
                "unified framework",
                "discrete class labels",
                "free-form language prompts",
                "input scaling",
                "network architecture",
                "hyperparameters",
                "diffusion generators"
            ],
            "githubStars": 91,
            "organization": {
                "_id": "6808e7522a4d69d5111da55f",
                "name": "Alibaba-DAMO-Academy",
                "fullname": "DAMO Academy",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6808e64de5dd22427c006e10/9J3vdB62CdeTOd_YrGh9w.jpeg"
            }
        },
        "publishedAt": "2025-12-15T00:58:36.000Z",
        "title": "Few-Step Distillation for Text-to-Image Generation: A Practical Guide",
        "summary": "Diffusion distillation has dramatically accelerated class-conditional image synthesis, but its applicability to open-ended text-to-image (T2I) generation is still unclear. We present the first systematic study that adapts and compares state-of-the-art distillation techniques on a strong T2I teacher model, FLUX.1-lite. By casting existing methods into a unified framework, we identify the key obstacles that arise when moving from discrete class labels to free-form language prompts. Beyond a thorough methodological analysis, we offer practical guidelines on input scaling, network architecture, and hyperparameters, accompanied by an open-source implementation and pretrained student models. Our findings establish a solid foundation for deploying fast, high-fidelity, and resource-efficient diffusion generators in real-world T2I applications. Code is available on github.com/alibaba-damo-academy/T2I-Distill.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13006.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "692815374e8f98aed81747d4",
            "avatarUrl": "/avatars/9a973e4b74369eba59b369af7a449566.svg",
            "fullname": "Tang",
            "name": "Jiasheng1110",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "6808e7522a4d69d5111da55f",
            "name": "Alibaba-DAMO-Academy",
            "fullname": "DAMO Academy",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6808e64de5dd22427c006e10/9J3vdB62CdeTOd_YrGh9w.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.11438",
            "authors": [
                {
                    "_id": "694070f4b21c43ffbf4b7658",
                    "name": "Tariq Berrada Ifriqi",
                    "hidden": false
                },
                {
                    "_id": "694070f4b21c43ffbf4b7659",
                    "user": {
                        "_id": "648a14e002c8497f58ebff62",
                        "avatarUrl": "/avatars/0d30f7bd843ac94f317d8cfc53256450.svg",
                        "isPro": false,
                        "fullname": "John Nguyen",
                        "user": "ngjhn",
                        "type": "user"
                    },
                    "name": "John Nguyen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-16T15:09:20.244Z",
                    "hidden": false
                },
                {
                    "_id": "694070f4b21c43ffbf4b765a",
                    "name": "Karteek Alahari",
                    "hidden": false
                },
                {
                    "_id": "694070f4b21c43ffbf4b765b",
                    "name": "Jakob Verbeek",
                    "hidden": false
                },
                {
                    "_id": "694070f4b21c43ffbf4b765c",
                    "name": "Ricky T. Q. Chen",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/65858134e07563d421117279/fXhMnNxrYkqcsHay_pYLn.gif"
            ],
            "publishedAt": "2025-12-12T10:23:47.000Z",
            "submittedOnDailyAt": "2025-12-16T11:47:05.849Z",
            "title": "Flowception: Temporally Expansive Flow Matching for Video Generation",
            "submittedOnDailyBy": {
                "_id": "65858134e07563d421117279",
                "avatarUrl": "/avatars/08fb3f98cd96aa1828f5b9929b30cbb7.svg",
                "isPro": false,
                "fullname": "Tariq Berrada",
                "user": "TariqBerrada",
                "type": "user"
            },
            "summary": "We present Flowception, a novel non-autoregressive and variable-length video generation framework. Flowception learns a probability path that interleaves discrete frame insertions with continuous frame denoising. Compared to autoregressive methods, Flowception alleviates error accumulation/drift as the frame insertion mechanism during sampling serves as an efficient compression mechanism to handle long-term context. Compared to full-sequence flows, our method reduces FLOPs for training three-fold, while also being more amenable to local attention variants, and allowing to learn the length of videos jointly with their content. Quantitative experimental results show improved FVD and VBench metrics over autoregressive and full-sequence baselines, which is further validated with qualitative results. Finally, by learning to insert and denoise frames in a sequence, Flowception seamlessly integrates different tasks such as image-to-video generation and video interpolation.",
            "upvotes": 3,
            "discussionId": "694070f4b21c43ffbf4b765d",
            "projectPage": "https://flowception-meta.github.io/",
            "ai_summary": "Flowception, a non-autoregressive video generation framework, interleaves discrete frame insertions with continuous denoising, improving efficiency and performance over existing methods.",
            "ai_keywords": [
                "non-autoregressive",
                "variable-length video generation",
                "probability path",
                "discrete frame insertions",
                "continuous frame denoising",
                "error accumulation/drift",
                "full-sequence flows",
                "FLOPs",
                "local attention",
                "FVD",
                "VBench",
                "image-to-video generation",
                "video interpolation"
            ],
            "organization": {
                "_id": "5e63d8713071d5be688861b8",
                "name": "facebook",
                "fullname": "AI at Meta",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
            }
        },
        "publishedAt": "2025-12-12T05:23:47.000Z",
        "title": "Flowception: Temporally Expansive Flow Matching for Video Generation",
        "summary": "We present Flowception, a novel non-autoregressive and variable-length video generation framework. Flowception learns a probability path that interleaves discrete frame insertions with continuous frame denoising. Compared to autoregressive methods, Flowception alleviates error accumulation/drift as the frame insertion mechanism during sampling serves as an efficient compression mechanism to handle long-term context. Compared to full-sequence flows, our method reduces FLOPs for training three-fold, while also being more amenable to local attention variants, and allowing to learn the length of videos jointly with their content. Quantitative experimental results show improved FVD and VBench metrics over autoregressive and full-sequence baselines, which is further validated with qualitative results. Finally, by learning to insert and denoise frames in a sequence, Flowception seamlessly integrates different tasks such as image-to-video generation and video interpolation.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/65858134e07563d421117279/fXhMnNxrYkqcsHay_pYLn.gif"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.11438.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65858134e07563d421117279",
            "avatarUrl": "/avatars/08fb3f98cd96aa1828f5b9929b30cbb7.svg",
            "fullname": "Tariq Berrada",
            "name": "TariqBerrada",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "5e63d8713071d5be688861b8",
            "name": "facebook",
            "fullname": "AI at Meta",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.10794",
            "authors": [
                {
                    "_id": "693c141c9874a2a5e4ffb49c",
                    "name": "Jaskirat Singh",
                    "hidden": false
                },
                {
                    "_id": "693c141c9874a2a5e4ffb49d",
                    "name": "Xingjian Leng",
                    "hidden": false
                },
                {
                    "_id": "693c141c9874a2a5e4ffb49e",
                    "name": "Zongze Wu",
                    "hidden": false
                },
                {
                    "_id": "693c141c9874a2a5e4ffb49f",
                    "name": "Liang Zheng",
                    "hidden": false
                },
                {
                    "_id": "693c141c9874a2a5e4ffb4a0",
                    "name": "Richard Zhang",
                    "hidden": false
                },
                {
                    "_id": "693c141c9874a2a5e4ffb4a1",
                    "name": "Eli Shechtman",
                    "hidden": false
                },
                {
                    "_id": "693c141c9874a2a5e4ffb4a2",
                    "name": "Saining Xie",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-11T16:39:53.000Z",
            "submittedOnDailyAt": "2025-12-16T16:38:02.483Z",
            "title": "What matters for Representation Alignment: Global Information or Spatial Structure?",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": true,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "Representation alignment (REPA) guides generative training by distilling representations from a strong, pretrained vision encoder to intermediate diffusion features. We investigate a fundamental question: what aspect of the target representation matters for generation, its global semantic information (e.g., measured by ImageNet-1K accuracy) or its spatial structure (i.e. pairwise cosine similarity between patch tokens)? Prevalent wisdom holds that stronger global semantic performance leads to better generation as a target representation. To study this, we first perform a large-scale empirical analysis across 27 different vision encoders and different model scales. The results are surprising; spatial structure, rather than global performance, drives the generation performance of a target representation. To further study this, we introduce two straightforward modifications, which specifically accentuate the transfer of spatial information. We replace the standard MLP projection layer in REPA with a simple convolution layer and introduce a spatial normalization layer for the external representation. Surprisingly, our simple method (implemented in <4 lines of code), termed iREPA, consistently improves convergence speed of REPA, across a diverse set of vision encoders, model sizes, and training variants (such as REPA, REPA-E, Meanflow, JiT etc). %, etc. Our work motivates revisiting the fundamental working mechanism of representational alignment and how it can be leveraged for improved training of generative models. The code and project page are available at https://end2end-diffusion.github.io/irepa",
            "upvotes": 3,
            "discussionId": "693c141d9874a2a5e4ffb4a3",
            "projectPage": "https://end2end-diffusion.github.io/irepa",
            "githubRepo": "https://github.com/end2end-diffusion/irepa",
            "githubRepoAddedBy": "auto",
            "ai_summary": "Representation alignment enhances generative training by transferring spatial structure from pretrained vision encoders to diffusion models, surpassing the importance of global semantic performance.",
            "ai_keywords": [
                "representation alignment",
                "generative training",
                "diffusion features",
                "global semantic information",
                "spatial structure",
                "patch tokens",
                "MLP projection layer",
                "convolution layer",
                "spatial normalization layer",
                "iREPA",
                "convergence speed",
                "REPA",
                "REPA-E",
                "Meanflow",
                "JiT"
            ],
            "githubStars": 80
        },
        "publishedAt": "2025-12-11T11:39:53.000Z",
        "title": "What matters for Representation Alignment: Global Information or Spatial Structure?",
        "summary": "Representation alignment (REPA) guides generative training by distilling representations from a strong, pretrained vision encoder to intermediate diffusion features. We investigate a fundamental question: what aspect of the target representation matters for generation, its global semantic information (e.g., measured by ImageNet-1K accuracy) or its spatial structure (i.e. pairwise cosine similarity between patch tokens)? Prevalent wisdom holds that stronger global semantic performance leads to better generation as a target representation. To study this, we first perform a large-scale empirical analysis across 27 different vision encoders and different model scales. The results are surprising; spatial structure, rather than global performance, drives the generation performance of a target representation. To further study this, we introduce two straightforward modifications, which specifically accentuate the transfer of spatial information. We replace the standard MLP projection layer in REPA with a simple convolution layer and introduce a spatial normalization layer for the external representation. Surprisingly, our simple method (implemented in <4 lines of code), termed iREPA, consistently improves convergence speed of REPA, across a diverse set of vision encoders, model sizes, and training variants (such as REPA, REPA-E, Meanflow, JiT etc). %, etc. Our work motivates revisiting the fundamental working mechanism of representational alignment and how it can be leveraged for improved training of generative models. The code and project page are available at https://end2end-diffusion.github.io/irepa",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.10794.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": true,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8933
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.10655",
            "authors": [
                {
                    "_id": "694166c965f1e24a117808aa",
                    "name": "Tong Zhang",
                    "hidden": false
                },
                {
                    "_id": "694166c965f1e24a117808ab",
                    "name": "Carlos Hinojosa",
                    "hidden": false
                },
                {
                    "_id": "694166c965f1e24a117808ac",
                    "name": "Bernard Ghanem",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-11T14:01:47.000Z",
            "submittedOnDailyAt": "2025-12-16T11:35:18.742Z",
            "title": "CAPTAIN: Semantic Feature Injection for Memorization Mitigation in Text-to-Image Diffusion Models",
            "submittedOnDailyBy": {
                "_id": "65c898197faf326d067e2c0d",
                "avatarUrl": "/avatars/dde3c047e627bacd69e0f063a4a833b2.svg",
                "isPro": false,
                "fullname": "Tong Zhang",
                "user": "Tong98Zhang",
                "type": "user"
            },
            "summary": "Diffusion models can unintentionally reproduce training examples, raising privacy and copyright concerns as these systems are increasingly deployed at scale. Existing inference-time mitigation methods typically manipulate classifier-free guidance (CFG) or perturb prompt embeddings; however, they often struggle to reduce memorization without compromising alignment with the conditioning prompt. We introduce CAPTAIN, a training-free framework that mitigates memorization by directly modifying latent features during denoising. CAPTAIN first applies frequency-based noise initialization to reduce the tendency to replicate memorized patterns early in the denoising process. It then identifies the optimal denoising timesteps for feature injection and localizes memorized regions. Finally, CAPTAIN injects semantically aligned features from non-memorized reference images into localized latent regions, suppressing memorization while preserving prompt fidelity and visual quality. Our experiments show that CAPTAIN achieves substantial reductions in memorization compared to CFG-based baselines while maintaining strong alignment with the intended prompt.",
            "upvotes": 3,
            "discussionId": "694166c965f1e24a117808ad",
            "ai_summary": "CAPTAIN, a training-free framework, mitigates memorization in diffusion models by modifying latent features during denoising, ensuring prompt fidelity and visual quality.",
            "ai_keywords": [
                "diffusion models",
                "classifier-free guidance",
                "prompt embeddings",
                "latent features",
                "denoising",
                "frequency-based noise initialization",
                "denoising timesteps",
                "feature injection",
                "memorized patterns",
                "semantically aligned features",
                "reference images"
            ],
            "organization": {
                "_id": "6808bff3dd4333f8fe87db70",
                "name": "IVUL-KAUST",
                "fullname": "Image and Video Understanding Lab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6808bf97ffadd78ec71cb721/Xwkc8YwWKDbZj0fHSGXoa.png"
            }
        },
        "publishedAt": "2025-12-11T09:01:47.000Z",
        "title": "CAPTAIN: Semantic Feature Injection for Memorization Mitigation in Text-to-Image Diffusion Models",
        "summary": "Diffusion models can unintentionally reproduce training examples, raising privacy and copyright concerns as these systems are increasingly deployed at scale. Existing inference-time mitigation methods typically manipulate classifier-free guidance (CFG) or perturb prompt embeddings; however, they often struggle to reduce memorization without compromising alignment with the conditioning prompt. We introduce CAPTAIN, a training-free framework that mitigates memorization by directly modifying latent features during denoising. CAPTAIN first applies frequency-based noise initialization to reduce the tendency to replicate memorized patterns early in the denoising process. It then identifies the optimal denoising timesteps for feature injection and localizes memorized regions. Finally, CAPTAIN injects semantically aligned features from non-memorized reference images into localized latent regions, suppressing memorization while preserving prompt fidelity and visual quality. Our experiments show that CAPTAIN achieves substantial reductions in memorization compared to CFG-based baselines while maintaining strong alignment with the intended prompt.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.10655.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65c898197faf326d067e2c0d",
            "avatarUrl": "/avatars/dde3c047e627bacd69e0f063a4a833b2.svg",
            "fullname": "Tong Zhang",
            "name": "Tong98Zhang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "6808bff3dd4333f8fe87db70",
            "name": "IVUL-KAUST",
            "fullname": "Image and Video Understanding Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6808bf97ffadd78ec71cb721/Xwkc8YwWKDbZj0fHSGXoa.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.13690",
            "authors": [
                {
                    "_id": "6940fa2065f1e24a117806a6",
                    "name": "Susung Hong",
                    "hidden": false
                },
                {
                    "_id": "6940fa2065f1e24a117806a7",
                    "name": "Chongjian Ge",
                    "hidden": false
                },
                {
                    "_id": "6940fa2065f1e24a117806a8",
                    "name": "Zhifei Zhang",
                    "hidden": false
                },
                {
                    "_id": "6940fa2065f1e24a117806a9",
                    "name": "Jui-Hsien Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-15T18:59:57.000Z",
            "submittedOnDailyAt": "2025-12-16T03:53:33.128Z",
            "title": "DiffusionBrowser: Interactive Diffusion Previews via Multi-Branch Decoders",
            "submittedOnDailyBy": {
                "_id": "635a6dd21668c4ead3ed19fa",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1679159537671-635a6dd21668c4ead3ed19fa.jpeg",
                "isPro": false,
                "fullname": "Susung Hong",
                "user": "susunghong",
                "type": "user"
            },
            "summary": "Video diffusion models have revolutionized generative video synthesis, but they are imprecise, slow, and can be opaque during generation -- keeping users in the dark for a prolonged period. In this work, we propose DiffusionBrowser, a model-agnostic, lightweight decoder framework that allows users to interactively generate previews at any point (timestep or transformer block) during the denoising process. Our model can generate multi-modal preview representations that include RGB and scene intrinsics at more than 4times real-time speed (less than 1 second for a 4-second video) that convey consistent appearance and motion to the final video. With the trained decoder, we show that it is possible to interactively guide the generation at intermediate noise steps via stochasticity reinjection and modal steering, unlocking a new control capability. Moreover, we systematically probe the model using the learned decoders, revealing how scene, object, and other details are composed and assembled during the otherwise black-box denoising process.",
            "upvotes": 2,
            "discussionId": "6940fa2065f1e24a117806aa",
            "ai_summary": "DiffusionBrowser enables interactive video preview generation and control during the denoising process, enhancing user experience and revealing model composition details.",
            "ai_keywords": [
                "video diffusion models",
                "generative video synthesis",
                "denoising process",
                "DiffusionBrowser",
                "model-agnostic",
                "lightweight decoder",
                "interactive generation",
                "previews",
                "timestep",
                "transformer block",
                "multi-modal preview representations",
                "RGB",
                "scene intrinsics",
                "real-time speed",
                "stochasticity reinjection",
                "modal steering",
                "control capability",
                "learned decoders",
                "black-box denoising process"
            ]
        },
        "publishedAt": "2025-12-15T13:59:57.000Z",
        "title": "DiffusionBrowser: Interactive Diffusion Previews via Multi-Branch Decoders",
        "summary": "Video diffusion models have revolutionized generative video synthesis, but they are imprecise, slow, and can be opaque during generation -- keeping users in the dark for a prolonged period. In this work, we propose DiffusionBrowser, a model-agnostic, lightweight decoder framework that allows users to interactively generate previews at any point (timestep or transformer block) during the denoising process. Our model can generate multi-modal preview representations that include RGB and scene intrinsics at more than 4times real-time speed (less than 1 second for a 4-second video) that convey consistent appearance and motion to the final video. With the trained decoder, we show that it is possible to interactively guide the generation at intermediate noise steps via stochasticity reinjection and modal steering, unlocking a new control capability. Moreover, we systematically probe the model using the learned decoders, revealing how scene, object, and other details are composed and assembled during the otherwise black-box denoising process.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13690.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "635a6dd21668c4ead3ed19fa",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1679159537671-635a6dd21668c4ead3ed19fa.jpeg",
            "fullname": "Susung Hong",
            "name": "susunghong",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.13689",
            "authors": [
                {
                    "_id": "6940de9365f1e24a11780640",
                    "name": "Yuanwen Yue",
                    "hidden": false
                },
                {
                    "_id": "6940de9365f1e24a11780641",
                    "name": "Damien Robert",
                    "hidden": false
                },
                {
                    "_id": "6940de9365f1e24a11780642",
                    "name": "Jianyuan Wang",
                    "hidden": false
                },
                {
                    "_id": "6940de9365f1e24a11780643",
                    "name": "Sunghwan Hong",
                    "hidden": false
                },
                {
                    "_id": "6940de9365f1e24a11780644",
                    "name": "Jan Dirk Wegner",
                    "hidden": false
                },
                {
                    "_id": "6940de9365f1e24a11780645",
                    "name": "Christian Rupprecht",
                    "hidden": false
                },
                {
                    "_id": "6940de9365f1e24a11780646",
                    "name": "Konrad Schindler",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-15T18:59:57.000Z",
            "submittedOnDailyAt": "2025-12-16T01:55:26.757Z",
            "title": "LitePT: Lighter Yet Stronger Point Transformer",
            "submittedOnDailyBy": {
                "_id": "65b0b33a403a23a2fda96cbc",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b0b33a403a23a2fda96cbc/f-45UN20BixueeJPAuJUj.png",
                "isPro": true,
                "fullname": "Yuanwen Yue",
                "user": "yuanwenyue",
                "type": "user"
            },
            "summary": "Modern neural architectures for 3D point cloud processing contain both convolutional layers and attention blocks, but the best way to assemble them remains unclear. We analyse the role of different computational blocks in 3D point cloud networks and find an intuitive behaviour: convolution is adequate to extract low-level geometry at high-resolution in early layers, where attention is expensive without bringing any benefits; attention captures high-level semantics and context in low-resolution, deep layers more efficiently. Guided by this design principle, we propose a new, improved 3D point cloud backbone that employs convolutions in early stages and switches to attention for deeper layers. To avoid the loss of spatial layout information when discarding redundant convolution layers, we introduce a novel, training-free 3D positional encoding, PointROPE. The resulting LitePT model has 3.6times fewer parameters, runs 2times faster, and uses 2times less memory than the state-of-the-art Point Transformer V3, but nonetheless matches or even outperforms it on a range of tasks and datasets. Code and models are available at: https://github.com/prs-eth/LitePT.",
            "upvotes": 2,
            "discussionId": "6940de9365f1e24a11780647",
            "projectPage": "https://litept.github.io/",
            "githubRepo": "https://github.com/prs-eth/LitePT",
            "githubRepoAddedBy": "user",
            "ai_summary": "A new 3D point cloud backbone model, LitePT, uses convolutions for early stages and attention for deeper layers, incorporating PointROPE for positional encoding, achieving efficient performance with fewer resources.",
            "ai_keywords": [
                "convolutional layers",
                "attention blocks",
                "3D point cloud networks",
                "low-level geometry",
                "high-resolution",
                "high-level semantics",
                "context",
                "spatial layout information",
                "LitePT",
                "PointROPE",
                "Point Transformer V3"
            ],
            "githubStars": 31,
            "organization": {
                "_id": "63263d7db8e57aab1a778773",
                "name": "ethz",
                "fullname": "ETH Zurich",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/xMcrQI8Yx8o697uhiCcoA.jpeg"
            }
        },
        "publishedAt": "2025-12-15T13:59:57.000Z",
        "title": "LitePT: Lighter Yet Stronger Point Transformer",
        "summary": "Modern neural architectures for 3D point cloud processing contain both convolutional layers and attention blocks, but the best way to assemble them remains unclear. We analyse the role of different computational blocks in 3D point cloud networks and find an intuitive behaviour: convolution is adequate to extract low-level geometry at high-resolution in early layers, where attention is expensive without bringing any benefits; attention captures high-level semantics and context in low-resolution, deep layers more efficiently. Guided by this design principle, we propose a new, improved 3D point cloud backbone that employs convolutions in early stages and switches to attention for deeper layers. To avoid the loss of spatial layout information when discarding redundant convolution layers, we introduce a novel, training-free 3D positional encoding, PointROPE. The resulting LitePT model has 3.6times fewer parameters, runs 2times faster, and uses 2times less memory than the state-of-the-art Point Transformer V3, but nonetheless matches or even outperforms it on a range of tasks and datasets. Code and models are available at: https://github.com/prs-eth/LitePT.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13689.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65b0b33a403a23a2fda96cbc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b0b33a403a23a2fda96cbc/f-45UN20BixueeJPAuJUj.png",
            "fullname": "Yuanwen Yue",
            "name": "yuanwenyue",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "organization": {
            "_id": "63263d7db8e57aab1a778773",
            "name": "ethz",
            "fullname": "ETH Zurich",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/xMcrQI8Yx8o697uhiCcoA.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.13683",
            "authors": [
                {
                    "_id": "694137b865f1e24a1178076d",
                    "name": "Lu Ling",
                    "hidden": false
                },
                {
                    "_id": "694137b865f1e24a1178076e",
                    "name": "Yunhao Ge",
                    "hidden": false
                },
                {
                    "_id": "694137b865f1e24a1178076f",
                    "name": "Yichen Sheng",
                    "hidden": false
                },
                {
                    "_id": "694137b865f1e24a11780770",
                    "name": "Aniket Bera",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-15T18:59:13.000Z",
            "submittedOnDailyAt": "2025-12-16T22:51:19.923Z",
            "title": "I-Scene: 3D Instance Models are Implicit Generalizable Spatial Learners",
            "submittedOnDailyBy": {
                "_id": "6347935d79e42766e212fa53",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6347935d79e42766e212fa53/9BBADaR0yV1wjC91pGYmZ.png",
                "isPro": false,
                "fullname": "Lu Ling",
                "user": "LuLing",
                "type": "user"
            },
            "summary": "Generalization remains the central challenge for interactive 3D scene generation. Existing learning-based approaches ground spatial understanding in limited scene dataset, restricting generalization to new layouts. We instead reprogram a pre-trained 3D instance generator to act as a scene level learner, replacing dataset-bounded supervision with model-centric spatial supervision. This reprogramming unlocks the generator transferable spatial knowledge, enabling generalization to unseen layouts and novel object compositions. Remarkably, spatial reasoning still emerges even when the training scenes are randomly composed objects. This demonstrates that the generator's transferable scene prior provides a rich learning signal for inferring proximity, support, and symmetry from purely geometric cues. Replacing widely used canonical space, we instantiate this insight with a view-centric formulation of the scene space, yielding a fully feed-forward, generalizable scene generator that learns spatial relations directly from the instance model. Quantitative and qualitative results show that a 3D instance generator is an implicit spatial learner and reasoner, pointing toward foundation models for interactive 3D scene understanding and generation. Project page: https://luling06.github.io/I-Scene-project/",
            "upvotes": 2,
            "discussionId": "694137b965f1e24a11780771",
            "ai_summary": "A pre-trained 3D instance generator is reprogrammed to generalize spatial understanding in new layouts by learning directly from geometric cues, demonstrating its potential as a foundation model for interactive 3D scene generation.",
            "ai_keywords": [
                "3D instance generator",
                "spatial understanding",
                "dataset-bounded supervision",
                "model-centric spatial supervision",
                "transferable spatial knowledge",
                "spatial reasoning",
                "canonical space",
                "view-centric formulation",
                "scene space",
                "feed-forward",
                "generalizable scene generator",
                "spatial relations",
                "implicit spatial learner",
                "interactive 3D scene understanding"
            ]
        },
        "publishedAt": "2025-12-15T13:59:13.000Z",
        "title": "I-Scene: 3D Instance Models are Implicit Generalizable Spatial Learners",
        "summary": "Generalization remains the central challenge for interactive 3D scene generation. Existing learning-based approaches ground spatial understanding in limited scene dataset, restricting generalization to new layouts. We instead reprogram a pre-trained 3D instance generator to act as a scene level learner, replacing dataset-bounded supervision with model-centric spatial supervision. This reprogramming unlocks the generator transferable spatial knowledge, enabling generalization to unseen layouts and novel object compositions. Remarkably, spatial reasoning still emerges even when the training scenes are randomly composed objects. This demonstrates that the generator's transferable scene prior provides a rich learning signal for inferring proximity, support, and symmetry from purely geometric cues. Replacing widely used canonical space, we instantiate this insight with a view-centric formulation of the scene space, yielding a fully feed-forward, generalizable scene generator that learns spatial relations directly from the instance model. Quantitative and qualitative results show that a 3D instance generator is an implicit spatial learner and reasoner, pointing toward foundation models for interactive 3D scene understanding and generation. Project page: https://luling06.github.io/I-Scene-project/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13683.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6347935d79e42766e212fa53",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6347935d79e42766e212fa53/9BBADaR0yV1wjC91pGYmZ.png",
            "fullname": "Lu Ling",
            "name": "LuLing",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.13672",
            "authors": [
                {
                    "_id": "69410b0665f1e24a117806ff",
                    "name": "Kunhee Kim",
                    "hidden": false
                },
                {
                    "_id": "69410b0665f1e24a11780700",
                    "name": "NaHyeon Park",
                    "hidden": false
                },
                {
                    "_id": "69410b0665f1e24a11780701",
                    "name": "Kibeom Hong",
                    "hidden": false
                },
                {
                    "_id": "69410b0665f1e24a11780702",
                    "name": "Hyunjung Shim",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-15T18:57:07.000Z",
            "submittedOnDailyAt": "2025-12-16T05:10:49.363Z",
            "title": "Directional Textual Inversion for Personalized Text-to-Image Generation",
            "submittedOnDailyBy": {
                "_id": "6314968cd8dc27b2f7f1f3e2",
                "avatarUrl": "/avatars/3830bcff76b0408b1849fd9a44f7aedc.svg",
                "isPro": false,
                "fullname": "Kunhee Kim",
                "user": "kunheekim",
                "type": "user"
            },
            "summary": "Textual Inversion (TI) is an efficient approach to text-to-image personalization but often fails on complex prompts. We trace these failures to embedding norm inflation: learned tokens drift to out-of-distribution magnitudes, degrading prompt conditioning in pre-norm Transformers. Empirically, we show semantics are primarily encoded by direction in CLIP token space, while inflated norms harm contextualization; theoretically, we analyze how large magnitudes attenuate positional information and hinder residual updates in pre-norm blocks. We propose Directional Textual Inversion (DTI), which fixes the embedding magnitude to an in-distribution scale and optimizes only direction on the unit hypersphere via Riemannian SGD. We cast direction learning as MAP with a von Mises-Fisher prior, yielding a constant-direction prior gradient that is simple and efficient to incorporate. Across personalization tasks, DTI improves text fidelity over TI and TI-variants while maintaining subject similarity. Crucially, DTI's hyperspherical parameterization enables smooth, semantically coherent interpolation between learned concepts (slerp), a capability that is absent in standard TI. Our findings suggest that direction-only optimization is a robust and scalable path for prompt-faithful personalization.",
            "upvotes": 2,
            "discussionId": "69410b0665f1e24a11780703",
            "projectPage": "https://kunheek.github.io/dti",
            "githubRepo": "https://github.com/kunheek/dti",
            "githubRepoAddedBy": "user",
            "ai_summary": "Directional Textual Inversion (DTI) improves text-to-image personalization by constraining learned tokens to unit magnitude, enhancing prompt conditioning and enabling smooth interpolation between concepts.",
            "ai_keywords": [
                "Textual Inversion",
                "embedding norm inflation",
                "pre-norm Transformers",
                "CLIP token space",
                "Riemannian SGD",
                "von Mises-Fisher prior",
                "positional information",
                "residual updates",
                "Directional Textual Inversion",
                "hyperspherical parameterization",
                "semantically coherent interpolation",
                "slerp"
            ],
            "githubStars": 2,
            "organization": {
                "_id": "6475760c33192631bad2bb38",
                "name": "kaist-ai",
                "fullname": "KAIST AI",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6469949654873f0043b09c22/aaZFiyXe1qR-Dmy_xq67m.png"
            }
        },
        "publishedAt": "2025-12-15T13:57:07.000Z",
        "title": "Directional Textual Inversion for Personalized Text-to-Image Generation",
        "summary": "Textual Inversion (TI) is an efficient approach to text-to-image personalization but often fails on complex prompts. We trace these failures to embedding norm inflation: learned tokens drift to out-of-distribution magnitudes, degrading prompt conditioning in pre-norm Transformers. Empirically, we show semantics are primarily encoded by direction in CLIP token space, while inflated norms harm contextualization; theoretically, we analyze how large magnitudes attenuate positional information and hinder residual updates in pre-norm blocks. We propose Directional Textual Inversion (DTI), which fixes the embedding magnitude to an in-distribution scale and optimizes only direction on the unit hypersphere via Riemannian SGD. We cast direction learning as MAP with a von Mises-Fisher prior, yielding a constant-direction prior gradient that is simple and efficient to incorporate. Across personalization tasks, DTI improves text fidelity over TI and TI-variants while maintaining subject similarity. Crucially, DTI's hyperspherical parameterization enables smooth, semantically coherent interpolation between learned concepts (slerp), a capability that is absent in standard TI. Our findings suggest that direction-only optimization is a robust and scalable path for prompt-faithful personalization.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13672.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6314968cd8dc27b2f7f1f3e2",
            "avatarUrl": "/avatars/3830bcff76b0408b1849fd9a44f7aedc.svg",
            "fullname": "Kunhee Kim",
            "name": "kunheekim",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "organization": {
            "_id": "6475760c33192631bad2bb38",
            "name": "kaist-ai",
            "fullname": "KAIST AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6469949654873f0043b09c22/aaZFiyXe1qR-Dmy_xq67m.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.12196",
            "authors": [
                {
                    "_id": "6940da6f65f1e24a117805d3",
                    "name": "Xiaoxuan Tang",
                    "hidden": false
                },
                {
                    "_id": "6940da6f65f1e24a117805d4",
                    "name": "Xinping Lei",
                    "hidden": false
                },
                {
                    "_id": "6940da6f65f1e24a117805d5",
                    "name": "Chaoran Zhu",
                    "hidden": false
                },
                {
                    "_id": "6940da6f65f1e24a117805d6",
                    "name": "Shiyun Chen",
                    "hidden": false
                },
                {
                    "_id": "6940da6f65f1e24a117805d7",
                    "name": "Ruibin Yuan",
                    "hidden": false
                },
                {
                    "_id": "6940da6f65f1e24a117805d8",
                    "name": "Yizhi Li",
                    "hidden": false
                },
                {
                    "_id": "6940da6f65f1e24a117805d9",
                    "name": "Changjae Oh",
                    "hidden": false
                },
                {
                    "_id": "6940da6f65f1e24a117805da",
                    "name": "Ge Zhang",
                    "hidden": false
                },
                {
                    "_id": "6940da6f65f1e24a117805db",
                    "name": "Wenhao Huang",
                    "hidden": false
                },
                {
                    "_id": "6940da6f65f1e24a117805dc",
                    "name": "Emmanouil Benetos",
                    "hidden": false
                },
                {
                    "_id": "6940da6f65f1e24a117805dd",
                    "name": "Yang Liu",
                    "hidden": false
                },
                {
                    "_id": "6940da6f65f1e24a117805de",
                    "name": "Jiaheng Liu",
                    "hidden": false
                },
                {
                    "_id": "6940da6f65f1e24a117805df",
                    "name": "Yinghao Ma",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6410665d5364a661bee22524/gEyyEIPiTjB8mTyQxf3I8.mp4"
            ],
            "publishedAt": "2025-12-13T05:53:50.000Z",
            "submittedOnDailyAt": "2025-12-16T20:44:33.162Z",
            "title": "AutoMV: An Automatic Multi-Agent System for Music Video Generation",
            "submittedOnDailyBy": {
                "_id": "6410665d5364a661bee22524",
                "avatarUrl": "/avatars/f1cb0e07f36933187ceccbd5dcbeff79.svg",
                "isPro": false,
                "fullname": "Yinghao Ma",
                "user": "nicolaus625",
                "type": "user"
            },
            "summary": "Music-to-Video (M2V) generation for full-length songs faces significant challenges. Existing methods produce short, disjointed clips, failing to align visuals with musical structure, beats, or lyrics, and lack temporal consistency. We propose AutoMV, a multi-agent system that generates full music videos (MVs) directly from a song. AutoMV first applies music processing tools to extract musical attributes, such as structure, vocal tracks, and time-aligned lyrics, and constructs these features as contextual inputs for following agents. The screenwriter Agent and director Agent then use this information to design short script, define character profiles in a shared external bank, and specify camera instructions. Subsequently, these agents call the image generator for keyframes and different video generators for \"story\" or \"singer\" scenes. A Verifier Agent evaluates their output, enabling multi-agent collaboration to produce a coherent longform MV. To evaluate M2V generation, we further propose a benchmark with four high-level categories (Music Content, Technical, Post-production, Art) and twelve ine-grained criteria. This benchmark was applied to compare commercial products, AutoMV, and human-directed MVs with expert human raters: AutoMV outperforms current baselines significantly across all four categories, narrowing the gap to professional MVs. Finally, we investigate using large multimodal models as automatic MV judges; while promising, they still lag behind human expert, highlighting room for future work.",
            "upvotes": 2,
            "discussionId": "6940da7065f1e24a117805e0",
            "projectPage": "https://m-a-p.ai/AutoMV/",
            "githubRepo": "https://github.com/multimodal-art-projection/AutoMV",
            "githubRepoAddedBy": "user",
            "ai_summary": "AutoMV, a multi-agent system, generates coherent full-length music videos directly from songs, outperforming existing methods and narrowing the gap to professional videos.",
            "ai_keywords": [
                "multi-agent system",
                "music processing",
                "musical attributes",
                "contextual inputs",
                "screenwriter Agent",
                "director Agent",
                "image generator",
                "video generators",
                "Verifier Agent",
                "benchmark",
                "Music Content",
                "Technical",
                "Post-production",
                "Art"
            ],
            "githubStars": 4,
            "organization": {
                "_id": "6384ee7fdfffab482400b938",
                "name": "m-a-p",
                "fullname": "Multimodal Art Projection",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6382252f54421460665ec501/oNH4MDqpSiMWJpxbaSLOv.png"
            }
        },
        "publishedAt": "2025-12-13T00:53:50.000Z",
        "title": "AutoMV: An Automatic Multi-Agent System for Music Video Generation",
        "summary": "Music-to-Video (M2V) generation for full-length songs faces significant challenges. Existing methods produce short, disjointed clips, failing to align visuals with musical structure, beats, or lyrics, and lack temporal consistency. We propose AutoMV, a multi-agent system that generates full music videos (MVs) directly from a song. AutoMV first applies music processing tools to extract musical attributes, such as structure, vocal tracks, and time-aligned lyrics, and constructs these features as contextual inputs for following agents. The screenwriter Agent and director Agent then use this information to design short script, define character profiles in a shared external bank, and specify camera instructions. Subsequently, these agents call the image generator for keyframes and different video generators for \"story\" or \"singer\" scenes. A Verifier Agent evaluates their output, enabling multi-agent collaboration to produce a coherent longform MV. To evaluate M2V generation, we further propose a benchmark with four high-level categories (Music Content, Technical, Post-production, Art) and twelve ine-grained criteria. This benchmark was applied to compare commercial products, AutoMV, and human-directed MVs with expert human raters: AutoMV outperforms current baselines significantly across all four categories, narrowing the gap to professional MVs. Finally, we investigate using large multimodal models as automatic MV judges; while promising, they still lag behind human expert, highlighting room for future work.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6410665d5364a661bee22524/gEyyEIPiTjB8mTyQxf3I8.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.12196.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6410665d5364a661bee22524",
            "avatarUrl": "/avatars/f1cb0e07f36933187ceccbd5dcbeff79.svg",
            "fullname": "Yinghao Ma",
            "name": "nicolaus625",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 13
        },
        "organization": {
            "_id": "6384ee7fdfffab482400b938",
            "name": "m-a-p",
            "fullname": "Multimodal Art Projection",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6382252f54421460665ec501/oNH4MDqpSiMWJpxbaSLOv.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.10927",
            "authors": [
                {
                    "_id": "69417bc965f1e24a117808db",
                    "name": "Yulu Gan",
                    "hidden": false
                },
                {
                    "_id": "69417bc965f1e24a117808dc",
                    "name": "Ligeng Zhu",
                    "hidden": false
                },
                {
                    "_id": "69417bc965f1e24a117808dd",
                    "name": "Dandan Shan",
                    "hidden": false
                },
                {
                    "_id": "69417bc965f1e24a117808de",
                    "name": "Baifeng Shi",
                    "hidden": false
                },
                {
                    "_id": "69417bc965f1e24a117808df",
                    "name": "Hongxu Yin",
                    "hidden": false
                },
                {
                    "_id": "69417bc965f1e24a117808e0",
                    "name": "Boris Ivanovic",
                    "hidden": false
                },
                {
                    "_id": "69417bc965f1e24a117808e1",
                    "name": "Song Han",
                    "hidden": false
                },
                {
                    "_id": "69417bc965f1e24a117808e2",
                    "name": "Trevor Darrell",
                    "hidden": false
                },
                {
                    "_id": "69417bc965f1e24a117808e3",
                    "name": "Jitendra Malik",
                    "hidden": false
                },
                {
                    "_id": "69417bc965f1e24a117808e4",
                    "name": "Marco Pavone",
                    "hidden": false
                },
                {
                    "_id": "69417bc965f1e24a117808e5",
                    "name": "Boyi Li",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63fde8116315a264aba6d48f/3lR797q5esd-bCt_UtA1B.mp4"
            ],
            "publishedAt": "2025-12-11T18:53:15.000Z",
            "submittedOnDailyAt": "2025-12-16T13:11:11.058Z",
            "title": "FoundationMotion: Auto-Labeling and Reasoning about Spatial Movement in Videos",
            "submittedOnDailyBy": {
                "_id": "63fde8116315a264aba6d48f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fde8116315a264aba6d48f/h7fg5sG_QF5Tx6bZr4P8V.png",
                "isPro": true,
                "fullname": "Yulu Gan",
                "user": "yulu2",
                "type": "user"
            },
            "summary": "Motion understanding is fundamental to physical reasoning, enabling models to infer dynamics and predict future states. However, state-of-the-art models still struggle on recent motion benchmarks, primarily due to the scarcity of large-scale, fine-grained motion datasets. Existing motion datasets are often constructed from costly manual annotation, severely limiting scalability. To address this challenge, we introduce FoundationMotion, a fully automated data curation pipeline that constructs large-scale motion datasets. Our approach first detects and tracks objects in videos to extract their trajectories, then leverages these trajectories and video frames with Large Language Models (LLMs) to generate fine-grained captions and diverse question-answer pairs about motion and spatial reasoning. Using datasets produced by this pipeline, we fine-tune open-source models including NVILA-Video-15B and Qwen2.5-7B, achieving substantial improvements in motion understanding without compromising performance on other tasks. Notably, our models outperform strong closed-source baselines like Gemini-2.5 Flash and large open-source models such as Qwen2.5-VL-72B across diverse motion understanding datasets and benchmarks. FoundationMotion thus provides a scalable solution for curating fine-grained motion datasets that enable effective fine-tuning of diverse models to enhance motion understanding and spatial reasoning capabilities.",
            "upvotes": 2,
            "discussionId": "69417bca65f1e24a117808e6",
            "projectPage": "https://yulugan.com/projects/FoundationMotion.html",
            "githubRepo": "https://github.com/Wolfv0/FoundationMotion/tree/main",
            "githubRepoAddedBy": "user",
            "ai_summary": "FoundationMotion is an automated pipeline for creating large-scale motion datasets using object detection, trajectory extraction, and LLM-generated captions, improving motion understanding in models.",
            "ai_keywords": [
                "motion understanding",
                "state-of-the-art models",
                "motion benchmarks",
                "motion datasets",
                "manual annotation",
                "data curation pipeline",
                "object detection",
                "trajectory extraction",
                "Large Language Models",
                "fine-grained captions",
                "question-answer pairs",
                "NVILA-Video-15B",
                "Qwen2.5-7B",
                "Gemini-2.5 Flash",
                "Qwen2.5-VL-72B",
                "spatial reasoning"
            ],
            "githubStars": 12,
            "organization": {
                "_id": "63728bde14d543d507ae970d",
                "name": "MIT",
                "fullname": "Massachusetts Institute of Technology",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"
            }
        },
        "publishedAt": "2025-12-11T13:53:15.000Z",
        "title": "FoundationMotion: Auto-Labeling and Reasoning about Spatial Movement in Videos",
        "summary": "Motion understanding is fundamental to physical reasoning, enabling models to infer dynamics and predict future states. However, state-of-the-art models still struggle on recent motion benchmarks, primarily due to the scarcity of large-scale, fine-grained motion datasets. Existing motion datasets are often constructed from costly manual annotation, severely limiting scalability. To address this challenge, we introduce FoundationMotion, a fully automated data curation pipeline that constructs large-scale motion datasets. Our approach first detects and tracks objects in videos to extract their trajectories, then leverages these trajectories and video frames with Large Language Models (LLMs) to generate fine-grained captions and diverse question-answer pairs about motion and spatial reasoning. Using datasets produced by this pipeline, we fine-tune open-source models including NVILA-Video-15B and Qwen2.5-7B, achieving substantial improvements in motion understanding without compromising performance on other tasks. Notably, our models outperform strong closed-source baselines like Gemini-2.5 Flash and large open-source models such as Qwen2.5-VL-72B across diverse motion understanding datasets and benchmarks. FoundationMotion thus provides a scalable solution for curating fine-grained motion datasets that enable effective fine-tuning of diverse models to enhance motion understanding and spatial reasoning capabilities.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63fde8116315a264aba6d48f/3lR797q5esd-bCt_UtA1B.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.10927.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63fde8116315a264aba6d48f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fde8116315a264aba6d48f/h7fg5sG_QF5Tx6bZr4P8V.png",
            "fullname": "Yulu Gan",
            "name": "yulu2",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "63728bde14d543d507ae970d",
            "name": "MIT",
            "fullname": "Massachusetts Institute of Technology",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.07186",
            "authors": [
                {
                    "_id": "6938e8acdfc35938ba12a001",
                    "user": {
                        "_id": "666f2534a119281ee0e39783",
                        "avatarUrl": "/avatars/1de737da5cdf68d9bb1f98db8d5ad8b9.svg",
                        "isPro": false,
                        "fullname": "Zhuoming Liu",
                        "user": "zhuomingliu",
                        "type": "user"
                    },
                    "name": "Zhuoming Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-10T08:55:45.932Z",
                    "hidden": false
                },
                {
                    "_id": "6938e8acdfc35938ba12a002",
                    "name": "Xiaofeng Gao",
                    "hidden": false
                },
                {
                    "_id": "6938e8acdfc35938ba12a003",
                    "name": "Feiyang Niu",
                    "hidden": false
                },
                {
                    "_id": "6938e8acdfc35938ba12a004",
                    "name": "Qiaozi Gao",
                    "hidden": false
                },
                {
                    "_id": "6938e8acdfc35938ba12a005",
                    "name": "Liu Liu",
                    "hidden": false
                },
                {
                    "_id": "6938e8acdfc35938ba12a006",
                    "name": "Robinson Piramuthu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-08T05:43:14.000Z",
            "submittedOnDailyAt": "2025-12-16T01:06:58.171Z",
            "title": "START: Spatial and Textual Learning for Chart Understanding",
            "submittedOnDailyBy": {
                "_id": "666f2534a119281ee0e39783",
                "avatarUrl": "/avatars/1de737da5cdf68d9bb1f98db8d5ad8b9.svg",
                "isPro": false,
                "fullname": "Zhuoming Liu",
                "user": "zhuomingliu",
                "type": "user"
            },
            "summary": "Chart understanding is crucial for deploying multimodal large language models (MLLMs) in real-world scenarios such as analyzing scientific papers and technical reports. Unlike natural images, charts pair a structured visual layout (spatial property) with an underlying data representation (textual property) -- grasping both is essential for precise, fine-grained chart reasoning. Motivated by this observation, we propose START, the Spatial and Textual learning for chART understanding. Specifically, we introduce (i) chart-element grounding and (ii) chart-to-code generation to strengthen an MLLM's understanding of both chart visual layout and data details. To facilitate spatial and textual learning, we propose the START-Dataset generated with a novel data-generation pipeline that first leverages an MLLM to translate real chart images into executable chart code, recovering the underlying data representation while preserving the visual distribution of real-world charts. We then evolve the code with a Large Language Model (LLM) to ascertain the positions of chart elements that capture the chart's visual structure, addressing challenges that existing methods cannot handle. To evaluate a model's ability to understand chart spatial structures, we propose the Chart Spatial understanding Benchmark (CS-Bench), filling a critical gap in comprehensive chart understanding evaluation. Leveraging spatial and textual learning, START delivers consistent gains across model sizes and benchmarks over the base models and surpasses prior state-of-the-art by a clear margin. Code, data and models will be publicly available.",
            "upvotes": 2,
            "discussionId": "6938e8addfc35938ba12a007",
            "githubRepo": "https://github.com/dragonlzm/START",
            "githubRepoAddedBy": "user",
            "ai_summary": "START enhances multimodal large language models by integrating spatial and textual learning through chart-element grounding and chart-to-code generation, improving chart understanding and performance across benchmarks.",
            "ai_keywords": [
                "chart-element grounding",
                "chart-to-code generation",
                "spatial learning",
                "textual learning",
                "START-Dataset",
                "data-generation pipeline",
                "Large Language Model",
                "Chart Spatial understanding Benchmark (CS-Bench)"
            ],
            "githubStars": 2,
            "organization": {
                "_id": "674e3f226f1597e933139875",
                "name": "amazon-agi",
                "fullname": "Amazon AGI",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6163872970554df7422ef784/DZhaUsJ9C5C2G6vHKUzLD.png"
            }
        },
        "publishedAt": "2025-12-08T00:43:14.000Z",
        "title": "START: Spatial and Textual Learning for Chart Understanding",
        "summary": "Chart understanding is crucial for deploying multimodal large language models (MLLMs) in real-world scenarios such as analyzing scientific papers and technical reports. Unlike natural images, charts pair a structured visual layout (spatial property) with an underlying data representation (textual property) -- grasping both is essential for precise, fine-grained chart reasoning. Motivated by this observation, we propose START, the Spatial and Textual learning for chART understanding. Specifically, we introduce (i) chart-element grounding and (ii) chart-to-code generation to strengthen an MLLM's understanding of both chart visual layout and data details. To facilitate spatial and textual learning, we propose the START-Dataset generated with a novel data-generation pipeline that first leverages an MLLM to translate real chart images into executable chart code, recovering the underlying data representation while preserving the visual distribution of real-world charts. We then evolve the code with a Large Language Model (LLM) to ascertain the positions of chart elements that capture the chart's visual structure, addressing challenges that existing methods cannot handle. To evaluate a model's ability to understand chart spatial structures, we propose the Chart Spatial understanding Benchmark (CS-Bench), filling a critical gap in comprehensive chart understanding evaluation. Leveraging spatial and textual learning, START delivers consistent gains across model sizes and benchmarks over the base models and surpasses prior state-of-the-art by a clear margin. Code, data and models will be publicly available.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.07186.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "666f2534a119281ee0e39783",
            "avatarUrl": "/avatars/1de737da5cdf68d9bb1f98db8d5ad8b9.svg",
            "fullname": "Zhuoming Liu",
            "name": "zhuomingliu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "674e3f226f1597e933139875",
            "name": "amazon-agi",
            "fullname": "Amazon AGI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6163872970554df7422ef784/DZhaUsJ9C5C2G6vHKUzLD.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.05272",
            "authors": [
                {
                    "_id": "6941484465f1e24a1178087f",
                    "user": {
                        "_id": "662fcfeddeb646171f1df312",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/V1WdIbDyd20COpAAF70qf.jpeg",
                        "isPro": false,
                        "fullname": "Berke Gkmen",
                        "user": "berkegokmen1",
                        "type": "user"
                    },
                    "name": "Ahmet Berke Gokmen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-16T14:10:23.454Z",
                    "hidden": false
                },
                {
                    "_id": "6941484465f1e24a11780880",
                    "name": "Ajad Chhatkuli",
                    "hidden": false
                },
                {
                    "_id": "6941484465f1e24a11780881",
                    "name": "Luc Van Gool",
                    "hidden": false
                },
                {
                    "_id": "6941484465f1e24a11780882",
                    "name": "Danda Pani Paudel",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-04T21:51:47.000Z",
            "submittedOnDailyAt": "2025-12-16T09:24:08.305Z",
            "title": "Inferring Compositional 4D Scenes without Ever Seeing One",
            "submittedOnDailyBy": {
                "_id": "662fcfeddeb646171f1df312",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/V1WdIbDyd20COpAAF70qf.jpeg",
                "isPro": false,
                "fullname": "Berke Gkmen",
                "user": "berkegokmen1",
                "type": "user"
            },
            "summary": "Scenes in the real world are often composed of several static and dynamic objects. Capturing their 4-dimensional structures, composition and spatio-temporal configuration in-the-wild, though extremely interesting, is equally hard. Therefore, existing works often focus on one object at a time, while relying on some category-specific parametric shape model for dynamic objects. This can lead to inconsistent scene configurations, in addition to being limited to the modeled object categories. We propose COM4D (Compositional 4D), a method that consistently and jointly predicts the structure and spatio-temporal configuration of 4D/3D objects using only static multi-object or dynamic single object supervision. We achieve this by a carefully designed training of spatial and temporal attentions on 2D video input. The training is disentangled into learning from object compositions on the one hand, and single object dynamics throughout the video on the other, thus completely avoiding reliance on 4D compositional training data. At inference time, our proposed attention mixing mechanism combines these independently learned attentions, without requiring any 4D composition examples. By alternating between spatial and temporal reasoning, COM4D reconstructs complete and persistent 4D scenes with multiple interacting objects directly from monocular videos. Furthermore, COM4D provides state-of-the-art results in existing separate problems of 4D object and composed 3D reconstruction despite being purely data-driven.",
            "upvotes": 2,
            "discussionId": "6941484465f1e24a11780883",
            "projectPage": "https://berkegokmen1.github.io/com4d/",
            "githubRepo": "https://github.com/insait-institute/COM4D",
            "githubRepoAddedBy": "user",
            "ai_summary": "COM4D predicts the structure and spatio-temporal configuration of 4D/3D objects from 2D video without using 4D compositional training data, achieving state-of-the-art results in 4D object and composed 3D reconstruction.",
            "ai_keywords": [
                "4D structures",
                "spatio-temporal configuration",
                "category-specific parametric shape model",
                "COM4D",
                "Compositional 4D",
                "spatial attention",
                "temporal attention",
                "attention mixing mechanism",
                "spatial reasoning",
                "temporal reasoning",
                "4D reconstruction",
                "3D reconstruction"
            ],
            "githubStars": 8,
            "organization": {
                "_id": "6585b575110c3eb77dabaa93",
                "name": "INSAIT-Institute",
                "fullname": "Institute for Computer Science, Artificial intelligence and Technology ",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64f1a0700af832a73d0f3e6f/KwpuATq29U2-Fu55OvUHR.png"
            }
        },
        "publishedAt": "2025-12-04T16:51:47.000Z",
        "title": "Inferring Compositional 4D Scenes without Ever Seeing One",
        "summary": "Scenes in the real world are often composed of several static and dynamic objects. Capturing their 4-dimensional structures, composition and spatio-temporal configuration in-the-wild, though extremely interesting, is equally hard. Therefore, existing works often focus on one object at a time, while relying on some category-specific parametric shape model for dynamic objects. This can lead to inconsistent scene configurations, in addition to being limited to the modeled object categories. We propose COM4D (Compositional 4D), a method that consistently and jointly predicts the structure and spatio-temporal configuration of 4D/3D objects using only static multi-object or dynamic single object supervision. We achieve this by a carefully designed training of spatial and temporal attentions on 2D video input. The training is disentangled into learning from object compositions on the one hand, and single object dynamics throughout the video on the other, thus completely avoiding reliance on 4D compositional training data. At inference time, our proposed attention mixing mechanism combines these independently learned attentions, without requiring any 4D composition examples. By alternating between spatial and temporal reasoning, COM4D reconstructs complete and persistent 4D scenes with multiple interacting objects directly from monocular videos. Furthermore, COM4D provides state-of-the-art results in existing separate problems of 4D object and composed 3D reconstruction despite being purely data-driven.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.05272.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "662fcfeddeb646171f1df312",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/V1WdIbDyd20COpAAF70qf.jpeg",
            "fullname": "Berke Gkmen",
            "name": "berkegokmen1",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "6585b575110c3eb77dabaa93",
            "name": "INSAIT-Institute",
            "fullname": "Institute for Computer Science, Artificial intelligence and Technology ",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64f1a0700af832a73d0f3e6f/KwpuATq29U2-Fu55OvUHR.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.13330",
            "authors": [
                {
                    "_id": "6941504965f1e24a1178088d",
                    "user": {
                        "_id": "64798c5c4be04c76ce4508db",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64798c5c4be04c76ce4508db/k-8-2wagNuckcQ4jygu1p.jpeg",
                        "isPro": false,
                        "fullname": "Joona Kytniemi",
                        "user": "kjoona",
                        "type": "user"
                    },
                    "name": "Joona Kytniemi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-16T14:10:20.947Z",
                    "hidden": false
                },
                {
                    "_id": "6941504965f1e24a1178088e",
                    "name": "Jousia Piha",
                    "hidden": false
                },
                {
                    "_id": "6941504965f1e24a1178088f",
                    "user": {
                        "_id": "62e77fc7d4b0b8d818b1272b",
                        "avatarUrl": "/avatars/29b62154dc69c8ac66197736561073d0.svg",
                        "isPro": false,
                        "fullname": "Akseli Reunamo",
                        "user": "akseli-reunamo",
                        "type": "user"
                    },
                    "name": "Akseli Reunamo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-16T22:32:53.707Z",
                    "hidden": false
                },
                {
                    "_id": "6941504965f1e24a11780890",
                    "name": "Fedor Vitiugin",
                    "hidden": false
                },
                {
                    "_id": "6941504965f1e24a11780891",
                    "name": "Farrokh Mehryary",
                    "hidden": false
                },
                {
                    "_id": "6941504965f1e24a11780892",
                    "name": "Sampo Pyysalo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-15T13:41:41.000Z",
            "submittedOnDailyAt": "2025-12-16T10:32:58.142Z",
            "title": "FIN-bench-v2: A Unified and Robust Benchmark Suite for Evaluating Finnish Large Language Models",
            "submittedOnDailyBy": {
                "_id": "64798c5c4be04c76ce4508db",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64798c5c4be04c76ce4508db/k-8-2wagNuckcQ4jygu1p.jpeg",
                "isPro": false,
                "fullname": "Joona Kytniemi",
                "user": "kjoona",
                "type": "user"
            },
            "summary": "We introduce FIN-bench-v2, a unified benchmark suite for evaluating large language models in Finnish. FIN-bench-v2 consolidates Finnish versions of widely used benchmarks together with an updated and expanded version of the original FIN-bench into a single, consistently formatted collection, covering multiple-choice and generative tasks across reading comprehension, commonsense reasoning, sentiment analysis, world knowledge, and alignment. All datasets are converted to HuggingFace Datasets, which include both cloze and multiple-choice prompt formulations with five variants per task, and we incorporate human annotation or review for machine-translated resources such as GoldenSwag and XED. To select robust tasks, we pretrain a set of 2.15B-parameter decoder-only models and use their learning curves to compute monotonicity, signal-to-noise, non-random performance, and model ordering consistency, retaining only tasks that satisfy all criteria. We further evaluate a set of larger instruction-tuned models to characterize performance across tasks and prompt formulations. All datasets, prompts, and evaluation configurations are publicly available via our fork of the Language Model Evaluation Harness at https://github.com/LumiOpen/lm-evaluation-harness. Supplementary resources are released in a separate repository at https://github.com/TurkuNLP/FIN-bench-v2.",
            "upvotes": 1,
            "discussionId": "6941504965f1e24a11780893",
            "ai_summary": "FIN-bench-v2 is a unified benchmark suite for evaluating Finnish large language models, incorporating diverse datasets and evaluation criteria.",
            "ai_keywords": [
                "large language models",
                "benchmark suite",
                "reading comprehension",
                "commonsense reasoning",
                "sentiment analysis",
                "world knowledge",
                "alignment",
                "HuggingFace Datasets",
                "cloze",
                "multiple-choice",
                "pretrain",
                "decoder-only models",
                "learning curves",
                "monotonicity",
                "signal-to-noise",
                "non-random performance",
                "model ordering consistency",
                "instruction-tuned models",
                "evaluation configurations",
                "Language Model Evaluation Harness"
            ],
            "organization": {
                "_id": "618a9c8dc5884924d6825efc",
                "name": "TurkuNLP",
                "fullname": "TurkuNLP Research Group",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1636712482548-618e380849baf26530e19574.png"
            }
        },
        "publishedAt": "2025-12-15T08:41:41.000Z",
        "title": "FIN-bench-v2: A Unified and Robust Benchmark Suite for Evaluating Finnish Large Language Models",
        "summary": "We introduce FIN-bench-v2, a unified benchmark suite for evaluating large language models in Finnish. FIN-bench-v2 consolidates Finnish versions of widely used benchmarks together with an updated and expanded version of the original FIN-bench into a single, consistently formatted collection, covering multiple-choice and generative tasks across reading comprehension, commonsense reasoning, sentiment analysis, world knowledge, and alignment. All datasets are converted to HuggingFace Datasets, which include both cloze and multiple-choice prompt formulations with five variants per task, and we incorporate human annotation or review for machine-translated resources such as GoldenSwag and XED. To select robust tasks, we pretrain a set of 2.15B-parameter decoder-only models and use their learning curves to compute monotonicity, signal-to-noise, non-random performance, and model ordering consistency, retaining only tasks that satisfy all criteria. We further evaluate a set of larger instruction-tuned models to characterize performance across tasks and prompt formulations. All datasets, prompts, and evaluation configurations are publicly available via our fork of the Language Model Evaluation Harness at https://github.com/LumiOpen/lm-evaluation-harness. Supplementary resources are released in a separate repository at https://github.com/TurkuNLP/FIN-bench-v2.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13330.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64798c5c4be04c76ce4508db",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64798c5c4be04c76ce4508db/k-8-2wagNuckcQ4jygu1p.jpeg",
            "fullname": "Joona Kytniemi",
            "name": "kjoona",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "organization": {
            "_id": "618a9c8dc5884924d6825efc",
            "name": "TurkuNLP",
            "fullname": "TurkuNLP Research Group",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1636712482548-618e380849baf26530e19574.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.12777",
            "authors": [
                {
                    "_id": "69415bb065f1e24a11780895",
                    "name": "Mosh Levy",
                    "hidden": false
                },
                {
                    "_id": "69415bb065f1e24a11780896",
                    "name": "Zohar Elyoseph",
                    "hidden": false
                },
                {
                    "_id": "69415bb065f1e24a11780897",
                    "name": "Shauli Ravfogel",
                    "hidden": false
                },
                {
                    "_id": "69415bb065f1e24a11780898",
                    "name": "Yoav Goldberg",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/622f35a2bc2a392eaf21b3e7/5a4Ewb6co7N8eBgxtJukG.png"
            ],
            "publishedAt": "2025-12-14T17:30:34.000Z",
            "submittedOnDailyAt": "2025-12-16T10:48:15.077Z",
            "title": "State over Tokens: Characterizing the Role of Reasoning Tokens",
            "submittedOnDailyBy": {
                "_id": "622f35a2bc2a392eaf21b3e7",
                "avatarUrl": "/avatars/383409ebd912ba90d8e7966e61a3910d.svg",
                "isPro": false,
                "fullname": "Mosh Levy",
                "user": "Mosh",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) can generate reasoning tokens before their final answer to boost performance on complex tasks. While these sequences seem like human thought processes, empirical evidence reveals that they are not a faithful explanation of the model's actual reasoning process. To address this gap between appearance and function, we introduce the State over Tokens (SoT) conceptual framework. SoT reframes reasoning tokens not as a linguistic narrative, but as an externalized computational state -- the sole persistent information carrier across the model's stateless generation cycles. This explains how the tokens can drive correct reasoning without being a faithful explanation when read as text and surfaces previously overlooked research questions on these tokens. We argue that to truly understand the process that LLMs do, research must move beyond reading the reasoning tokens as text and focus on decoding them as state.",
            "upvotes": 1,
            "discussionId": "69415bb065f1e24a11780899",
            "ai_summary": "The State over Tokens (SoT) framework reinterprets reasoning tokens in large language models as computational states rather than linguistic narratives, highlighting the need for a new focus in research.",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "reasoning tokens",
                "State over Tokens (SoT)",
                "computational state",
                "generation cycles"
            ]
        },
        "publishedAt": "2025-12-14T12:30:34.000Z",
        "title": "State over Tokens: Characterizing the Role of Reasoning Tokens",
        "summary": "Large Language Models (LLMs) can generate reasoning tokens before their final answer to boost performance on complex tasks. While these sequences seem like human thought processes, empirical evidence reveals that they are not a faithful explanation of the model's actual reasoning process. To address this gap between appearance and function, we introduce the State over Tokens (SoT) conceptual framework. SoT reframes reasoning tokens not as a linguistic narrative, but as an externalized computational state -- the sole persistent information carrier across the model's stateless generation cycles. This explains how the tokens can drive correct reasoning without being a faithful explanation when read as text and surfaces previously overlooked research questions on these tokens. We argue that to truly understand the process that LLMs do, research must move beyond reading the reasoning tokens as text and focus on decoding them as state.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/622f35a2bc2a392eaf21b3e7/5a4Ewb6co7N8eBgxtJukG.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.12777.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "622f35a2bc2a392eaf21b3e7",
            "avatarUrl": "/avatars/383409ebd912ba90d8e7966e61a3910d.svg",
            "fullname": "Mosh Levy",
            "name": "Mosh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.12768",
            "authors": [
                {
                    "_id": "6941d69e5d5b2dc10527473f",
                    "name": "Tianjiao Yu",
                    "hidden": false
                },
                {
                    "_id": "6941d69e5d5b2dc105274740",
                    "name": "Xinzhuo Li",
                    "hidden": false
                },
                {
                    "_id": "6941d69e5d5b2dc105274741",
                    "name": "Yifan Shen",
                    "hidden": false
                },
                {
                    "_id": "6941d69e5d5b2dc105274742",
                    "name": "Yuanzhe Liu",
                    "hidden": false
                },
                {
                    "_id": "6941d69e5d5b2dc105274743",
                    "name": "Ismini Lourentzou",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/678ac3b31cbaa0b4bc295885/8ipU7v9hC3McDrrDej_ke.jpeg"
            ],
            "publishedAt": "2025-12-14T17:05:11.000Z",
            "submittedOnDailyAt": "2025-12-16T19:39:53.624Z",
            "title": "CoRe3D: Collaborative Reasoning as a Foundation for 3D Intelligence",
            "submittedOnDailyBy": {
                "_id": "678ac3b31cbaa0b4bc295885",
                "avatarUrl": "/avatars/1244fc1b305c9c6383df9bb5e4707347.svg",
                "isPro": false,
                "fullname": "Ismini Lourentzou",
                "user": "isminoula",
                "type": "user"
            },
            "summary": "Recent advances in large multimodal models suggest that explicit reasoning mechanisms play a critical role in improving model reliability, interpretability, and cross-modal alignment. While such reasoning-centric approaches have been proven effective in language and vision tasks, their extension to 3D remains underdeveloped. CoRe3D introduces a unified 3D understanding and generation reasoning framework that jointly operates over semantic and spatial abstractions, enabling high-level intent inferred from language to directly guide low-level 3D content formation. Central to this design is a spatially grounded reasoning representation that decomposes 3D latent space into localized regions, allowing the model to reason over geometry in a compositional and procedural manner. By tightly coupling semantic chain-of-thought inference with structured spatial reasoning, CoRe3D produces 3D outputs that exhibit strong local consistency and faithful alignment with linguistic descriptions.",
            "upvotes": 1,
            "discussionId": "6941d69f5d5b2dc105274744",
            "ai_summary": "A reasoning framework for 3D understanding and generation, CoRe3D, uses spatially grounded reasoning to align high-level language intent with low-level 3D content, ensuring local consistency and alignment with descriptions.",
            "ai_keywords": [
                "multimodal models",
                "reasoning mechanisms",
                "language tasks",
                "vision tasks",
                "3D understanding",
                "3D generation",
                "semantic abstractions",
                "spatial abstractions",
                "high-level intent",
                "low-level 3D content",
                "spatially grounded reasoning",
                "3D latent space",
                "localized regions",
                "compositional reasoning",
                "procedural reasoning",
                "semantic chain-of-thought inference",
                "structured spatial reasoning",
                "local consistency",
                "linguistic descriptions"
            ],
            "organization": {
                "_id": "67dd70ec2153d3e99c437bb2",
                "name": "planlab",
                "fullname": "Plan Lab @ UIUC",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65bac814c19f1b23459c9475/oX43HL5KEbOQSVqmSQom-.png"
            }
        },
        "publishedAt": "2025-12-14T12:05:11.000Z",
        "title": "CoRe3D: Collaborative Reasoning as a Foundation for 3D Intelligence",
        "summary": "Recent advances in large multimodal models suggest that explicit reasoning mechanisms play a critical role in improving model reliability, interpretability, and cross-modal alignment. While such reasoning-centric approaches have been proven effective in language and vision tasks, their extension to 3D remains underdeveloped. CoRe3D introduces a unified 3D understanding and generation reasoning framework that jointly operates over semantic and spatial abstractions, enabling high-level intent inferred from language to directly guide low-level 3D content formation. Central to this design is a spatially grounded reasoning representation that decomposes 3D latent space into localized regions, allowing the model to reason over geometry in a compositional and procedural manner. By tightly coupling semantic chain-of-thought inference with structured spatial reasoning, CoRe3D produces 3D outputs that exhibit strong local consistency and faithful alignment with linguistic descriptions.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/678ac3b31cbaa0b4bc295885/8ipU7v9hC3McDrrDej_ke.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.12768.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "678ac3b31cbaa0b4bc295885",
            "avatarUrl": "/avatars/1244fc1b305c9c6383df9bb5e4707347.svg",
            "fullname": "Ismini Lourentzou",
            "name": "isminoula",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "67dd70ec2153d3e99c437bb2",
            "name": "planlab",
            "fullname": "Plan Lab @ UIUC",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65bac814c19f1b23459c9475/oX43HL5KEbOQSVqmSQom-.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.11470",
            "authors": [
                {
                    "_id": "69417c3e65f1e24a117808e8",
                    "name": "Bowen Ding",
                    "hidden": false
                },
                {
                    "_id": "69417c3e65f1e24a117808e9",
                    "name": "Yuhan Chen",
                    "hidden": false
                },
                {
                    "_id": "69417c3e65f1e24a117808ea",
                    "name": "Jiayang Lv",
                    "hidden": false
                },
                {
                    "_id": "69417c3e65f1e24a117808eb",
                    "name": "Jiyao Yuan",
                    "hidden": false
                },
                {
                    "_id": "69417c3e65f1e24a117808ec",
                    "name": "Qi Zhu",
                    "hidden": false
                },
                {
                    "_id": "69417c3e65f1e24a117808ed",
                    "name": "Shuangshuang Tian",
                    "hidden": false
                },
                {
                    "_id": "69417c3e65f1e24a117808ee",
                    "name": "Dantong Zhu",
                    "hidden": false
                },
                {
                    "_id": "69417c3e65f1e24a117808ef",
                    "name": "Futing Wang",
                    "hidden": false
                },
                {
                    "_id": "69417c3e65f1e24a117808f0",
                    "name": "Heyuan Deng",
                    "hidden": false
                },
                {
                    "_id": "69417c3e65f1e24a117808f1",
                    "name": "Fei Mi",
                    "hidden": false
                },
                {
                    "_id": "69417c3e65f1e24a117808f2",
                    "name": "Lifeng Shang",
                    "hidden": false
                },
                {
                    "_id": "69417c3e65f1e24a117808f3",
                    "name": "Tao Lin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-12T11:13:00.000Z",
            "submittedOnDailyAt": "2025-12-16T13:06:22.031Z",
            "title": "Rethinking Expert Trajectory Utilization in LLM Post-training",
            "submittedOnDailyBy": {
                "_id": "64127b9fac08ffb707937231",
                "avatarUrl": "/avatars/33d406b8d1f319af5a4e3c2dc59ea7f2.svg",
                "isPro": false,
                "fullname": "Ding Bowen",
                "user": "Daniel21Ding",
                "type": "user"
            },
            "summary": "While effective post-training integrates Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), the optimal mechanism for utilizing expert trajectories remains unresolved. We propose the Plasticity-Ceiling Framework to theoretically ground this landscape, decomposing performance into foundational SFT performance and the subsequent RL plasticity. Through extensive benchmarking, we establish the Sequential SFT-then-RL pipeline as the superior standard, overcoming the stability deficits of synchronized approaches. Furthermore, we derive precise scaling guidelines: (1) Transitioning to RL at the SFT Stable or Mild Overfitting Sub-phase maximizes the final ceiling by securing foundational SFT performance without compromising RL plasticity; (2) Refuting ``Less is More'' in the context of SFT-then-RL scaling, we demonstrate that Data Scale determines the primary post-training potential, while Trajectory Difficulty acts as a performance multiplier; and (3) Identifying that the Minimum SFT Validation Loss serves as a robust indicator for selecting the expert trajectories that maximize the final performance ceiling. Our findings provide actionable guidelines for maximizing the value extracted from expert trajectories.",
            "upvotes": 1,
            "discussionId": "69417c3e65f1e24a117808f4",
            "githubRepo": "https://github.com/LINs-lab/RETU",
            "githubRepoAddedBy": "user",
            "ai_summary": "The Sequential SFT-then-RL pipeline is identified as optimal for integrating expert trajectories, with guidelines for scaling and trajectory selection based on performance metrics.",
            "ai_keywords": [
                "Supervised Fine-Tuning",
                "Reinforcement Learning",
                "Plasticity-Ceiling Framework",
                "Sequential SFT-then-RL",
                "SFT Stable",
                "Mild Overfitting Sub-phase",
                "Data Scale",
                "Trajectory Difficulty",
                "Minimum SFT Validation Loss"
            ],
            "githubStars": 5
        },
        "publishedAt": "2025-12-12T06:13:00.000Z",
        "title": "Rethinking Expert Trajectory Utilization in LLM Post-training",
        "summary": "While effective post-training integrates Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), the optimal mechanism for utilizing expert trajectories remains unresolved. We propose the Plasticity-Ceiling Framework to theoretically ground this landscape, decomposing performance into foundational SFT performance and the subsequent RL plasticity. Through extensive benchmarking, we establish the Sequential SFT-then-RL pipeline as the superior standard, overcoming the stability deficits of synchronized approaches. Furthermore, we derive precise scaling guidelines: (1) Transitioning to RL at the SFT Stable or Mild Overfitting Sub-phase maximizes the final ceiling by securing foundational SFT performance without compromising RL plasticity; (2) Refuting ``Less is More'' in the context of SFT-then-RL scaling, we demonstrate that Data Scale determines the primary post-training potential, while Trajectory Difficulty acts as a performance multiplier; and (3) Identifying that the Minimum SFT Validation Loss serves as a robust indicator for selecting the expert trajectories that maximize the final performance ceiling. Our findings provide actionable guidelines for maximizing the value extracted from expert trajectories.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.11470.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64127b9fac08ffb707937231",
            "avatarUrl": "/avatars/33d406b8d1f319af5a4e3c2dc59ea7f2.svg",
            "fullname": "Ding Bowen",
            "name": "Daniel21Ding",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.08405",
            "authors": [
                {
                    "_id": "69412e3a65f1e24a1178073f",
                    "user": {
                        "_id": "64e43a3f8b5060e515b490f9",
                        "avatarUrl": "/avatars/ff890611ee8f7870dc4ad487b6d7b85d.svg",
                        "isPro": false,
                        "fullname": "Fan Zhang",
                        "user": "Fanzhri",
                        "type": "user"
                    },
                    "name": "Fan Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-16T10:18:15.229Z",
                    "hidden": false
                },
                {
                    "_id": "69412e3a65f1e24a11780740",
                    "name": "Michael Gienger",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-09T09:36:51.000Z",
            "submittedOnDailyAt": "2025-12-16T07:34:07.691Z",
            "title": "Learning Robot Manipulation from Audio World Models",
            "submittedOnDailyBy": {
                "_id": "64e43a3f8b5060e515b490f9",
                "avatarUrl": "/avatars/ff890611ee8f7870dc4ad487b6d7b85d.svg",
                "isPro": false,
                "fullname": "Fan Zhang",
                "user": "Fanzhri",
                "type": "user"
            },
            "summary": "World models have demonstrated impressive performance on robotic learning tasks. Many such tasks inherently demand multimodal reasoning; for example, filling a bottle with water will lead to visual information alone being ambiguous or incomplete, thereby requiring reasoning over the temporal evolution of audio, accounting for its underlying physical properties and pitch patterns. In this paper, we propose a generative latent flow matching model to anticipate future audio observations, enabling the system to reason about long-term consequences when integrated into a robot policy. We demonstrate the superior capabilities of our system through two manipulation tasks that require perceiving in-the-wild audio or music signals, compared to methods without future lookahead. We further emphasize that successful robot action learning for these tasks relies not merely on multi-modal input, but critically on the accurate prediction of future audio states that embody intrinsic rhythmic patterns.",
            "upvotes": 1,
            "discussionId": "69412e3a65f1e24a11780741",
            "ai_summary": "A generative latent flow matching model is proposed to predict future audio for robotic manipulation tasks, improving performance over methods without future lookahead by accurately capturing intrinsic rhythmic patterns.",
            "ai_keywords": [
                "generative latent flow matching model",
                "multimodal reasoning",
                "audio observations",
                "temporal evolution",
                "physical properties",
                "pitch patterns",
                "robot policy",
                "manipulation tasks",
                "audio signals",
                "rhythmic patterns"
            ]
        },
        "publishedAt": "2025-12-09T04:36:51.000Z",
        "title": "Learning Robot Manipulation from Audio World Models",
        "summary": "World models have demonstrated impressive performance on robotic learning tasks. Many such tasks inherently demand multimodal reasoning; for example, filling a bottle with water will lead to visual information alone being ambiguous or incomplete, thereby requiring reasoning over the temporal evolution of audio, accounting for its underlying physical properties and pitch patterns. In this paper, we propose a generative latent flow matching model to anticipate future audio observations, enabling the system to reason about long-term consequences when integrated into a robot policy. We demonstrate the superior capabilities of our system through two manipulation tasks that require perceiving in-the-wild audio or music signals, compared to methods without future lookahead. We further emphasize that successful robot action learning for these tasks relies not merely on multi-modal input, but critically on the accurate prediction of future audio states that embody intrinsic rhythmic patterns.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.08405.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64e43a3f8b5060e515b490f9",
            "avatarUrl": "/avatars/ff890611ee8f7870dc4ad487b6d7b85d.svg",
            "fullname": "Fan Zhang",
            "name": "Fanzhri",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.08400",
            "authors": [
                {
                    "_id": "6941306f65f1e24a11780751",
                    "user": {
                        "_id": "6798f3dd14760a7b6c18722d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/0tA-73JLuKIdbpuW_NLyZ.png",
                        "isPro": false,
                        "fullname": "samitha Thilakarathna",
                        "user": "samDK",
                        "type": "user"
                    },
                    "name": "Samitha Nuwan Thilakarathna",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-16T10:18:13.036Z",
                    "hidden": false
                },
                {
                    "_id": "6941306f65f1e24a11780752",
                    "user": {
                        "_id": "6790bc4be7ad4d0bbc502ca0",
                        "avatarUrl": "/avatars/c0084b93cf4fef393dc31abd730dd91f.svg",
                        "isPro": false,
                        "fullname": "Ercan",
                        "user": "ercaaqua",
                        "type": "user"
                    },
                    "name": "Ercan Avsar",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-16T14:10:34.082Z",
                    "hidden": false
                },
                {
                    "_id": "6941306f65f1e24a11780753",
                    "name": "Martin Mathias Nielsen",
                    "hidden": false
                },
                {
                    "_id": "6941306f65f1e24a11780754",
                    "name": "Malte Pedersen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-09T09:33:53.000Z",
            "submittedOnDailyAt": "2025-12-16T07:44:35.351Z",
            "title": "Towards Visual Re-Identification of Fish using Fine-Grained Classification for Electronic Monitoring in Fisheries",
            "submittedOnDailyBy": {
                "_id": "66c460d8ae70890c90923dd6",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ocbXyDoPO9FThXrA_f1Dq.png",
                "isPro": false,
                "fullname": "Malte Pedersen",
                "user": "mapeAAU",
                "type": "user"
            },
            "summary": "Accurate fisheries data are crucial for effective and sustainable marine resource management. With the recent adoption of Electronic Monitoring (EM) systems, more video data is now being collected than can be feasibly reviewed manually. This paper addresses this challenge by developing an optimized deep learning pipeline for automated fish re-identification (Re-ID) using the novel AutoFish dataset, which simulates EM systems with conveyor belts with six similarly looking fish species. We demonstrate that key Re-ID metrics (R1 and mAP@k) are substantially improved by using hard triplet mining in conjunction with a custom image transformation pipeline that includes dataset-specific normalization. By employing these strategies, we demonstrate that the Vision Transformer-based Swin-T architecture consistently outperforms the Convolutional Neural Network-based ResNet-50, achieving peak performance of 41.65% mAP@k and 90.43% Rank-1 accuracy. An in-depth analysis reveals that the primary challenge is distinguishing visually similar individuals of the same species (Intra-species errors), where viewpoint inconsistency proves significantly more detrimental than partial occlusion. The source code and documentation are available at: https://github.com/msamdk/Fish_Re_Identification.git",
            "upvotes": 1,
            "discussionId": "6941307065f1e24a11780755",
            "githubRepo": "https://github.com/msamdk/Fish_Re_Identification.git",
            "githubRepoAddedBy": "user",
            "ai_summary": "The study presents an optimized deep learning pipeline using the AutoFish dataset and Swin-T architecture to improve fish re-identification metrics in electronic monitoring systems.",
            "ai_keywords": [
                "fish re-identification",
                "Re-ID",
                "hard triplet mining",
                "image transformation pipeline",
                "dataset-specific normalization",
                "Vision Transformer",
                "Swin-T",
                "Convolutional Neural Network",
                "ResNet-50",
                "intra-species errors",
                "viewpoint inconsistency",
                "partial occlusion"
            ],
            "githubStars": 3,
            "organization": {
                "_id": "65206b4136008ecc886b085d",
                "name": "dtudk",
                "fullname": "Technical University of Denmark",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/xJq65HwJYs32ove1JVx3n.png"
            }
        },
        "publishedAt": "2025-12-09T04:33:53.000Z",
        "title": "Towards Visual Re-Identification of Fish using Fine-Grained Classification for Electronic Monitoring in Fisheries",
        "summary": "Accurate fisheries data are crucial for effective and sustainable marine resource management. With the recent adoption of Electronic Monitoring (EM) systems, more video data is now being collected than can be feasibly reviewed manually. This paper addresses this challenge by developing an optimized deep learning pipeline for automated fish re-identification (Re-ID) using the novel AutoFish dataset, which simulates EM systems with conveyor belts with six similarly looking fish species. We demonstrate that key Re-ID metrics (R1 and mAP@k) are substantially improved by using hard triplet mining in conjunction with a custom image transformation pipeline that includes dataset-specific normalization. By employing these strategies, we demonstrate that the Vision Transformer-based Swin-T architecture consistently outperforms the Convolutional Neural Network-based ResNet-50, achieving peak performance of 41.65% mAP@k and 90.43% Rank-1 accuracy. An in-depth analysis reveals that the primary challenge is distinguishing visually similar individuals of the same species (Intra-species errors), where viewpoint inconsistency proves significantly more detrimental than partial occlusion. The source code and documentation are available at: https://github.com/msamdk/Fish_Re_Identification.git",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.08400.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66c460d8ae70890c90923dd6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ocbXyDoPO9FThXrA_f1Dq.png",
            "fullname": "Malte Pedersen",
            "name": "mapeAAU",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "65206b4136008ecc886b085d",
            "name": "dtudk",
            "fullname": "Technical University of Denmark",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/xJq65HwJYs32ove1JVx3n.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.09069",
            "authors": [
                {
                    "_id": "693faa8ff516c6932468128a",
                    "user": {
                        "_id": "6824d49eae455f19806a14bf",
                        "avatarUrl": "/avatars/fa5389c805e53d9600326658da14478d.svg",
                        "isPro": false,
                        "fullname": "Erfan Nourbakhsh",
                        "user": "Erfan-Nourbakhsh",
                        "type": "user"
                    },
                    "name": "Erfan Nourbakhsh",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-16T09:48:48.896Z",
                    "hidden": false
                },
                {
                    "_id": "693faa8ff516c6932468128b",
                    "name": "Nasrin Sanjari",
                    "hidden": false
                },
                {
                    "_id": "693faa8ff516c6932468128c",
                    "name": "Ali Nourbakhsh",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6824d49eae455f19806a14bf/cg8MR2N4BjBjfjLpbgaP5.png"
            ],
            "publishedAt": "2025-12-09T19:34:30.000Z",
            "submittedOnDailyAt": "2025-12-16T13:18:23.376Z",
            "title": "KD-OCT: Efficient Knowledge Distillation for Clinical-Grade Retinal OCT Classification",
            "submittedOnDailyBy": {
                "_id": "6824d49eae455f19806a14bf",
                "avatarUrl": "/avatars/fa5389c805e53d9600326658da14478d.svg",
                "isPro": false,
                "fullname": "Erfan Nourbakhsh",
                "user": "Erfan-Nourbakhsh",
                "type": "user"
            },
            "summary": "Age-related macular degeneration (AMD) and choroidal neovascularization (CNV)-related conditions are leading causes of vision loss worldwide, with optical coherence tomography (OCT) serving as a cornerstone for early detection and management. However, deploying state-of-the-art deep learning models like ConvNeXtV2-Large in clinical settings is hindered by their computational demands. Therefore, it is desirable to develop efficient models that maintain high diagnostic performance while enabling real-time deployment. In this study, a novel knowledge distillation framework, termed KD-OCT, is proposed to compress a high-performance ConvNeXtV2-Large teacher model, enhanced with advanced augmentations, stochastic weight averaging, and focal loss, into a lightweight EfficientNet-B2 student for classifying normal, drusen, and CNV cases. KD-OCT employs real-time distillation with a combined loss balancing soft teacher knowledge transfer and hard ground-truth supervision. The effectiveness of the proposed method is evaluated on the Noor Eye Hospital (NEH) dataset using patient-level cross-validation. Experimental results demonstrate that KD-OCT outperforms comparable multi-scale or feature-fusion OCT classifiers in efficiency- accuracy balance, achieving near-teacher performance with substantial reductions in model size and inference time. Despite the compression, the student model exceeds most existing frameworks, facilitating edge deployment for AMD screening. Code is available at https://github.com/erfan-nourbakhsh/KD- OCT.",
            "upvotes": 0,
            "discussionId": "693faa90f516c6932468128d",
            "githubRepo": "https://github.com/erfan-nourbakhsh/KD-OCT",
            "githubRepoAddedBy": "auto",
            "ai_summary": "A novel knowledge distillation framework compresses a high-performance ConvNeXtV2-Large model into a lightweight EfficientNet-B2 for efficient AMD and CNV classification in real-time clinical settings.",
            "ai_keywords": [
                "ConvNeXtV2-Large",
                "knowledge distillation",
                "KD-OCT",
                "EfficientNet-B2",
                "advanced augmentations",
                "stochastic weight averaging",
                "focal loss",
                "real-time distillation",
                "patient-level cross-validation",
                "edge deployment"
            ],
            "githubStars": 0
        },
        "publishedAt": "2025-12-09T14:34:30.000Z",
        "title": "KD-OCT: Efficient Knowledge Distillation for Clinical-Grade Retinal OCT Classification",
        "summary": "Age-related macular degeneration (AMD) and choroidal neovascularization (CNV)-related conditions are leading causes of vision loss worldwide, with optical coherence tomography (OCT) serving as a cornerstone for early detection and management. However, deploying state-of-the-art deep learning models like ConvNeXtV2-Large in clinical settings is hindered by their computational demands. Therefore, it is desirable to develop efficient models that maintain high diagnostic performance while enabling real-time deployment. In this study, a novel knowledge distillation framework, termed KD-OCT, is proposed to compress a high-performance ConvNeXtV2-Large teacher model, enhanced with advanced augmentations, stochastic weight averaging, and focal loss, into a lightweight EfficientNet-B2 student for classifying normal, drusen, and CNV cases. KD-OCT employs real-time distillation with a combined loss balancing soft teacher knowledge transfer and hard ground-truth supervision. The effectiveness of the proposed method is evaluated on the Noor Eye Hospital (NEH) dataset using patient-level cross-validation. Experimental results demonstrate that KD-OCT outperforms comparable multi-scale or feature-fusion OCT classifiers in efficiency- accuracy balance, achieving near-teacher performance with substantial reductions in model size and inference time. Despite the compression, the student model exceeds most existing frameworks, facilitating edge deployment for AMD screening. Code is available at https://github.com/erfan-nourbakhsh/KD- OCT.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6824d49eae455f19806a14bf/cg8MR2N4BjBjfjLpbgaP5.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.09069.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6824d49eae455f19806a14bf",
            "avatarUrl": "/avatars/fa5389c805e53d9600326658da14478d.svg",
            "fullname": "Erfan Nourbakhsh",
            "name": "Erfan-Nourbakhsh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    }
]