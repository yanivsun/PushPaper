[
    {
        "paper": {
            "id": "2512.15431",
            "authors": [
                {
                    "_id": "69437417542d62d58a7bf6c4",
                    "name": "Haolong Yan",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6c5",
                    "name": "Jia Wang",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6c6",
                    "name": "Xin Huang",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6c7",
                    "name": "Yeqing Shen",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6c8",
                    "user": {
                        "_id": "653614073f4248157d60ccdc",
                        "avatarUrl": "/avatars/c9298bab1cdc1d0b6ffe4c7c5ef18bd5.svg",
                        "isPro": false,
                        "fullname": "mengziyang",
                        "user": "zylate",
                        "type": "user"
                    },
                    "name": "Ziyang Meng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-18T07:59:53.033Z",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6c9",
                    "name": "Zhimin Fan",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6ca",
                    "name": "Kaijun Tan",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6cb",
                    "name": "Jin Gao",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6cc",
                    "name": "Lieyu Shi",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6cd",
                    "name": "Mi Yang",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6ce",
                    "name": "Shiliang Yang",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6cf",
                    "name": "Zhirui Wang",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6d0",
                    "name": "Brian Li",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6d1",
                    "name": "Kang An",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6d2",
                    "name": "Chenyang Li",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6d3",
                    "name": "Lei Lei",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6d4",
                    "name": "Mengmeng Duan",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6d5",
                    "name": "Danxun Liang",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6d6",
                    "name": "Guodong Liu",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6d7",
                    "name": "Hang Cheng",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6d8",
                    "name": "Hao Wu",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6d9",
                    "name": "Jie Dong",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6da",
                    "name": "Junhao Huang",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6db",
                    "name": "Mei Chen",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6dc",
                    "name": "Renjie Yu",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6dd",
                    "name": "Shunshan Li",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6de",
                    "name": "Xu Zhou",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6df",
                    "name": "Yiting Dai",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6e0",
                    "name": "Yineng Deng",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6e1",
                    "name": "Yingdan Liang",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6e2",
                    "name": "Zelin Chen",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6e3",
                    "name": "Wen Sun",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6e4",
                    "name": "Chengxu Yan",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6e5",
                    "name": "Chunqin Xu",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6e6",
                    "name": "Dong Li",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6e7",
                    "name": "Fengqiong Xiao",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6e8",
                    "name": "Guanghao Fan",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6e9",
                    "name": "Guopeng Li",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6ea",
                    "name": "Guozhen Peng",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6eb",
                    "name": "Hongbing Li",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6ec",
                    "name": "Hang Li",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6ed",
                    "name": "Hongming Chen",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6ee",
                    "name": "Jingjing Xie",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6ef",
                    "name": "Jianyong Li",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6f0",
                    "name": "Jingyang Zhang",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6f1",
                    "name": "Jiaju Ren",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6f2",
                    "name": "Jiayu Yuan",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6f3",
                    "name": "Jianpeng Yin",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6f4",
                    "name": "Kai Cao",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6f5",
                    "name": "Liang Zhao",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6f6",
                    "name": "Liguo Tan",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6f7",
                    "name": "Liying Shi",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6f8",
                    "name": "Mengqiang Ren",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6f9",
                    "name": "Min Xu",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6fa",
                    "name": "Manjiao Liu",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6fb",
                    "name": "Mao Luo",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6fc",
                    "name": "Mingxin Wan",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6fd",
                    "name": "Na Wang",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6fe",
                    "name": "Nan Wu",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf6ff",
                    "name": "Ning Wang",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf700",
                    "name": "Peiyao Ma",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf701",
                    "name": "Qingzhou Zhang",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf702",
                    "name": "Qiao Wang",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf703",
                    "name": "Qinlin Zeng",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf704",
                    "name": "Qiong Gao",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf705",
                    "name": "Qiongyao Li",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf706",
                    "name": "Shangwu Zhong",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf707",
                    "name": "Shuli Gao",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf708",
                    "name": "Shaofan Liu",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf709",
                    "name": "Shisi Gao",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf70a",
                    "name": "Shuang Luo",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf70b",
                    "name": "Xingbin Liu",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf70c",
                    "name": "Xiaojia Liu",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf70d",
                    "name": "Xiaojie Hou",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf70e",
                    "name": "Xin Liu",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf70f",
                    "name": "Xuanti Feng",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf710",
                    "name": "Xuedan Cai",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf711",
                    "name": "Xuan Wen",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf712",
                    "name": "Xianwei Zhu",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf713",
                    "name": "Xin Liang",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf714",
                    "name": "Xin Liu",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf715",
                    "name": "Xin Zhou",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf716",
                    "name": "Yingxiu Zhao",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf717",
                    "name": "Yukang Shi",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf718",
                    "name": "Yunfang Xu",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf719",
                    "name": "Yuqing Zeng",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf71a",
                    "name": "Yixun Zhang",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf71b",
                    "name": "Zejia Weng",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf71c",
                    "name": "Zhonghao Yan",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf71d",
                    "name": "Zhiguo Huang",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf71e",
                    "name": "Zhuoyu Wang",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf71f",
                    "name": "Zheng Ge",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf720",
                    "name": "Jing Li",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf721",
                    "name": "Yibo Zhu",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf722",
                    "name": "Binxing Jiao",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf723",
                    "name": "Xiangyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "69437417542d62d58a7bf724",
                    "name": "Daxin Jiang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-17T13:26:30.000Z",
            "submittedOnDailyAt": "2025-12-18T00:55:26.804Z",
            "title": "Step-GUI Technical Report",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Recent advances in multimodal large language models unlock unprecedented opportunities for GUI automation. However, a fundamental challenge remains: how to efficiently acquire high-quality training data while maintaining annotation reliability? We introduce a self-evolving training pipeline powered by the Calibrated Step Reward System, which converts model-generated trajectories into reliable training signals through trajectory-level calibration, achieving >90% annotation accuracy with 10-100x lower cost. Leveraging this pipeline, we introduce Step-GUI, a family of models (4B/8B) that achieves state-of-the-art GUI performance (8B: 80.2% AndroidWorld, 48.5% OSWorld, 62.6% ScreenShot-Pro) while maintaining robust general capabilities. As GUI agent capabilities improve, practical deployment demands standardized interfaces across heterogeneous devices while protecting user privacy. To this end, we propose GUI-MCP, the first Model Context Protocol for GUI automation with hierarchical architecture that combines low-level atomic operations and high-level task delegation to local specialist models, enabling high-privacy execution where sensitive data stays on-device. Finally, to assess whether agents can handle authentic everyday usage, we introduce AndroidDaily, a benchmark grounded in real-world mobile usage patterns with 3146 static actions and 235 end-to-end tasks across high-frequency daily scenarios (8B: static 89.91%, end-to-end 52.50%). Our work advances the development of practical GUI agents and demonstrates strong potential for real-world deployment in everyday digital interactions.",
            "upvotes": 80,
            "discussionId": "69437418542d62d58a7bf725",
            "projectPage": "https://opengelab.github.io/",
            "githubRepo": "https://github.com/stepfun-ai/gelab-zero",
            "githubRepoAddedBy": "user",
            "ai_summary": "A self-evolving training pipeline with the Calibrated Step Reward System and GUI-MCP protocol improve GUI automation efficiency, accuracy, and privacy in real-world scenarios.",
            "ai_keywords": [
                "multimodal large language models",
                "GUI automation",
                "self-evolving training pipeline",
                "Calibrated Step Reward System",
                "trajectory-level calibration",
                "Step-GUI",
                "GUI performance",
                "GUI-MCP",
                "Model Context Protocol",
                "AndroidWorld",
                "OSWorld",
                "ScreenShot-Pro",
                "AndroidDaily",
                "real-world mobile usage patterns",
                "hierarchical architecture",
                "low-level atomic operations",
                "high-level task delegation",
                "local specialist models",
                "high-privacy execution"
            ],
            "githubStars": 1442,
            "organization": {
                "_id": "66e43eae9d477f566f937935",
                "name": "stepfun-ai",
                "fullname": "StepFun",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"
            }
        },
        "publishedAt": "2025-12-17T08:26:30.000Z",
        "title": "Step-GUI Technical Report",
        "summary": "Recent advances in multimodal large language models unlock unprecedented opportunities for GUI automation. However, a fundamental challenge remains: how to efficiently acquire high-quality training data while maintaining annotation reliability? We introduce a self-evolving training pipeline powered by the Calibrated Step Reward System, which converts model-generated trajectories into reliable training signals through trajectory-level calibration, achieving >90% annotation accuracy with 10-100x lower cost. Leveraging this pipeline, we introduce Step-GUI, a family of models (4B/8B) that achieves state-of-the-art GUI performance (8B: 80.2% AndroidWorld, 48.5% OSWorld, 62.6% ScreenShot-Pro) while maintaining robust general capabilities. As GUI agent capabilities improve, practical deployment demands standardized interfaces across heterogeneous devices while protecting user privacy. To this end, we propose GUI-MCP, the first Model Context Protocol for GUI automation with hierarchical architecture that combines low-level atomic operations and high-level task delegation to local specialist models, enabling high-privacy execution where sensitive data stays on-device. Finally, to assess whether agents can handle authentic everyday usage, we introduce AndroidDaily, a benchmark grounded in real-world mobile usage patterns with 3146 static actions and 235 end-to-end tasks across high-frequency daily scenarios (8B: static 89.91%, end-to-end 52.50%). Our work advances the development of practical GUI agents and demonstrates strong potential for real-world deployment in everyday digital interactions.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.15431.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 186
        },
        "organization": {
            "_id": "66e43eae9d477f566f937935",
            "name": "stepfun-ai",
            "fullname": "StepFun",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.15176",
            "authors": [
                {
                    "_id": "694370d7542d62d58a7bf698",
                    "name": "Zicong Cheng",
                    "hidden": false
                },
                {
                    "_id": "694370d7542d62d58a7bf699",
                    "name": "Guo-Wei Yang",
                    "hidden": false
                },
                {
                    "_id": "694370d7542d62d58a7bf69a",
                    "name": "Jia Li",
                    "hidden": false
                },
                {
                    "_id": "694370d7542d62d58a7bf69b",
                    "name": "Zhijie Deng",
                    "hidden": false
                },
                {
                    "_id": "694370d7542d62d58a7bf69c",
                    "user": {
                        "_id": "6571b51fd5c6a6d3b0ba68ad",
                        "avatarUrl": "/avatars/0ccd8fe8de857753b534356a90eb10f0.svg",
                        "isPro": false,
                        "fullname": "gmh",
                        "user": "menghao22",
                        "type": "user"
                    },
                    "name": "Meng-Hao Guo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-18T07:59:54.790Z",
                    "hidden": false
                },
                {
                    "_id": "694370d7542d62d58a7bf69d",
                    "name": "Shi-Min Hu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-17T08:19:04.000Z",
            "submittedOnDailyAt": "2025-12-18T00:44:13.153Z",
            "title": "DEER: Draft with Diffusion, Verify with Autoregressive Models",
            "submittedOnDailyBy": {
                "_id": "6571b51fd5c6a6d3b0ba68ad",
                "avatarUrl": "/avatars/0ccd8fe8de857753b534356a90eb10f0.svg",
                "isPro": false,
                "fullname": "gmh",
                "user": "menghao22",
                "type": "user"
            },
            "summary": "Efficiency, as a critical practical challenge for LLM-driven agentic and reasoning systems, is increasingly constrained by the inherent latency of autoregressive (AR) decoding. Speculative decoding mitigates this cost through a draft-verify scheme, yet existing approaches rely on AR draft models (a.k.a., drafters), which introduce two fundamental issues: (1) step-wise uncertainty accumulation leads to a progressive collapse of trust between the target model and the drafter, and (2) inherently sequential decoding of AR drafters. Together, these factors cause limited speedups. In this paper, we show that a diffusion large language model (dLLM) drafters can naturally overcome these issues through its fundamentally different probabilistic modeling and efficient parallel decoding strategy. Building on this insight, we introduce DEER, an efficient speculative decoding framework that drafts with diffusion and verifies with AR models. To enable high-quality drafting, DEER employs a two-stage training pipeline to align the dLLM-based drafters with the target AR model, and further adopts single-step decoding to generate long draft segments. Experiments show DEER reaches draft acceptance lengths of up to 32 tokens, far surpassing the 10 tokens achieved by EAGLE-3. Moreover, on HumanEval with Qwen3-30B-A3B, DEER attains a 5.54x speedup, while EAGLE-3 achieves only 2.41x. Code, model, demo, etc, will be available at https://czc726.github.io/DEER/",
            "upvotes": 37,
            "discussionId": "694370d8542d62d58a7bf69e",
            "projectPage": "https://czc726.github.io/DEER/",
            "ai_summary": "DEER framework uses diffusion large language models for efficient speculative decoding, overcoming the limitations of autoregressive drafters with better speed and draft quality.",
            "ai_keywords": [
                "autoregressive decoding",
                "speculative decoding",
                "diffusion large language model",
                "dLLM",
                "draft-verify scheme",
                "step-wise uncertainty accumulation",
                "parallel decoding",
                "two-stage training pipeline",
                "single-step decoding",
                "HumanEval",
                "Qwen3-30B-A3B"
            ]
        },
        "publishedAt": "2025-12-17T03:19:04.000Z",
        "title": "DEER: Draft with Diffusion, Verify with Autoregressive Models",
        "summary": "Efficiency, as a critical practical challenge for LLM-driven agentic and reasoning systems, is increasingly constrained by the inherent latency of autoregressive (AR) decoding. Speculative decoding mitigates this cost through a draft-verify scheme, yet existing approaches rely on AR draft models (a.k.a., drafters), which introduce two fundamental issues: (1) step-wise uncertainty accumulation leads to a progressive collapse of trust between the target model and the drafter, and (2) inherently sequential decoding of AR drafters. Together, these factors cause limited speedups. In this paper, we show that a diffusion large language model (dLLM) drafters can naturally overcome these issues through its fundamentally different probabilistic modeling and efficient parallel decoding strategy. Building on this insight, we introduce DEER, an efficient speculative decoding framework that drafts with diffusion and verifies with AR models. To enable high-quality drafting, DEER employs a two-stage training pipeline to align the dLLM-based drafters with the target AR model, and further adopts single-step decoding to generate long draft segments. Experiments show DEER reaches draft acceptance lengths of up to 32 tokens, far surpassing the 10 tokens achieved by EAGLE-3. Moreover, on HumanEval with Qwen3-30B-A3B, DEER attains a 5.54x speedup, while EAGLE-3 achieves only 2.41x. Code, model, demo, etc, will be available at https://czc726.github.io/DEER/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.15176.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6571b51fd5c6a6d3b0ba68ad",
            "avatarUrl": "/avatars/0ccd8fe8de857753b534356a90eb10f0.svg",
            "fullname": "gmh",
            "name": "menghao22",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.14681",
            "authors": [
                {
                    "_id": "69437375542d62d58a7bf6aa",
                    "name": "Lanxiang Hu",
                    "hidden": false
                },
                {
                    "_id": "69437375542d62d58a7bf6ab",
                    "name": "Siqi Kou",
                    "hidden": false
                },
                {
                    "_id": "69437375542d62d58a7bf6ac",
                    "name": "Yichao Fu",
                    "hidden": false
                },
                {
                    "_id": "69437375542d62d58a7bf6ad",
                    "name": "Samyam Rajbhandari",
                    "hidden": false
                },
                {
                    "_id": "69437375542d62d58a7bf6ae",
                    "name": "Tajana Rosing",
                    "hidden": false
                },
                {
                    "_id": "69437375542d62d58a7bf6af",
                    "name": "Yuxiong He",
                    "hidden": false
                },
                {
                    "_id": "69437375542d62d58a7bf6b0",
                    "name": "Zhijie Deng",
                    "hidden": false
                },
                {
                    "_id": "69437375542d62d58a7bf6b1",
                    "name": "Hao Zhang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6301d6455e305a35cb0846a7/X_SGgKHd6c9A0-I-9QNyn.gif",
                "https://cdn-uploads.huggingface.co/production/uploads/6301d6455e305a35cb0846a7/MidLBXBtdUmG0fY7Wyh27.gif",
                "https://cdn-uploads.huggingface.co/production/uploads/6301d6455e305a35cb0846a7/qVt5G_vvppSDGMXPPG8Sf.png",
                "https://cdn-uploads.huggingface.co/production/uploads/6301d6455e305a35cb0846a7/tJhlEhoQSGi5hoe8KvTId.gif"
            ],
            "publishedAt": "2025-12-16T18:45:18.000Z",
            "submittedOnDailyAt": "2025-12-18T00:57:47.270Z",
            "title": "Fast and Accurate Causal Parallel Decoding using Jacobi Forcing",
            "submittedOnDailyBy": {
                "_id": "6301d6455e305a35cb0846a7",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6301d6455e305a35cb0846a7/aT2AtzRMSY_T3y02MIUap.jpeg",
                "isPro": true,
                "fullname": "Lanxiang Hu",
                "user": "Snyhlxde",
                "type": "user"
            },
            "summary": "Multi-token generation has emerged as a promising paradigm for accelerating transformer-based large model inference. Recent efforts primarily explore diffusion Large Language Models (dLLMs) for parallel decoding to reduce inference latency. To achieve AR-level generation quality, many techniques adapt AR models into dLLMs to enable parallel decoding. However, they suffer from limited speedup compared to AR models due to a pretrain-to-posttrain mismatch. Specifically, the masked data distribution in post-training deviates significantly from the real-world data distribution seen during pretraining, and dLLMs rely on bidirectional attention, which conflicts with the causal prior learned during pretraining and hinders the integration of exact KV cache reuse. To address this, we introduce Jacobi Forcing, a progressive distillation paradigm where models are trained on their own generated parallel decoding trajectories, smoothly shifting AR models into efficient parallel decoders while preserving their pretrained causal inference property. The models trained under this paradigm, Jacobi Forcing Model, achieves 3.8x wall-clock speedup on coding and math benchmarks with minimal loss in performance. Based on Jacobi Forcing Models' trajectory characteristics, we introduce multi-block decoding with rejection recycling, which enables up to 4.5x higher token acceptance count per iteration and nearly 4.0x wall-clock speedup, effectively trading additional compute for lower inference latency. Our code is available at https://github.com/hao-ai-lab/JacobiForcing.",
            "upvotes": 36,
            "discussionId": "69437375542d62d58a7bf6b2",
            "githubRepo": "https://github.com/hao-ai-lab/JacobiForcing",
            "githubRepoAddedBy": "user",
            "ai_summary": "Jacobi Forcing is a progressive distillation method that enables efficient parallel decoding of transformer-based models while maintaining performance, significantly reducing inference latency.",
            "ai_keywords": [
                "Multi-token generation",
                "diffusion Large Language Models (dLLMs)",
                "parallel decoding",
                "inference latency",
                "adaptive autoregressive (AR) models",
                "masked data distribution",
                "bidirectional attention",
                "causal inference",
                "pretrained causal inference property",
                "trajectory characteristics",
                "multi-block decoding",
                "rejection recycling"
            ],
            "githubStars": 52
        },
        "publishedAt": "2025-12-16T13:45:18.000Z",
        "title": "Fast and Accurate Causal Parallel Decoding using Jacobi Forcing",
        "summary": "Multi-token generation has emerged as a promising paradigm for accelerating transformer-based large model inference. Recent efforts primarily explore diffusion Large Language Models (dLLMs) for parallel decoding to reduce inference latency. To achieve AR-level generation quality, many techniques adapt AR models into dLLMs to enable parallel decoding. However, they suffer from limited speedup compared to AR models due to a pretrain-to-posttrain mismatch. Specifically, the masked data distribution in post-training deviates significantly from the real-world data distribution seen during pretraining, and dLLMs rely on bidirectional attention, which conflicts with the causal prior learned during pretraining and hinders the integration of exact KV cache reuse. To address this, we introduce Jacobi Forcing, a progressive distillation paradigm where models are trained on their own generated parallel decoding trajectories, smoothly shifting AR models into efficient parallel decoders while preserving their pretrained causal inference property. The models trained under this paradigm, Jacobi Forcing Model, achieves 3.8x wall-clock speedup on coding and math benchmarks with minimal loss in performance. Based on Jacobi Forcing Models' trajectory characteristics, we introduce multi-block decoding with rejection recycling, which enables up to 4.5x higher token acceptance count per iteration and nearly 4.0x wall-clock speedup, effectively trading additional compute for lower inference latency. Our code is available at https://github.com/hao-ai-lab/JacobiForcing.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6301d6455e305a35cb0846a7/X_SGgKHd6c9A0-I-9QNyn.gif",
            "https://cdn-uploads.huggingface.co/production/uploads/6301d6455e305a35cb0846a7/MidLBXBtdUmG0fY7Wyh27.gif",
            "https://cdn-uploads.huggingface.co/production/uploads/6301d6455e305a35cb0846a7/qVt5G_vvppSDGMXPPG8Sf.png",
            "https://cdn-uploads.huggingface.co/production/uploads/6301d6455e305a35cb0846a7/tJhlEhoQSGi5hoe8KvTId.gif"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.14681.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6301d6455e305a35cb0846a7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6301d6455e305a35cb0846a7/aT2AtzRMSY_T3y02MIUap.jpeg",
            "fullname": "Lanxiang Hu",
            "name": "Snyhlxde",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.14944",
            "authors": [
                {
                    "_id": "6943feea47055eb55aade03e",
                    "name": "Ahmadreza Jeddi",
                    "hidden": false
                },
                {
                    "_id": "6943feea47055eb55aade03f",
                    "name": "Hakki Can Karaimer",
                    "hidden": false
                },
                {
                    "_id": "6943feea47055eb55aade040",
                    "name": "Hue Nguyen",
                    "hidden": false
                },
                {
                    "_id": "6943feea47055eb55aade041",
                    "name": "Zhongling Wang",
                    "hidden": false
                },
                {
                    "_id": "6943feea47055eb55aade042",
                    "name": "Ke Zhao",
                    "hidden": false
                },
                {
                    "_id": "6943feea47055eb55aade043",
                    "name": "Javad Rajabi",
                    "hidden": false
                },
                {
                    "_id": "6943feea47055eb55aade044",
                    "name": "Ran Zhang",
                    "hidden": false
                },
                {
                    "_id": "6943feea47055eb55aade045",
                    "name": "Raghav Goyal",
                    "hidden": false
                },
                {
                    "_id": "6943feea47055eb55aade046",
                    "name": "Babak Taati",
                    "hidden": false
                },
                {
                    "_id": "6943feea47055eb55aade047",
                    "name": "Radek Grzeszczuk",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/65e8df170cda621164769f6f/Lu0YITas0nGkl6AvQfh8U.png",
                "https://cdn-uploads.huggingface.co/production/uploads/65e8df170cda621164769f6f/8lUn87mKrq_eRbQytBIj3.png",
                "https://cdn-uploads.huggingface.co/production/uploads/65e8df170cda621164769f6f/U9XN-imAoO1qiYvlUX1VM.png",
                "https://cdn-uploads.huggingface.co/production/uploads/65e8df170cda621164769f6f/zrAdLC21nkwWubq6s-6JM.png"
            ],
            "publishedAt": "2025-12-16T22:17:25.000Z",
            "submittedOnDailyAt": "2025-12-18T13:40:40.167Z",
            "title": "Puzzle Curriculum GRPO for Vision-Centric Reasoning",
            "submittedOnDailyBy": {
                "_id": "65e8df170cda621164769f6f",
                "avatarUrl": "/avatars/b5d7c7c49d1a777a1bd41d06d299868a.svg",
                "isPro": false,
                "fullname": "Armen Jeddi",
                "user": "armenjeddi",
                "type": "user"
            },
            "summary": "Recent reinforcement learning (RL) approaches like outcome-supervised GRPO have advanced chain-of-thought reasoning in Vision Language Models (VLMs), yet key issues linger: (i) reliance on costly and noisy hand-curated annotations or external verifiers; (ii) flat and sparse reward schemes in GRPO; and (iii) logical inconsistency between a chain's reasoning and its final answer. We present Puzzle Curriculum GRPO (PC-GRPO), a supervision-free recipe for RL with Verifiable Rewards (RLVR) that strengthens visual reasoning in VLMs without annotations or external verifiers. PC-GRPO replaces labels with three self-supervised puzzle environments: PatchFit, Rotation (with binary rewards) and Jigsaw (with graded partial credit mitigating reward sparsity). To counter flat rewards and vanishing group-relative advantages, we introduce a difficulty-aware curriculum that dynamically weights samples and peaks at medium difficulty. We further monitor Reasoning-Answer Consistency (RAC) during post-training: mirroring reports for vanilla GRPO in LLMs, RAC typically rises early then degrades; our curriculum delays this decline, and consistency-enforcing reward schemes further boost RAC. RAC correlates with downstream accuracy. Across diverse benchmarks and on Qwen-7B and Qwen-3B backbones, PC-GRPO improves reasoning quality, training stability, and end-task accuracy, offering a practical path to scalable, verifiable, and interpretable RL post-training for VLMs.",
            "upvotes": 30,
            "discussionId": "6943feea47055eb55aade048",
            "projectPage": "https://pcgrpo.github.io/",
            "ai_summary": "Puzzle Curriculum GRPO enhances visual reasoning in Vision Language Models through self-supervised environments and a difficulty-aware curriculum, improving consistency and accuracy without external annotations.",
            "ai_keywords": [
                "reinforcement learning",
                "outcome-supervised GRPO",
                "chain-of-thought reasoning",
                "Vision Language Models",
                "self-supervised",
                "PatchFit",
                "Rotation",
                "Jigsaw",
                "graded partial credit",
                "reward sparsity",
                "difficulty-aware curriculum",
                "Reasoning-Answer Consistency",
                "LLMs",
                "Qwen-7B",
                "Qwen-3B",
                "verifiable rewards",
                "post-training"
            ],
            "organization": {
                "_id": "686df54910a52f2c2cf03c06",
                "name": "SamsungResearch",
                "fullname": "Samsung Research",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60ffc3e62403168abcae811d/lBrkzrpjrJ8k-3CGLKRLr.jpeg"
            }
        },
        "publishedAt": "2025-12-16T17:17:25.000Z",
        "title": "Puzzle Curriculum GRPO for Vision-Centric Reasoning",
        "summary": "Recent reinforcement learning (RL) approaches like outcome-supervised GRPO have advanced chain-of-thought reasoning in Vision Language Models (VLMs), yet key issues linger: (i) reliance on costly and noisy hand-curated annotations or external verifiers; (ii) flat and sparse reward schemes in GRPO; and (iii) logical inconsistency between a chain's reasoning and its final answer. We present Puzzle Curriculum GRPO (PC-GRPO), a supervision-free recipe for RL with Verifiable Rewards (RLVR) that strengthens visual reasoning in VLMs without annotations or external verifiers. PC-GRPO replaces labels with three self-supervised puzzle environments: PatchFit, Rotation (with binary rewards) and Jigsaw (with graded partial credit mitigating reward sparsity). To counter flat rewards and vanishing group-relative advantages, we introduce a difficulty-aware curriculum that dynamically weights samples and peaks at medium difficulty. We further monitor Reasoning-Answer Consistency (RAC) during post-training: mirroring reports for vanilla GRPO in LLMs, RAC typically rises early then degrades; our curriculum delays this decline, and consistency-enforcing reward schemes further boost RAC. RAC correlates with downstream accuracy. Across diverse benchmarks and on Qwen-7B and Qwen-3B backbones, PC-GRPO improves reasoning quality, training stability, and end-task accuracy, offering a practical path to scalable, verifiable, and interpretable RL post-training for VLMs.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/65e8df170cda621164769f6f/Lu0YITas0nGkl6AvQfh8U.png",
            "https://cdn-uploads.huggingface.co/production/uploads/65e8df170cda621164769f6f/8lUn87mKrq_eRbQytBIj3.png",
            "https://cdn-uploads.huggingface.co/production/uploads/65e8df170cda621164769f6f/U9XN-imAoO1qiYvlUX1VM.png",
            "https://cdn-uploads.huggingface.co/production/uploads/65e8df170cda621164769f6f/zrAdLC21nkwWubq6s-6JM.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.14944.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65e8df170cda621164769f6f",
            "avatarUrl": "/avatars/b5d7c7c49d1a777a1bd41d06d299868a.svg",
            "fullname": "Armen Jeddi",
            "name": "armenjeddi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "organization": {
            "_id": "686df54910a52f2c2cf03c06",
            "name": "SamsungResearch",
            "fullname": "Samsung Research",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60ffc3e62403168abcae811d/lBrkzrpjrJ8k-3CGLKRLr.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.14052",
            "authors": [
                {
                    "_id": "6942650d5d5b2dc1052749c8",
                    "name": "HyperAI Team",
                    "hidden": false
                },
                {
                    "_id": "6942650d5d5b2dc1052749c9",
                    "name": "Yuchen Liu",
                    "hidden": false
                },
                {
                    "_id": "6942650d5d5b2dc1052749ca",
                    "name": "Kaiyang Han",
                    "hidden": false
                },
                {
                    "_id": "6942650d5d5b2dc1052749cb",
                    "name": "Zhiqiang Xia",
                    "hidden": false
                },
                {
                    "_id": "6942650d5d5b2dc1052749cc",
                    "name": "Yuhang Dong",
                    "hidden": false
                },
                {
                    "_id": "6942650d5d5b2dc1052749cd",
                    "name": "Chen Song",
                    "hidden": false
                },
                {
                    "_id": "6942650d5d5b2dc1052749ce",
                    "name": "Kangyu Tang",
                    "hidden": false
                },
                {
                    "_id": "6942650d5d5b2dc1052749cf",
                    "name": "Jiaming Xu",
                    "hidden": false
                },
                {
                    "_id": "6942650d5d5b2dc1052749d0",
                    "name": "Xiushi Feng",
                    "hidden": false
                },
                {
                    "_id": "6942650d5d5b2dc1052749d1",
                    "name": "WenXuan Yu",
                    "hidden": false
                },
                {
                    "_id": "6942650d5d5b2dc1052749d2",
                    "name": "Li Peng",
                    "hidden": false
                },
                {
                    "_id": "6942650d5d5b2dc1052749d3",
                    "name": "Mingyang Wang",
                    "hidden": false
                },
                {
                    "_id": "6942650d5d5b2dc1052749d4",
                    "name": "Kai Wang",
                    "hidden": false
                },
                {
                    "_id": "6942650d5d5b2dc1052749d5",
                    "name": "Changpeng Yang",
                    "hidden": false
                },
                {
                    "_id": "6942650d5d5b2dc1052749d6",
                    "name": "Yang Li",
                    "hidden": false
                },
                {
                    "_id": "6942650d5d5b2dc1052749d7",
                    "name": "Haoyu Lu",
                    "hidden": false
                },
                {
                    "_id": "6942650d5d5b2dc1052749d8",
                    "name": "Hao Wang",
                    "hidden": false
                },
                {
                    "_id": "6942650d5d5b2dc1052749d9",
                    "name": "Bingna Xu",
                    "hidden": false
                },
                {
                    "_id": "6942650d5d5b2dc1052749da",
                    "name": "Guangyao Liu",
                    "hidden": false
                },
                {
                    "_id": "6942650d5d5b2dc1052749db",
                    "name": "Long Huang",
                    "hidden": false
                },
                {
                    "_id": "6942650d5d5b2dc1052749dc",
                    "name": "Kaibin Guo",
                    "hidden": false
                },
                {
                    "_id": "6942650d5d5b2dc1052749dd",
                    "name": "Jinyang Wu",
                    "hidden": false
                },
                {
                    "_id": "6942650d5d5b2dc1052749de",
                    "name": "Dan Wu",
                    "hidden": false
                },
                {
                    "_id": "6942650d5d5b2dc1052749df",
                    "name": "Hongzhen Wang",
                    "hidden": false
                },
                {
                    "_id": "6942650d5d5b2dc1052749e0",
                    "name": "Peng Zhou",
                    "hidden": false
                },
                {
                    "_id": "6942650d5d5b2dc1052749e1",
                    "name": "Shuai Nie",
                    "hidden": false
                },
                {
                    "_id": "6942650d5d5b2dc1052749e2",
                    "name": "Shande Wang",
                    "hidden": false
                },
                {
                    "_id": "6942650d5d5b2dc1052749e3",
                    "name": "Runyu Shi",
                    "hidden": false
                },
                {
                    "_id": "6942650d5d5b2dc1052749e4",
                    "name": "Ying Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-16T03:36:41.000Z",
            "submittedOnDailyAt": "2025-12-18T05:34:36.317Z",
            "title": "HyperVL: An Efficient and Dynamic Multimodal Large Language Model for Edge Devices",
            "submittedOnDailyBy": {
                "_id": "6747de57f8cab58c22ec94a2",
                "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg",
                "isPro": false,
                "fullname": "Jinyang Wu",
                "user": "Jinyang23",
                "type": "user"
            },
            "summary": "Current multimodal large lanauge models possess strong perceptual and reasoning capabilities, however high computational and memory requirements make them difficult to deploy directly on on-device environments. While small-parameter models are progressively endowed with strong general capabilities, standard Vision Transformer (ViT) encoders remain a critical bottleneck, suffering from excessive latency and memory consumption when processing high-resolution inputs.To address these challenges, we introduce HyperVL, an efficient multimodal large language model tailored for on-device inference. HyperVL adopts an image-tiling strategy to cap peak memory usage and incorporates two novel techniques: (1) a Visual Resolution Compressor (VRC) that adaptively predicts optimal encoding resolutions to eliminate redundant computation, and (2) Dual Consistency Learning (DCL), which aligns multi-scale ViT encoders within a unified framework, enabling dynamic switching between visual branches under a shared LLM. Extensive experiments demonstrate that HyperVL achieves state-of-the-art performance among models of comparable size across multiple benchmarks. Furthermore, it significantly significantly reduces latency and power consumption on real mobile devices, demonstrating its practicality for on-device multimodal inference.",
            "upvotes": 30,
            "discussionId": "6942650e5d5b2dc1052749e5",
            "ai_summary": "HyperVL, an efficient multimodal large language model for on-device inference, uses image tiling, Visual Resolution Compressor, and Dual Consistency Learning to reduce memory usage, latency, and power consumption while maintaining performance.",
            "ai_keywords": [
                "multimodal large language models",
                "Vision Transformer (ViT)",
                "image-tiling strategy",
                "Visual Resolution Compressor",
                "Dual Consistency Learning",
                "on-device inference",
                "latency",
                "power consumption",
                "benchmarks"
            ]
        },
        "publishedAt": "2025-12-15T22:36:41.000Z",
        "title": "HyperVL: An Efficient and Dynamic Multimodal Large Language Model for Edge Devices",
        "summary": "Current multimodal large lanauge models possess strong perceptual and reasoning capabilities, however high computational and memory requirements make them difficult to deploy directly on on-device environments. While small-parameter models are progressively endowed with strong general capabilities, standard Vision Transformer (ViT) encoders remain a critical bottleneck, suffering from excessive latency and memory consumption when processing high-resolution inputs.To address these challenges, we introduce HyperVL, an efficient multimodal large language model tailored for on-device inference. HyperVL adopts an image-tiling strategy to cap peak memory usage and incorporates two novel techniques: (1) a Visual Resolution Compressor (VRC) that adaptively predicts optimal encoding resolutions to eliminate redundant computation, and (2) Dual Consistency Learning (DCL), which aligns multi-scale ViT encoders within a unified framework, enabling dynamic switching between visual branches under a shared LLM. Extensive experiments demonstrate that HyperVL achieves state-of-the-art performance among models of comparable size across multiple benchmarks. Furthermore, it significantly significantly reduces latency and power consumption on real mobile devices, demonstrating its practicality for on-device multimodal inference.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.14052.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6747de57f8cab58c22ec94a2",
            "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg",
            "fullname": "Jinyang Wu",
            "name": "Jinyang23",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.14693",
            "authors": [
                {
                    "_id": "6942c644fb33037a39577c15",
                    "user": {
                        "_id": "641ddac5be3bd3a5a06ed4a4",
                        "avatarUrl": "/avatars/14969dff861d53b0a75305606495eca7.svg",
                        "isPro": false,
                        "fullname": "Zitian Gao",
                        "user": "zgao3186",
                        "type": "user"
                    },
                    "name": "Zitian Gao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-18T08:20:55.038Z",
                    "hidden": false
                },
                {
                    "_id": "6942c644fb33037a39577c16",
                    "name": "Lynx Chen",
                    "hidden": false
                },
                {
                    "_id": "6942c644fb33037a39577c17",
                    "name": "Yihao Xiao",
                    "hidden": false
                },
                {
                    "_id": "6942c644fb33037a39577c18",
                    "name": "He Xing",
                    "hidden": false
                },
                {
                    "_id": "6942c644fb33037a39577c19",
                    "name": "Ran Tao",
                    "hidden": false
                },
                {
                    "_id": "6942c644fb33037a39577c1a",
                    "name": "Haoming Luo",
                    "hidden": false
                },
                {
                    "_id": "6942c644fb33037a39577c1b",
                    "name": "Joey Zhou",
                    "hidden": false
                },
                {
                    "_id": "6942c644fb33037a39577c1c",
                    "name": "Bryan Dai",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/641ddac5be3bd3a5a06ed4a4/0uPUEtb7mYGoZ0RDxJUnI.png"
            ],
            "publishedAt": "2025-12-16T18:58:45.000Z",
            "submittedOnDailyAt": "2025-12-18T00:22:44.947Z",
            "title": "Universal Reasoning Model",
            "submittedOnDailyBy": {
                "_id": "641ddac5be3bd3a5a06ed4a4",
                "avatarUrl": "/avatars/14969dff861d53b0a75305606495eca7.svg",
                "isPro": false,
                "fullname": "Zitian Gao",
                "user": "zgao3186",
                "type": "user"
            },
            "summary": "Universal transformers (UTs) have been widely used for complex reasoning tasks such as ARC-AGI and Sudoku, yet the specific sources of their performance gains remain underexplored. In this work, we systematically analyze UTs variants and show that improvements on ARC-AGI primarily arise from the recurrent inductive bias and strong nonlinear components of Transformer, rather than from elaborate architectural designs. Motivated by this finding, we propose the Universal Reasoning Model (URM), which enhances the UT with short convolution and truncated backpropagation. Our approach substantially improves reasoning performance, achieving state-of-the-art 53.8% pass@1 on ARC-AGI 1 and 16.0% pass@1 on ARC-AGI 2. Our code is avaliable at https://github.com/zitian-gao/URM.",
            "upvotes": 19,
            "discussionId": "6942c644fb33037a39577c1d",
            "githubRepo": "https://github.com/zitian-gao/URM",
            "githubRepoAddedBy": "user",
            "ai_summary": "The Universal Reasoning Model enhances Universal Transformers with short convolution and truncated backpropagation to improve reasoning performance on ARC-AGI tasks.",
            "ai_keywords": [
                "Universal Transformers",
                "ARC-AGI",
                "recurrent inductive bias",
                "nonlinear components",
                "truncated backpropagation",
                "Universal Reasoning Model"
            ],
            "githubStars": 23,
            "organization": {
                "_id": "68c8248dda0e62cc830e7e49",
                "name": "UbiquantAI",
                "fullname": "Ubiquant",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68c80cc3a1ca9a73c17d29f7/AmD26BH3h3lGxFN_SgWra.jpeg"
            }
        },
        "publishedAt": "2025-12-16T13:58:45.000Z",
        "title": "Universal Reasoning Model",
        "summary": "Universal transformers (UTs) have been widely used for complex reasoning tasks such as ARC-AGI and Sudoku, yet the specific sources of their performance gains remain underexplored. In this work, we systematically analyze UTs variants and show that improvements on ARC-AGI primarily arise from the recurrent inductive bias and strong nonlinear components of Transformer, rather than from elaborate architectural designs. Motivated by this finding, we propose the Universal Reasoning Model (URM), which enhances the UT with short convolution and truncated backpropagation. Our approach substantially improves reasoning performance, achieving state-of-the-art 53.8% pass@1 on ARC-AGI 1 and 16.0% pass@1 on ARC-AGI 2. Our code is avaliable at https://github.com/zitian-gao/URM.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/641ddac5be3bd3a5a06ed4a4/0uPUEtb7mYGoZ0RDxJUnI.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.14693.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "641ddac5be3bd3a5a06ed4a4",
            "avatarUrl": "/avatars/14969dff861d53b0a75305606495eca7.svg",
            "fullname": "Zitian Gao",
            "name": "zgao3186",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "68c8248dda0e62cc830e7e49",
            "name": "UbiquantAI",
            "fullname": "Ubiquant",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68c80cc3a1ca9a73c17d29f7/AmD26BH3h3lGxFN_SgWra.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.15635",
            "authors": [
                {
                    "_id": "6943902a542d62d58a7bf7a0",
                    "user": {
                        "_id": "64aa2210e04e7f92245f54d2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64aa2210e04e7f92245f54d2/OE43T22bLWBVgmqcJyUtu.png",
                        "isPro": false,
                        "fullname": "Li",
                        "user": "kotion",
                        "type": "user"
                    },
                    "name": "Yuanhang Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-18T07:59:34.872Z",
                    "hidden": false
                },
                {
                    "_id": "6943902a542d62d58a7bf7a1",
                    "name": "Yiren Song",
                    "hidden": false
                },
                {
                    "_id": "6943902a542d62d58a7bf7a2",
                    "name": "Junzhe Bai",
                    "hidden": false
                },
                {
                    "_id": "6943902a542d62d58a7bf7a3",
                    "name": "Xinran Liang",
                    "hidden": false
                },
                {
                    "_id": "6943902a542d62d58a7bf7a4",
                    "name": "Hu Yang",
                    "hidden": false
                },
                {
                    "_id": "6943902a542d62d58a7bf7a5",
                    "name": "Libiao Jin",
                    "hidden": false
                },
                {
                    "_id": "6943902a542d62d58a7bf7a6",
                    "user": {
                        "_id": "6388a7e98a5dbe2f3dc61faa",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6388a7e98a5dbe2f3dc61faa/Zj8pR7BG_wOQjsYzosb5p.jpeg",
                        "isPro": false,
                        "fullname": "Qi Mao",
                        "user": "HelenMao",
                        "type": "user"
                    },
                    "name": "Qi Mao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-18T07:59:37.087Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-17T17:47:18.000Z",
            "submittedOnDailyAt": "2025-12-18T03:01:26.657Z",
            "title": "IC-Effect: Precise and Efficient Video Effects Editing via In-Context Learning",
            "submittedOnDailyBy": {
                "_id": "6388a7e98a5dbe2f3dc61faa",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6388a7e98a5dbe2f3dc61faa/Zj8pR7BG_wOQjsYzosb5p.jpeg",
                "isPro": false,
                "fullname": "Qi Mao",
                "user": "HelenMao",
                "type": "user"
            },
            "summary": "We propose IC-Effect, an instruction-guided, DiT-based framework for few-shot video VFX editing that synthesizes complex effects (\\eg flames, particles and cartoon characters) while strictly preserving spatial and temporal consistency. Video VFX editing is highly challenging because injected effects must blend seamlessly with the background, the background must remain entirely unchanged, and effect patterns must be learned efficiently from limited paired data. However, existing video editing models fail to satisfy these requirements. IC-Effect leverages the source video as clean contextual conditions, exploiting the contextual learning capability of DiT models to achieve precise background preservation and natural effect injection. A two-stage training strategy, consisting of general editing adaptation followed by effect-specific learning via Effect-LoRA, ensures strong instruction following and robust effect modeling. To further improve efficiency, we introduce spatiotemporal sparse tokenization, enabling high fidelity with substantially reduced computation. We also release a paired VFX editing dataset spanning 15 high-quality visual styles. Extensive experiments show that IC-Effect delivers high-quality, controllable, and temporally consistent VFX editing, opening new possibilities for video creation.",
            "upvotes": 17,
            "discussionId": "6943902a542d62d58a7bf7a7",
            "projectPage": "https://cuc-mipg.github.io/IC-Effect/",
            "githubRepo": "https://github.com/CUC-MIPG/IC-Effect",
            "githubRepoAddedBy": "user",
            "ai_summary": "IC-Effect, an instruction-guided DiT-based framework, synthesizes complex video VFX effects while preserving spatial and temporal consistency using a two-stage training strategy and spatiotemporal sparse tokenization.",
            "ai_keywords": [
                "DiT",
                "few-shot video VFX editing",
                "flames",
                "particles",
                "cartoon characters",
                "spatial consistency",
                "temporal consistency",
                "DiT models",
                "contextual learning",
                "general editing adaptation",
                "Effect-LoRA",
                "spatiotemporal sparse tokenization",
                "VFX editing dataset"
            ],
            "githubStars": 25,
            "organization": {
                "_id": "67dab498ed21a53369f5de73",
                "name": "CUC-MIPG",
                "fullname": "Multimedia Intelligent Processing Group in Communication University of China",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/640d704c8036cc2142299c19/B85B31gd7-0kjK_Rpvv3g.jpeg"
            }
        },
        "publishedAt": "2025-12-17T12:47:18.000Z",
        "title": "IC-Effect: Precise and Efficient Video Effects Editing via In-Context Learning",
        "summary": "We propose IC-Effect, an instruction-guided, DiT-based framework for few-shot video VFX editing that synthesizes complex effects (\\eg flames, particles and cartoon characters) while strictly preserving spatial and temporal consistency. Video VFX editing is highly challenging because injected effects must blend seamlessly with the background, the background must remain entirely unchanged, and effect patterns must be learned efficiently from limited paired data. However, existing video editing models fail to satisfy these requirements. IC-Effect leverages the source video as clean contextual conditions, exploiting the contextual learning capability of DiT models to achieve precise background preservation and natural effect injection. A two-stage training strategy, consisting of general editing adaptation followed by effect-specific learning via Effect-LoRA, ensures strong instruction following and robust effect modeling. To further improve efficiency, we introduce spatiotemporal sparse tokenization, enabling high fidelity with substantially reduced computation. We also release a paired VFX editing dataset spanning 15 high-quality visual styles. Extensive experiments show that IC-Effect delivers high-quality, controllable, and temporally consistent VFX editing, opening new possibilities for video creation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.15635.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6388a7e98a5dbe2f3dc61faa",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6388a7e98a5dbe2f3dc61faa/Zj8pR7BG_wOQjsYzosb5p.jpeg",
            "fullname": "Qi Mao",
            "name": "HelenMao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "67dab498ed21a53369f5de73",
            "name": "CUC-MIPG",
            "fullname": "Multimedia Intelligent Processing Group in Communication University of China",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/640d704c8036cc2142299c19/B85B31gd7-0kjK_Rpvv3g.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.15603",
            "authors": [
                {
                    "_id": "694373cc542d62d58a7bf6b4",
                    "name": "Shengming Yin",
                    "hidden": false
                },
                {
                    "_id": "694373cc542d62d58a7bf6b5",
                    "name": "Zekai Zhang",
                    "hidden": false
                },
                {
                    "_id": "694373cc542d62d58a7bf6b6",
                    "name": "Zecheng Tang",
                    "hidden": false
                },
                {
                    "_id": "694373cc542d62d58a7bf6b7",
                    "name": "Kaiyuan Gao",
                    "hidden": false
                },
                {
                    "_id": "694373cc542d62d58a7bf6b8",
                    "name": "Xiao Xu",
                    "hidden": false
                },
                {
                    "_id": "694373cc542d62d58a7bf6b9",
                    "name": "Kun Yan",
                    "hidden": false
                },
                {
                    "_id": "694373cc542d62d58a7bf6ba",
                    "name": "Jiahao Li",
                    "hidden": false
                },
                {
                    "_id": "694373cc542d62d58a7bf6bb",
                    "name": "Yilei Chen",
                    "hidden": false
                },
                {
                    "_id": "694373cc542d62d58a7bf6bc",
                    "name": "Yuxiang Chen",
                    "hidden": false
                },
                {
                    "_id": "694373cc542d62d58a7bf6bd",
                    "name": "Heung-Yeung Shum",
                    "hidden": false
                },
                {
                    "_id": "694373cc542d62d58a7bf6be",
                    "name": "Lionel M. Ni",
                    "hidden": false
                },
                {
                    "_id": "694373cc542d62d58a7bf6bf",
                    "name": "Jingren Zhou",
                    "hidden": false
                },
                {
                    "_id": "694373cc542d62d58a7bf6c0",
                    "name": "Junyang Lin",
                    "hidden": false
                },
                {
                    "_id": "694373cc542d62d58a7bf6c1",
                    "name": "Chenfei Wu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-17T17:12:42.000Z",
            "submittedOnDailyAt": "2025-12-18T00:54:15.231Z",
            "title": "Qwen-Image-Layered: Towards Inherent Editability via Layer Decomposition",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Recent visual generative models often struggle with consistency during image editing due to the entangled nature of raster images, where all visual content is fused into a single canvas. In contrast, professional design tools employ layered representations, allowing isolated edits while preserving consistency. Motivated by this, we propose Qwen-Image-Layered, an end-to-end diffusion model that decomposes a single RGB image into multiple semantically disentangled RGBA layers, enabling inherent editability, where each RGBA layer can be independently manipulated without affecting other content. To support variable-length decomposition, we introduce three key components: (1) an RGBA-VAE to unify the latent representations of RGB and RGBA images; (2) a VLD-MMDiT (Variable Layers Decomposition MMDiT) architecture capable of decomposing a variable number of image layers; and (3) a Multi-stage Training strategy to adapt a pretrained image generation model into a multilayer image decomposer. Furthermore, to address the scarcity of high-quality multilayer training images, we build a pipeline to extract and annotate multilayer images from Photoshop documents (PSD). Experiments demonstrate that our method significantly surpasses existing approaches in decomposition quality and establishes a new paradigm for consistent image editing. Our code and models are released on https://github.com/QwenLM/Qwen-Image-Layered{https://github.com/QwenLM/Qwen-Image-Layered}",
            "upvotes": 17,
            "discussionId": "694373cc542d62d58a7bf6c2",
            "ai_summary": "Qwen-Image-Layered decomposes images into semantically disentangled RGBA layers using a diffusion model, enabling independent editing of each layer and improving decomposition quality and consistency.",
            "ai_keywords": [
                "diffusion model",
                "RGBA layers",
                "inherent editability",
                "RGBA-VAE",
                "VLD-MMDiT",
                "Multi-stage Training",
                "PSD",
                "decomposition quality",
                "consistent image editing"
            ]
        },
        "publishedAt": "2025-12-17T12:12:42.000Z",
        "title": "Qwen-Image-Layered: Towards Inherent Editability via Layer Decomposition",
        "summary": "Recent visual generative models often struggle with consistency during image editing due to the entangled nature of raster images, where all visual content is fused into a single canvas. In contrast, professional design tools employ layered representations, allowing isolated edits while preserving consistency. Motivated by this, we propose Qwen-Image-Layered, an end-to-end diffusion model that decomposes a single RGB image into multiple semantically disentangled RGBA layers, enabling inherent editability, where each RGBA layer can be independently manipulated without affecting other content. To support variable-length decomposition, we introduce three key components: (1) an RGBA-VAE to unify the latent representations of RGB and RGBA images; (2) a VLD-MMDiT (Variable Layers Decomposition MMDiT) architecture capable of decomposing a variable number of image layers; and (3) a Multi-stage Training strategy to adapt a pretrained image generation model into a multilayer image decomposer. Furthermore, to address the scarcity of high-quality multilayer training images, we build a pipeline to extract and annotate multilayer images from Photoshop documents (PSD). Experiments demonstrate that our method significantly surpasses existing approaches in decomposition quality and establishes a new paradigm for consistent image editing. Our code and models are released on https://github.com/QwenLM/Qwen-Image-Layered{https://github.com/QwenLM/Qwen-Image-Layered}",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.15603.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 186
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.15693",
            "authors": [
                {
                    "_id": "69436a60542d62d58a7bf65b",
                    "user": {
                        "_id": "64adfeac4beffa272dfaef21",
                        "avatarUrl": "/avatars/883f6ba38b993476115dfafcef9ce3c1.svg",
                        "isPro": false,
                        "fullname": "Yifei Li",
                        "user": "JoeLeelyf",
                        "type": "user"
                    },
                    "name": "Yifei Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-18T08:00:01.339Z",
                    "hidden": false
                },
                {
                    "_id": "69436a60542d62d58a7bf65c",
                    "name": "Wenzhao Zheng",
                    "hidden": false
                },
                {
                    "_id": "69436a60542d62d58a7bf65d",
                    "name": "Yanran Zhang",
                    "hidden": false
                },
                {
                    "_id": "69436a60542d62d58a7bf65e",
                    "name": "Runze Sun",
                    "hidden": false
                },
                {
                    "_id": "69436a60542d62d58a7bf65f",
                    "name": "Yu Zheng",
                    "hidden": false
                },
                {
                    "_id": "69436a60542d62d58a7bf660",
                    "name": "Lei Chen",
                    "hidden": false
                },
                {
                    "_id": "69436a60542d62d58a7bf661",
                    "name": "Jie Zhou",
                    "hidden": false
                },
                {
                    "_id": "69436a60542d62d58a7bf662",
                    "name": "Jiwen Lu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-17T18:48:26.000Z",
            "submittedOnDailyAt": "2025-12-18T00:31:01.463Z",
            "title": "Skyra: AI-Generated Video Detection via Grounded Artifact Reasoning",
            "submittedOnDailyBy": {
                "_id": "64adfeac4beffa272dfaef21",
                "avatarUrl": "/avatars/883f6ba38b993476115dfafcef9ce3c1.svg",
                "isPro": false,
                "fullname": "Yifei Li",
                "user": "JoeLeelyf",
                "type": "user"
            },
            "summary": "The misuse of AI-driven video generation technologies has raised serious social concerns, highlighting the urgent need for reliable AI-generated video detectors. However, most existing methods are limited to binary classification and lack the necessary explanations for human interpretation. In this paper, we present Skyra, a specialized multimodal large language model (MLLM) that identifies human-perceivable visual artifacts in AI-generated videos and leverages them as grounded evidence for both detection and explanation. To support this objective, we construct ViF-CoT-4K for Supervised Fine-Tuning (SFT), which represents the first large-scale AI-generated video artifact dataset with fine-grained human annotations. We then develop a two-stage training strategy that systematically enhances our model's spatio-temporal artifact perception, explanation capability, and detection accuracy. To comprehensively evaluate Skyra, we introduce ViF-Bench, a benchmark comprising 3K high-quality samples generated by over ten state-of-the-art video generators. Extensive experiments demonstrate that Skyra surpasses existing methods across multiple benchmarks, while our evaluation yields valuable insights for advancing explainable AI-generated video detection.",
            "upvotes": 16,
            "discussionId": "69436a60542d62d58a7bf663",
            "projectPage": "https://joeleelyf.github.io/Skyra/",
            "githubRepo": "https://github.com/JoeLeelyf/Skyra",
            "githubRepoAddedBy": "user",
            "ai_summary": "Skyra, a specialized multimodal large language model, detects and explains visual artifacts in AI-generated videos using a novel dataset and two-stage training strategy, outperforming existing methods.",
            "ai_keywords": [
                "multimodal large language model",
                "AI-generated video detectors",
                "visual artifacts",
                "ViF-CoT-4K",
                "supervised fine-tuning",
                "spatio-temporal artifact perception",
                "explanation capability",
                "detection accuracy",
                "ViF-Bench",
                "explainable AI-generated video detection"
            ],
            "githubStars": 19,
            "organization": {
                "_id": "693649ff6df58c411109e13e",
                "name": "Tsinghua-IVG",
                "fullname": "Tsinghua-IVG",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/661cfae9a853782abad2a495/dYWQZAl7ZOpDET4PRQyDD.png"
            }
        },
        "publishedAt": "2025-12-17T13:48:26.000Z",
        "title": "Skyra: AI-Generated Video Detection via Grounded Artifact Reasoning",
        "summary": "The misuse of AI-driven video generation technologies has raised serious social concerns, highlighting the urgent need for reliable AI-generated video detectors. However, most existing methods are limited to binary classification and lack the necessary explanations for human interpretation. In this paper, we present Skyra, a specialized multimodal large language model (MLLM) that identifies human-perceivable visual artifacts in AI-generated videos and leverages them as grounded evidence for both detection and explanation. To support this objective, we construct ViF-CoT-4K for Supervised Fine-Tuning (SFT), which represents the first large-scale AI-generated video artifact dataset with fine-grained human annotations. We then develop a two-stage training strategy that systematically enhances our model's spatio-temporal artifact perception, explanation capability, and detection accuracy. To comprehensively evaluate Skyra, we introduce ViF-Bench, a benchmark comprising 3K high-quality samples generated by over ten state-of-the-art video generators. Extensive experiments demonstrate that Skyra surpasses existing methods across multiple benchmarks, while our evaluation yields valuable insights for advancing explainable AI-generated video detection.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.15693.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64adfeac4beffa272dfaef21",
            "avatarUrl": "/avatars/883f6ba38b993476115dfafcef9ce3c1.svg",
            "fullname": "Yifei Li",
            "name": "JoeLeelyf",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "693649ff6df58c411109e13e",
            "name": "Tsinghua-IVG",
            "fullname": "Tsinghua-IVG",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/661cfae9a853782abad2a495/dYWQZAl7ZOpDET4PRQyDD.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.15182",
            "authors": [
                {
                    "_id": "69437830542d62d58a7bf752",
                    "user": {
                        "_id": "62676a94dacab364889bb36c",
                        "avatarUrl": "/avatars/0ead41b44957eb30564ea685ed22781a.svg",
                        "isPro": false,
                        "fullname": "SARIM HASHMI",
                        "user": "Sarim-Hash",
                        "type": "user"
                    },
                    "name": "Sarim Hashmi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-18T07:59:45.380Z",
                    "hidden": false
                },
                {
                    "_id": "69437830542d62d58a7bf753",
                    "name": "Abdelrahman Elsayed",
                    "hidden": false
                },
                {
                    "_id": "69437830542d62d58a7bf754",
                    "name": "Mohammed Talha Alam",
                    "hidden": false
                },
                {
                    "_id": "69437830542d62d58a7bf755",
                    "name": "Samuele Poppi",
                    "hidden": false
                },
                {
                    "_id": "69437830542d62d58a7bf756",
                    "name": "Nils Lukas",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-17T08:31:40.000Z",
            "submittedOnDailyAt": "2025-12-18T01:20:18.954Z",
            "title": "Robust and Calibrated Detection of Authentic Multimedia Content",
            "submittedOnDailyBy": {
                "_id": "62676a94dacab364889bb36c",
                "avatarUrl": "/avatars/0ead41b44957eb30564ea685ed22781a.svg",
                "isPro": false,
                "fullname": "SARIM HASHMI",
                "user": "Sarim-Hash",
                "type": "user"
            },
            "summary": "Generative models can synthesize highly realistic content, so-called deepfakes, that are already being misused at scale to undermine digital media authenticity. Current deepfake detection methods are unreliable for two reasons: (i) distinguishing inauthentic content post-hoc is often impossible (e.g., with memorized samples), leading to an unbounded false positive rate (FPR); and (ii) detection lacks robustness, as adversaries can adapt to known detectors with near-perfect accuracy using minimal computational resources. To address these limitations, we propose a resynthesis framework to determine if a sample is authentic or if its authenticity can be plausibly denied. We make two key contributions focusing on the high-precision, low-recall setting against efficient (i.e., compute-restricted) adversaries. First, we demonstrate that our calibrated resynthesis method is the most reliable approach for verifying authentic samples while maintaining controllable, low FPRs. Second, we show that our method achieves adversarial robustness against efficient adversaries, whereas prior methods are easily evaded under identical compute budgets. Our approach supports multiple modalities and leverages state-of-the-art inversion techniques.",
            "upvotes": 15,
            "discussionId": "69437830542d62d58a7bf757",
            "ai_summary": "A resynthesis framework enhances deepfake detection by verifying authenticity with low false positive rates and robustness against efficient adversaries, supporting multiple modalities.",
            "ai_keywords": [
                "deepfakes",
                "deepfake detection",
                "resynthesis framework",
                "false positive rate",
                "adversarial robustness",
                "inversion techniques"
            ],
            "organization": {
                "_id": "61fb9e24dc607a42af5f193f",
                "name": "MBZUAI",
                "fullname": "Mohamed Bin Zayed University of Artificial Intelligence",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1643879908583-603ab5664a944b99e81476e8.jpeg"
            }
        },
        "publishedAt": "2025-12-17T03:31:40.000Z",
        "title": "Robust and Calibrated Detection of Authentic Multimedia Content",
        "summary": "Generative models can synthesize highly realistic content, so-called deepfakes, that are already being misused at scale to undermine digital media authenticity. Current deepfake detection methods are unreliable for two reasons: (i) distinguishing inauthentic content post-hoc is often impossible (e.g., with memorized samples), leading to an unbounded false positive rate (FPR); and (ii) detection lacks robustness, as adversaries can adapt to known detectors with near-perfect accuracy using minimal computational resources. To address these limitations, we propose a resynthesis framework to determine if a sample is authentic or if its authenticity can be plausibly denied. We make two key contributions focusing on the high-precision, low-recall setting against efficient (i.e., compute-restricted) adversaries. First, we demonstrate that our calibrated resynthesis method is the most reliable approach for verifying authentic samples while maintaining controllable, low FPRs. Second, we show that our method achieves adversarial robustness against efficient adversaries, whereas prior methods are easily evaded under identical compute budgets. Our approach supports multiple modalities and leverages state-of-the-art inversion techniques.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.15182.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62676a94dacab364889bb36c",
            "avatarUrl": "/avatars/0ead41b44957eb30564ea685ed22781a.svg",
            "fullname": "SARIM HASHMI",
            "name": "Sarim-Hash",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "61fb9e24dc607a42af5f193f",
            "name": "MBZUAI",
            "fullname": "Mohamed Bin Zayed University of Artificial Intelligence",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1643879908583-603ab5664a944b99e81476e8.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.13874",
            "authors": [
                {
                    "_id": "694376f5542d62d58a7bf73e",
                    "user": {
                        "_id": "623dfe96dcda6a715304cbca",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623dfe96dcda6a715304cbca/B7V_IbQNXAcotOKE_mJQ4.jpeg",
                        "isPro": false,
                        "fullname": "Jitesh Jain",
                        "user": "praeclarumjj3",
                        "type": "user"
                    },
                    "name": "Jitesh Jain",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-18T07:59:47.204Z",
                    "hidden": false
                },
                {
                    "_id": "694376f5542d62d58a7bf73f",
                    "name": "Jialuo Li",
                    "hidden": false
                },
                {
                    "_id": "694376f5542d62d58a7bf740",
                    "name": "Zixian Ma",
                    "hidden": false
                },
                {
                    "_id": "694376f5542d62d58a7bf741",
                    "name": "Jieyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "694376f5542d62d58a7bf742",
                    "name": "Chris Dongjoo Kim",
                    "hidden": false
                },
                {
                    "_id": "694376f5542d62d58a7bf743",
                    "name": "Sangho Lee",
                    "hidden": false
                },
                {
                    "_id": "694376f5542d62d58a7bf744",
                    "name": "Rohun Tripathi",
                    "hidden": false
                },
                {
                    "_id": "694376f5542d62d58a7bf745",
                    "name": "Tanmay Gupta",
                    "hidden": false
                },
                {
                    "_id": "694376f5542d62d58a7bf746",
                    "name": "Christopher Clark",
                    "hidden": false
                },
                {
                    "_id": "694376f5542d62d58a7bf747",
                    "name": "Humphrey Shi",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/623dfe96dcda6a715304cbca/KCKV4KgcOblJiEDVnbS22.mp4"
            ],
            "publishedAt": "2025-12-15T20:14:19.000Z",
            "submittedOnDailyAt": "2025-12-18T01:09:31.107Z",
            "title": "SAGE: Training Smart Any-Horizon Agents for Long Video Reasoning with Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "623dfe96dcda6a715304cbca",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623dfe96dcda6a715304cbca/B7V_IbQNXAcotOKE_mJQ4.jpeg",
                "isPro": false,
                "fullname": "Jitesh Jain",
                "user": "praeclarumjj3",
                "type": "user"
            },
            "summary": "As humans, we are natural any-horizon reasoners, i.e., we can decide whether to iteratively skim long videos or watch short ones in full when necessary for a given task. With this in mind, one would expect video reasoning models to reason flexibly across different durations. However, SOTA models are still trained to predict answers in a single turn while processing a large number of frames, akin to watching an entire long video, requiring significant resources. This raises the question: Is it possible to develop performant any-horizon video reasoning systems? Inspired by human behavior, we first propose SAGE, an agent system that performs multi-turn reasoning on long videos while handling simpler problems in a single turn. Secondly, we introduce an easy synthetic data generation pipeline using Gemini-2.5-Flash to train the orchestrator, SAGE-MM, which lies at the core of SAGE. We further propose an effective RL post-training recipe essential for instilling any-horizon reasoning ability in SAGE-MM. Thirdly, we curate SAGE-Bench with an average duration of greater than 700 seconds for evaluating video reasoning ability in real-world entertainment use cases. Lastly, we empirically validate the effectiveness of our system, data, and RL recipe, observing notable improvements of up to 6.1% on open-ended video reasoning tasks, as well as an impressive 8.2% improvement on videos longer than 10 minutes.",
            "upvotes": 13,
            "discussionId": "694376f6542d62d58a7bf748",
            "projectPage": "https://praeclarumjj3.github.io/sage/",
            "githubRepo": "https://github.com/allenai/SAGE",
            "githubRepoAddedBy": "user",
            "ai_summary": "The paper proposes SAGE, a multi-turn reasoning system for video that mimics human behavior, using synthetic data and reinforcement learning to improve performance on long videos.",
            "ai_keywords": [
                "SAGE",
                "multi-turn reasoning",
                "Gemini-2.5-Flash",
                "SAGE-MM",
                "reinforcement learning",
                "SAGE-Bench",
                "open-ended video reasoning"
            ],
            "githubStars": 22,
            "organization": {
                "_id": "5e70f3648ce3c604d78fe132",
                "name": "allenai",
                "fullname": "Ai2",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/652db071b62cf1f8463221e2/CxxwFiaomTa1MCX_B7-pT.png"
            }
        },
        "publishedAt": "2025-12-15T15:14:19.000Z",
        "title": "SAGE: Training Smart Any-Horizon Agents for Long Video Reasoning with Reinforcement Learning",
        "summary": "As humans, we are natural any-horizon reasoners, i.e., we can decide whether to iteratively skim long videos or watch short ones in full when necessary for a given task. With this in mind, one would expect video reasoning models to reason flexibly across different durations. However, SOTA models are still trained to predict answers in a single turn while processing a large number of frames, akin to watching an entire long video, requiring significant resources. This raises the question: Is it possible to develop performant any-horizon video reasoning systems? Inspired by human behavior, we first propose SAGE, an agent system that performs multi-turn reasoning on long videos while handling simpler problems in a single turn. Secondly, we introduce an easy synthetic data generation pipeline using Gemini-2.5-Flash to train the orchestrator, SAGE-MM, which lies at the core of SAGE. We further propose an effective RL post-training recipe essential for instilling any-horizon reasoning ability in SAGE-MM. Thirdly, we curate SAGE-Bench with an average duration of greater than 700 seconds for evaluating video reasoning ability in real-world entertainment use cases. Lastly, we empirically validate the effectiveness of our system, data, and RL recipe, observing notable improvements of up to 6.1% on open-ended video reasoning tasks, as well as an impressive 8.2% improvement on videos longer than 10 minutes.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/623dfe96dcda6a715304cbca/KCKV4KgcOblJiEDVnbS22.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13874.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "623dfe96dcda6a715304cbca",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623dfe96dcda6a715304cbca/B7V_IbQNXAcotOKE_mJQ4.jpeg",
            "fullname": "Jitesh Jain",
            "name": "praeclarumjj3",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "organization": {
            "_id": "5e70f3648ce3c604d78fe132",
            "name": "allenai",
            "fullname": "Ai2",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/652db071b62cf1f8463221e2/CxxwFiaomTa1MCX_B7-pT.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.15687",
            "authors": [
                {
                    "_id": "69436bcb542d62d58a7bf682",
                    "user": {
                        "_id": "62ffa3f8311cad266f9af236",
                        "avatarUrl": "/avatars/203dac40bc546ee25a01d8715a4b3049.svg",
                        "isPro": false,
                        "fullname": "Zhenwen Liang",
                        "user": "invokerliang",
                        "type": "user"
                    },
                    "name": "Zhenwen Liang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-18T07:59:59.027Z",
                    "hidden": false
                },
                {
                    "_id": "69436bcb542d62d58a7bf683",
                    "user": {
                        "_id": "65f2a015f79caa96bcde9de4",
                        "avatarUrl": "/avatars/b989383d43162a42e264001ff940ea73.svg",
                        "isPro": false,
                        "fullname": "Sidi Lu",
                        "user": "desire2020",
                        "type": "user"
                    },
                    "name": "Sidi Lu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-18T07:59:56.635Z",
                    "hidden": false
                },
                {
                    "_id": "69436bcb542d62d58a7bf684",
                    "name": "Wenhao Yu",
                    "hidden": false
                },
                {
                    "_id": "69436bcb542d62d58a7bf685",
                    "name": "Kishan Panaganti",
                    "hidden": false
                },
                {
                    "_id": "69436bcb542d62d58a7bf686",
                    "name": "Yujun Zhou",
                    "hidden": false
                },
                {
                    "_id": "69436bcb542d62d58a7bf687",
                    "name": "Haitao Mi",
                    "hidden": false
                },
                {
                    "_id": "69436bcb542d62d58a7bf688",
                    "name": "Dong Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-17T18:44:45.000Z",
            "submittedOnDailyAt": "2025-12-18T00:22:58.457Z",
            "title": "Can LLMs Guide Their Own Exploration? Gradient-Guided Reinforcement Learning for LLM Reasoning",
            "submittedOnDailyBy": {
                "_id": "65a05abf07184d32fa002d41",
                "avatarUrl": "/avatars/3a23e7e568d2024381ed31b56c1c461a.svg",
                "isPro": false,
                "fullname": "Yujun Zhou",
                "user": "yujunzhou",
                "type": "user"
            },
            "summary": "Reinforcement learning has become essential for strengthening the reasoning abilities of large language models, yet current exploration mechanisms remain fundamentally misaligned with how these models actually learn. Entropy bonuses and external semantic comparators encourage surface level variation but offer no guarantee that sampled trajectories differ in the update directions that shape optimization. We propose G2RL, a gradient guided reinforcement learning framework in which exploration is driven not by external heuristics but by the model own first order update geometry. For each response, G2RL constructs a sequence level feature from the model final layer sensitivity, obtainable at negligible cost from a standard forward pass, and measures how each trajectory would reshape the policy by comparing these features within a sampled group. Trajectories that introduce novel gradient directions receive a bounded multiplicative reward scaler, while redundant or off manifold updates are deemphasized, yielding a self referential exploration signal that is naturally aligned with PPO style stability and KL control. Across math and general reasoning benchmarks (MATH500, AMC, AIME24, AIME25, GPQA, MMLUpro) on Qwen3 base 1.7B and 4B models, G2RL consistently improves pass@1, maj@16, and pass@k over entropy based GRPO and external embedding methods. Analyzing the induced geometry, we find that G2RL expands exploration into substantially more orthogonal and often opposing gradient directions while maintaining semantic coherence, revealing that a policy own update space provides a far more faithful and effective basis for guiding exploration in large language model reinforcement learning.",
            "upvotes": 12,
            "discussionId": "69436bcc542d62d58a7bf689",
            "ai_summary": "G2RL, a gradient-guided reinforcement learning framework, enhances exploration in large language models by leveraging the model's own update geometry, leading to improved performance on various reasoning benchmarks.",
            "ai_keywords": [
                "gradient guided reinforcement learning",
                "entropy bonuses",
                "semantic comparators",
                "surface level variation",
                "update directions",
                "sequence level feature",
                "model sensitivity",
                "policy reshaping",
                "trajectory reward scaler",
                "PPO style stability",
                "KL control",
                "math and general reasoning benchmarks",
                "Qwen3",
                "pass@1",
                "maj@16",
                "pass@k",
                "entropy based GRPO",
                "external embedding methods",
                "orthogonal gradient directions",
                "semantic coherence"
            ],
            "organization": {
                "_id": "66543b6e420092799d2f625c",
                "name": "tencent",
                "fullname": "Tencent",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
            }
        },
        "publishedAt": "2025-12-17T13:44:45.000Z",
        "title": "Can LLMs Guide Their Own Exploration? Gradient-Guided Reinforcement Learning for LLM Reasoning",
        "summary": "Reinforcement learning has become essential for strengthening the reasoning abilities of large language models, yet current exploration mechanisms remain fundamentally misaligned with how these models actually learn. Entropy bonuses and external semantic comparators encourage surface level variation but offer no guarantee that sampled trajectories differ in the update directions that shape optimization. We propose G2RL, a gradient guided reinforcement learning framework in which exploration is driven not by external heuristics but by the model own first order update geometry. For each response, G2RL constructs a sequence level feature from the model final layer sensitivity, obtainable at negligible cost from a standard forward pass, and measures how each trajectory would reshape the policy by comparing these features within a sampled group. Trajectories that introduce novel gradient directions receive a bounded multiplicative reward scaler, while redundant or off manifold updates are deemphasized, yielding a self referential exploration signal that is naturally aligned with PPO style stability and KL control. Across math and general reasoning benchmarks (MATH500, AMC, AIME24, AIME25, GPQA, MMLUpro) on Qwen3 base 1.7B and 4B models, G2RL consistently improves pass@1, maj@16, and pass@k over entropy based GRPO and external embedding methods. Analyzing the induced geometry, we find that G2RL expands exploration into substantially more orthogonal and often opposing gradient directions while maintaining semantic coherence, revealing that a policy own update space provides a far more faithful and effective basis for guiding exploration in large language model reinforcement learning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.15687.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65a05abf07184d32fa002d41",
            "avatarUrl": "/avatars/3a23e7e568d2024381ed31b56c1c461a.svg",
            "fullname": "Yujun Zhou",
            "name": "yujunzhou",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "66543b6e420092799d2f625c",
            "name": "tencent",
            "fullname": "Tencent",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.13884",
            "authors": [
                {
                    "_id": "694283a25d5b2dc105274a8f",
                    "name": "Jonas Golde",
                    "hidden": false
                },
                {
                    "_id": "694283a25d5b2dc105274a90",
                    "name": "Patrick Haller",
                    "hidden": false
                },
                {
                    "_id": "694283a25d5b2dc105274a91",
                    "name": "Alan Akbik",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-15T20:36:39.000Z",
            "submittedOnDailyAt": "2025-12-18T10:02:08.678Z",
            "title": "FiNERweb: Datasets and Artifacts for Scalable Multilingual Named Entity Recognition",
            "submittedOnDailyBy": {
                "_id": "6454fd79e4952d1c6cb4281e",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6454fd79e4952d1c6cb4281e/_vXaQ-Qx8Z-AUtYtw76dV.jpeg",
                "isPro": false,
                "fullname": "Jonas Golde",
                "user": "whoisjones",
                "type": "user"
            },
            "summary": "Recent multilingual named entity recognition (NER) work has shown that large language models (LLMs) can provide effective synthetic supervision, yet such datasets have mostly appeared as by-products of broader experiments rather than as systematic, reusable resources. We introduce FiNERweb, a dataset-creation pipeline that scales the teacher-student paradigm to 91 languages and 25 scripts. Building on FineWeb-Edu, our approach trains regression models to identify NER-relevant passages and annotates them with multilingual LLMs, resulting in about 225k passages with 235k distinct entity labels. Our experiments show that the regression model achieves more than 84 F1, and that models trained on FiNERweb obtain comparable or improved performance in zero shot transfer settings on English, Thai, and Swahili, despite being trained on 19x less data than strong baselines. In addition, we assess annotation quality using LLM-as-a-judge and observe consistently high scores for both faithfulness (3.99 out of 5) and completeness (4.05 out of 5), indicating reliable and informative annotations. Further, we release the dataset with both English labels and translated label sets in the respective target languages because we observe that the performance of current state-of-the-art models drops by 0.02 to 0.09 F1 when evaluated using target language labels instead of English ones. We release FiNERweb together with all accompanying artifacts to the research community in order to facilitate more effective student-teacher training for multilingual named entity recognition.",
            "upvotes": 12,
            "discussionId": "694283a25d5b2dc105274a92",
            "githubRepo": "https://github.com/whoisjones/FiNERweb",
            "githubRepoAddedBy": "user",
            "githubStars": 5,
            "organization": {
                "_id": "5fce0bd40931aa16b3c5dc97",
                "name": "flair",
                "fullname": "flair",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1607338919026-5fce0b030931aa16b3c5dc94.png"
            }
        },
        "publishedAt": "2025-12-15T15:36:39.000Z",
        "title": "FiNERweb: Datasets and Artifacts for Scalable Multilingual Named Entity Recognition",
        "summary": "Recent multilingual named entity recognition (NER) work has shown that large language models (LLMs) can provide effective synthetic supervision, yet such datasets have mostly appeared as by-products of broader experiments rather than as systematic, reusable resources. We introduce FiNERweb, a dataset-creation pipeline that scales the teacher-student paradigm to 91 languages and 25 scripts. Building on FineWeb-Edu, our approach trains regression models to identify NER-relevant passages and annotates them with multilingual LLMs, resulting in about 225k passages with 235k distinct entity labels. Our experiments show that the regression model achieves more than 84 F1, and that models trained on FiNERweb obtain comparable or improved performance in zero shot transfer settings on English, Thai, and Swahili, despite being trained on 19x less data than strong baselines. In addition, we assess annotation quality using LLM-as-a-judge and observe consistently high scores for both faithfulness (3.99 out of 5) and completeness (4.05 out of 5), indicating reliable and informative annotations. Further, we release the dataset with both English labels and translated label sets in the respective target languages because we observe that the performance of current state-of-the-art models drops by 0.02 to 0.09 F1 when evaluated using target language labels instead of English ones. We release FiNERweb together with all accompanying artifacts to the research community in order to facilitate more effective student-teacher training for multilingual named entity recognition.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13884.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6454fd79e4952d1c6cb4281e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6454fd79e4952d1c6cb4281e/_vXaQ-Qx8Z-AUtYtw76dV.jpeg",
            "fullname": "Jonas Golde",
            "name": "whoisjones",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "organization": {
            "_id": "5fce0bd40931aa16b3c5dc97",
            "name": "flair",
            "fullname": "flair",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1607338919026-5fce0b030931aa16b3c5dc94.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.10863",
            "authors": [
                {
                    "_id": "6942c775fb33037a39577dcc",
                    "user": {
                        "_id": "666b0210b307f882e0724b7d",
                        "avatarUrl": "/avatars/80ad8ac0a4cec885158607fc634c1291.svg",
                        "isPro": false,
                        "fullname": "Jingli Lin",
                        "user": "rbler",
                        "type": "user"
                    },
                    "name": "Jingli Lin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-18T08:00:14.115Z",
                    "hidden": false
                },
                {
                    "_id": "6942c775fb33037a39577dcd",
                    "name": "Runsen Xu",
                    "hidden": false
                },
                {
                    "_id": "6942c775fb33037a39577dce",
                    "name": "Shaohao Zhu",
                    "hidden": false
                },
                {
                    "_id": "6942c775fb33037a39577dcf",
                    "name": "Sihan Yang",
                    "hidden": false
                },
                {
                    "_id": "6942c775fb33037a39577dd0",
                    "name": "Peizhou Cao",
                    "hidden": false
                },
                {
                    "_id": "6942c775fb33037a39577dd1",
                    "name": "Yunlong Ran",
                    "hidden": false
                },
                {
                    "_id": "6942c775fb33037a39577dd2",
                    "name": "Miao Hu",
                    "hidden": false
                },
                {
                    "_id": "6942c775fb33037a39577dd3",
                    "name": "Chenming Zhu",
                    "hidden": false
                },
                {
                    "_id": "6942c775fb33037a39577dd4",
                    "name": "Yiman Xie",
                    "hidden": false
                },
                {
                    "_id": "6942c775fb33037a39577dd5",
                    "name": "Yilin Long",
                    "hidden": false
                },
                {
                    "_id": "6942c775fb33037a39577dd6",
                    "name": "Wenbo Hu",
                    "hidden": false
                },
                {
                    "_id": "6942c775fb33037a39577dd7",
                    "name": "Dahua Lin",
                    "hidden": false
                },
                {
                    "_id": "6942c775fb33037a39577dd8",
                    "name": "Tai Wang",
                    "hidden": false
                },
                {
                    "_id": "6942c775fb33037a39577dd9",
                    "name": "Jiangmiao Pang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/666b0210b307f882e0724b7d/mfwppM9eRQr9mhjcCuTNu.mp4"
            ],
            "publishedAt": "2025-12-11T17:57:24.000Z",
            "submittedOnDailyAt": "2025-12-18T03:51:32.723Z",
            "title": "MMSI-Video-Bench: A Holistic Benchmark for Video-Based Spatial Intelligence",
            "submittedOnDailyBy": {
                "_id": "666b0210b307f882e0724b7d",
                "avatarUrl": "/avatars/80ad8ac0a4cec885158607fc634c1291.svg",
                "isPro": false,
                "fullname": "Jingli Lin",
                "user": "rbler",
                "type": "user"
            },
            "summary": "Spatial understanding over continuous visual input is crucial for MLLMs to evolve into general-purpose assistants in physical environments. Yet there is still no comprehensive benchmark that holistically assesses the progress toward this goal. In this work, we introduce MMSI-Video-Bench, a fully human-annotated benchmark for video-based spatial intelligence in MLLMs. It operationalizes a four-level framework, Perception, Planning, Prediction, and Cross-Video Reasoning, through 1,106 questions grounded in 1,278 clips from 25 datasets and in-house videos. Each item is carefully designed and reviewed by 3DV experts with explanatory rationales to ensure precise, unambiguous grounding. Leveraging its diverse data sources and holistic task coverage, MMSI-Video-Bench also supports three domain-oriented sub-benchmarks (Indoor Scene Perception Bench, Robot Bench and Grounding Bench) for targeted capability assessment. We evaluate 25 strong open-source and proprietary MLLMs, revealing a striking human--AI gap: many models perform near chance, and the best reasoning model lags humans by nearly 60%. We further find that spatially fine-tuned models still fail to generalize effectively on our benchmark. Fine-grained error analysis exposes systematic failures in geometric reasoning, motion grounding, long-horizon prediction, and cross-video correspondence. We also show that typical frame-sampling strategies transfer poorly to our reasoning-intensive benchmark, and that neither 3D spatial cues nor chain-of-thought prompting yields meaningful gains. We expect our benchmark to establish a solid testbed for advancing video-based spatial intelligence.",
            "upvotes": 12,
            "discussionId": "6942c776fb33037a39577dda",
            "ai_summary": "MMSI-Video-Bench is a comprehensive benchmark for video-based spatial intelligence in MLLMs, revealing significant gaps between human and AI performance and highlighting challenges in geometric reasoning, motion grounding, and cross-video correspondence.",
            "ai_keywords": [
                "MMSI-Video-Bench",
                "spatial intelligence",
                "MLLMs",
                "Perception",
                "Planning",
                "Prediction",
                "Cross-Video Reasoning",
                "3DV experts",
                "fine-grained error analysis",
                "geometric reasoning",
                "motion grounding",
                "long-horizon prediction",
                "cross-video correspondence",
                "frame-sampling strategies",
                "3D spatial cues",
                "chain-of-thought prompting"
            ]
        },
        "publishedAt": "2025-12-11T12:57:24.000Z",
        "title": "MMSI-Video-Bench: A Holistic Benchmark for Video-Based Spatial Intelligence",
        "summary": "Spatial understanding over continuous visual input is crucial for MLLMs to evolve into general-purpose assistants in physical environments. Yet there is still no comprehensive benchmark that holistically assesses the progress toward this goal. In this work, we introduce MMSI-Video-Bench, a fully human-annotated benchmark for video-based spatial intelligence in MLLMs. It operationalizes a four-level framework, Perception, Planning, Prediction, and Cross-Video Reasoning, through 1,106 questions grounded in 1,278 clips from 25 datasets and in-house videos. Each item is carefully designed and reviewed by 3DV experts with explanatory rationales to ensure precise, unambiguous grounding. Leveraging its diverse data sources and holistic task coverage, MMSI-Video-Bench also supports three domain-oriented sub-benchmarks (Indoor Scene Perception Bench, Robot Bench and Grounding Bench) for targeted capability assessment. We evaluate 25 strong open-source and proprietary MLLMs, revealing a striking human--AI gap: many models perform near chance, and the best reasoning model lags humans by nearly 60%. We further find that spatially fine-tuned models still fail to generalize effectively on our benchmark. Fine-grained error analysis exposes systematic failures in geometric reasoning, motion grounding, long-horizon prediction, and cross-video correspondence. We also show that typical frame-sampling strategies transfer poorly to our reasoning-intensive benchmark, and that neither 3D spatial cues nor chain-of-thought prompting yields meaningful gains. We expect our benchmark to establish a solid testbed for advancing video-based spatial intelligence.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/666b0210b307f882e0724b7d/mfwppM9eRQr9mhjcCuTNu.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.10863.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "666b0210b307f882e0724b7d",
            "avatarUrl": "/avatars/80ad8ac0a4cec885158607fc634c1291.svg",
            "fullname": "Jingli Lin",
            "name": "rbler",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.15713",
            "authors": [
                {
                    "_id": "69437656542d62d58a7bf736",
                    "user": {
                        "_id": "6375dfa7f9aafd41ce145254",
                        "avatarUrl": "/avatars/19e7460dd7ea6c60e1c52d5707660cc8.svg",
                        "isPro": false,
                        "fullname": "Lunbin Zeng",
                        "user": "xiazhi",
                        "type": "user"
                    },
                    "name": "Lunbin Zeng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-18T07:59:49.237Z",
                    "hidden": false
                },
                {
                    "_id": "69437656542d62d58a7bf737",
                    "name": "Jingfeng Yao",
                    "hidden": false
                },
                {
                    "_id": "69437656542d62d58a7bf738",
                    "name": "Bencheng Liao",
                    "hidden": false
                },
                {
                    "_id": "69437656542d62d58a7bf739",
                    "user": {
                        "_id": "66a105bb456284adf458d656",
                        "avatarUrl": "/avatars/b543a324f7e159d6e84bc68915e93d24.svg",
                        "isPro": false,
                        "fullname": "Tao Hongyuan",
                        "user": "HongyuanTao",
                        "type": "user"
                    },
                    "name": "Hongyuan Tao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-18T07:59:50.879Z",
                    "hidden": false
                },
                {
                    "_id": "69437656542d62d58a7bf73a",
                    "name": "Wenyu Liu",
                    "hidden": false
                },
                {
                    "_id": "69437656542d62d58a7bf73b",
                    "name": "Xinggang Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-17T18:59:55.000Z",
            "submittedOnDailyAt": "2025-12-18T01:05:13.622Z",
            "title": "DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models",
            "submittedOnDailyBy": {
                "_id": "6375dfa7f9aafd41ce145254",
                "avatarUrl": "/avatars/19e7460dd7ea6c60e1c52d5707660cc8.svg",
                "isPro": false,
                "fullname": "Lunbin Zeng",
                "user": "xiazhi",
                "type": "user"
            },
            "summary": "In recent multimodal research, the diffusion paradigm has emerged as a promising alternative to the autoregressive paradigm (AR), owing to its unique decoding advantages. However, due to the capability limitations of the base diffusion language model, the performance of the diffusion vision language model (dVLM) still lags significantly behind that of mainstream models. This leads to a simple yet fundamental question: Is it possible to construct dVLMs based on existing powerful AR models? In response, we propose DiffusionVL, a dVLM family that could be translated from any powerful AR models. Through simple fine-tuning, we successfully adapt AR pre-trained models into the diffusion paradigm. This approach yields two key observations: (1) The paradigm shift from AR-based multimodal models to diffusion is remarkably effective. (2) Direct conversion of an AR language model to a dVLM is also feasible, achieving performance competitive with LLaVA-style visual-instruction-tuning. Further, we introduce a block-decoding design into dVLMs that supports arbitrary-length generation and KV cache reuse, achieving a significant inference speedup. We conduct a large number of experiments. Despite training with less than 5% of the data required by prior methods, DiffusionVL achieves a comprehensive performance improvement-a 34.4% gain on the MMMU-Pro (vision) bench and 37.5% gain on the MME (Cog.) bench-alongside a 2x inference speedup. The model and code are released at https://github.com/hustvl/DiffusionVL.",
            "upvotes": 11,
            "discussionId": "69437656542d62d58a7bf73c",
            "githubRepo": "https://github.com/hustvl/DiffusionVL",
            "githubRepoAddedBy": "user",
            "ai_summary": "DiffusionVL, a family of diffusion vision language models derived from autoregressive models through fine-tuning, achieves performance improvements and faster inference speeds compared to existing models.",
            "ai_keywords": [
                "diffusion paradigm",
                "autoregressive paradigm",
                "diffusion vision language model",
                "dVLM",
                "DiffusionVL",
                "fine-tuning",
                "block-decoding design",
                "KV cache reuse",
                "MMMU-Pro",
                "MME",
                "Cog."
            ],
            "githubStars": 30,
            "organization": {
                "_id": "62600e67ffe8827cb1d6180b",
                "name": "hustvl",
                "fullname": "HUST Vision Lab"
            }
        },
        "publishedAt": "2025-12-17T13:59:55.000Z",
        "title": "DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models",
        "summary": "In recent multimodal research, the diffusion paradigm has emerged as a promising alternative to the autoregressive paradigm (AR), owing to its unique decoding advantages. However, due to the capability limitations of the base diffusion language model, the performance of the diffusion vision language model (dVLM) still lags significantly behind that of mainstream models. This leads to a simple yet fundamental question: Is it possible to construct dVLMs based on existing powerful AR models? In response, we propose DiffusionVL, a dVLM family that could be translated from any powerful AR models. Through simple fine-tuning, we successfully adapt AR pre-trained models into the diffusion paradigm. This approach yields two key observations: (1) The paradigm shift from AR-based multimodal models to diffusion is remarkably effective. (2) Direct conversion of an AR language model to a dVLM is also feasible, achieving performance competitive with LLaVA-style visual-instruction-tuning. Further, we introduce a block-decoding design into dVLMs that supports arbitrary-length generation and KV cache reuse, achieving a significant inference speedup. We conduct a large number of experiments. Despite training with less than 5% of the data required by prior methods, DiffusionVL achieves a comprehensive performance improvement-a 34.4% gain on the MMMU-Pro (vision) bench and 37.5% gain on the MME (Cog.) bench-alongside a 2x inference speedup. The model and code are released at https://github.com/hustvl/DiffusionVL.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.15713.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6375dfa7f9aafd41ce145254",
            "avatarUrl": "/avatars/19e7460dd7ea6c60e1c52d5707660cc8.svg",
            "fullname": "Lunbin Zeng",
            "name": "xiazhi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "62600e67ffe8827cb1d6180b",
            "name": "hustvl",
            "fullname": "HUST Vision Lab"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.12072",
            "authors": [
                {
                    "_id": "6941b5d65d5b2dc105274718",
                    "user": {
                        "_id": "65c129488aedd6edd26d3e35",
                        "avatarUrl": "/avatars/897cb5bd4915d6ded23419d5d12623db.svg",
                        "isPro": false,
                        "fullname": "Avinash Amballa",
                        "user": "AvinashAmballa",
                        "type": "user"
                    },
                    "name": "Avinash Amballa",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-16T22:32:37.221Z",
                    "hidden": false
                },
                {
                    "_id": "6941b5d65d5b2dc105274719",
                    "name": "Yashas Malur Saidutta",
                    "hidden": false
                },
                {
                    "_id": "6941b5d65d5b2dc10527471a",
                    "name": "Chi-Heng Lin",
                    "hidden": false
                },
                {
                    "_id": "6941b5d65d5b2dc10527471b",
                    "name": "Vivek Kulkarni",
                    "hidden": false
                },
                {
                    "_id": "6941b5d65d5b2dc10527471c",
                    "name": "Srinivas Chappidi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-12T22:39:01.000Z",
            "submittedOnDailyAt": "2025-12-18T04:43:59.725Z",
            "title": "VOYAGER: A Training Free Approach for Generating Diverse Datasets using LLMs",
            "submittedOnDailyBy": {
                "_id": "65c129488aedd6edd26d3e35",
                "avatarUrl": "/avatars/897cb5bd4915d6ded23419d5d12623db.svg",
                "isPro": false,
                "fullname": "Avinash Amballa",
                "user": "AvinashAmballa",
                "type": "user"
            },
            "summary": "Large language models (LLMs) are increasingly being used to generate synthetic datasets for the evaluation and training of downstream models. However, prior work has noted that such generated data lacks diversity. In this paper, we propose Voyager, a novel principled approach to generate diverse datasets. Our approach is iterative and directly optimizes a mathematical quantity that optimizes the diversity of the dataset using the machinery of determinantal point processes. Furthermore, our approach is training-free, applicable to closed-source models, and scalable. In addition to providing theoretical justification for the working of our method, we also demonstrate through comprehensive experiments that Voyager significantly outperforms popular baseline approaches by providing a 1.5-3x improvement in diversity.",
            "upvotes": 11,
            "discussionId": "6941b5d75d5b2dc10527471d",
            "ai_summary": "Voyager is a method that uses determinantal point processes to iteratively generate diverse synthetic datasets for model evaluation and training.",
            "ai_keywords": [
                "determinantal point processes"
            ],
            "organization": {
                "_id": "686df54910a52f2c2cf03c06",
                "name": "SamsungResearch",
                "fullname": "Samsung Research",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60ffc3e62403168abcae811d/lBrkzrpjrJ8k-3CGLKRLr.jpeg"
            }
        },
        "publishedAt": "2025-12-12T17:39:01.000Z",
        "title": "VOYAGER: A Training Free Approach for Generating Diverse Datasets using LLMs",
        "summary": "Large language models (LLMs) are increasingly being used to generate synthetic datasets for the evaluation and training of downstream models. However, prior work has noted that such generated data lacks diversity. In this paper, we propose Voyager, a novel principled approach to generate diverse datasets. Our approach is iterative and directly optimizes a mathematical quantity that optimizes the diversity of the dataset using the machinery of determinantal point processes. Furthermore, our approach is training-free, applicable to closed-source models, and scalable. In addition to providing theoretical justification for the working of our method, we also demonstrate through comprehensive experiments that Voyager significantly outperforms popular baseline approaches by providing a 1.5-3x improvement in diversity.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.12072.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65c129488aedd6edd26d3e35",
            "avatarUrl": "/avatars/897cb5bd4915d6ded23419d5d12623db.svg",
            "fullname": "Avinash Amballa",
            "name": "AvinashAmballa",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "686df54910a52f2c2cf03c06",
            "name": "SamsungResearch",
            "fullname": "Samsung Research",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60ffc3e62403168abcae811d/lBrkzrpjrJ8k-3CGLKRLr.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.15702",
            "authors": [
                {
                    "_id": "69437cdb542d62d58a7bf766",
                    "name": "Yuwei Guo",
                    "hidden": false
                },
                {
                    "_id": "69437cdb542d62d58a7bf767",
                    "name": "Ceyuan Yang",
                    "hidden": false
                },
                {
                    "_id": "69437cdb542d62d58a7bf768",
                    "name": "Hao He",
                    "hidden": false
                },
                {
                    "_id": "69437cdb542d62d58a7bf769",
                    "name": "Yang Zhao",
                    "hidden": false
                },
                {
                    "_id": "69437cdb542d62d58a7bf76a",
                    "name": "Meng Wei",
                    "hidden": false
                },
                {
                    "_id": "69437cdb542d62d58a7bf76b",
                    "name": "Zhenheng Yang",
                    "hidden": false
                },
                {
                    "_id": "69437cdb542d62d58a7bf76c",
                    "name": "Weilin Huang",
                    "hidden": false
                },
                {
                    "_id": "69437cdb542d62d58a7bf76d",
                    "name": "Dahua Lin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-17T18:53:29.000Z",
            "submittedOnDailyAt": "2025-12-18T01:34:14.444Z",
            "title": "End-to-End Training for Autoregressive Video Diffusion via Self-Resampling",
            "submittedOnDailyBy": {
                "_id": "6371f83d5ffa922d638ef486",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6371f83d5ffa922d638ef486/UzWl3xhVvtol3Rz1mjQ9t.jpeg",
                "isPro": false,
                "fullname": "Yuwei Guo",
                "user": "guoyww",
                "type": "user"
            },
            "summary": "Autoregressive video diffusion models hold promise for world simulation but are vulnerable to exposure bias arising from the train-test mismatch. While recent works address this via post-training, they typically rely on a bidirectional teacher model or online discriminator. To achieve an end-to-end solution, we introduce Resampling Forcing, a teacher-free framework that enables training autoregressive video models from scratch and at scale. Central to our approach is a self-resampling scheme that simulates inference-time model errors on history frames during training. Conditioned on these degraded histories, a sparse causal mask enforces temporal causality while enabling parallel training with frame-level diffusion loss. To facilitate efficient long-horizon generation, we further introduce history routing, a parameter-free mechanism that dynamically retrieves the top-k most relevant history frames for each query. Experiments demonstrate that our approach achieves performance comparable to distillation-based baselines while exhibiting superior temporal consistency on longer videos owing to native-length training.",
            "upvotes": 8,
            "discussionId": "69437cdb542d62d58a7bf76e",
            "projectPage": "https://guoyww.github.io/projects/resampling-forcing/",
            "ai_summary": "Resampling Forcing is introduced as a teacher-free framework to train autoregressive video diffusion models with improved temporal consistency using self-resampling and history routing.",
            "ai_keywords": [
                "autoregressive video diffusion models",
                "exposure bias",
                "train-test mismatch",
                "Resampling Forcing",
                "self-resampling scheme",
                "sparse causal mask",
                "frame-level diffusion loss",
                "history routing",
                "temporal consistency"
            ],
            "organization": {
                "_id": "67d1140985ea0644e2f14b99",
                "name": "ByteDance-Seed",
                "fullname": "ByteDance Seed",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
            }
        },
        "publishedAt": "2025-12-17T13:53:29.000Z",
        "title": "End-to-End Training for Autoregressive Video Diffusion via Self-Resampling",
        "summary": "Autoregressive video diffusion models hold promise for world simulation but are vulnerable to exposure bias arising from the train-test mismatch. While recent works address this via post-training, they typically rely on a bidirectional teacher model or online discriminator. To achieve an end-to-end solution, we introduce Resampling Forcing, a teacher-free framework that enables training autoregressive video models from scratch and at scale. Central to our approach is a self-resampling scheme that simulates inference-time model errors on history frames during training. Conditioned on these degraded histories, a sparse causal mask enforces temporal causality while enabling parallel training with frame-level diffusion loss. To facilitate efficient long-horizon generation, we further introduce history routing, a parameter-free mechanism that dynamically retrieves the top-k most relevant history frames for each query. Experiments demonstrate that our approach achieves performance comparable to distillation-based baselines while exhibiting superior temporal consistency on longer videos owing to native-length training.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.15702.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6371f83d5ffa922d638ef486",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6371f83d5ffa922d638ef486/UzWl3xhVvtol3Rz1mjQ9t.jpeg",
            "fullname": "Yuwei Guo",
            "name": "guoyww",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 203
        },
        "organization": {
            "_id": "67d1140985ea0644e2f14b99",
            "name": "ByteDance-Seed",
            "fullname": "ByteDance Seed",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.09299",
            "authors": [
                {
                    "_id": "69436b5c542d62d58a7bf677",
                    "name": "Daili Hua",
                    "hidden": false
                },
                {
                    "_id": "69436b5c542d62d58a7bf678",
                    "name": "Xizhi Wang",
                    "hidden": false
                },
                {
                    "_id": "69436b5c542d62d58a7bf679",
                    "name": "Bohan Zeng",
                    "hidden": false
                },
                {
                    "_id": "69436b5c542d62d58a7bf67a",
                    "name": "Xinyi Huang",
                    "hidden": false
                },
                {
                    "_id": "69436b5c542d62d58a7bf67b",
                    "name": "Hao Liang",
                    "hidden": false
                },
                {
                    "_id": "69436b5c542d62d58a7bf67c",
                    "name": "Junbo Niu",
                    "hidden": false
                },
                {
                    "_id": "69436b5c542d62d58a7bf67d",
                    "name": "Xinlong Chen",
                    "hidden": false
                },
                {
                    "_id": "69436b5c542d62d58a7bf67e",
                    "name": "Quanqing Xu",
                    "hidden": false
                },
                {
                    "_id": "69436b5c542d62d58a7bf67f",
                    "name": "Wentao Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-10T03:57:29.000Z",
            "submittedOnDailyAt": "2025-12-18T00:18:50.316Z",
            "title": "VABench: A Comprehensive Benchmark for Audio-Video Generation",
            "submittedOnDailyBy": {
                "_id": "6671214c92412fd4640714eb",
                "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg",
                "isPro": false,
                "fullname": "bohan zeng",
                "user": "zbhpku",
                "type": "user"
            },
            "summary": "Recent advances in video generation have been remarkable, enabling models to produce visually compelling videos with synchronized audio. While existing video generation benchmarks provide comprehensive metrics for visual quality, they lack convincing evaluations for audio-video generation, especially for models aiming to generate synchronized audio-video outputs. To address this gap, we introduce VABench, a comprehensive and multi-dimensional benchmark framework designed to systematically evaluate the capabilities of synchronous audio-video generation. VABench encompasses three primary task types: text-to-audio-video (T2AV), image-to-audio-video (I2AV), and stereo audio-video generation. It further establishes two major evaluation modules covering 15 dimensions. These dimensions specifically assess pairwise similarities (text-video, text-audio, video-audio), audio-video synchronization, lip-speech consistency, and carefully curated audio and video question-answering (QA) pairs, among others. Furthermore, VABench covers seven major content categories: animals, human sounds, music, environmental sounds, synchronous physical sounds, complex scenes, and virtual worlds. We provide a systematic analysis and visualization of the evaluation results, aiming to establish a new standard for assessing video generation models with synchronous audio capabilities and to promote the comprehensive advancement of the field.",
            "upvotes": 7,
            "discussionId": "69436b5c542d62d58a7bf680",
            "githubRepo": "https://github.com/tanABCC/VABench",
            "githubRepoAddedBy": "user",
            "ai_summary": "VABench is a benchmark framework for evaluating audio-video generation models, covering text-to-audio-video, image-to-audio-video, and stereo audio-video tasks with 15 evaluation dimensions.",
            "ai_keywords": [
                "text-to-audio-video",
                "image-to-audio-video",
                "stereo audio-video generation",
                "pairwise similarities",
                "audio-video synchronization",
                "lip-speech consistency",
                "question-answering pairs",
                "animals",
                "human sounds",
                "music",
                "environmental sounds",
                "synchronous physical sounds",
                "complex scenes",
                "virtual worlds"
            ],
            "githubStars": 4,
            "organization": {
                "_id": "61dcd8e344f59573371b5cb6",
                "name": "PekingUniversity",
                "fullname": "Peking University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
            }
        },
        "publishedAt": "2025-12-09T22:57:29.000Z",
        "title": "VABench: A Comprehensive Benchmark for Audio-Video Generation",
        "summary": "Recent advances in video generation have been remarkable, enabling models to produce visually compelling videos with synchronized audio. While existing video generation benchmarks provide comprehensive metrics for visual quality, they lack convincing evaluations for audio-video generation, especially for models aiming to generate synchronized audio-video outputs. To address this gap, we introduce VABench, a comprehensive and multi-dimensional benchmark framework designed to systematically evaluate the capabilities of synchronous audio-video generation. VABench encompasses three primary task types: text-to-audio-video (T2AV), image-to-audio-video (I2AV), and stereo audio-video generation. It further establishes two major evaluation modules covering 15 dimensions. These dimensions specifically assess pairwise similarities (text-video, text-audio, video-audio), audio-video synchronization, lip-speech consistency, and carefully curated audio and video question-answering (QA) pairs, among others. Furthermore, VABench covers seven major content categories: animals, human sounds, music, environmental sounds, synchronous physical sounds, complex scenes, and virtual worlds. We provide a systematic analysis and visualization of the evaluation results, aiming to establish a new standard for assessing video generation models with synchronous audio capabilities and to promote the comprehensive advancement of the field.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.09299.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6671214c92412fd4640714eb",
            "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg",
            "fullname": "bohan zeng",
            "name": "zbhpku",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "organization": {
            "_id": "61dcd8e344f59573371b5cb6",
            "name": "PekingUniversity",
            "fullname": "Peking University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.15715",
            "authors": [
                {
                    "_id": "694370f5542d62d58a7bf6a0",
                    "name": "Lihe Yang",
                    "hidden": false
                },
                {
                    "_id": "694370f5542d62d58a7bf6a1",
                    "name": "Shang-Wen Li",
                    "hidden": false
                },
                {
                    "_id": "694370f5542d62d58a7bf6a2",
                    "name": "Yang Li",
                    "hidden": false
                },
                {
                    "_id": "694370f5542d62d58a7bf6a3",
                    "name": "Xinjie Lei",
                    "hidden": false
                },
                {
                    "_id": "694370f5542d62d58a7bf6a4",
                    "name": "Dong Wang",
                    "hidden": false
                },
                {
                    "_id": "694370f5542d62d58a7bf6a5",
                    "name": "Abdelrahman Mohamed",
                    "hidden": false
                },
                {
                    "_id": "694370f5542d62d58a7bf6a6",
                    "name": "Hengshuang Zhao",
                    "hidden": false
                },
                {
                    "_id": "694370f5542d62d58a7bf6a7",
                    "name": "Hu Xu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-17T18:59:58.000Z",
            "submittedOnDailyAt": "2025-12-18T00:56:18.397Z",
            "title": "In Pursuit of Pixel Supervision for Visual Pre-training",
            "submittedOnDailyBy": {
                "_id": "65a3a0342548c41ad9f4e4e7",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65a3a0342548c41ad9f4e4e7/80Yod9z7O95nC-Y0o-5UI.jpeg",
                "isPro": false,
                "fullname": "Lihe Yang",
                "user": "LiheYoung",
                "type": "user"
            },
            "summary": "At the most basic level, pixels are the source of the visual information through which we perceive the world. Pixels contain information at all levels, ranging from low-level attributes to high-level concepts. Autoencoders represent a classical and long-standing paradigm for learning representations from pixels or other raw inputs. In this work, we demonstrate that autoencoder-based self-supervised learning remains competitive today and can produce strong representations for downstream tasks, while remaining simple, stable, and efficient. Our model, codenamed \"Pixio\", is an enhanced masked autoencoder (MAE) with more challenging pre-training tasks and more capable architectures. The model is trained on 2B web-crawled images with a self-curation strategy with minimal human curation. Pixio performs competitively across a wide range of downstream tasks in the wild, including monocular depth estimation (e.g., Depth Anything), feed-forward 3D reconstruction (i.e., MapAnything), semantic segmentation, and robot learning, outperforming or matching DINOv3 trained at similar scales. Our results suggest that pixel-space self-supervised learning can serve as a promising alternative and a complement to latent-space approaches.",
            "upvotes": 6,
            "discussionId": "694370f5542d62d58a7bf6a8",
            "projectPage": "https://github.com/facebookresearch/pixio",
            "githubRepo": "https://github.com/facebookresearch/pixio",
            "githubRepoAddedBy": "user",
            "ai_summary": "Pixio, an enhanced masked autoencoder, demonstrates competitive performance across various downstream tasks using pixel-space self-supervised learning, outperforming latent-space approaches.",
            "ai_keywords": [
                "autoencoders",
                "masked autoencoder",
                "self-supervised learning",
                "monocular depth estimation",
                "feed-forward 3D reconstruction",
                "semantic segmentation",
                "robot learning",
                "DINOv3"
            ],
            "githubStars": 50
        },
        "publishedAt": "2025-12-17T13:59:58.000Z",
        "title": "In Pursuit of Pixel Supervision for Visual Pre-training",
        "summary": "At the most basic level, pixels are the source of the visual information through which we perceive the world. Pixels contain information at all levels, ranging from low-level attributes to high-level concepts. Autoencoders represent a classical and long-standing paradigm for learning representations from pixels or other raw inputs. In this work, we demonstrate that autoencoder-based self-supervised learning remains competitive today and can produce strong representations for downstream tasks, while remaining simple, stable, and efficient. Our model, codenamed \"Pixio\", is an enhanced masked autoencoder (MAE) with more challenging pre-training tasks and more capable architectures. The model is trained on 2B web-crawled images with a self-curation strategy with minimal human curation. Pixio performs competitively across a wide range of downstream tasks in the wild, including monocular depth estimation (e.g., Depth Anything), feed-forward 3D reconstruction (i.e., MapAnything), semantic segmentation, and robot learning, outperforming or matching DINOv3 trained at similar scales. Our results suggest that pixel-space self-supervised learning can serve as a promising alternative and a complement to latent-space approaches.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.15715.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65a3a0342548c41ad9f4e4e7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65a3a0342548c41ad9f4e4e7/80Yod9z7O95nC-Y0o-5UI.jpeg",
            "fullname": "Lihe Yang",
            "name": "LiheYoung",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 108
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.15649",
            "authors": [
                {
                    "_id": "69436833542d62d58a7bf648",
                    "user": {
                        "_id": "64100834c025ddf6189c415e",
                        "avatarUrl": "/avatars/9b9bbecef5d5815540abf92d74012f55.svg",
                        "isPro": false,
                        "fullname": "Hongbo Zhao",
                        "user": "z-hb",
                        "type": "user"
                    },
                    "name": "Hongbo Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-18T08:00:05.012Z",
                    "hidden": false
                },
                {
                    "_id": "69436833542d62d58a7bf649",
                    "user": {
                        "_id": "655698848d9e07913d6c8620",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655698848d9e07913d6c8620/UeLV4MGfOmpEHJHKFms2r.jpeg",
                        "isPro": false,
                        "fullname": "Meng Wang",
                        "user": "Moenupa",
                        "type": "user"
                    },
                    "name": "Meng Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-18T08:00:03.214Z",
                    "hidden": false
                },
                {
                    "_id": "69436833542d62d58a7bf64a",
                    "name": "Fei Zhu",
                    "hidden": false
                },
                {
                    "_id": "69436833542d62d58a7bf64b",
                    "name": "Wenzhuo Liu",
                    "hidden": false
                },
                {
                    "_id": "69436833542d62d58a7bf64c",
                    "name": "Bolin Ni",
                    "hidden": false
                },
                {
                    "_id": "69436833542d62d58a7bf64d",
                    "name": "Fanhu Zeng",
                    "hidden": false
                },
                {
                    "_id": "69436833542d62d58a7bf64e",
                    "name": "Gaofeng Meng",
                    "hidden": false
                },
                {
                    "_id": "69436833542d62d58a7bf64f",
                    "name": "Zhaoxiang Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-17T17:58:35.000Z",
            "submittedOnDailyAt": "2025-12-18T00:15:18.107Z",
            "title": "VTCBench: Can Vision-Language Models Understand Long Context with Vision-Text Compression?",
            "submittedOnDailyBy": {
                "_id": "64100834c025ddf6189c415e",
                "avatarUrl": "/avatars/9b9bbecef5d5815540abf92d74012f55.svg",
                "isPro": false,
                "fullname": "Hongbo Zhao",
                "user": "z-hb",
                "type": "user"
            },
            "summary": "The computational and memory overheads associated with expanding the context window of LLMs severely limit their scalability. A noteworthy solution is vision-text compression (VTC), exemplified by frameworks like DeepSeek-OCR and Glyph, which convert long texts into dense 2D visual representations, thereby achieving token compression ratios of 3x-20x. However, the impact of this high information density on the core long-context capabilities of vision-language models (VLMs) remains under-investigated. To address this gap, we introduce the first benchmark for VTC and systematically assess the performance of VLMs across three long-context understanding settings: VTC-Retrieval, which evaluates the model's ability to retrieve and aggregate information; VTC-Reasoning, which requires models to infer latent associations to locate facts with minimal lexical overlap; and VTC-Memory, which measures comprehensive question answering within long-term dialogue memory. Furthermore, we establish the VTCBench-Wild to simulate diverse input scenarios.We comprehensively evaluate leading open-source and proprietary models on our benchmarks. The results indicate that, despite being able to decode textual information (e.g., OCR) well, most VLMs exhibit a surprisingly poor long-context understanding ability with VTC-compressed information, failing to capture long associations or dependencies in the context.This study provides a deep understanding of VTC and serves as a foundation for designing more efficient and scalable VLMs.",
            "upvotes": 5,
            "discussionId": "69436834542d62d58a7bf650",
            "projectPage": "https://github.com/Moenupa/VTCBench",
            "githubRepo": "https://github.com/Moenupa/VTCBench",
            "githubRepoAddedBy": "user",
            "ai_summary": "A benchmark evaluates the performance of vision-language models on understanding long-context information compressed into dense visual representations, revealing significant limitations in capturing long-term dependencies.",
            "ai_keywords": [
                "vision-text compression",
                "VTC",
                "DeepSeek-OCR",
                "Glyph",
                "token compression",
                "vision-language models",
                "VLMs",
                "VTC-Retrieval",
                "VTC-Reasoning",
                "VTC-Memory",
                "VTCBench-Wild",
                "OCR",
                "long-context understanding"
            ],
            "githubStars": 5,
            "organization": {
                "_id": "640a887796aae649741a586f",
                "name": "CASIA",
                "fullname": "Chinese Academic of Science Institute of Automation",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1678411888885-6388984e8a5dbe2f3dc5afee.jpeg"
            }
        },
        "publishedAt": "2025-12-17T12:58:35.000Z",
        "title": "VTCBench: Can Vision-Language Models Understand Long Context with Vision-Text Compression?",
        "summary": "The computational and memory overheads associated with expanding the context window of LLMs severely limit their scalability. A noteworthy solution is vision-text compression (VTC), exemplified by frameworks like DeepSeek-OCR and Glyph, which convert long texts into dense 2D visual representations, thereby achieving token compression ratios of 3x-20x. However, the impact of this high information density on the core long-context capabilities of vision-language models (VLMs) remains under-investigated. To address this gap, we introduce the first benchmark for VTC and systematically assess the performance of VLMs across three long-context understanding settings: VTC-Retrieval, which evaluates the model's ability to retrieve and aggregate information; VTC-Reasoning, which requires models to infer latent associations to locate facts with minimal lexical overlap; and VTC-Memory, which measures comprehensive question answering within long-term dialogue memory. Furthermore, we establish the VTCBench-Wild to simulate diverse input scenarios.We comprehensively evaluate leading open-source and proprietary models on our benchmarks. The results indicate that, despite being able to decode textual information (e.g., OCR) well, most VLMs exhibit a surprisingly poor long-context understanding ability with VTC-compressed information, failing to capture long associations or dependencies in the context.This study provides a deep understanding of VTC and serves as a foundation for designing more efficient and scalable VLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.15649.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "64100834c025ddf6189c415e",
            "avatarUrl": "/avatars/9b9bbecef5d5815540abf92d74012f55.svg",
            "fullname": "Hongbo Zhao",
            "name": "z-hb",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "640a887796aae649741a586f",
            "name": "CASIA",
            "fullname": "Chinese Academic of Science Institute of Automation",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1678411888885-6388984e8a5dbe2f3dc5afee.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.15110",
            "authors": [
                {
                    "_id": "69437484542d62d58a7bf727",
                    "name": "Jialong Zuo",
                    "hidden": false
                },
                {
                    "_id": "69437484542d62d58a7bf728",
                    "name": "Haoyou Deng",
                    "hidden": false
                },
                {
                    "_id": "69437484542d62d58a7bf729",
                    "name": "Hanyu Zhou",
                    "hidden": false
                },
                {
                    "_id": "69437484542d62d58a7bf72a",
                    "name": "Jiaxin Zhu",
                    "hidden": false
                },
                {
                    "_id": "69437484542d62d58a7bf72b",
                    "name": "Yicheng Zhang",
                    "hidden": false
                },
                {
                    "_id": "69437484542d62d58a7bf72c",
                    "name": "Yiwei Zhang",
                    "hidden": false
                },
                {
                    "_id": "69437484542d62d58a7bf72d",
                    "name": "Yongxin Yan",
                    "hidden": false
                },
                {
                    "_id": "69437484542d62d58a7bf72e",
                    "name": "Kaixing Huang",
                    "hidden": false
                },
                {
                    "_id": "69437484542d62d58a7bf72f",
                    "name": "Weisen Chen",
                    "hidden": false
                },
                {
                    "_id": "69437484542d62d58a7bf730",
                    "name": "Yongtai Deng",
                    "hidden": false
                },
                {
                    "_id": "69437484542d62d58a7bf731",
                    "name": "Rui Jin",
                    "hidden": false
                },
                {
                    "_id": "69437484542d62d58a7bf732",
                    "name": "Nong Sang",
                    "hidden": false
                },
                {
                    "_id": "69437484542d62d58a7bf733",
                    "name": "Changxin Gao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-17T06:02:25.000Z",
            "submittedOnDailyAt": "2025-12-18T00:57:28.457Z",
            "title": "Is Nano Banana Pro a Low-Level Vision All-Rounder? A Comprehensive Evaluation on 14 Tasks and 40 Datasets",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "The rapid evolution of text-to-image generation models has revolutionized visual content creation. While commercial products like Nano Banana Pro have garnered significant attention, their potential as generalist solvers for traditional low-level vision challenges remains largely underexplored. In this study, we investigate the critical question: Is Nano Banana Pro a Low-Level Vision All-Rounder? We conducted a comprehensive zero-shot evaluation across 14 distinct low-level tasks spanning 40 diverse datasets. By utilizing simple textual prompts without fine-tuning, we benchmarked Nano Banana Pro against state-of-the-art specialist models. Our extensive analysis reveals a distinct performance dichotomy: while Nano Banana Pro demonstrates superior subjective visual quality, often hallucinating plausible high-frequency details that surpass specialist models, it lags behind in traditional reference-based quantitative metrics. We attribute this discrepancy to the inherent stochasticity of generative models, which struggle to maintain the strict pixel-level consistency required by conventional metrics. This report identifies Nano Banana Pro as a capable zero-shot contender for low-level vision tasks, while highlighting that achieving the high fidelity of domain specialists remains a significant hurdle.",
            "upvotes": 5,
            "discussionId": "69437484542d62d58a7bf734",
            "projectPage": "https://lowlevelbanana.github.io/",
            "ai_summary": "Nano Banana Pro excels in subjective visual quality across low-level vision tasks without fine-tuning but struggles with traditional reference-based quantitative metrics due to generative model stochasticity.",
            "ai_keywords": [
                "text-to-image generation",
                "low-level vision",
                "zero-shot evaluation",
                "textual prompts",
                "reference-based quantitative metrics",
                "generative models",
                "stochasticity",
                "pixel-level consistency"
            ]
        },
        "publishedAt": "2025-12-17T01:02:25.000Z",
        "title": "Is Nano Banana Pro a Low-Level Vision All-Rounder? A Comprehensive Evaluation on 14 Tasks and 40 Datasets",
        "summary": "The rapid evolution of text-to-image generation models has revolutionized visual content creation. While commercial products like Nano Banana Pro have garnered significant attention, their potential as generalist solvers for traditional low-level vision challenges remains largely underexplored. In this study, we investigate the critical question: Is Nano Banana Pro a Low-Level Vision All-Rounder? We conducted a comprehensive zero-shot evaluation across 14 distinct low-level tasks spanning 40 diverse datasets. By utilizing simple textual prompts without fine-tuning, we benchmarked Nano Banana Pro against state-of-the-art specialist models. Our extensive analysis reveals a distinct performance dichotomy: while Nano Banana Pro demonstrates superior subjective visual quality, often hallucinating plausible high-frequency details that surpass specialist models, it lags behind in traditional reference-based quantitative metrics. We attribute this discrepancy to the inherent stochasticity of generative models, which struggle to maintain the strict pixel-level consistency required by conventional metrics. This report identifies Nano Banana Pro as a capable zero-shot contender for low-level vision tasks, while highlighting that achieving the high fidelity of domain specialists remains a significant hurdle.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.15110.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 186
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.13190",
            "authors": [
                {
                    "_id": "694191a02de1a8e32ad666ce",
                    "user": {
                        "_id": "69418f2b68f48f199f47c0fc",
                        "avatarUrl": "/avatars/2e8618043c8a79c18b4492c8bf7f9ea8.svg",
                        "isPro": false,
                        "fullname": "Jin Sob Kim",
                        "user": "sadPororo",
                        "type": "user"
                    },
                    "name": "Jin Sob Kim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-16T22:32:48.469Z",
                    "hidden": false
                },
                {
                    "_id": "694191a02de1a8e32ad666cf",
                    "name": "Hyun Joon Park",
                    "hidden": false
                },
                {
                    "_id": "694191a02de1a8e32ad666d0",
                    "name": "Wooseok Shin",
                    "hidden": false
                },
                {
                    "_id": "694191a02de1a8e32ad666d1",
                    "name": "Dongil Park",
                    "hidden": false
                },
                {
                    "_id": "694191a02de1a8e32ad666d2",
                    "name": "Sung Won Han",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-15T10:55:20.000Z",
            "submittedOnDailyAt": "2025-12-18T03:29:07.692Z",
            "title": "WAY: Estimation of Vessel Destination in Worldwide AIS Trajectory",
            "submittedOnDailyBy": {
                "_id": "69418f2b68f48f199f47c0fc",
                "avatarUrl": "/avatars/2e8618043c8a79c18b4492c8bf7f9ea8.svg",
                "isPro": false,
                "fullname": "Jin Sob Kim",
                "user": "sadPororo",
                "type": "user"
            },
            "summary": "The Automatic Identification System (AIS) enables data-driven maritime surveillance but suffers from reliability issues and irregular intervals. We address vessel destination estimation using global-scope AIS data by proposing a differentiated approach that recasts long port-to-port trajectories as a nested sequence structure. Using spatial grids, this method mitigates spatio-temporal bias while preserving detailed resolution. We introduce a novel deep learning architecture, WAY, designed to process these reformulated trajectories for long-term destination estimation days to weeks in advance. WAY comprises a trajectory representation layer and Channel-Aggregative Sequential Processing (CASP) blocks. The representation layer generates multi-channel vector sequences from kinematic and non-kinematic features. CASP blocks utilize multi-headed channel- and self-attention for aggregation and sequential information delivery. Additionally, we propose a task-specialized Gradient Dropout (GD) technique to enable many-to-many training on single labels, preventing biased feedback surges by stochastically blocking gradient flow based on sample length. Experiments on 5-year AIS data demonstrate WAY's superiority over conventional spatial grid-based approaches regardless of trajectory progression. Results further confirm that adopting GD leads to performance gains. Finally, we explore WAY's potential for real-world application through multitask learning for ETA estimation.",
            "upvotes": 5,
            "discussionId": "694191a12de1a8e32ad666d3",
            "githubRepo": "https://github.com/sadPororo/WAY",
            "githubRepoAddedBy": "user",
            "ai_summary": "A novel deep learning architecture, WAY, uses nested sequence structures and spatial grids for accurate long-term vessel destination estimation from AIS data, incorporating CASP blocks and Gradient Dropout for improved performance.",
            "ai_keywords": [
                "WAY",
                "trajectory representation layer",
                "Channel-Aggregative Sequential Processing (CASP)",
                "multi-headed channel-attention",
                "multi-headed self-attention",
                "Gradient Dropout (GD)"
            ],
            "githubStars": 3
        },
        "publishedAt": "2025-12-15T05:55:20.000Z",
        "title": "WAY: Estimation of Vessel Destination in Worldwide AIS Trajectory",
        "summary": "The Automatic Identification System (AIS) enables data-driven maritime surveillance but suffers from reliability issues and irregular intervals. We address vessel destination estimation using global-scope AIS data by proposing a differentiated approach that recasts long port-to-port trajectories as a nested sequence structure. Using spatial grids, this method mitigates spatio-temporal bias while preserving detailed resolution. We introduce a novel deep learning architecture, WAY, designed to process these reformulated trajectories for long-term destination estimation days to weeks in advance. WAY comprises a trajectory representation layer and Channel-Aggregative Sequential Processing (CASP) blocks. The representation layer generates multi-channel vector sequences from kinematic and non-kinematic features. CASP blocks utilize multi-headed channel- and self-attention for aggregation and sequential information delivery. Additionally, we propose a task-specialized Gradient Dropout (GD) technique to enable many-to-many training on single labels, preventing biased feedback surges by stochastically blocking gradient flow based on sample length. Experiments on 5-year AIS data demonstrate WAY's superiority over conventional spatial grid-based approaches regardless of trajectory progression. Results further confirm that adopting GD leads to performance gains. Finally, we explore WAY's potential for real-world application through multitask learning for ETA estimation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13190.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "69418f2b68f48f199f47c0fc",
            "avatarUrl": "/avatars/2e8618043c8a79c18b4492c8bf7f9ea8.svg",
            "fullname": "Jin Sob Kim",
            "name": "sadPororo",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.15699",
            "authors": [
                {
                    "_id": "69446a6bfbf17e708e185e64",
                    "name": "Qiuyang Mang",
                    "hidden": false
                },
                {
                    "_id": "69446a6bfbf17e708e185e65",
                    "name": "Wenhao Chai",
                    "hidden": false
                },
                {
                    "_id": "69446a6bfbf17e708e185e66",
                    "name": "Zhifei Li",
                    "hidden": false
                },
                {
                    "_id": "69446a6bfbf17e708e185e67",
                    "name": "Huanzhi Mao",
                    "hidden": false
                },
                {
                    "_id": "69446a6bfbf17e708e185e68",
                    "name": "Shang Zhou",
                    "hidden": false
                },
                {
                    "_id": "69446a6bfbf17e708e185e69",
                    "name": "Alexander Du",
                    "hidden": false
                },
                {
                    "_id": "69446a6bfbf17e708e185e6a",
                    "name": "Hanchen Li",
                    "hidden": false
                },
                {
                    "_id": "69446a6bfbf17e708e185e6b",
                    "name": "Shu Liu",
                    "hidden": false
                },
                {
                    "_id": "69446a6bfbf17e708e185e6c",
                    "name": "Edwin Chen",
                    "hidden": false
                },
                {
                    "_id": "69446a6bfbf17e708e185e6d",
                    "name": "Yichuan Wang",
                    "hidden": false
                },
                {
                    "_id": "69446a6bfbf17e708e185e6e",
                    "name": "Xieting Chu",
                    "hidden": false
                },
                {
                    "_id": "69446a6bfbf17e708e185e6f",
                    "name": "Zerui Cheng",
                    "hidden": false
                },
                {
                    "_id": "69446a6bfbf17e708e185e70",
                    "name": "Yuan Xu",
                    "hidden": false
                },
                {
                    "_id": "69446a6bfbf17e708e185e71",
                    "name": "Tian Xia",
                    "hidden": false
                },
                {
                    "_id": "69446a6bfbf17e708e185e72",
                    "name": "Zirui Wang",
                    "hidden": false
                },
                {
                    "_id": "69446a6bfbf17e708e185e73",
                    "name": "Tianneng Shi",
                    "hidden": false
                },
                {
                    "_id": "69446a6bfbf17e708e185e74",
                    "name": "Jianzhu Yao",
                    "hidden": false
                },
                {
                    "_id": "69446a6bfbf17e708e185e75",
                    "name": "Yilong Zhao",
                    "hidden": false
                },
                {
                    "_id": "69446a6bfbf17e708e185e76",
                    "name": "Qizheng Zhang",
                    "hidden": false
                },
                {
                    "_id": "69446a6bfbf17e708e185e77",
                    "name": "Charlie Ruan",
                    "hidden": false
                },
                {
                    "_id": "69446a6bfbf17e708e185e78",
                    "name": "Zeyu Shen",
                    "hidden": false
                },
                {
                    "_id": "69446a6bfbf17e708e185e79",
                    "name": "Kaiyuan Liu",
                    "hidden": false
                },
                {
                    "_id": "69446a6bfbf17e708e185e7a",
                    "name": "Runyuan He",
                    "hidden": false
                },
                {
                    "_id": "69446a6bfbf17e708e185e7b",
                    "name": "Dong Xing",
                    "hidden": false
                },
                {
                    "_id": "69446a6bfbf17e708e185e7c",
                    "name": "Zerui Li",
                    "hidden": false
                },
                {
                    "_id": "69446a6bfbf17e708e185e7d",
                    "name": "Zirong Zeng",
                    "hidden": false
                },
                {
                    "_id": "69446a6bfbf17e708e185e7e",
                    "name": "Yige Jiang",
                    "hidden": false
                },
                {
                    "_id": "69446a6bfbf17e708e185e7f",
                    "name": "Lufeng Cheng",
                    "hidden": false
                },
                {
                    "_id": "69446a6bfbf17e708e185e80",
                    "name": "Ziyi Zhao",
                    "hidden": false
                },
                {
                    "_id": "69446a6bfbf17e708e185e81",
                    "name": "Youran Sun",
                    "hidden": false
                },
                {
                    "_id": "69446a6bfbf17e708e185e82",
                    "name": "Wesley Zheng",
                    "hidden": false
                },
                {
                    "_id": "69446a6bfbf17e708e185e83",
                    "name": "Meiyuwang Zhang",
                    "hidden": false
                },
                {
                    "_id": "69446a6bfbf17e708e185e84",
                    "name": "Ruyi Ji",
                    "hidden": false
                },
                {
                    "_id": "69446a6bfbf17e708e185e85",
                    "name": "Xuechang Tu",
                    "hidden": false
                },
                {
                    "_id": "69446a6bfbf17e708e185e86",
                    "name": "Zihan Zheng",
                    "hidden": false
                },
                {
                    "_id": "69446a6bfbf17e708e185e87",
                    "name": "Zexing Chen",
                    "hidden": false
                },
                {
                    "_id": "69446a6bfbf17e708e185e88",
                    "name": "Kangyang Zhou",
                    "hidden": false
                },
                {
                    "_id": "69446a6bfbf17e708e185e89",
                    "name": "Zhaozi Wang",
                    "hidden": false
                },
                {
                    "_id": "69446a6bfbf17e708e185e8a",
                    "name": "Jingbang Chen",
                    "hidden": false
                },
                {
                    "_id": "69446a6bfbf17e708e185e8b",
                    "name": "Aleksandra Korolova",
                    "hidden": false
                },
                {
                    "_id": "69446a6bfbf17e708e185e8c",
                    "name": "Peter Henderson",
                    "hidden": false
                },
                {
                    "_id": "69446a6bfbf17e708e185e8d",
                    "name": "Pramod Viswanath",
                    "hidden": false
                },
                {
                    "_id": "69446a6bfbf17e708e185e8e",
                    "name": "Vijay Ganesh",
                    "hidden": false
                },
                {
                    "_id": "69446a6bfbf17e708e185e8f",
                    "name": "Saining Xie",
                    "hidden": false
                },
                {
                    "_id": "69446a6bfbf17e708e185e90",
                    "name": "Zhuang Liu",
                    "hidden": false
                },
                {
                    "_id": "69446a6bfbf17e708e185e91",
                    "name": "Dawn Song",
                    "hidden": false
                },
                {
                    "_id": "69446a6bfbf17e708e185e92",
                    "name": "Sewon Min",
                    "hidden": false
                },
                {
                    "_id": "69446a6bfbf17e708e185e93",
                    "name": "Ion Stoica",
                    "hidden": false
                },
                {
                    "_id": "69446a6bfbf17e708e185e94",
                    "name": "Joseph E. Gonzalez",
                    "hidden": false
                },
                {
                    "_id": "69446a6bfbf17e708e185e95",
                    "name": "Jingbo Shang",
                    "hidden": false
                },
                {
                    "_id": "69446a6bfbf17e708e185e96",
                    "name": "Alvin Cheung",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-17T18:52:45.000Z",
            "submittedOnDailyAt": "2025-12-18T18:26:59.894Z",
            "title": "FrontierCS: Evolving Challenges for Evolving Intelligence",
            "submittedOnDailyBy": {
                "_id": "637c7503fe115289cfecbe6b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676361945047-637c7503fe115289cfecbe6b.jpeg",
                "isPro": false,
                "fullname": "Wenhao Chai",
                "user": "wchai",
                "type": "user"
            },
            "summary": "We introduce FrontierCS, a benchmark of 156 open-ended problems across diverse areas of computer science, designed and reviewed by experts, including CS PhDs and top-tier competitive programming participants and problem setters. Unlike existing benchmarks that focus on tasks with known optimal solutions, FrontierCS targets problems where the optimal solution is unknown, but the quality of a solution can be objectively evaluated. Models solve these tasks by implementing executable programs rather than outputting a direct answer. FrontierCS includes algorithmic problems, which are often NP-hard variants of competitive programming problems with objective partial scoring, and research problems with the same property. For each problem we provide an expert reference solution and an automatic evaluator. Combining open-ended design, measurable progress, and expert curation, FrontierCS provides a benchmark at the frontier of computer-science difficulty. Empirically, we find that frontier reasoning models still lag far behind human experts on both the algorithmic and research tracks, that increasing reasoning budgets alone does not close this gap, and that models often over-optimize for generating merely workable code instead of discovering high-quality algorithms and system designs.",
            "upvotes": 4,
            "discussionId": "69446a6bfbf17e708e185e97",
            "ai_summary": "FrontierCS is a benchmark for evaluating models on open-ended computer science problems with unknown optimal solutions, where tasks involve implementing executable programs.",
            "ai_keywords": [
                "algorithmic problems",
                "NP-hard",
                "competitive programming",
                "objective partial scoring",
                "research problems",
                "expert reference solution",
                "automatic evaluator",
                "frontier reasoning models"
            ]
        },
        "publishedAt": "2025-12-17T13:52:45.000Z",
        "title": "FrontierCS: Evolving Challenges for Evolving Intelligence",
        "summary": "We introduce FrontierCS, a benchmark of 156 open-ended problems across diverse areas of computer science, designed and reviewed by experts, including CS PhDs and top-tier competitive programming participants and problem setters. Unlike existing benchmarks that focus on tasks with known optimal solutions, FrontierCS targets problems where the optimal solution is unknown, but the quality of a solution can be objectively evaluated. Models solve these tasks by implementing executable programs rather than outputting a direct answer. FrontierCS includes algorithmic problems, which are often NP-hard variants of competitive programming problems with objective partial scoring, and research problems with the same property. For each problem we provide an expert reference solution and an automatic evaluator. Combining open-ended design, measurable progress, and expert curation, FrontierCS provides a benchmark at the frontier of computer-science difficulty. Empirically, we find that frontier reasoning models still lag far behind human experts on both the algorithmic and research tracks, that increasing reasoning budgets alone does not close this gap, and that models often over-optimize for generating merely workable code instead of discovering high-quality algorithms and system designs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.15699.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "637c7503fe115289cfecbe6b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676361945047-637c7503fe115289cfecbe6b.jpeg",
            "fullname": "Wenhao Chai",
            "name": "wchai",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 41
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.15374",
            "authors": [
                {
                    "_id": "69436b43542d62d58a7bf665",
                    "name": "Zehua Pei",
                    "hidden": false
                },
                {
                    "_id": "69436b43542d62d58a7bf666",
                    "name": "Hui-Ling Zhen",
                    "hidden": false
                },
                {
                    "_id": "69436b43542d62d58a7bf667",
                    "name": "Shixiong Kai",
                    "hidden": false
                },
                {
                    "_id": "69436b43542d62d58a7bf668",
                    "name": "Sinno Jialin Pan",
                    "hidden": false
                },
                {
                    "_id": "69436b43542d62d58a7bf669",
                    "name": "Yunhe Wang",
                    "hidden": false
                },
                {
                    "_id": "69436b43542d62d58a7bf66a",
                    "name": "Mingxuan Yuan",
                    "hidden": false
                },
                {
                    "_id": "69436b43542d62d58a7bf66b",
                    "name": "Bei Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-17T12:25:05.000Z",
            "submittedOnDailyAt": "2025-12-18T00:19:42.276Z",
            "title": "SCOPE: Prompt Evolution for Enhancing Agent Effectiveness",
            "submittedOnDailyBy": {
                "_id": "6527c063e86758eb6ca800a1",
                "avatarUrl": "/avatars/9091be87eea518209c1de9eebfa663c0.svg",
                "isPro": false,
                "fullname": "JarvisPei",
                "user": "Eleven-P",
                "type": "user"
            },
            "summary": "Large Language Model (LLM) agents are increasingly deployed in environments that generate massive, dynamic contexts. However, a critical bottleneck remains: while agents have access to this context, their static prompts lack the mechanisms to manage it effectively, leading to recurring Corrective and Enhancement failures. To address this capability gap, we introduce SCOPE (Self-evolving Context Optimization via Prompt Evolution). SCOPE frames context management as an online optimization problem, synthesizing guidelines from execution traces to automatically evolve the agent's prompt. We propose a Dual-Stream mechanism that balances tactical specificity (resolving immediate errors) with strategic generality (evolving long-term principles). Furthermore, we introduce Perspective-Driven Exploration to maximize strategy coverage, increasing the likelihood that the agent has the correct strategy for any given task. Experiments on the HLE benchmark show that SCOPE improves task success rates from 14.23\\% to 38.64\\% without human intervention. We make our code publicly available at https://github.com/JarvisPei/SCOPE.",
            "upvotes": 4,
            "discussionId": "69436b43542d62d58a7bf66c",
            "githubRepo": "https://github.com/JarvisPei/SCOPE",
            "githubRepoAddedBy": "user",
            "ai_summary": "SCOPE enhances LLM agents' context management through prompt evolution, improving task success rates in dynamic environments without human intervention.",
            "ai_keywords": [
                "Large Language Model (LLM)",
                "context management",
                "online optimization",
                "execution traces",
                "prompt evolution",
                "Dual-Stream mechanism",
                "tactical specificity",
                "strategic generality",
                "Perspective-Driven Exploration",
                "HLE benchmark"
            ],
            "githubStars": 4
        },
        "publishedAt": "2025-12-17T07:25:05.000Z",
        "title": "SCOPE: Prompt Evolution for Enhancing Agent Effectiveness",
        "summary": "Large Language Model (LLM) agents are increasingly deployed in environments that generate massive, dynamic contexts. However, a critical bottleneck remains: while agents have access to this context, their static prompts lack the mechanisms to manage it effectively, leading to recurring Corrective and Enhancement failures. To address this capability gap, we introduce SCOPE (Self-evolving Context Optimization via Prompt Evolution). SCOPE frames context management as an online optimization problem, synthesizing guidelines from execution traces to automatically evolve the agent's prompt. We propose a Dual-Stream mechanism that balances tactical specificity (resolving immediate errors) with strategic generality (evolving long-term principles). Furthermore, we introduce Perspective-Driven Exploration to maximize strategy coverage, increasing the likelihood that the agent has the correct strategy for any given task. Experiments on the HLE benchmark show that SCOPE improves task success rates from 14.23\\% to 38.64\\% without human intervention. We make our code publicly available at https://github.com/JarvisPei/SCOPE.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.15374.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6527c063e86758eb6ca800a1",
            "avatarUrl": "/avatars/9091be87eea518209c1de9eebfa663c0.svg",
            "fullname": "JarvisPei",
            "name": "Eleven-P",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.14202",
            "authors": [
                {
                    "_id": "6942c782fb33037a39577de8",
                    "user": {
                        "_id": "64ca3d44ec160b67ca5325e4",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/drL9iiIpl8qN6qweKzsE4.jpeg",
                        "isPro": false,
                        "fullname": "Timo K.",
                        "user": "X3N4",
                        "type": "user"
                    },
                    "name": "Timo Klein",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-17T16:18:53.644Z",
                    "hidden": false
                },
                {
                    "_id": "6942c782fb33037a39577de9",
                    "name": "Thomas Lang",
                    "hidden": false
                },
                {
                    "_id": "6942c782fb33037a39577dea",
                    "name": "Andrii Shkabrii",
                    "hidden": false
                },
                {
                    "_id": "6942c782fb33037a39577deb",
                    "name": "Alexander Sturm",
                    "hidden": false
                },
                {
                    "_id": "6942c782fb33037a39577dec",
                    "name": "Kevin Sidak",
                    "hidden": false
                },
                {
                    "_id": "6942c782fb33037a39577ded",
                    "name": "Lukas Miklautz",
                    "hidden": false
                },
                {
                    "_id": "6942c782fb33037a39577dee",
                    "name": "Claudia Plant",
                    "hidden": false
                },
                {
                    "_id": "6942c782fb33037a39577def",
                    "name": "Yllka Velaj",
                    "hidden": false
                },
                {
                    "_id": "6942c782fb33037a39577df0",
                    "name": "Sebastian Tschiatschek",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64ca3d44ec160b67ca5325e4/I1Fry9k2qpKyjWBDTmzDg.png"
            ],
            "publishedAt": "2025-12-16T08:49:24.000Z",
            "submittedOnDailyAt": "2025-12-18T09:47:26.610Z",
            "title": "Understanding and Improving Hyperbolic Deep Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "64ca3d44ec160b67ca5325e4",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/drL9iiIpl8qN6qweKzsE4.jpeg",
                "isPro": false,
                "fullname": "Timo K.",
                "user": "X3N4",
                "type": "user"
            },
            "summary": "The performance of reinforcement learning (RL) agents depends critically on the quality of the underlying feature representations. Hyperbolic feature spaces are well-suited for this purpose, as they naturally capture hierarchical and relational structure often present in complex RL environments. However, leveraging these spaces commonly faces optimization challenges due to the nonstationarity of RL. In this work, we identify key factors that determine the success and failure of training hyperbolic deep RL agents. By analyzing the gradients of core operations in the Poincar Ball and Hyperboloid models of hyperbolic geometry, we show that large-norm embeddings destabilize gradient-based training, leading to trust-region violations in proximal policy optimization (PPO). Based on these insights, we introduce Hyper++, a new hyperbolic PPO agent that consists of three components: (i) stable critic training through a categorical value loss instead of regression; (ii) feature regularization guaranteeing bounded norms while avoiding the curse of dimensionality from clipping; and (iii) using a more optimization-friendly formulation of hyperbolic network layers. In experiments on ProcGen, we show that Hyper++ guarantees stable learning, outperforms prior hyperbolic agents, and reduces wall-clock time by approximately 30%. On Atari-5 with Double DQN, Hyper++ strongly outperforms Euclidean and hyperbolic baselines. We release our code at https://github.com/Probabilistic-and-Interactive-ML/hyper-rl .",
            "upvotes": 4,
            "discussionId": "6942c783fb33037a39577df1",
            "ai_summary": "Hyper++ is a hyperbolic deep RL agent that improves stability and performance by addressing gradient issues and norm constraints in hyperbolic feature spaces.",
            "ai_keywords": [
                "hyperbolic feature spaces",
                "hierarchical structure",
                "relational structure",
                "Poincar Ball",
                "Hyperboloid models",
                "hyperbolic geometry",
                "gradient-based training",
                "trust-region violations",
                "proximal policy optimization (PPO)",
                "stable critic training",
                "categorical value loss",
                "feature regularization",
                "bounded norms",
                "hyperbolic network layers",
                "ProcGen",
                "Atari-5",
                "Double DQN",
                "Euclidean baselines"
            ],
            "organization": {
                "_id": "63493ce03caab0136df11216",
                "name": "univie",
                "fullname": "University of Vienna",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1678690670536-63493cbfb53c4d0fbe46b6fb.png"
            }
        },
        "publishedAt": "2025-12-16T03:49:24.000Z",
        "title": "Understanding and Improving Hyperbolic Deep Reinforcement Learning",
        "summary": "The performance of reinforcement learning (RL) agents depends critically on the quality of the underlying feature representations. Hyperbolic feature spaces are well-suited for this purpose, as they naturally capture hierarchical and relational structure often present in complex RL environments. However, leveraging these spaces commonly faces optimization challenges due to the nonstationarity of RL. In this work, we identify key factors that determine the success and failure of training hyperbolic deep RL agents. By analyzing the gradients of core operations in the Poincar Ball and Hyperboloid models of hyperbolic geometry, we show that large-norm embeddings destabilize gradient-based training, leading to trust-region violations in proximal policy optimization (PPO). Based on these insights, we introduce Hyper++, a new hyperbolic PPO agent that consists of three components: (i) stable critic training through a categorical value loss instead of regression; (ii) feature regularization guaranteeing bounded norms while avoiding the curse of dimensionality from clipping; and (iii) using a more optimization-friendly formulation of hyperbolic network layers. In experiments on ProcGen, we show that Hyper++ guarantees stable learning, outperforms prior hyperbolic agents, and reduces wall-clock time by approximately 30%. On Atari-5 with Double DQN, Hyper++ strongly outperforms Euclidean and hyperbolic baselines. We release our code at https://github.com/Probabilistic-and-Interactive-ML/hyper-rl .",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64ca3d44ec160b67ca5325e4/I1Fry9k2qpKyjWBDTmzDg.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.14202.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64ca3d44ec160b67ca5325e4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/drL9iiIpl8qN6qweKzsE4.jpeg",
            "fullname": "Timo K.",
            "name": "X3N4",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "63493ce03caab0136df11216",
            "name": "univie",
            "fullname": "University of Vienna",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1678690670536-63493cbfb53c4d0fbe46b6fb.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.14719",
            "authors": [
                {
                    "_id": "694399a5542d62d58a7bf7b7",
                    "name": "Zhuoran Zhang",
                    "hidden": false
                },
                {
                    "_id": "694399a5542d62d58a7bf7b8",
                    "name": "Feng Zhang",
                    "hidden": false
                },
                {
                    "_id": "694399a5542d62d58a7bf7b9",
                    "name": "Shangyuan Li",
                    "hidden": false
                },
                {
                    "_id": "694399a5542d62d58a7bf7ba",
                    "user": {
                        "_id": "673c7319d11b1c2e246ead9c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673c7319d11b1c2e246ead9c/IjFIO--N7Hm_BOEafhEQv.jpeg",
                        "isPro": false,
                        "fullname": "Yang Shi",
                        "user": "DogNeverSleep",
                        "type": "user"
                    },
                    "name": "Yang Shi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-18T07:59:33.021Z",
                    "hidden": false
                },
                {
                    "_id": "694399a5542d62d58a7bf7bb",
                    "name": "Yuanxing Zhang",
                    "hidden": false
                },
                {
                    "_id": "694399a5542d62d58a7bf7bc",
                    "name": "Wei Chen",
                    "hidden": false
                },
                {
                    "_id": "694399a5542d62d58a7bf7bd",
                    "name": "Tengjiao Wang",
                    "hidden": false
                },
                {
                    "_id": "694399a5542d62d58a7bf7be",
                    "name": "Kam-Fai Wong",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-09T07:52:47.000Z",
            "submittedOnDailyAt": "2025-12-18T03:35:43.475Z",
            "title": "Hybrid Attribution Priors for Explainable and Robust Model Training",
            "submittedOnDailyBy": {
                "_id": "673c7319d11b1c2e246ead9c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673c7319d11b1c2e246ead9c/IjFIO--N7Hm_BOEafhEQv.jpeg",
                "isPro": false,
                "fullname": "Yang Shi",
                "user": "DogNeverSleep",
                "type": "user"
            },
            "summary": "Small language models (SLMs) are widely used in tasks that require low latency and lightweight deployment, particularly classification. As interpretability and robustness gain increasing importance, explanation-guided learning has emerged as an effective framework by introducing attribution-based supervision during training; however, deriving general and reliable attribution priors remains a significant challenge. Through an analysis of representative attribution methods in classification settings, we find that although these methods can reliably highlight class-relevant tokens, they often focus on common keywords shared by semantically similar classes. Because such classes are already difficult to distinguish under standard training, these attributions provide insufficient discriminative cues, limiting their ability to improve model differentiation. To overcome this limitation, we propose Class-Aware Attribution Prior (CAP), a novel attribution prior extraction framework that guides language models toward capturing fine-grained class distinctions and producing more salient, discriminative attribution priors. Building on this idea, we further introduce CAP Hybrid, which combines priors from CAP with those from existing attribution techniques to form a more comprehensive and balanced supervisory signal. By aligning a model's self-attribution with these enriched priors, our approach encourages the learning of diverse, decision-relevant features. Extensive experiments in full-data, few-shot, and adversarial scenarios demonstrate that our method consistently enhances both interpretability and robustness.",
            "upvotes": 2,
            "discussionId": "694399a5542d62d58a7bf7bf",
            "ai_summary": "A novel framework, Class-Aware Attribution Prior (CAP), enhances language model interpretability and robustness by guiding the model to capture fine-grained class distinctions and combining with existing attribution methods.",
            "ai_keywords": [
                "Small language models",
                "explanation-guided learning",
                "attribution-based supervision",
                "class-relevant tokens",
                "self-attribution",
                "fine-grained class distinctions",
                "CAP Hybrid",
                "interpretability",
                "robustness",
                "full-data scenarios",
                "few-shot scenarios",
                "adversarial scenarios"
            ]
        },
        "publishedAt": "2025-12-09T02:52:47.000Z",
        "title": "Hybrid Attribution Priors for Explainable and Robust Model Training",
        "summary": "Small language models (SLMs) are widely used in tasks that require low latency and lightweight deployment, particularly classification. As interpretability and robustness gain increasing importance, explanation-guided learning has emerged as an effective framework by introducing attribution-based supervision during training; however, deriving general and reliable attribution priors remains a significant challenge. Through an analysis of representative attribution methods in classification settings, we find that although these methods can reliably highlight class-relevant tokens, they often focus on common keywords shared by semantically similar classes. Because such classes are already difficult to distinguish under standard training, these attributions provide insufficient discriminative cues, limiting their ability to improve model differentiation. To overcome this limitation, we propose Class-Aware Attribution Prior (CAP), a novel attribution prior extraction framework that guides language models toward capturing fine-grained class distinctions and producing more salient, discriminative attribution priors. Building on this idea, we further introduce CAP Hybrid, which combines priors from CAP with those from existing attribution techniques to form a more comprehensive and balanced supervisory signal. By aligning a model's self-attribution with these enriched priors, our approach encourages the learning of diverse, decision-relevant features. Extensive experiments in full-data, few-shot, and adversarial scenarios demonstrate that our method consistently enhances both interpretability and robustness.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.14719.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "673c7319d11b1c2e246ead9c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673c7319d11b1c2e246ead9c/IjFIO--N7Hm_BOEafhEQv.jpeg",
            "fullname": "Yang Shi",
            "name": "DogNeverSleep",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.13077",
            "authors": [
                {
                    "_id": "69437fac542d62d58a7bf770",
                    "user": {
                        "_id": "60d34de513f774189902f547",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1640713172129-60d34de513f774189902f547.png",
                        "isPro": false,
                        "fullname": "Awsaf",
                        "user": "awsaf49",
                        "type": "user"
                    },
                    "name": "Md Awsafur Rahman",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-18T07:59:42.130Z",
                    "hidden": false
                },
                {
                    "_id": "69437fac542d62d58a7bf771",
                    "name": "Adam Gabrys",
                    "hidden": false
                },
                {
                    "_id": "69437fac542d62d58a7bf772",
                    "name": "Doug Kang",
                    "hidden": false
                },
                {
                    "_id": "69437fac542d62d58a7bf773",
                    "name": "Jingjing Sun",
                    "hidden": false
                },
                {
                    "_id": "69437fac542d62d58a7bf774",
                    "name": "Tian Tan",
                    "hidden": false
                },
                {
                    "_id": "69437fac542d62d58a7bf775",
                    "name": "Ashwin Chandramouli",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/60d34de513f774189902f547/NL_FAeT7nOawp9GlTpzNp.png",
                "https://cdn-uploads.huggingface.co/production/uploads/60d34de513f774189902f547/fJttQhre1646aGtcx9A4Z.png"
            ],
            "publishedAt": "2025-12-15T08:18:42.000Z",
            "submittedOnDailyAt": "2025-12-18T02:08:22.352Z",
            "title": "LikeBench: Evaluating Subjective Likability in LLMs for Personalization",
            "submittedOnDailyBy": {
                "_id": "60d34de513f774189902f547",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1640713172129-60d34de513f774189902f547.png",
                "isPro": false,
                "fullname": "Awsaf",
                "user": "awsaf49",
                "type": "user"
            },
            "summary": "A personalized LLM should remember user facts, apply them correctly, and adapt over time to provide responses that the user prefers. Existing LLM personalization benchmarks are largely centered on two axes: accurately recalling user information and accurately applying remembered information in downstream tasks. We argue that a third axis, likability, is both subjective and central to user experience, yet under-measured by current benchmarks. To measure likability holistically, we introduce LikeBench, a multi-session, dynamic evaluation framework that measures likability across multiple dimensions by how much an LLM can adapt over time to a user's preferences to provide more likable responses. In LikeBench, the LLMs engage in conversation with a simulated user and learn preferences only from the ongoing dialogue. As the interaction unfolds, models try to adapt to responses, and after each turn, they are evaluated for likability across seven dimensions by the same simulated user. To the best of our knowledge, we are the first to decompose likability into multiple diagnostic metrics: emotional adaptation, formality matching, knowledge adaptation, reference understanding, conversation length fit, humor fit, and callback, which makes it easier to pinpoint where a model falls short. To make the simulated user more realistic and discriminative, LikeBench uses fine-grained, psychologically grounded descriptive personas rather than the coarse high/low trait rating based personas used in prior work. Our benchmark shows that strong memory performance does not guarantee high likability: DeepSeek R1, with lower memory accuracy (86%, 17 facts/profile), outperformed Qwen3 by 28% on likability score despite Qwen3's higher memory accuracy (93%, 43 facts/profile). Even SOTA models like GPT-5 adapt well in short exchanges but show only limited robustness in longer, noisier interactions.",
            "upvotes": 1,
            "discussionId": "69437fac542d62d58a7bf776",
            "ai_summary": "LikeBench introduces a multi-session evaluation framework to measure the likability of LLMs by their ability to adapt to user preferences across multiple dimensions, demonstrating that strong memory performance does not necessarily equate to higher likability.",
            "ai_keywords": [
                "LLM",
                "LikeBench",
                "multi-session",
                "dynamic evaluation",
                "emotional adaptation",
                "formality matching",
                "knowledge adaptation",
                "reference understanding",
                "conversation length fit",
                "humor fit",
                "callback",
                "simulated user",
                "fine-grained",
                "psychologically grounded personas",
                "DeepSeek R1",
                "Qwen3",
                "GPT-5"
            ],
            "organization": {
                "_id": "5ffdfbadbba2ae614d771970",
                "name": "amazon",
                "fullname": "Amazon",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66f19ed428ae41c20c470792/8y7msN6A6W82LdQhQd85a.png"
            }
        },
        "publishedAt": "2025-12-15T03:18:42.000Z",
        "title": "LikeBench: Evaluating Subjective Likability in LLMs for Personalization",
        "summary": "A personalized LLM should remember user facts, apply them correctly, and adapt over time to provide responses that the user prefers. Existing LLM personalization benchmarks are largely centered on two axes: accurately recalling user information and accurately applying remembered information in downstream tasks. We argue that a third axis, likability, is both subjective and central to user experience, yet under-measured by current benchmarks. To measure likability holistically, we introduce LikeBench, a multi-session, dynamic evaluation framework that measures likability across multiple dimensions by how much an LLM can adapt over time to a user's preferences to provide more likable responses. In LikeBench, the LLMs engage in conversation with a simulated user and learn preferences only from the ongoing dialogue. As the interaction unfolds, models try to adapt to responses, and after each turn, they are evaluated for likability across seven dimensions by the same simulated user. To the best of our knowledge, we are the first to decompose likability into multiple diagnostic metrics: emotional adaptation, formality matching, knowledge adaptation, reference understanding, conversation length fit, humor fit, and callback, which makes it easier to pinpoint where a model falls short. To make the simulated user more realistic and discriminative, LikeBench uses fine-grained, psychologically grounded descriptive personas rather than the coarse high/low trait rating based personas used in prior work. Our benchmark shows that strong memory performance does not guarantee high likability: DeepSeek R1, with lower memory accuracy (86%, 17 facts/profile), outperformed Qwen3 by 28% on likability score despite Qwen3's higher memory accuracy (93%, 43 facts/profile). Even SOTA models like GPT-5 adapt well in short exchanges but show only limited robustness in longer, noisier interactions.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/60d34de513f774189902f547/NL_FAeT7nOawp9GlTpzNp.png",
            "https://cdn-uploads.huggingface.co/production/uploads/60d34de513f774189902f547/fJttQhre1646aGtcx9A4Z.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13077.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60d34de513f774189902f547",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1640713172129-60d34de513f774189902f547.png",
            "fullname": "Awsaf",
            "name": "awsaf49",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "5ffdfbadbba2ae614d771970",
            "name": "amazon",
            "fullname": "Amazon",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66f19ed428ae41c20c470792/8y7msN6A6W82LdQhQd85a.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.09851",
            "authors": [
                {
                    "_id": "69436b58542d62d58a7bf66e",
                    "name": "Yuyang Li",
                    "hidden": false
                },
                {
                    "_id": "69436b58542d62d58a7bf66f",
                    "name": "Yinghan Chen",
                    "hidden": false
                },
                {
                    "_id": "69436b58542d62d58a7bf670",
                    "name": "Zihang Zhao",
                    "hidden": false
                },
                {
                    "_id": "69436b58542d62d58a7bf671",
                    "name": "Puhao Li",
                    "hidden": false
                },
                {
                    "_id": "69436b58542d62d58a7bf672",
                    "name": "Tengyu Liu",
                    "hidden": false
                },
                {
                    "_id": "69436b58542d62d58a7bf673",
                    "name": "Siyuan Huang",
                    "hidden": false
                },
                {
                    "_id": "69436b58542d62d58a7bf674",
                    "name": "Yixin Zhu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-10T17:35:13.000Z",
            "submittedOnDailyAt": "2025-12-18T00:18:26.223Z",
            "title": "Simultaneous Tactile-Visual Perception for Learning Multimodal Robot Manipulation",
            "submittedOnDailyBy": {
                "_id": "637cef70b8e573d75beb5748",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637cef70b8e573d75beb5748/hMl_pKUO3BanR47RQqN6o.png",
                "isPro": false,
                "fullname": "Yuyang Li",
                "user": "aidenli",
                "type": "user"
            },
            "summary": "Robotic manipulation requires both rich multimodal perception and effective learning frameworks to handle complex real-world tasks. See-through-skin (STS) sensors, which combine tactile and visual perception, offer promising sensing capabilities, while modern imitation learning provides powerful tools for policy acquisition. However, existing STS designs lack simultaneous multimodal perception and suffer from unreliable tactile tracking. Furthermore, integrating these rich multimodal signals into learning-based manipulation pipelines remains an open challenge. We introduce TacThru, an STS sensor enabling simultaneous visual perception and robust tactile signal extraction, and TacThru-UMI, an imitation learning framework that leverages these multimodal signals for manipulation. Our sensor features a fully transparent elastomer, persistent illumination, novel keyline markers, and efficient tracking, while our learning system integrates these signals through a Transformer-based Diffusion Policy. Experiments on five challenging real-world tasks show that TacThru-UMI achieves an average success rate of 85.5%, significantly outperforming the baselines of alternating tactile-visual (66.3%) and vision-only (55.4%). The system excels in critical scenarios, including contact detection with thin and soft objects and precision manipulation requiring multimodal coordination. This work demonstrates that combining simultaneous multimodal perception with modern learning frameworks enables more precise, adaptable robotic manipulation.",
            "upvotes": 1,
            "discussionId": "69436b59542d62d58a7bf675",
            "projectPage": "https://tacthru.yuyang.li/",
            "githubRepo": "https://github.com/YuyangLee/TacThru",
            "githubRepoAddedBy": "user",
            "ai_summary": "TacThru-UMI, a system combining a TacThru sensor with a Transformer-based Diffusion Policy, achieves superior performance in robotic manipulation tasks by integrating simultaneous multimodal perception.",
            "ai_keywords": [
                "STS sensors",
                "tactile perception",
                "visual perception",
                "imitation learning",
                "multimodal signals",
                "Transformer-based Diffusion Policy"
            ],
            "githubStars": 19,
            "organization": {
                "_id": "61dcd8e344f59573371b5cb6",
                "name": "PekingUniversity",
                "fullname": "Peking University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
            }
        },
        "publishedAt": "2025-12-10T12:35:13.000Z",
        "title": "Simultaneous Tactile-Visual Perception for Learning Multimodal Robot Manipulation",
        "summary": "Robotic manipulation requires both rich multimodal perception and effective learning frameworks to handle complex real-world tasks. See-through-skin (STS) sensors, which combine tactile and visual perception, offer promising sensing capabilities, while modern imitation learning provides powerful tools for policy acquisition. However, existing STS designs lack simultaneous multimodal perception and suffer from unreliable tactile tracking. Furthermore, integrating these rich multimodal signals into learning-based manipulation pipelines remains an open challenge. We introduce TacThru, an STS sensor enabling simultaneous visual perception and robust tactile signal extraction, and TacThru-UMI, an imitation learning framework that leverages these multimodal signals for manipulation. Our sensor features a fully transparent elastomer, persistent illumination, novel keyline markers, and efficient tracking, while our learning system integrates these signals through a Transformer-based Diffusion Policy. Experiments on five challenging real-world tasks show that TacThru-UMI achieves an average success rate of 85.5%, significantly outperforming the baselines of alternating tactile-visual (66.3%) and vision-only (55.4%). The system excels in critical scenarios, including contact detection with thin and soft objects and precision manipulation requiring multimodal coordination. This work demonstrates that combining simultaneous multimodal perception with modern learning frameworks enables more precise, adaptable robotic manipulation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.09851.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "637cef70b8e573d75beb5748",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637cef70b8e573d75beb5748/hMl_pKUO3BanR47RQqN6o.png",
            "fullname": "Yuyang Li",
            "name": "aidenli",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "61dcd8e344f59573371b5cb6",
            "name": "PekingUniversity",
            "fullname": "Peking University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.15340",
            "authors": [
                {
                    "_id": "694360bd542d62d58a7bf61f",
                    "name": "Junjie Chen",
                    "hidden": false
                },
                {
                    "_id": "694360bd542d62d58a7bf620",
                    "name": "Fei Wang",
                    "hidden": false
                },
                {
                    "_id": "694360bd542d62d58a7bf621",
                    "name": "Zhihao Huang",
                    "hidden": false
                },
                {
                    "_id": "694360bd542d62d58a7bf622",
                    "name": "Qing Zhou",
                    "hidden": false
                },
                {
                    "_id": "694360bd542d62d58a7bf623",
                    "name": "Kun Li",
                    "hidden": false
                },
                {
                    "_id": "694360bd542d62d58a7bf624",
                    "name": "Dan Guo",
                    "hidden": false
                },
                {
                    "_id": "694360bd542d62d58a7bf625",
                    "name": "Linfeng Zhang",
                    "hidden": false
                },
                {
                    "_id": "694360bd542d62d58a7bf626",
                    "name": "Xun Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-17T11:37:35.000Z",
            "submittedOnDailyAt": "2025-12-18T08:30:14.294Z",
            "title": "Towards Seamless Interaction: Causal Turn-Level Modeling of Interactive 3D Conversational Head Dynamics",
            "submittedOnDailyBy": {
                "_id": "652f8642338c761caf474169",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/mq5jjqqNaFxVboWGDEocJ.jpeg",
                "isPro": false,
                "fullname": "Junjie Chen",
                "user": "coderchen01",
                "type": "user"
            },
            "summary": "Human conversation involves continuous exchanges of speech and nonverbal cues such as head nods, gaze shifts, and facial expressions that convey attention and emotion. Modeling these bidirectional dynamics in 3D is essential for building expressive avatars and interactive robots. However, existing frameworks often treat talking and listening as independent processes or rely on non-causal full-sequence modeling, hindering temporal coherence across turns. We present TIMAR (Turn-level Interleaved Masked AutoRegression), a causal framework for 3D conversational head generation that models dialogue as interleaved audio-visual contexts. It fuses multimodal information within each turn and applies turn-level causal attention to accumulate conversational history, while a lightweight diffusion head predicts continuous 3D head dynamics that captures both coordination and expressive variability. Experiments on the DualTalk benchmark show that TIMAR reduces Frchet Distance and MSE by 15-30% on the test set, and achieves similar gains on out-of-distribution data. The source code will be released in the GitHub repository https://github.com/CoderChen01/towards-seamleass-interaction.",
            "upvotes": 0,
            "discussionId": "694360bd542d62d58a7bf627",
            "projectPage": "https://github.com/CoderChen01/towards-seamleass-interaction/blob/main/README.md",
            "githubRepo": "https://github.com/CoderChen01/towards-seamleass-interaction",
            "githubRepoAddedBy": "user",
            "ai_summary": "TIMAR, a causal framework for 3D conversational head generation, models dialogue as interleaved audio-visual contexts and predicts continuous 3D head dynamics, improving coherence and expressive variability.",
            "ai_keywords": [
                "Turn-level Interleaved Masked AutoRegression",
                "causal framework",
                "3D conversational head generation",
                "dialogue",
                "interleaved audio-visual contexts",
                "turn-level causal attention",
                "conversational history",
                "lightweight diffusion head",
                "Frchet Distance",
                "MSE",
                "DualTalk benchmark"
            ],
            "githubStars": 6
        },
        "publishedAt": "2025-12-17T06:37:35.000Z",
        "title": "Towards Seamless Interaction: Causal Turn-Level Modeling of Interactive 3D Conversational Head Dynamics",
        "summary": "Human conversation involves continuous exchanges of speech and nonverbal cues such as head nods, gaze shifts, and facial expressions that convey attention and emotion. Modeling these bidirectional dynamics in 3D is essential for building expressive avatars and interactive robots. However, existing frameworks often treat talking and listening as independent processes or rely on non-causal full-sequence modeling, hindering temporal coherence across turns. We present TIMAR (Turn-level Interleaved Masked AutoRegression), a causal framework for 3D conversational head generation that models dialogue as interleaved audio-visual contexts. It fuses multimodal information within each turn and applies turn-level causal attention to accumulate conversational history, while a lightweight diffusion head predicts continuous 3D head dynamics that captures both coordination and expressive variability. Experiments on the DualTalk benchmark show that TIMAR reduces Frchet Distance and MSE by 15-30% on the test set, and achieves similar gains on out-of-distribution data. The source code will be released in the GitHub repository https://github.com/CoderChen01/towards-seamleass-interaction.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.15340.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "652f8642338c761caf474169",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/mq5jjqqNaFxVboWGDEocJ.jpeg",
            "fullname": "Junjie Chen",
            "name": "coderchen01",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 12
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.14080",
            "authors": [
                {
                    "_id": "69449631fbf17e708e185ef4",
                    "name": "Wentao Guo",
                    "hidden": false
                },
                {
                    "_id": "69449631fbf17e708e185ef5",
                    "name": "Mayank Mishra",
                    "hidden": false
                },
                {
                    "_id": "69449631fbf17e708e185ef6",
                    "name": "Xinle Cheng",
                    "hidden": false
                },
                {
                    "_id": "69449631fbf17e708e185ef7",
                    "name": "Ion Stoica",
                    "hidden": false
                },
                {
                    "_id": "69449631fbf17e708e185ef8",
                    "name": "Tri Dao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-16T04:39:10.000Z",
            "submittedOnDailyAt": "2025-12-18T21:33:56.478Z",
            "title": "SonicMoE: Accelerating MoE with IO and Tile-aware Optimizations",
            "submittedOnDailyBy": {
                "_id": "651e96991b97c9f33d26bde6",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/651e96991b97c9f33d26bde6/-Bqs6qrmz0yCfwtB2e-6q.jpeg",
                "isPro": true,
                "fullname": "Elie Bakouch",
                "user": "eliebak",
                "type": "user"
            },
            "summary": "Mixture of Experts (MoE) models have emerged as the de facto architecture for scaling up language models without significantly increasing the computational cost. Recent MoE models demonstrate a clear trend towards high expert granularity (smaller expert intermediate dimension) and higher sparsity (constant number of activated experts with higher number of total experts), which improve model quality per FLOP. However, fine-grained MoEs suffer from increased activation memory footprint and reduced hardware efficiency due to higher IO costs, while sparser MoEs suffer from wasted computations due to padding in Grouped GEMM kernels. In response, we propose a memory-efficient algorithm to compute the forward and backward passes of MoEs with minimal activation caching for the backward pass. We also design GPU kernels that overlap memory IO with computation benefiting all MoE architectures. Finally, we propose a novel \"token rounding\" method that minimizes the wasted compute due to padding in Grouped GEMM kernels. As a result, our method SonicMoE reduces activation memory by 45% and achieves a 1.86x compute throughput improvement on Hopper GPUs compared to ScatterMoE's BF16 MoE kernel for a fine-grained 7B MoE. Concretely, SonicMoE on 64 H100s achieves a training throughput of 213 billion tokens per day comparable to ScatterMoE's 225 billion tokens per day on 96 H100s for a 7B MoE model training with FSDP-2 using the lm-engine codebase. Under high MoE sparsity settings, our tile-aware token rounding algorithm yields an additional 1.16x speedup on kernel execution time compared to vanilla top-K routing while maintaining similar downstream performance. We open-source all our kernels to enable faster MoE model training.",
            "upvotes": 0,
            "discussionId": "69449632fbf17e708e185ef9",
            "ai_summary": "SonicMoE optimizes memory usage and computational efficiency in Mixture of Experts models through minimal activation caching, IO-computation overlap, and token rounding, achieving high throughput and performance.",
            "ai_keywords": [
                "Mixture of Experts (MoE)",
                "expert granularity",
                "sparsity",
                "activation memory footprint",
                "hardware efficiency",
                "IO costs",
                "forward and backward passes",
                "GPU kernels",
                "memory IO overlap",
                "Grouped GEMM kernels",
                "token rounding",
                "ScatterMoE",
                "Hopper GPUs",
                "training throughput",
                "FSDP-2",
                "lm-engine",
                "top-$K$ routing",
                "tile-aware token rounding"
            ]
        },
        "publishedAt": "2025-12-15T23:39:10.000Z",
        "title": "SonicMoE: Accelerating MoE with IO and Tile-aware Optimizations",
        "summary": "Mixture of Experts (MoE) models have emerged as the de facto architecture for scaling up language models without significantly increasing the computational cost. Recent MoE models demonstrate a clear trend towards high expert granularity (smaller expert intermediate dimension) and higher sparsity (constant number of activated experts with higher number of total experts), which improve model quality per FLOP. However, fine-grained MoEs suffer from increased activation memory footprint and reduced hardware efficiency due to higher IO costs, while sparser MoEs suffer from wasted computations due to padding in Grouped GEMM kernels. In response, we propose a memory-efficient algorithm to compute the forward and backward passes of MoEs with minimal activation caching for the backward pass. We also design GPU kernels that overlap memory IO with computation benefiting all MoE architectures. Finally, we propose a novel \"token rounding\" method that minimizes the wasted compute due to padding in Grouped GEMM kernels. As a result, our method SonicMoE reduces activation memory by 45% and achieves a 1.86x compute throughput improvement on Hopper GPUs compared to ScatterMoE's BF16 MoE kernel for a fine-grained 7B MoE. Concretely, SonicMoE on 64 H100s achieves a training throughput of 213 billion tokens per day comparable to ScatterMoE's 225 billion tokens per day on 96 H100s for a 7B MoE model training with FSDP-2 using the lm-engine codebase. Under high MoE sparsity settings, our tile-aware token rounding algorithm yields an additional 1.16x speedup on kernel execution time compared to vanilla top-K routing while maintaining similar downstream performance. We open-source all our kernels to enable faster MoE model training.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.14080.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "651e96991b97c9f33d26bde6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/651e96991b97c9f33d26bde6/-Bqs6qrmz0yCfwtB2e-6q.jpeg",
            "fullname": "Elie Bakouch",
            "name": "eliebak",
            "type": "user",
            "isPro": true,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 368
        },
        "isAuthorParticipating": false
    }
]