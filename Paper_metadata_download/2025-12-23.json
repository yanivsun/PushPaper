[
    {
        "paper": {
            "id": "2512.16676",
            "authors": [
                {
                    "_id": "6949026334f46eaf46cbb3d1",
                    "name": "Hao Liang",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3d2",
                    "name": "Xiaochen Ma",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3d3",
                    "name": "Zhou Liu",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3d4",
                    "name": "Zhen Hao Wong",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3d5",
                    "name": "Zhengyang Zhao",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3d6",
                    "name": "Zimo Meng",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3d7",
                    "name": "Runming He",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3d8",
                    "name": "Chengyu Shen",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3d9",
                    "name": "Qifeng Cai",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3da",
                    "name": "Zhaoyang Han",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3db",
                    "name": "Meiyi Qiang",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3dc",
                    "name": "Yalin Feng",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3dd",
                    "name": "Tianyi Bai",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3de",
                    "name": "Zewei Pan",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3df",
                    "name": "Ziyi Guo",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3e0",
                    "name": "Yizhen Jiang",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3e1",
                    "name": "Jingwen Deng",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3e2",
                    "name": "Qijie You",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3e3",
                    "name": "Peichao Lai",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3e4",
                    "name": "Tianyu Guo",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3e5",
                    "name": "Chi Hsu Tsai",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3e6",
                    "name": "Hengyi Feng",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3e7",
                    "name": "Rui Hu",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3e8",
                    "name": "Wenkai Yu",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3e9",
                    "name": "Junbo Niu",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3ea",
                    "name": "Bohan Zeng",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3eb",
                    "name": "Ruichuan An",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3ec",
                    "name": "Lu Ma",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3ed",
                    "name": "Jihao Huang",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3ee",
                    "name": "Yaowei Zheng",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3ef",
                    "name": "Conghui He",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3f0",
                    "name": "Linpeng Tang",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3f1",
                    "name": "Bin Cui",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3f2",
                    "name": "Weinan E",
                    "hidden": false
                },
                {
                    "_id": "6949026334f46eaf46cbb3f3",
                    "name": "Wentao Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-18T15:46:15.000Z",
            "submittedOnDailyAt": "2025-12-23T01:07:14.287Z",
            "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI",
            "submittedOnDailyBy": {
                "_id": "6671214c92412fd4640714eb",
                "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg",
                "isPro": false,
                "fullname": "bohan zeng",
                "user": "zbhpku",
                "type": "user"
            },
            "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.",
            "upvotes": 158,
            "discussionId": "6949026334f46eaf46cbb3f4",
            "projectPage": "https://github.com/OpenDCAI/DataFlow",
            "ai_summary": "DataFlow is an LLM-driven data preparation framework that enhances data quality and reproducibility for various tasks, improving LLM performance with automatically generated pipelines.",
            "ai_keywords": [
                "DataFlow",
                "Large Language Models (LLMs)",
                "data preparation pipelines",
                "system-level abstractions",
                "PyTorch-style pipeline construction API",
                "reusable operators",
                "domain-general pipelines",
                "Text-to-SQL",
                "agentic RAG",
                "large-scale knowledge extraction",
                "DataFlow-Agent",
                "operator synthesis",
                "pipeline planning",
                "iterative verification"
            ],
            "organization": {
                "_id": "61dcd8e344f59573371b5cb6",
                "name": "PekingUniversity",
                "fullname": "Peking University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
            }
        },
        "publishedAt": "2025-12-18T10:46:15.000Z",
        "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI",
        "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16676.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "6671214c92412fd4640714eb",
            "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg",
            "fullname": "bohan zeng",
            "name": "zbhpku",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "organization": {
            "_id": "61dcd8e344f59573371b5cb6",
            "name": "PekingUniversity",
            "fullname": "Peking University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.19693",
            "authors": [
                {
                    "_id": "694a0ffa335742716e93227d",
                    "name": "Weichen Fan",
                    "hidden": false
                },
                {
                    "_id": "694a0ffa335742716e93227e",
                    "name": "Haiwen Diao",
                    "hidden": false
                },
                {
                    "_id": "694a0ffa335742716e93227f",
                    "name": "Quan Wang",
                    "hidden": false
                },
                {
                    "_id": "694a0ffa335742716e932280",
                    "name": "Dahua Lin",
                    "hidden": false
                },
                {
                    "_id": "694a0ffa335742716e932281",
                    "name": "Ziwei Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-22T18:59:57.000Z",
            "submittedOnDailyAt": "2025-12-23T01:15:14.379Z",
            "title": "The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Deep representations across modalities are inherently intertwined. In this paper, we systematically analyze the spectral characteristics of various semantic and pixel encoders. Interestingly, our study uncovers a highly inspiring and rarely explored correspondence between an encoder's feature spectrum and its functional role: semantic encoders primarily capture low-frequency components that encode abstract meaning, whereas pixel encoders additionally retain high-frequency information that conveys fine-grained detail. This heuristic finding offers a unifying perspective that ties encoder behavior to its underlying spectral structure. We define it as the Prism Hypothesis, where each data modality can be viewed as a projection of the natural world onto a shared feature spectrum, just like the prism. Building on this insight, we propose Unified Autoencoding (UAE), a model that harmonizes semantic structure and pixel details via an innovative frequency-band modulator, enabling their seamless coexistence. Extensive experiments on ImageNet and MS-COCO benchmarks validate that our UAE effectively unifies semantic abstraction and pixel-level fidelity into a single latent space with state-of-the-art performance.",
            "upvotes": 53,
            "discussionId": "694a0ffa335742716e932282",
            "githubRepo": "https://github.com/WeichenFan/UAE",
            "githubRepoAddedBy": "user",
            "ai_summary": "Unified Autoencoding combines semantic and pixel-level information through a frequency-band modulator, resulting in a latent space with state-of-the-art performance on image benchmarks.",
            "ai_keywords": [
                "spectral characteristics",
                "semantic encoders",
                "pixel encoders",
                "feature spectrum",
                "low-frequency components",
                "high-frequency information",
                "Prism Hypothesis",
                "Unified Autoencoding",
                "frequency-band modulator",
                "ImageNet",
                "MS-COCO",
                "latent space"
            ],
            "githubStars": 56
        },
        "publishedAt": "2025-12-22T13:59:57.000Z",
        "title": "The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding",
        "summary": "Deep representations across modalities are inherently intertwined. In this paper, we systematically analyze the spectral characteristics of various semantic and pixel encoders. Interestingly, our study uncovers a highly inspiring and rarely explored correspondence between an encoder's feature spectrum and its functional role: semantic encoders primarily capture low-frequency components that encode abstract meaning, whereas pixel encoders additionally retain high-frequency information that conveys fine-grained detail. This heuristic finding offers a unifying perspective that ties encoder behavior to its underlying spectral structure. We define it as the Prism Hypothesis, where each data modality can be viewed as a projection of the natural world onto a shared feature spectrum, just like the prism. Building on this insight, we propose Unified Autoencoding (UAE), a model that harmonizes semantic structure and pixel details via an innovative frequency-band modulator, enabling their seamless coexistence. Extensive experiments on ImageNet and MS-COCO benchmarks validate that our UAE effectively unifies semantic abstraction and pixel-level fidelity into a single latent space with state-of-the-art performance.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.19693.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 189
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.17650",
            "authors": [
                {
                    "_id": "6948c8f334f46eaf46cbb325",
                    "name": "Zhongwei Zhang",
                    "hidden": false
                },
                {
                    "_id": "6948c8f334f46eaf46cbb326",
                    "name": "Fuchen Long",
                    "hidden": false
                },
                {
                    "_id": "6948c8f334f46eaf46cbb327",
                    "name": "Wei Li",
                    "hidden": false
                },
                {
                    "_id": "6948c8f334f46eaf46cbb328",
                    "name": "Zhaofan Qiu",
                    "hidden": false
                },
                {
                    "_id": "6948c8f334f46eaf46cbb329",
                    "name": "Wu Liu",
                    "hidden": false
                },
                {
                    "_id": "6948c8f334f46eaf46cbb32a",
                    "name": "Ting Yao",
                    "hidden": false
                },
                {
                    "_id": "6948c8f334f46eaf46cbb32b",
                    "name": "Tao Mei",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6496f5754a3c31df8e3139f6/3S3unvdbINRHQFW85psrn.mp4"
            ],
            "publishedAt": "2025-12-19T14:49:30.000Z",
            "submittedOnDailyAt": "2025-12-23T01:38:37.820Z",
            "title": "Region-Constraint In-Context Generation for Instructional Video Editing",
            "submittedOnDailyBy": {
                "_id": "6496f5754a3c31df8e3139f6",
                "avatarUrl": "/avatars/cf789d1986f976373c82b2976df4542a.svg",
                "isPro": false,
                "fullname": "Zhongwei Zhang",
                "user": "zzwustc",
                "type": "user"
            },
            "summary": "The In-context generation paradigm recently has demonstrated strong power in instructional image editing with both data efficiency and synthesis quality. Nevertheless, shaping such in-context learning for instruction-based video editing is not trivial. Without specifying editing regions, the results can suffer from the problem of inaccurate editing regions and the token interference between editing and non-editing areas during denoising. To address these, we present ReCo, a new instructional video editing paradigm that novelly delves into constraint modeling between editing and non-editing regions during in-context generation. Technically, ReCo width-wise concatenates source and target video for joint denoising. To calibrate video diffusion learning, ReCo capitalizes on two regularization terms, i.e., latent and attention regularization, conducting on one-step backward denoised latents and attention maps, respectively. The former increases the latent discrepancy of the editing region between source and target videos while reducing that of non-editing areas, emphasizing the modification on editing area and alleviating outside unexpected content generation. The latter suppresses the attention of tokens in the editing region to the tokens in counterpart of the source video, thereby mitigating their interference during novel object generation in target video. Furthermore, we propose a large-scale, high-quality video editing dataset, i.e., ReCo-Data, comprising 500K instruction-video pairs to benefit model training. Extensive experiments conducted on four major instruction-based video editing tasks demonstrate the superiority of our proposal.",
            "upvotes": 39,
            "discussionId": "6948c8f334f46eaf46cbb32c",
            "projectPage": "https://zhw-zhang.github.io/ReCo-page/",
            "githubRepo": "https://github.com/HiDream-ai/ReCo",
            "githubRepoAddedBy": "user",
            "ai_summary": "ReCo is a novel instructional video editing paradigm that enhances accuracy and reduces token interference by incorporating constraint modeling and regularization techniques during in-context generation.",
            "ai_keywords": [
                "in-context generation",
                "instructional video editing",
                "denoising",
                "ReCo",
                "constraint modeling",
                "latent regularization",
                "attention regularization",
                "backward denoised latents",
                "attention maps",
                "ReCo-Data",
                "video diffusion learning",
                "instruction-video pairs",
                "video editing tasks"
            ],
            "githubStars": 32,
            "organization": {
                "_id": "61d8000084231b832e5bbd99",
                "name": "ustc",
                "fullname": "university of science and technology  of china",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1641545773772-61d7fdeb22a383817a543b68.png"
            }
        },
        "publishedAt": "2025-12-19T09:49:30.000Z",
        "title": "Region-Constraint In-Context Generation for Instructional Video Editing",
        "summary": "The In-context generation paradigm recently has demonstrated strong power in instructional image editing with both data efficiency and synthesis quality. Nevertheless, shaping such in-context learning for instruction-based video editing is not trivial. Without specifying editing regions, the results can suffer from the problem of inaccurate editing regions and the token interference between editing and non-editing areas during denoising. To address these, we present ReCo, a new instructional video editing paradigm that novelly delves into constraint modeling between editing and non-editing regions during in-context generation. Technically, ReCo width-wise concatenates source and target video for joint denoising. To calibrate video diffusion learning, ReCo capitalizes on two regularization terms, i.e., latent and attention regularization, conducting on one-step backward denoised latents and attention maps, respectively. The former increases the latent discrepancy of the editing region between source and target videos while reducing that of non-editing areas, emphasizing the modification on editing area and alleviating outside unexpected content generation. The latter suppresses the attention of tokens in the editing region to the tokens in counterpart of the source video, thereby mitigating their interference during novel object generation in target video. Furthermore, we propose a large-scale, high-quality video editing dataset, i.e., ReCo-Data, comprising 500K instruction-video pairs to benefit model training. Extensive experiments conducted on four major instruction-based video editing tasks demonstrate the superiority of our proposal.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6496f5754a3c31df8e3139f6/3S3unvdbINRHQFW85psrn.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.17650.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6496f5754a3c31df8e3139f6",
            "avatarUrl": "/avatars/cf789d1986f976373c82b2976df4542a.svg",
            "fullname": "Zhongwei Zhang",
            "name": "zzwustc",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "organization": {
            "_id": "61d8000084231b832e5bbd99",
            "name": "ustc",
            "fullname": "university of science and technology  of china",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1641545773772-61d7fdeb22a383817a543b68.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.17040",
            "authors": [
                {
                    "_id": "6948b33d34f46eaf46cbb293",
                    "user": {
                        "_id": "6449db44df4e6cb7eaef912a",
                        "avatarUrl": "/avatars/777cea252e06933863bda10dc3543f59.svg",
                        "isPro": false,
                        "fullname": "Min-Jung Kim",
                        "user": "emjay73",
                        "type": "user"
                    },
                    "name": "Min-Jung Kim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-22T10:58:39.498Z",
                    "hidden": false
                },
                {
                    "_id": "6948b33d34f46eaf46cbb294",
                    "name": "Jeongho Kim",
                    "hidden": false
                },
                {
                    "_id": "6948b33d34f46eaf46cbb295",
                    "name": "Hoiyeong Jin",
                    "hidden": false
                },
                {
                    "_id": "6948b33d34f46eaf46cbb296",
                    "name": "Junha Hyung",
                    "hidden": false
                },
                {
                    "_id": "6948b33d34f46eaf46cbb297",
                    "name": "Jaegul Choo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-18T20:03:05.000Z",
            "submittedOnDailyAt": "2025-12-23T03:45:12.735Z",
            "title": "Infinite-Homography as Robust Conditioning for Camera-Controlled Video Generation",
            "submittedOnDailyBy": {
                "_id": "6449db44df4e6cb7eaef912a",
                "avatarUrl": "/avatars/777cea252e06933863bda10dc3543f59.svg",
                "isPro": false,
                "fullname": "Min-Jung Kim",
                "user": "emjay73",
                "type": "user"
            },
            "summary": "Recent progress in video diffusion models has spurred growing interest in camera-controlled novel-view video generation for dynamic scenes, aiming to provide creators with cinematic camera control capabilities in post-production. A key challenge in camera-controlled video generation is ensuring fidelity to the specified camera pose, while maintaining view consistency and reasoning about occluded geometry from limited observations. To address this, existing methods either train trajectory-conditioned video generation model on trajectory-video pair dataset, or estimate depth from the input video to reproject it along a target trajectory and generate the unprojected regions. Nevertheless, existing methods struggle to generate camera-pose-faithful, high-quality videos for two main reasons: (1) reprojection-based approaches are highly susceptible to errors caused by inaccurate depth estimation; and (2) the limited diversity of camera trajectories in existing datasets restricts learned models. To address these limitations, we present InfCam, a depth-free, camera-controlled video-to-video generation framework with high pose fidelity. The framework integrates two key components: (1) infinite homography warping, which encodes 3D camera rotations directly within the 2D latent space of a video diffusion model. Conditioning on this noise-free rotational information, the residual parallax term is predicted through end-to-end training to achieve high camera-pose fidelity; and (2) a data augmentation pipeline that transforms existing synthetic multiview datasets into sequences with diverse trajectories and focal lengths. Experimental results demonstrate that InfCam outperforms baseline methods in camera-pose accuracy and visual fidelity, generalizing well from synthetic to real-world data. Link to our project page:https://emjay73.github.io/InfCam/",
            "upvotes": 26,
            "discussionId": "6948b33d34f46eaf46cbb298",
            "projectPage": "https://emjay73.github.io/InfCam/",
            "githubRepo": "https://github.com/emjay73/InfCam",
            "githubRepoAddedBy": "user",
            "ai_summary": "InfCam generates high-fidelity videos with accurate camera poses by using infinite homography warping and augmenting synthetic datasets with diverse trajectories.",
            "ai_keywords": [
                "video diffusion models",
                "camera-controlled video generation",
                "trajectory-conditioned video generation",
                "depth-free",
                "infinite homography warping",
                "3D camera rotations",
                "2D latent space",
                "residual parallax term",
                "data augmentation pipeline",
                "multiview datasets",
                "focal lengths"
            ],
            "githubStars": 21,
            "organization": {
                "_id": "6475760c33192631bad2bb38",
                "name": "kaist-ai",
                "fullname": "KAIST AI",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6469949654873f0043b09c22/aaZFiyXe1qR-Dmy_xq67m.png"
            }
        },
        "publishedAt": "2025-12-18T15:03:05.000Z",
        "title": "Infinite-Homography as Robust Conditioning for Camera-Controlled Video Generation",
        "summary": "Recent progress in video diffusion models has spurred growing interest in camera-controlled novel-view video generation for dynamic scenes, aiming to provide creators with cinematic camera control capabilities in post-production. A key challenge in camera-controlled video generation is ensuring fidelity to the specified camera pose, while maintaining view consistency and reasoning about occluded geometry from limited observations. To address this, existing methods either train trajectory-conditioned video generation model on trajectory-video pair dataset, or estimate depth from the input video to reproject it along a target trajectory and generate the unprojected regions. Nevertheless, existing methods struggle to generate camera-pose-faithful, high-quality videos for two main reasons: (1) reprojection-based approaches are highly susceptible to errors caused by inaccurate depth estimation; and (2) the limited diversity of camera trajectories in existing datasets restricts learned models. To address these limitations, we present InfCam, a depth-free, camera-controlled video-to-video generation framework with high pose fidelity. The framework integrates two key components: (1) infinite homography warping, which encodes 3D camera rotations directly within the 2D latent space of a video diffusion model. Conditioning on this noise-free rotational information, the residual parallax term is predicted through end-to-end training to achieve high camera-pose fidelity; and (2) a data augmentation pipeline that transforms existing synthetic multiview datasets into sequences with diverse trajectories and focal lengths. Experimental results demonstrate that InfCam outperforms baseline methods in camera-pose accuracy and visual fidelity, generalizing well from synthetic to real-world data. Link to our project page:https://emjay73.github.io/InfCam/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.17040.png",
        "numComments": 5,
        "submittedBy": {
            "_id": "6449db44df4e6cb7eaef912a",
            "avatarUrl": "/avatars/777cea252e06933863bda10dc3543f59.svg",
            "fullname": "Min-Jung Kim",
            "name": "emjay73",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "6475760c33192631bad2bb38",
            "name": "kaist-ai",
            "fullname": "KAIST AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6469949654873f0043b09c22/aaZFiyXe1qR-Dmy_xq67m.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.19134",
            "authors": [
                {
                    "_id": "694a1765335742716e9322b7",
                    "name": "Dehai Min",
                    "hidden": false
                },
                {
                    "_id": "694a1765335742716e9322b8",
                    "name": "Kailin Zhang",
                    "hidden": false
                },
                {
                    "_id": "694a1765335742716e9322b9",
                    "name": "Tongtong Wu",
                    "hidden": false
                },
                {
                    "_id": "694a1765335742716e9322ba",
                    "name": "Lu Cheng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-22T08:28:05.000Z",
            "submittedOnDailyAt": "2025-12-23T01:46:57.477Z",
            "title": "QuCo-RAG: Quantifying Uncertainty from the Pre-training Corpus for Dynamic Retrieval-Augmented Generation",
            "submittedOnDailyBy": {
                "_id": "629c6ee73a3221bb210afc2d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629c6ee73a3221bb210afc2d/Mg-VymVvHQn_pDrTgks0s.jpeg",
                "isPro": false,
                "fullname": "Dehai Min",
                "user": "ZhishanQ",
                "type": "user"
            },
            "summary": "Dynamic Retrieval-Augmented Generation adaptively determines when to retrieve during generation to mitigate hallucinations in large language models (LLMs). However, existing methods rely on model-internal signals (e.g., logits, entropy), which are fundamentally unreliable because LLMs are typically ill-calibrated and often exhibit high confidence in erroneous outputs. We propose QuCo-RAG, which shifts from subjective confidence to objective statistics computed from pre-training data. Our method quantifies uncertainty through two stages: (1) before generation, we identify low-frequency entities indicating long-tail knowledge gaps; (2) during generation, we verify entity co-occurrence in the pre-training corpus, where zero co-occurrence often signals hallucination risk. Both stages leverage Infini-gram for millisecond-latency queries over 4 trillion tokens, triggering retrieval when uncertainty is high. Experiments on multi-hop QA benchmarks show QuCo-RAG achieves EM gains of 5--12 points over state-of-the-art baselines with OLMo-2 models, and transfers effectively to models with undisclosed pre-training data (Llama, Qwen, GPT), improving EM by up to 14 points. Domain generalization on biomedical QA further validates the robustness of our paradigm. These results establish corpus-grounded verification as a principled, practically model-agnostic paradigm for dynamic RAG. Our code is publicly available at https://github.com/ZhishanQ/QuCo-RAG.",
            "upvotes": 25,
            "discussionId": "694a1765335742716e9322bb",
            "githubRepo": "https://github.com/ZhishanQ/QuCo-RAG",
            "githubRepoAddedBy": "user",
            "ai_summary": "QuCo-RAG uses objective corpus statistics to mitigate hallucinations in large language models during generation, improving accuracy across various benchmarks.",
            "ai_keywords": [
                "dynamic retrieval-augmented generation",
                "large language models",
                "hallucinations",
                "model-internal signals",
                "logits",
                "entropy",
                "pre-training data",
                "uncertainty quantification",
                "low-frequency entities",
                "entity co-occurrence",
                "Infini-gram",
                "multi-hop QA",
                "EM gains",
                "OLMo-2",
                "Llama",
                "Qwen",
                "GPT",
                "biomedical QA",
                "domain generalization",
                "corpus-grounded verification"
            ],
            "githubStars": 8
        },
        "publishedAt": "2025-12-22T03:28:05.000Z",
        "title": "QuCo-RAG: Quantifying Uncertainty from the Pre-training Corpus for Dynamic Retrieval-Augmented Generation",
        "summary": "Dynamic Retrieval-Augmented Generation adaptively determines when to retrieve during generation to mitigate hallucinations in large language models (LLMs). However, existing methods rely on model-internal signals (e.g., logits, entropy), which are fundamentally unreliable because LLMs are typically ill-calibrated and often exhibit high confidence in erroneous outputs. We propose QuCo-RAG, which shifts from subjective confidence to objective statistics computed from pre-training data. Our method quantifies uncertainty through two stages: (1) before generation, we identify low-frequency entities indicating long-tail knowledge gaps; (2) during generation, we verify entity co-occurrence in the pre-training corpus, where zero co-occurrence often signals hallucination risk. Both stages leverage Infini-gram for millisecond-latency queries over 4 trillion tokens, triggering retrieval when uncertainty is high. Experiments on multi-hop QA benchmarks show QuCo-RAG achieves EM gains of 5--12 points over state-of-the-art baselines with OLMo-2 models, and transfers effectively to models with undisclosed pre-training data (Llama, Qwen, GPT), improving EM by up to 14 points. Domain generalization on biomedical QA further validates the robustness of our paradigm. These results establish corpus-grounded verification as a principled, practically model-agnostic paradigm for dynamic RAG. Our code is publicly available at https://github.com/ZhishanQ/QuCo-RAG.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.19134.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "629c6ee73a3221bb210afc2d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629c6ee73a3221bb210afc2d/Mg-VymVvHQn_pDrTgks0s.jpeg",
            "fullname": "Dehai Min",
            "name": "ZhishanQ",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.18880",
            "authors": [
                {
                    "_id": "694a0eb6335742716e932275",
                    "name": "Ming Li",
                    "hidden": false
                },
                {
                    "_id": "694a0eb6335742716e932276",
                    "name": "Han Chen",
                    "hidden": false
                },
                {
                    "_id": "694a0eb6335742716e932277",
                    "name": "Yunze Xiao",
                    "hidden": false
                },
                {
                    "_id": "694a0eb6335742716e932278",
                    "name": "Jian Chen",
                    "hidden": false
                },
                {
                    "_id": "694a0eb6335742716e932279",
                    "name": "Hong Jiao",
                    "hidden": false
                },
                {
                    "_id": "694a0eb6335742716e93227a",
                    "name": "Tianyi Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-21T20:41:36.000Z",
            "submittedOnDailyAt": "2025-12-23T01:11:10.771Z",
            "title": "Can LLMs Estimate Student Struggles? Human-AI Difficulty Alignment with Proficiency Simulation for Item Difficulty Prediction",
            "submittedOnDailyBy": {
                "_id": "647f5af5b0e96764589f3b2a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
                "isPro": false,
                "fullname": "Tianyi Zhou",
                "user": "zhoutianyi",
                "type": "user"
            },
            "summary": "Accurate estimation of item (question or task) difficulty is critical for educational assessment but suffers from the cold start problem. While Large Language Models demonstrate superhuman problem-solving capabilities, it remains an open question whether they can perceive the cognitive struggles of human learners. In this work, we present a large-scale empirical analysis of Human-AI Difficulty Alignment for over 20 models across diverse domains such as medical knowledge and mathematical reasoning. Our findings reveal a systematic misalignment where scaling up model size is not reliably helpful; instead of aligning with humans, models converge toward a shared machine consensus. We observe that high performance often impedes accurate difficulty estimation, as models struggle to simulate the capability limitations of students even when being explicitly prompted to adopt specific proficiency levels. Furthermore, we identify a critical lack of introspection, as models fail to predict their own limitations. These results suggest that general problem-solving capability does not imply an understanding of human cognitive struggles, highlighting the challenge of using current models for automated difficulty prediction.",
            "upvotes": 21,
            "discussionId": "694a0eb7335742716e93227b",
            "githubRepo": "https://github.com/MingLiiii/Difficulty_Alignment",
            "githubRepoAddedBy": "user",
            "ai_summary": "Large Language Models struggle to accurately estimate human cognitive difficulty due to a misalignment with human perceptions and a lack of introspection regarding their own limitations.",
            "ai_keywords": [
                "Human-AI Difficulty Alignment",
                "Large Language Models",
                "cognitive struggles",
                "machine consensus",
                "proficiency levels",
                "introspection"
            ],
            "githubStars": 0
        },
        "publishedAt": "2025-12-21T15:41:36.000Z",
        "title": "Can LLMs Estimate Student Struggles? Human-AI Difficulty Alignment with Proficiency Simulation for Item Difficulty Prediction",
        "summary": "Accurate estimation of item (question or task) difficulty is critical for educational assessment but suffers from the cold start problem. While Large Language Models demonstrate superhuman problem-solving capabilities, it remains an open question whether they can perceive the cognitive struggles of human learners. In this work, we present a large-scale empirical analysis of Human-AI Difficulty Alignment for over 20 models across diverse domains such as medical knowledge and mathematical reasoning. Our findings reveal a systematic misalignment where scaling up model size is not reliably helpful; instead of aligning with humans, models converge toward a shared machine consensus. We observe that high performance often impedes accurate difficulty estimation, as models struggle to simulate the capability limitations of students even when being explicitly prompted to adopt specific proficiency levels. Furthermore, we identify a critical lack of introspection, as models fail to predict their own limitations. These results suggest that general problem-solving capability does not imply an understanding of human cognitive struggles, highlighting the challenge of using current models for automated difficulty prediction.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.18880.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "647f5af5b0e96764589f3b2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
            "fullname": "Tianyi Zhou",
            "name": "zhoutianyi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 19
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.19678",
            "authors": [
                {
                    "_id": "694a087f335742716e9321e9",
                    "name": "Hanyang Kong",
                    "hidden": false
                },
                {
                    "_id": "694a087f335742716e9321ea",
                    "name": "Xingyi Yang",
                    "hidden": false
                },
                {
                    "_id": "694a087f335742716e9321eb",
                    "name": "Xiaoxu Zheng",
                    "hidden": false
                },
                {
                    "_id": "694a087f335742716e9321ec",
                    "name": "Xinchao Wang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6442882f8443bce4c98a88aa/mSmlhlqlIsBQJ1S0aEeSo.mp4"
            ],
            "publishedAt": "2025-12-22T18:53:50.000Z",
            "submittedOnDailyAt": "2025-12-23T01:31:35.792Z",
            "title": "WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion",
            "submittedOnDailyBy": {
                "_id": "6442882f8443bce4c98a88aa",
                "avatarUrl": "/avatars/70d5aa651b07b43629554096d76efd4c.svg",
                "isPro": false,
                "fullname": "Kong",
                "user": "imsuperkong",
                "type": "user"
            },
            "summary": "Generating long-range, geometrically consistent video presents a fundamental dilemma: while consistency demands strict adherence to 3D geometry in pixel space, state-of-the-art generative models operate most effectively in a camera-conditioned latent space. This disconnect causes current methods to struggle with occluded areas and complex camera trajectories. To bridge this gap, we propose WorldWarp, a framework that couples a 3D structural anchor with a 2D generative refiner. To establish geometric grounding, WorldWarp maintains an online 3D geometric cache built via Gaussian Splatting (3DGS). By explicitly warping historical content into novel views, this cache acts as a structural scaffold, ensuring each new frame respects prior geometry. However, static warping inevitably leaves holes and artifacts due to occlusions. We address this using a Spatio-Temporal Diffusion (ST-Diff) model designed for a \"fill-and-revise\" objective. Our key innovation is a spatio-temporal varying noise schedule: blank regions receive full noise to trigger generation, while warped regions receive partial noise to enable refinement. By dynamically updating the 3D cache at every step, WorldWarp maintains consistency across video chunks. Consequently, it achieves state-of-the-art fidelity by ensuring that 3D logic guides structure while diffusion logic perfects texture. Project page: https://hyokong.github.io/worldwarp-page/{https://hyokong.github.io/worldwarp-page/}.",
            "upvotes": 20,
            "discussionId": "694a087f335742716e9321ed",
            "projectPage": "https://hyokong.github.io/worldwarp-page/",
            "githubRepo": "https://github.com/HyoKong/WorldWarp",
            "githubRepoAddedBy": "user",
            "ai_summary": "WorldWarp addresses the challenge of generating consistent long-range videos by integrating a 3D geometric cache with a spatio-temporal diffusion model, ensuring structural consistency and textural refinement.",
            "ai_keywords": [
                "3D structural anchor",
                "2D generative refiner",
                "Gaussian Splatting",
                "Spatio-Temporal Diffusion",
                "spatio-temporal varying noise schedule",
                "3D cache",
                "structural scaffold",
                "fill-and-revise objective"
            ],
            "githubStars": 31,
            "organization": {
                "_id": "6508ab2b349930913196378b",
                "name": "NationalUniversityofSingapore",
                "fullname": "National University of Singapore",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZYUmpSMsa5Whihw3me2Bw.png"
            }
        },
        "publishedAt": "2025-12-22T13:53:50.000Z",
        "title": "WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion",
        "summary": "Generating long-range, geometrically consistent video presents a fundamental dilemma: while consistency demands strict adherence to 3D geometry in pixel space, state-of-the-art generative models operate most effectively in a camera-conditioned latent space. This disconnect causes current methods to struggle with occluded areas and complex camera trajectories. To bridge this gap, we propose WorldWarp, a framework that couples a 3D structural anchor with a 2D generative refiner. To establish geometric grounding, WorldWarp maintains an online 3D geometric cache built via Gaussian Splatting (3DGS). By explicitly warping historical content into novel views, this cache acts as a structural scaffold, ensuring each new frame respects prior geometry. However, static warping inevitably leaves holes and artifacts due to occlusions. We address this using a Spatio-Temporal Diffusion (ST-Diff) model designed for a \"fill-and-revise\" objective. Our key innovation is a spatio-temporal varying noise schedule: blank regions receive full noise to trigger generation, while warped regions receive partial noise to enable refinement. By dynamically updating the 3D cache at every step, WorldWarp maintains consistency across video chunks. Consequently, it achieves state-of-the-art fidelity by ensuring that 3D logic guides structure while diffusion logic perfects texture. Project page: https://hyokong.github.io/worldwarp-page/{https://hyokong.github.io/worldwarp-page/}.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6442882f8443bce4c98a88aa/mSmlhlqlIsBQJ1S0aEeSo.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.19678.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6442882f8443bce4c98a88aa",
            "avatarUrl": "/avatars/70d5aa651b07b43629554096d76efd4c.svg",
            "fullname": "Kong",
            "name": "imsuperkong",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "6508ab2b349930913196378b",
            "name": "NationalUniversityofSingapore",
            "fullname": "National University of Singapore",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZYUmpSMsa5Whihw3me2Bw.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.19629",
            "authors": [
                {
                    "_id": "694a13d0335742716e9322a0",
                    "name": "Jiaqi Peng",
                    "hidden": false
                },
                {
                    "_id": "694a13d0335742716e9322a1",
                    "name": "Wenzhe Cai",
                    "hidden": false
                },
                {
                    "_id": "694a13d0335742716e9322a2",
                    "name": "Yuqiang Yang",
                    "hidden": false
                },
                {
                    "_id": "694a13d0335742716e9322a3",
                    "name": "Tai Wang",
                    "hidden": false
                },
                {
                    "_id": "694a13d0335742716e9322a4",
                    "name": "Yuan Shen",
                    "hidden": false
                },
                {
                    "_id": "694a13d0335742716e9322a5",
                    "name": "Jiangmiao Pang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64e6d9d229a548f66aff6e5b/E5JZHmhqVQdd32fECzT8R.mp4"
            ],
            "publishedAt": "2025-12-22T18:03:08.000Z",
            "submittedOnDailyAt": "2025-12-23T03:05:41.880Z",
            "title": "LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry",
            "submittedOnDailyBy": {
                "_id": "64e6d9d229a548f66aff6e5b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6d9d229a548f66aff6e5b/yQ9E2TyzM4CfSjMPigcey.jpeg",
                "isPro": false,
                "fullname": "Tai Wang",
                "user": "taiwang",
                "type": "user"
            },
            "summary": "Trajectory planning in unstructured environments is a fundamental and challenging capability for mobile robots. Traditional modular pipelines suffer from latency and cascading errors across perception, localization, mapping, and planning modules. Recent end-to-end learning methods map raw visual observations directly to control signals or trajectories, promising greater performance and efficiency in open-world settings. However, most prior end-to-end approaches still rely on separate localization modules that depend on accurate sensor extrinsic calibration for self-state estimation, thereby limiting generalization across embodiments and environments. We introduce LoGoPlanner, a localization-grounded, end-to-end navigation framework that addresses these limitations by: (1) finetuning a long-horizon visual-geometry backbone to ground predictions with absolute metric scale, thereby providing implicit state estimation for accurate localization; (2) reconstructing surrounding scene geometry from historical observations to supply dense, fine-grained environmental awareness for reliable obstacle avoidance; and (3) conditioning the policy on implicit geometry bootstrapped by the aforementioned auxiliary tasks, thereby reducing error propagation.We evaluate LoGoPlanner in both simulation and real-world settings, where its fully end-to-end design reduces cumulative error while metric-aware geometry memory enhances planning consistency and obstacle avoidance, leading to more than a 27.3\\% improvement over oracle-localization baselines and strong generalization across embodiments and environments. The code and models have been made publicly available on the https://steinate.github.io/logoplanner.github.io/{project page}.",
            "upvotes": 18,
            "discussionId": "694a13d1335742716e9322a6",
            "ai_summary": "LoGoPlanner is an end-to-end navigation framework that improves trajectory planning in unstructured environments by integrating localization, scene geometry reconstruction, and policy conditioning.",
            "ai_keywords": [
                "trajectory planning",
                "unstructured environments",
                "end-to-end learning",
                "localization",
                "scene geometry reconstruction",
                "policy conditioning",
                "long-horizon visual-geometry backbone",
                "implicit state estimation",
                "obstacle avoidance",
                "metric-aware geometry memory"
            ]
        },
        "publishedAt": "2025-12-22T13:03:08.000Z",
        "title": "LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry",
        "summary": "Trajectory planning in unstructured environments is a fundamental and challenging capability for mobile robots. Traditional modular pipelines suffer from latency and cascading errors across perception, localization, mapping, and planning modules. Recent end-to-end learning methods map raw visual observations directly to control signals or trajectories, promising greater performance and efficiency in open-world settings. However, most prior end-to-end approaches still rely on separate localization modules that depend on accurate sensor extrinsic calibration for self-state estimation, thereby limiting generalization across embodiments and environments. We introduce LoGoPlanner, a localization-grounded, end-to-end navigation framework that addresses these limitations by: (1) finetuning a long-horizon visual-geometry backbone to ground predictions with absolute metric scale, thereby providing implicit state estimation for accurate localization; (2) reconstructing surrounding scene geometry from historical observations to supply dense, fine-grained environmental awareness for reliable obstacle avoidance; and (3) conditioning the policy on implicit geometry bootstrapped by the aforementioned auxiliary tasks, thereby reducing error propagation.We evaluate LoGoPlanner in both simulation and real-world settings, where its fully end-to-end design reduces cumulative error while metric-aware geometry memory enhances planning consistency and obstacle avoidance, leading to more than a 27.3\\% improvement over oracle-localization baselines and strong generalization across embodiments and environments. The code and models have been made publicly available on the https://steinate.github.io/logoplanner.github.io/{project page}.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64e6d9d229a548f66aff6e5b/E5JZHmhqVQdd32fECzT8R.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.19629.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64e6d9d229a548f66aff6e5b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6d9d229a548f66aff6e5b/yQ9E2TyzM4CfSjMPigcey.jpeg",
            "fullname": "Tai Wang",
            "name": "taiwang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.17385",
            "authors": [
                {
                    "_id": "6948fd1234f46eaf46cbb3be",
                    "name": "Jiajun Wu",
                    "hidden": false
                },
                {
                    "_id": "6948fd1234f46eaf46cbb3bf",
                    "name": "Jian Yang",
                    "hidden": false
                },
                {
                    "_id": "6948fd1234f46eaf46cbb3c0",
                    "name": "Wei Zhang",
                    "hidden": false
                },
                {
                    "_id": "6948fd1234f46eaf46cbb3c1",
                    "name": "Lin Jing",
                    "hidden": false
                },
                {
                    "_id": "6948fd1234f46eaf46cbb3c2",
                    "name": "Yuqing Ma",
                    "hidden": false
                },
                {
                    "_id": "6948fd1234f46eaf46cbb3c3",
                    "name": "Ensheng Shi",
                    "hidden": false
                },
                {
                    "_id": "6948fd1234f46eaf46cbb3c4",
                    "name": "Yuchi Ma",
                    "hidden": false
                },
                {
                    "_id": "6948fd1234f46eaf46cbb3c5",
                    "name": "Zhoujun Li",
                    "hidden": false
                },
                {
                    "_id": "6948fd1234f46eaf46cbb3c6",
                    "name": "Xianglong Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-19T09:42:04.000Z",
            "submittedOnDailyAt": "2025-12-23T03:58:44.703Z",
            "title": "UCoder: Unsupervised Code Generation by Internal Probing of Large Language Models",
            "submittedOnDailyBy": {
                "_id": "64ccb9bfead94891d12aef42",
                "avatarUrl": "/avatars/c54809d43d93d3f0766bd2555cecc4e3.svg",
                "isPro": false,
                "fullname": "Yang Jian",
                "user": "CSJianYang",
                "type": "user"
            },
            "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in code generation tasks. However, their effectiveness heavily relies on supervised training with extensive labeled (e.g., question-answering pairs) or unlabeled datasets (e.g., code snippets), which are often expensive and difficult to obtain at scale. To address this limitation, this paper introduces a method IPC, an unsupervised framework that leverages Internal Probing of LLMs for Code generation without any external corpus, even unlabeled code snippets. We introduce the problem space probing, test understanding probing, solution space probing, and knowledge consolidation and reinforcement to probe the internal knowledge and confidence patterns existing in LLMs. Further, IPC identifies reliable code candidates through self-consistency mechanisms and representation-based quality estimation to train UCoder (coder with unsupervised learning). We validate the proposed approach across multiple code benchmarks, demonstrating that unsupervised methods can achieve competitive performance compared to supervised approaches while significantly reducing the dependency on labeled data and computational resources. Analytic experiments reveal that internal model states contain rich signals about code quality and correctness, and that properly harnessing these signals enables effective unsupervised learning for code generation tasks, opening new directions for training code LLMs in resource-constrained scenarios.",
            "upvotes": 15,
            "discussionId": "6948fd1334f46eaf46cbb3c7",
            "ai_summary": "IPC is an unsupervised framework that uses internal probing of large language models to generate code without labeled datasets, achieving competitive performance with reduced resource dependency.",
            "ai_keywords": [
                "large language models",
                "unsupervised framework",
                "internal probing",
                "problem space probing",
                "test understanding probing",
                "solution space probing",
                "knowledge consolidation",
                "reinforcement",
                "self-consistency mechanisms",
                "representation-based quality estimation",
                "UCoder",
                "code benchmarks",
                "internal model states",
                "code quality",
                "correctness",
                "resource-constrained scenarios"
            ]
        },
        "publishedAt": "2025-12-19T04:42:04.000Z",
        "title": "UCoder: Unsupervised Code Generation by Internal Probing of Large Language Models",
        "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in code generation tasks. However, their effectiveness heavily relies on supervised training with extensive labeled (e.g., question-answering pairs) or unlabeled datasets (e.g., code snippets), which are often expensive and difficult to obtain at scale. To address this limitation, this paper introduces a method IPC, an unsupervised framework that leverages Internal Probing of LLMs for Code generation without any external corpus, even unlabeled code snippets. We introduce the problem space probing, test understanding probing, solution space probing, and knowledge consolidation and reinforcement to probe the internal knowledge and confidence patterns existing in LLMs. Further, IPC identifies reliable code candidates through self-consistency mechanisms and representation-based quality estimation to train UCoder (coder with unsupervised learning). We validate the proposed approach across multiple code benchmarks, demonstrating that unsupervised methods can achieve competitive performance compared to supervised approaches while significantly reducing the dependency on labeled data and computational resources. Analytic experiments reveal that internal model states contain rich signals about code quality and correctness, and that properly harnessing these signals enables effective unsupervised learning for code generation tasks, opening new directions for training code LLMs in resource-constrained scenarios.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.17385.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64ccb9bfead94891d12aef42",
            "avatarUrl": "/avatars/c54809d43d93d3f0766bd2555cecc4e3.svg",
            "fullname": "Yang Jian",
            "name": "CSJianYang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 22
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.19682",
            "authors": [
                {
                    "_id": "694a0dc4335742716e932249",
                    "name": "Jiacheng Guo",
                    "hidden": false
                },
                {
                    "_id": "694a0dc4335742716e93224a",
                    "name": "Ling Yang",
                    "hidden": false
                },
                {
                    "_id": "694a0dc4335742716e93224b",
                    "name": "Peter Chen",
                    "hidden": false
                },
                {
                    "_id": "694a0dc4335742716e93224c",
                    "name": "Qixin Xiao",
                    "hidden": false
                },
                {
                    "_id": "694a0dc4335742716e93224d",
                    "name": "Yinjie Wang",
                    "hidden": false
                },
                {
                    "_id": "694a0dc4335742716e93224e",
                    "name": "Xinzhe Juan",
                    "hidden": false
                },
                {
                    "_id": "694a0dc4335742716e93224f",
                    "name": "Jiahao Qiu",
                    "hidden": false
                },
                {
                    "_id": "694a0dc4335742716e932250",
                    "name": "Ke Shen",
                    "hidden": false
                },
                {
                    "_id": "694a0dc4335742716e932251",
                    "name": "Mengdi Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-22T18:57:13.000Z",
            "submittedOnDailyAt": "2025-12-23T01:04:41.574Z",
            "title": "GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Training capable Large Language Model (LLM) agents is critically bottlenecked by the high cost and static nature of real-world interaction data. We address this by introducing GenEnv, a framework that establishes a difficulty-aligned co-evolutionary game between an agent and a scalable, generative environment simulator. Unlike traditional methods that evolve models on static datasets, GenEnv instantiates a dataevolving: the simulator acts as a dynamic curriculum policy, continuously generating tasks specifically tailored to the agent's ``zone of proximal development''. This process is guided by a simple but effective -Curriculum Reward, which aligns task difficulty with the agent's current capabilities. We evaluate GenEnv on five benchmarks, including API-Bank, ALFWorld, BFCL, Bamboogle, and TravelPlanner. Across these tasks, GenEnv improves agent performance by up to +40.3\\% over 7B baselines and matches or exceeds the average performance of larger models. Compared to Gemini 2.5 Pro-based offline data augmentation, GenEnv achieves better performance while using 3.3times less data. By shifting from static supervision to adaptive simulation, GenEnv provides a data-efficient pathway for scaling agent capabilities.",
            "upvotes": 12,
            "discussionId": "694a0dc5335742716e932252",
            "githubRepo": "https://github.com/Gen-Verse/GenEnv",
            "githubRepoAddedBy": "user",
            "ai_summary": "GenEnv, a framework using a co-evolutionary game with a generative environment simulator, enhances LLM agent performance by 40.3% over 7B baselines and uses less data than offline augmentation.",
            "ai_keywords": [
                "Large Language Model",
                "LLM",
                "GenEnv",
                "co-evolutionary game",
                "generative environment simulator",
                "dynamic curriculum policy",
                "zone of proximal development",
                "-Curriculum Reward",
                "API-Bank",
                "ALFWorld",
                "BFCL",
                "Bamboogle",
                "TravelPlanner",
                "data-efficient",
                "adaptive simulation"
            ],
            "githubStars": 8,
            "organization": {
                "_id": "64374111a701a7e744c02b0e",
                "name": "princetonu",
                "fullname": "Princeton University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/b3xXusq8Zz3ej8Z6fRTSZ.png"
            }
        },
        "publishedAt": "2025-12-22T13:57:13.000Z",
        "title": "GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators",
        "summary": "Training capable Large Language Model (LLM) agents is critically bottlenecked by the high cost and static nature of real-world interaction data. We address this by introducing GenEnv, a framework that establishes a difficulty-aligned co-evolutionary game between an agent and a scalable, generative environment simulator. Unlike traditional methods that evolve models on static datasets, GenEnv instantiates a dataevolving: the simulator acts as a dynamic curriculum policy, continuously generating tasks specifically tailored to the agent's ``zone of proximal development''. This process is guided by a simple but effective -Curriculum Reward, which aligns task difficulty with the agent's current capabilities. We evaluate GenEnv on five benchmarks, including API-Bank, ALFWorld, BFCL, Bamboogle, and TravelPlanner. Across these tasks, GenEnv improves agent performance by up to +40.3\\% over 7B baselines and matches or exceeds the average performance of larger models. Compared to Gemini 2.5 Pro-based offline data augmentation, GenEnv achieves better performance while using 3.3times less data. By shifting from static supervision to adaptive simulation, GenEnv provides a data-efficient pathway for scaling agent capabilities.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.19682.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 189
        },
        "organization": {
            "_id": "64374111a701a7e744c02b0e",
            "name": "princetonu",
            "fullname": "Princeton University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/b3xXusq8Zz3ej8Z6fRTSZ.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.19539",
            "authors": [
                {
                    "_id": "694a1285335742716e932295",
                    "name": "Kaiwen Zhang",
                    "hidden": false
                },
                {
                    "_id": "694a1285335742716e932296",
                    "name": "Liming Jiang",
                    "hidden": false
                },
                {
                    "_id": "694a1285335742716e932297",
                    "name": "Angtian Wang",
                    "hidden": false
                },
                {
                    "_id": "694a1285335742716e932298",
                    "name": "Jacob Zhiyuan Fang",
                    "hidden": false
                },
                {
                    "_id": "694a1285335742716e932299",
                    "name": "Tiancheng Zhi",
                    "hidden": false
                },
                {
                    "_id": "694a1285335742716e93229a",
                    "name": "Qing Yan",
                    "hidden": false
                },
                {
                    "_id": "694a1285335742716e93229b",
                    "name": "Hao Kang",
                    "hidden": false
                },
                {
                    "_id": "694a1285335742716e93229c",
                    "name": "Xin Lu",
                    "hidden": false
                },
                {
                    "_id": "694a1285335742716e93229d",
                    "name": "Xingang Pan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-22T16:23:24.000Z",
            "submittedOnDailyAt": "2025-12-23T01:25:33.507Z",
            "title": "StoryMem: Multi-shot Long Video Storytelling with Memory",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Visual storytelling requires generating multi-shot videos with cinematic quality and long-range consistency. Inspired by human memory, we propose StoryMem, a paradigm that reformulates long-form video storytelling as iterative shot synthesis conditioned on explicit visual memory, transforming pre-trained single-shot video diffusion models into multi-shot storytellers. This is achieved by a novel Memory-to-Video (M2V) design, which maintains a compact and dynamically updated memory bank of keyframes from historical generated shots. The stored memory is then injected into single-shot video diffusion models via latent concatenation and negative RoPE shifts with only LoRA fine-tuning. A semantic keyframe selection strategy, together with aesthetic preference filtering, further ensures informative and stable memory throughout generation. Moreover, the proposed framework naturally accommodates smooth shot transitions and customized story generation applications. To facilitate evaluation, we introduce ST-Bench, a diverse benchmark for multi-shot video storytelling. Extensive experiments demonstrate that StoryMem achieves superior cross-shot consistency over previous methods while preserving high aesthetic quality and prompt adherence, marking a significant step toward coherent minute-long video storytelling.",
            "upvotes": 10,
            "discussionId": "694a1285335742716e93229e",
            "projectPage": "https://kevin-thu.github.io/StoryMem/",
            "githubRepo": "https://github.com/Kevin-thu/StoryMem",
            "githubRepoAddedBy": "user",
            "ai_summary": "StoryMem enhances multi-shot video generation with cinematic quality and long-range consistency using a memory bank and pre-trained single-shot video diffusion models.",
            "ai_keywords": [
                "StoryMem",
                "Memory-to-Video (M2V)",
                "memory bank",
                "keyframes",
                "latent concatenation",
                "negative RoPE shifts",
                "LoRA fine-tuning",
                "semantic keyframe selection",
                "aesthetic preference filtering",
                "shot transitions",
                "ST-Bench",
                "cross-shot consistency",
                "aesthetic quality",
                "prompt adherence"
            ],
            "githubStars": 7,
            "organization": {
                "_id": "653b817d32c97d0655575872",
                "name": "ByteDance",
                "fullname": "ByteDance",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
            }
        },
        "publishedAt": "2025-12-22T11:23:24.000Z",
        "title": "StoryMem: Multi-shot Long Video Storytelling with Memory",
        "summary": "Visual storytelling requires generating multi-shot videos with cinematic quality and long-range consistency. Inspired by human memory, we propose StoryMem, a paradigm that reformulates long-form video storytelling as iterative shot synthesis conditioned on explicit visual memory, transforming pre-trained single-shot video diffusion models into multi-shot storytellers. This is achieved by a novel Memory-to-Video (M2V) design, which maintains a compact and dynamically updated memory bank of keyframes from historical generated shots. The stored memory is then injected into single-shot video diffusion models via latent concatenation and negative RoPE shifts with only LoRA fine-tuning. A semantic keyframe selection strategy, together with aesthetic preference filtering, further ensures informative and stable memory throughout generation. Moreover, the proposed framework naturally accommodates smooth shot transitions and customized story generation applications. To facilitate evaluation, we introduce ST-Bench, a diverse benchmark for multi-shot video storytelling. Extensive experiments demonstrate that StoryMem achieves superior cross-shot consistency over previous methods while preserving high aesthetic quality and prompt adherence, marking a significant step toward coherent minute-long video storytelling.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.19539.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 189
        },
        "organization": {
            "_id": "653b817d32c97d0655575872",
            "name": "ByteDance",
            "fullname": "ByteDance",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.16229",
            "authors": [
                {
                    "_id": "694a70d6335742716e932442",
                    "name": "Chenkai Xu",
                    "hidden": false
                },
                {
                    "_id": "694a70d6335742716e932443",
                    "name": "Yijie Jin",
                    "hidden": false
                },
                {
                    "_id": "694a70d6335742716e932444",
                    "name": "Jiajun Li",
                    "hidden": false
                },
                {
                    "_id": "694a70d6335742716e932445",
                    "name": "Yi Tu",
                    "hidden": false
                },
                {
                    "_id": "694a70d6335742716e932446",
                    "name": "Guoping Long",
                    "hidden": false
                },
                {
                    "_id": "694a70d6335742716e932447",
                    "name": "Dandan Tu",
                    "hidden": false
                },
                {
                    "_id": "694a70d6335742716e932448",
                    "name": "Mingcong Song",
                    "hidden": false
                },
                {
                    "_id": "694a70d6335742716e932449",
                    "name": "Hongjie Si",
                    "hidden": false
                },
                {
                    "_id": "694a70d6335742716e93244a",
                    "name": "Tianqi Hou",
                    "hidden": false
                },
                {
                    "_id": "694a70d6335742716e93244b",
                    "name": "Junchi Yan",
                    "hidden": false
                },
                {
                    "_id": "694a70d6335742716e93244c",
                    "name": "Zhijie Deng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-18T06:22:01.000Z",
            "submittedOnDailyAt": "2025-12-23T08:09:53.334Z",
            "title": "LoPA: Scaling dLLM Inference via Lookahead Parallel Decoding",
            "submittedOnDailyBy": {
                "_id": "65708920806dee337da0eef5",
                "avatarUrl": "/avatars/945e328dedc8e1e3111f48c344ad5b03.svg",
                "isPro": false,
                "fullname": "xuchenkai",
                "user": "UnhurriedDawn",
                "type": "user"
            },
            "summary": "Diffusion Large Language Models (dLLMs) have demonstrated significant potential for high-speed inference. However, current confidence-driven decoding strategies are constrained by limited parallelism, typically achieving only 1--3 tokens per forward pass (TPF). In this work, we identify that the degree of parallelism during dLLM inference is highly sensitive to the Token Filling Order (TFO). Then, we introduce Lookahead PArallel Decoding LoPA, a training-free, plug-and-play algorithm, to identify a superior TFO and hence accelerate inference. LoPA concurrently explores distinct candidate TFOs via parallel branches, and selects the one with the highest potential for future parallelism based on branch confidence. We apply LoPA to the state-of-the-art D2F model and observe a substantial enhancement in decoding efficiency. Notably, LoPA increases the TPF of D2F-Dream to 10.1 on the GSM8K while maintaining performance superior to the Dream baseline. Furthermore, to facilitate this unprecedented degree of parallelism, we develop a specialized multi-device inference system featuring Branch Parallelism (BP), which achieves a single-sample throughput of 1073.9 tokens per second under multi-GPU deployment. The code is available at https://github.com/zhijie-group/LoPA.",
            "upvotes": 10,
            "discussionId": "694a70d6335742716e93244d",
            "ai_summary": "LoPA, a training-free algorithm, enhances the parallelism of diffusion large language models, doubling the tokens per forward pass and boosting throughput with multi-GPU deployment.",
            "ai_keywords": [
                "Diffusion Large Language Models",
                "dLLMs",
                "confidence-driven decoding",
                "Token Filling Order",
                "TFO",
                "Lookahead PArallel Decoding",
                "LoPA",
                "branch confidence",
                "D2F model",
                "GSM8K",
                "Dream baseline",
                "Branch Parallelism",
                "BP",
                "multi-device inference system",
                "single-sample throughput"
            ],
            "organization": {
                "_id": "673d5fe8d031224e947dc235",
                "name": "SJTU-Deng-Lab",
                "fullname": "DENG Lab @ SJTU",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64bba541da140e461924dfed/_WPqM9jCqIIkS73aTeZP-.png"
            }
        },
        "publishedAt": "2025-12-18T01:22:01.000Z",
        "title": "LoPA: Scaling dLLM Inference via Lookahead Parallel Decoding",
        "summary": "Diffusion Large Language Models (dLLMs) have demonstrated significant potential for high-speed inference. However, current confidence-driven decoding strategies are constrained by limited parallelism, typically achieving only 1--3 tokens per forward pass (TPF). In this work, we identify that the degree of parallelism during dLLM inference is highly sensitive to the Token Filling Order (TFO). Then, we introduce Lookahead PArallel Decoding LoPA, a training-free, plug-and-play algorithm, to identify a superior TFO and hence accelerate inference. LoPA concurrently explores distinct candidate TFOs via parallel branches, and selects the one with the highest potential for future parallelism based on branch confidence. We apply LoPA to the state-of-the-art D2F model and observe a substantial enhancement in decoding efficiency. Notably, LoPA increases the TPF of D2F-Dream to 10.1 on the GSM8K while maintaining performance superior to the Dream baseline. Furthermore, to facilitate this unprecedented degree of parallelism, we develop a specialized multi-device inference system featuring Branch Parallelism (BP), which achieves a single-sample throughput of 1073.9 tokens per second under multi-GPU deployment. The code is available at https://github.com/zhijie-group/LoPA.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16229.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65708920806dee337da0eef5",
            "avatarUrl": "/avatars/945e328dedc8e1e3111f48c344ad5b03.svg",
            "fullname": "xuchenkai",
            "name": "UnhurriedDawn",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "673d5fe8d031224e947dc235",
            "name": "SJTU-Deng-Lab",
            "fullname": "DENG Lab @ SJTU",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64bba541da140e461924dfed/_WPqM9jCqIIkS73aTeZP-.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.17206",
            "authors": [
                {
                    "_id": "694a20cc335742716e932304",
                    "name": "Rujiao Long",
                    "hidden": false
                },
                {
                    "_id": "694a20cc335742716e932305",
                    "name": "Yang Li",
                    "hidden": false
                },
                {
                    "_id": "694a20cc335742716e932306",
                    "name": "Xingyao Zhang",
                    "hidden": false
                },
                {
                    "_id": "694a20cc335742716e932307",
                    "name": "Weixun Wang",
                    "hidden": false
                },
                {
                    "_id": "694a20cc335742716e932308",
                    "name": "Tianqianjin Lin",
                    "hidden": false
                },
                {
                    "_id": "694a20cc335742716e932309",
                    "name": "Xi Zhao",
                    "hidden": false
                },
                {
                    "_id": "694a20cc335742716e93230a",
                    "name": "Yuchi Xu",
                    "hidden": false
                },
                {
                    "_id": "694a20cc335742716e93230b",
                    "name": "Wenbo Su",
                    "hidden": false
                },
                {
                    "_id": "694a20cc335742716e93230c",
                    "name": "Junchi Yan",
                    "hidden": false
                },
                {
                    "_id": "694a20cc335742716e93230d",
                    "name": "Bo Zheng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-19T03:32:53.000Z",
            "submittedOnDailyAt": "2025-12-23T02:26:28.177Z",
            "title": "Reasoning Palette: Modulating Reasoning via Latent Contextualization for Controllable Exploration for (V)LMs",
            "submittedOnDailyBy": {
                "_id": "6570587b44d0620d221c722b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/T-B6q6sqhWAyXM7Hk_qWp.jpeg",
                "isPro": false,
                "fullname": "Yang Li (SJTU & SII)",
                "user": "yangcole",
                "type": "user"
            },
            "summary": "Exploration capacity shapes both inference-time performance and reinforcement learning (RL) training for large (vision-) language models, as stochastic sampling often yields redundant reasoning paths with little high-level diversity. This paper proposes Reasoning Palette, a novel latent-modulation framework that endows the model with a stochastic latent variable for strategic contextualization, guiding its internal planning prior to token generation. This latent context is inferred from the mean-pooled embedding of a question-answer pair via a variational autoencoder (VAE), where each sampled latent potentially encodes a distinct reasoning context. During inference, a sampled latent is decoded into learnable token prefixes and prepended to the input prompt, modulating the model's internal reasoning trajectory. In this way, the model performs internal sampling over reasoning strategies prior to output generation, which shapes the style and structure of the entire response sequence. A brief supervised fine-tuning (SFT) warm-up phase allows the model to adapt to this latent conditioning. Within RL optimization, Reasoning Palette facilitates structured exploration by enabling on-demand injection for diverse reasoning modes, significantly enhancing exploration efficiency and sustained learning capability. Experiments across multiple reasoning benchmarks demonstrate that our method enables interpretable and controllable control over the (vision-) language model's strategic behavior, thereby achieving consistent performance gains over standard RL methods.",
            "upvotes": 9,
            "discussionId": "694a20cc335742716e93230e",
            "ai_summary": "Reasoning Palette enhances large language models by using a latent-modulation framework to guide internal planning and improve both inference and reinforcement learning performance.",
            "ai_keywords": [
                "latent-modulation framework",
                "stochastic latent variable",
                "strategic contextualization",
                "variational autoencoder (VAE)",
                "latent context",
                "mean-pooled embedding",
                "token prefixes",
                "internal reasoning trajectory",
                "supervised fine-tuning (SFT)",
                "structured exploration",
                "diverse reasoning modes",
                "reinforcement learning (RL)",
                "strategic behavior"
            ],
            "organization": {
                "_id": "64488b334988ee01f2a8d856",
                "name": "alibaba-inc",
                "fullname": "alibaba-inc",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"
            }
        },
        "publishedAt": "2025-12-18T22:32:53.000Z",
        "title": "Reasoning Palette: Modulating Reasoning via Latent Contextualization for Controllable Exploration for (V)LMs",
        "summary": "Exploration capacity shapes both inference-time performance and reinforcement learning (RL) training for large (vision-) language models, as stochastic sampling often yields redundant reasoning paths with little high-level diversity. This paper proposes Reasoning Palette, a novel latent-modulation framework that endows the model with a stochastic latent variable for strategic contextualization, guiding its internal planning prior to token generation. This latent context is inferred from the mean-pooled embedding of a question-answer pair via a variational autoencoder (VAE), where each sampled latent potentially encodes a distinct reasoning context. During inference, a sampled latent is decoded into learnable token prefixes and prepended to the input prompt, modulating the model's internal reasoning trajectory. In this way, the model performs internal sampling over reasoning strategies prior to output generation, which shapes the style and structure of the entire response sequence. A brief supervised fine-tuning (SFT) warm-up phase allows the model to adapt to this latent conditioning. Within RL optimization, Reasoning Palette facilitates structured exploration by enabling on-demand injection for diverse reasoning modes, significantly enhancing exploration efficiency and sustained learning capability. Experiments across multiple reasoning benchmarks demonstrate that our method enables interpretable and controllable control over the (vision-) language model's strategic behavior, thereby achieving consistent performance gains over standard RL methods.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.17206.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6570587b44d0620d221c722b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/T-B6q6sqhWAyXM7Hk_qWp.jpeg",
            "fullname": "Yang Li (SJTU & SII)",
            "name": "yangcole",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "64488b334988ee01f2a8d856",
            "name": "alibaba-inc",
            "fullname": "alibaba-inc",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.19432",
            "authors": [
                {
                    "_id": "694a0e4e335742716e932266",
                    "name": "Quyu Kong",
                    "hidden": false
                },
                {
                    "_id": "694a0e4e335742716e932267",
                    "name": "Xu Zhang",
                    "hidden": false
                },
                {
                    "_id": "694a0e4e335742716e932268",
                    "name": "Zhenyu Yang",
                    "hidden": false
                },
                {
                    "_id": "694a0e4e335742716e932269",
                    "name": "Nolan Gao",
                    "hidden": false
                },
                {
                    "_id": "694a0e4e335742716e93226a",
                    "name": "Chen Liu",
                    "hidden": false
                },
                {
                    "_id": "694a0e4e335742716e93226b",
                    "name": "Panrong Tong",
                    "hidden": false
                },
                {
                    "_id": "694a0e4e335742716e93226c",
                    "name": "Chenglin Cai",
                    "hidden": false
                },
                {
                    "_id": "694a0e4e335742716e93226d",
                    "name": "Hanzhang Zhou",
                    "hidden": false
                },
                {
                    "_id": "694a0e4e335742716e93226e",
                    "name": "Jianan Zhang",
                    "hidden": false
                },
                {
                    "_id": "694a0e4e335742716e93226f",
                    "name": "Liangyu Chen",
                    "hidden": false
                },
                {
                    "_id": "694a0e4e335742716e932270",
                    "name": "Zhidan Liu",
                    "hidden": false
                },
                {
                    "_id": "694a0e4e335742716e932271",
                    "name": "Steven Hoi",
                    "hidden": false
                },
                {
                    "_id": "694a0e4e335742716e932272",
                    "name": "Yue Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-22T14:31:28.000Z",
            "submittedOnDailyAt": "2025-12-23T01:07:02.570Z",
            "title": "MobileWorld: Benchmarking Autonomous Mobile Agents in Agent-User Interactive, and MCP-Augmented Environments",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Among existing online mobile-use benchmarks, AndroidWorld has emerged as the dominant benchmark due to its reproducible environment and deterministic evaluation; however, recent agents achieving over 90% success rates indicate its saturation and motivate the need for a more challenging benchmark. In addition, its environment lacks key application categories, such as e-commerce and enterprise communication, and does not reflect realistic mobile-use scenarios characterized by vague user instructions and hybrid tool usage. To bridge this gap, we introduce MobileWorld, a substantially more challenging benchmark designed to better reflect real-world mobile usage, comprising 201 tasks across 20 applications, while maintaining the same level of reproducible evaluation as AndroidWorld. The difficulty of MobileWorld is twofold. First, it emphasizes long-horizon tasks with cross-application interactions: MobileWorld requires nearly twice as many task-completion steps on average (27.8 vs. 14.3) and includes far more multi-application tasks (62.2% vs. 9.5%) compared to AndroidWorld. Second, MobileWorld extends beyond standard GUI manipulation by introducing novel task categories, including agent-user interaction and MCP-augmented tasks. To ensure robust evaluation, we provide snapshot-based container environment and precise functional verifications, including backend database inspection and task callback APIs. We further develop a planner-executor agentic framework with extended action spaces to support user interactions and MCP calls. Our results reveal a sharp performance drop compared to AndroidWorld, with the best agentic framework and end-to-end model achieving 51.7% and 20.9% success rates, respectively. Our analysis shows that current models struggle significantly with user interaction and MCP calls, offering a strategic roadmap toward more robust, next-generation mobile intelligence.",
            "upvotes": 7,
            "discussionId": "694a0e4e335742716e932273",
            "ai_summary": "MobileWorld, a more challenging benchmark than AndroidWorld, includes diverse real-world mobile tasks and interactions, revealing significant gaps in current model capabilities.",
            "ai_keywords": [
                "long-horizon tasks",
                "cross-application interactions",
                "agent-user interaction",
                "MCP-augmented tasks",
                "snapshot-based container environment",
                "planner-executor agentic framework",
                "extended action spaces"
            ],
            "organization": {
                "_id": "67d15cca6e2cf0e062dbfb54",
                "name": "AlibabaTongyiLab",
                "fullname": "TongyiLab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
            }
        },
        "publishedAt": "2025-12-22T09:31:28.000Z",
        "title": "MobileWorld: Benchmarking Autonomous Mobile Agents in Agent-User Interactive, and MCP-Augmented Environments",
        "summary": "Among existing online mobile-use benchmarks, AndroidWorld has emerged as the dominant benchmark due to its reproducible environment and deterministic evaluation; however, recent agents achieving over 90% success rates indicate its saturation and motivate the need for a more challenging benchmark. In addition, its environment lacks key application categories, such as e-commerce and enterprise communication, and does not reflect realistic mobile-use scenarios characterized by vague user instructions and hybrid tool usage. To bridge this gap, we introduce MobileWorld, a substantially more challenging benchmark designed to better reflect real-world mobile usage, comprising 201 tasks across 20 applications, while maintaining the same level of reproducible evaluation as AndroidWorld. The difficulty of MobileWorld is twofold. First, it emphasizes long-horizon tasks with cross-application interactions: MobileWorld requires nearly twice as many task-completion steps on average (27.8 vs. 14.3) and includes far more multi-application tasks (62.2% vs. 9.5%) compared to AndroidWorld. Second, MobileWorld extends beyond standard GUI manipulation by introducing novel task categories, including agent-user interaction and MCP-augmented tasks. To ensure robust evaluation, we provide snapshot-based container environment and precise functional verifications, including backend database inspection and task callback APIs. We further develop a planner-executor agentic framework with extended action spaces to support user interactions and MCP calls. Our results reveal a sharp performance drop compared to AndroidWorld, with the best agentic framework and end-to-end model achieving 51.7% and 20.9% success rates, respectively. Our analysis shows that current models struggle significantly with user interaction and MCP calls, offering a strategic roadmap toward more robust, next-generation mobile intelligence.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.19432.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 189
        },
        "organization": {
            "_id": "67d15cca6e2cf0e062dbfb54",
            "name": "AlibabaTongyiLab",
            "fullname": "TongyiLab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.18658",
            "authors": [
                {
                    "_id": "694a52e5335742716e9323de",
                    "name": "Pierre Colombo",
                    "hidden": false
                },
                {
                    "_id": "694a52e5335742716e9323df",
                    "name": "Malik Boudiaf",
                    "hidden": false
                },
                {
                    "_id": "694a52e5335742716e9323e0",
                    "name": "Allyn Sweet",
                    "hidden": false
                },
                {
                    "_id": "694a52e5335742716e9323e1",
                    "name": "Michael Desa",
                    "hidden": false
                },
                {
                    "_id": "694a52e5335742716e9323e2",
                    "name": "Hongxi Wang",
                    "hidden": false
                },
                {
                    "_id": "694a52e5335742716e9323e3",
                    "name": "Kevin Candra",
                    "hidden": false
                },
                {
                    "_id": "694a52e5335742716e9323e4",
                    "name": "Symon del Marmol",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-21T09:12:21.000Z",
            "submittedOnDailyAt": "2025-12-23T06:03:42.723Z",
            "title": "Does It Tie Out? Towards Autonomous Legal Agents in Venture Capital",
            "submittedOnDailyBy": {
                "_id": "644a900e3a619fe72b14af0f",
                "avatarUrl": "/avatars/e2d5dac3d92757ed48e37e126a3464a3.svg",
                "isPro": false,
                "fullname": "Colombo",
                "user": "PierreColombo",
                "type": "user"
            },
            "summary": "Before closing venture capital financing rounds, lawyers conduct diligence that includes tying out the capitalization table: verifying that every security (for example, shares, options, warrants) and issuance term (for example, vesting schedules, acceleration triggers, transfer restrictions) is supported by large sets of underlying legal documentation. While LLMs continue to improve on legal benchmarks, specialized legal workflows, such as capitalization tie-out, remain out of reach even for strong agentic systems. The task requires multi-document reasoning, strict evidence traceability, and deterministic outputs that current approaches fail to reliably deliver. We characterize capitalization tie-out as an instance of a real-world benchmark for legal AI, analyze and compare the performance of existing agentic systems, and propose a world model architecture toward tie-out automation-and more broadly as a foundation for applied legal intelligence.",
            "upvotes": 5,
            "discussionId": "694a52e5335742716e9323e5",
            "ai_summary": "Capitalization tie-out in legal diligence is a complex task that current agentic systems struggle to automate due to its multi-document reasoning and evidence traceability requirements.",
            "ai_keywords": [
                "capitalization table",
                "securities",
                "shares",
                "options",
                "warrants",
                "issuance terms",
                "vesting schedules",
                "acceleration triggers",
                "transfer restrictions",
                "multi-document reasoning",
                "evidence traceability",
                "deterministic outputs",
                "legal AI",
                "world model architecture",
                "tie-out automation",
                "applied legal intelligence"
            ],
            "organization": {
                "_id": "6437b55aa1fa778a4da68073",
                "name": "Equall",
                "fullname": "Equall",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62cee925e7f6014c0e9d7b93/BlO0g0k8-ziReNou8BksX.png"
            }
        },
        "publishedAt": "2025-12-21T04:12:21.000Z",
        "title": "Does It Tie Out? Towards Autonomous Legal Agents in Venture Capital",
        "summary": "Before closing venture capital financing rounds, lawyers conduct diligence that includes tying out the capitalization table: verifying that every security (for example, shares, options, warrants) and issuance term (for example, vesting schedules, acceleration triggers, transfer restrictions) is supported by large sets of underlying legal documentation. While LLMs continue to improve on legal benchmarks, specialized legal workflows, such as capitalization tie-out, remain out of reach even for strong agentic systems. The task requires multi-document reasoning, strict evidence traceability, and deterministic outputs that current approaches fail to reliably deliver. We characterize capitalization tie-out as an instance of a real-world benchmark for legal AI, analyze and compare the performance of existing agentic systems, and propose a world model architecture toward tie-out automation-and more broadly as a foundation for applied legal intelligence.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.18658.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "644a900e3a619fe72b14af0f",
            "avatarUrl": "/avatars/e2d5dac3d92757ed48e37e126a3464a3.svg",
            "fullname": "Colombo",
            "name": "PierreColombo",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 17
        },
        "organization": {
            "_id": "6437b55aa1fa778a4da68073",
            "name": "Equall",
            "fullname": "Equall",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62cee925e7f6014c0e9d7b93/BlO0g0k8-ziReNou8BksX.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.19402",
            "authors": [
                {
                    "_id": "694a1f4e335742716e9322d0",
                    "name": "Yujie Zhao",
                    "hidden": false
                },
                {
                    "_id": "694a1f4e335742716e9322d1",
                    "name": "Hongwei Fan",
                    "hidden": false
                },
                {
                    "_id": "694a1f4e335742716e9322d2",
                    "name": "Di Chen",
                    "hidden": false
                },
                {
                    "_id": "694a1f4e335742716e9322d3",
                    "name": "Shengcong Chen",
                    "hidden": false
                },
                {
                    "_id": "694a1f4e335742716e9322d4",
                    "name": "Liliang Chen",
                    "hidden": false
                },
                {
                    "_id": "694a1f4e335742716e9322d5",
                    "name": "Xiaoqi Li",
                    "hidden": false
                },
                {
                    "_id": "694a1f4e335742716e9322d6",
                    "name": "Guanghui Ren",
                    "hidden": false
                },
                {
                    "_id": "694a1f4e335742716e9322d7",
                    "name": "Hao Dong",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-22T13:53:25.000Z",
            "submittedOnDailyAt": "2025-12-23T02:24:26.860Z",
            "title": "Real2Edit2Real: Generating Robotic Demonstrations via a 3D Control Interface",
            "submittedOnDailyBy": {
                "_id": "6721acf7bbf9703bc285e840",
                "avatarUrl": "/avatars/487f5cae476758d318465aa9ed103568.svg",
                "isPro": false,
                "fullname": "Yujie Zhao",
                "user": "HomieZ",
                "type": "user"
            },
            "summary": "Recent progress in robot learning has been driven by large-scale datasets and powerful visuomotor policy architectures, yet policy robustness remains limited by the substantial cost of collecting diverse demonstrations, particularly for spatial generalization in manipulation tasks. To reduce repetitive data collection, we present Real2Edit2Real, a framework that generates new demonstrations by bridging 3D editability with 2D visual data through a 3D control interface. Our approach first reconstructs scene geometry from multi-view RGB observations with a metric-scale 3D reconstruction model. Based on the reconstructed geometry, we perform depth-reliable 3D editing on point clouds to generate new manipulation trajectories while geometrically correcting the robot poses to recover physically consistent depth, which serves as a reliable condition for synthesizing new demonstrations. Finally, we propose a multi-conditional video generation model guided by depth as the primary control signal, together with action, edge, and ray maps, to synthesize spatially augmented multi-view manipulation videos. Experiments on four real-world manipulation tasks demonstrate that policies trained on data generated from only 1-5 source demonstrations can match or outperform those trained on 50 real-world demonstrations, improving data efficiency by up to 10-50x. Moreover, experimental results on height and texture editing demonstrate the framework's flexibility and extensibility, indicating its potential to serve as a unified data generation framework.",
            "upvotes": 4,
            "discussionId": "694a1f4e335742716e9322d8",
            "ai_summary": "A framework called Real2Edit2Real generates new manipulation demonstrations by using 3D reconstruction, editing, and video synthesis, improving data efficiency in robot learning.",
            "ai_keywords": [
                "metric-scale 3D reconstruction model",
                "depth-reliable 3D editing",
                "multi-conditional video generation model",
                "depth",
                "action",
                "edge",
                "ray maps",
                "spatially augmented multi-view manipulation videos"
            ]
        },
        "publishedAt": "2025-12-22T08:53:25.000Z",
        "title": "Real2Edit2Real: Generating Robotic Demonstrations via a 3D Control Interface",
        "summary": "Recent progress in robot learning has been driven by large-scale datasets and powerful visuomotor policy architectures, yet policy robustness remains limited by the substantial cost of collecting diverse demonstrations, particularly for spatial generalization in manipulation tasks. To reduce repetitive data collection, we present Real2Edit2Real, a framework that generates new demonstrations by bridging 3D editability with 2D visual data through a 3D control interface. Our approach first reconstructs scene geometry from multi-view RGB observations with a metric-scale 3D reconstruction model. Based on the reconstructed geometry, we perform depth-reliable 3D editing on point clouds to generate new manipulation trajectories while geometrically correcting the robot poses to recover physically consistent depth, which serves as a reliable condition for synthesizing new demonstrations. Finally, we propose a multi-conditional video generation model guided by depth as the primary control signal, together with action, edge, and ray maps, to synthesize spatially augmented multi-view manipulation videos. Experiments on four real-world manipulation tasks demonstrate that policies trained on data generated from only 1-5 source demonstrations can match or outperform those trained on 50 real-world demonstrations, improving data efficiency by up to 10-50x. Moreover, experimental results on height and texture editing demonstrate the framework's flexibility and extensibility, indicating its potential to serve as a unified data generation framework.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.19402.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6721acf7bbf9703bc285e840",
            "avatarUrl": "/avatars/487f5cae476758d318465aa9ed103568.svg",
            "fullname": "Yujie Zhao",
            "name": "HomieZ",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.19535",
            "authors": [
                {
                    "_id": "694a4603335742716e9323bc",
                    "name": "Moritz Bhle",
                    "hidden": false
                },
                {
                    "_id": "694a4603335742716e9323bd",
                    "name": "Amlie Royer",
                    "hidden": false
                },
                {
                    "_id": "694a4603335742716e9323be",
                    "name": "Juliette Marrie",
                    "hidden": false
                },
                {
                    "_id": "694a4603335742716e9323bf",
                    "name": "Edouard Grave",
                    "hidden": false
                },
                {
                    "_id": "694a4603335742716e9323c0",
                    "name": "Patrick Prez",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-22T16:21:39.000Z",
            "submittedOnDailyAt": "2025-12-23T12:00:02.268Z",
            "title": "CASA: Cross-Attention via Self-Attention for Efficient Vision-Language Fusion",
            "submittedOnDailyBy": {
                "_id": "5f1158120c833276f61f1a84",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
                "isPro": false,
                "fullname": "Niels Rogge",
                "user": "nielsr",
                "type": "user"
            },
            "summary": "Vision-language models (VLMs) are commonly trained by inserting image tokens from a pretrained vision encoder into the textual stream of a language model. This allows text and image information to fully attend to one another within the model, but becomes extremely costly for high-resolution images, long conversations, or streaming videos, both in memory and compute. VLMs leveraging cross-attention are an efficient alternative to token insertion but exhibit a clear performance gap, in particular on tasks involving fine-grained visual details. We find that a key to improving such models is to also enable local text-to-text interaction in the dedicated cross-attention layers. Building on this, we propose CASA, Cross-Attention via Self-Attention, a simple and efficient paradigm which substantially reduces the gap with full token insertion on common image understanding benchmarks, while enjoying the same scalability as cross-attention models when applied to long-context multimodal tasks such as streaming video captioning. For samples and code, please see our project page at https://kyutai.org/casa .",
            "upvotes": 3,
            "discussionId": "694a4603335742716e9323c1",
            "projectPage": "https://kyutai.org/casa",
            "githubRepo": "https://github.com/kyutai-labs/casa",
            "githubRepoAddedBy": "user",
            "ai_summary": "CASA, a cross-attention method enhanced with self-attention, improves vision-language models' performance on detailed visual tasks while maintaining scalability for long-context multimodal applications.",
            "ai_keywords": [
                "vision-language models",
                "VLMs",
                "image tokens",
                "pretrained vision encoder",
                "textual stream",
                "cross-attention",
                "local text-to-text interaction",
                "CASA",
                "Cross-Attention via Self-Attention",
                "image understanding benchmarks",
                "streaming video captioning"
            ],
            "githubStars": 6,
            "organization": {
                "_id": "6683d6350b54a28aff6645fe",
                "name": "kyutai",
                "fullname": "Kyutai",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6355a3c1805be5a8f30fea49/8xGdIOlfkopZfhbMitw_k.jpeg"
            }
        },
        "publishedAt": "2025-12-22T11:21:39.000Z",
        "title": "CASA: Cross-Attention via Self-Attention for Efficient Vision-Language Fusion",
        "summary": "Vision-language models (VLMs) are commonly trained by inserting image tokens from a pretrained vision encoder into the textual stream of a language model. This allows text and image information to fully attend to one another within the model, but becomes extremely costly for high-resolution images, long conversations, or streaming videos, both in memory and compute. VLMs leveraging cross-attention are an efficient alternative to token insertion but exhibit a clear performance gap, in particular on tasks involving fine-grained visual details. We find that a key to improving such models is to also enable local text-to-text interaction in the dedicated cross-attention layers. Building on this, we propose CASA, Cross-Attention via Self-Attention, a simple and efficient paradigm which substantially reduces the gap with full token insertion on common image understanding benchmarks, while enjoying the same scalability as cross-attention models when applied to long-context multimodal tasks such as streaming video captioning. For samples and code, please see our project page at https://kyutai.org/casa .",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.19535.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "5f1158120c833276f61f1a84",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
            "fullname": "Niels Rogge",
            "name": "nielsr",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1065
        },
        "organization": {
            "_id": "6683d6350b54a28aff6645fe",
            "name": "kyutai",
            "fullname": "Kyutai",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6355a3c1805be5a8f30fea49/8xGdIOlfkopZfhbMitw_k.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.18003",
            "authors": [
                {
                    "_id": "694a10a1335742716e932284",
                    "name": "Soumava Paul",
                    "hidden": false
                },
                {
                    "_id": "694a10a1335742716e932285",
                    "name": "Prakhar Kaushik",
                    "hidden": false
                },
                {
                    "_id": "694a10a1335742716e932286",
                    "name": "Ankit Vaidya",
                    "hidden": false
                },
                {
                    "_id": "694a10a1335742716e932287",
                    "name": "Anand Bhattad",
                    "hidden": false
                },
                {
                    "_id": "694a10a1335742716e932288",
                    "name": "Alan Yuille",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/7uTYk9J-jm2phTbJIxaxw.png",
                "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/ilpc5us11-Kzc0GKoRKct.mp4"
            ],
            "publishedAt": "2025-12-19T19:02:36.000Z",
            "submittedOnDailyAt": "2025-12-23T01:17:52.937Z",
            "title": "Name That Part: 3D Part Segmentation and Naming",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We address semantic 3D part segmentation: decomposing objects into parts with meaningful names. While datasets exist with part annotations, their definitions are inconsistent across datasets, limiting robust training. Previous methods produce unlabeled decompositions or retrieve single parts without complete shape annotations. We propose ALIGN-Parts, which formulates part naming as a direct set alignment task. Our method decomposes shapes into partlets - implicit 3D part representations - matched to part descriptions via bipartite assignment. We combine geometric cues from 3D part fields, appearance from multi-view vision features, and semantic knowledge from language-model-generated affordance descriptions. Text-alignment loss ensures partlets share embedding space with text, enabling a theoretically open-vocabulary matching setup, given sufficient data. Our efficient and novel, one-shot, 3D part segmentation and naming method finds applications in several downstream tasks, including serving as a scalable annotation engine. As our model supports zero-shot matching to arbitrary descriptions and confidence-calibrated predictions for known categories, with human verification, we create a unified ontology that aligns PartNet, 3DCoMPaT++, and Find3D, consisting of 1,794 unique 3D parts. We also show examples from our newly created Tex-Parts dataset. We also introduce 2 novel metrics appropriate for the named 3D part segmentation task.",
            "upvotes": 3,
            "discussionId": "694a10a1335742716e932289",
            "projectPage": "https://name-that-part.github.io/",
            "ai_summary": "ALIGN-Parts addresses semantic 3D part segmentation by aligning implicit 3D part representations with part descriptions using geometric, appearance, and semantic cues, supporting open-vocabulary part naming and creating a unified ontology for multiple datasets.",
            "ai_keywords": [
                "3D part segmentation",
                "part naming",
                "set alignment",
                "partlets",
                "bipartite assignment",
                "3D part fields",
                "multi-view vision features",
                "language-model-generated affordance descriptions",
                "text-alignment loss",
                "zero-shot matching",
                "confidence-calibrated predictions",
                "unified ontology",
                "Tex-Parts dataset"
            ]
        },
        "publishedAt": "2025-12-19T14:02:36.000Z",
        "title": "Name That Part: 3D Part Segmentation and Naming",
        "summary": "We address semantic 3D part segmentation: decomposing objects into parts with meaningful names. While datasets exist with part annotations, their definitions are inconsistent across datasets, limiting robust training. Previous methods produce unlabeled decompositions or retrieve single parts without complete shape annotations. We propose ALIGN-Parts, which formulates part naming as a direct set alignment task. Our method decomposes shapes into partlets - implicit 3D part representations - matched to part descriptions via bipartite assignment. We combine geometric cues from 3D part fields, appearance from multi-view vision features, and semantic knowledge from language-model-generated affordance descriptions. Text-alignment loss ensures partlets share embedding space with text, enabling a theoretically open-vocabulary matching setup, given sufficient data. Our efficient and novel, one-shot, 3D part segmentation and naming method finds applications in several downstream tasks, including serving as a scalable annotation engine. As our model supports zero-shot matching to arbitrary descriptions and confidence-calibrated predictions for known categories, with human verification, we create a unified ontology that aligns PartNet, 3DCoMPaT++, and Find3D, consisting of 1,794 unique 3D parts. We also show examples from our newly created Tex-Parts dataset. We also introduce 2 novel metrics appropriate for the named 3D part segmentation task.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/7uTYk9J-jm2phTbJIxaxw.png",
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/ilpc5us11-Kzc0GKoRKct.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.18003.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 189
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.18314",
            "authors": [
                {
                    "_id": "694a5666335742716e9323ec",
                    "name": "Philipp Langsteiner",
                    "hidden": false
                },
                {
                    "_id": "694a5666335742716e9323ed",
                    "name": "Jan-Niklas Dihlmann",
                    "hidden": false
                },
                {
                    "_id": "694a5666335742716e9323ee",
                    "name": "Hendrik P. A. Lensch",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/631b6dbcbf1351ed2bd05be1/3Gu_-_iUsbd0nBbeyprVC.mp4"
            ],
            "publishedAt": "2025-12-20T10:58:45.000Z",
            "submittedOnDailyAt": "2025-12-23T06:14:59.893Z",
            "title": "MatSpray: Fusing 2D Material World Knowledge on 3D Geometry",
            "submittedOnDailyBy": {
                "_id": "631b6dbcbf1351ed2bd05be1",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631b6dbcbf1351ed2bd05be1/_kzcDD4qCifsb-651_LmY.png",
                "isPro": false,
                "fullname": "Jan-Niklas Dihlmann",
                "user": "JDihlmann",
                "type": "user"
            },
            "summary": "Manual modeling of material parameters and 3D geometry is a time consuming yet essential task in the gaming and film industries. While recent advances in 3D reconstruction have enabled accurate approximations of scene geometry and appearance, these methods often fall short in relighting scenarios due to the lack of precise, spatially varying material parameters. At the same time, diffusion models operating on 2D images have shown strong performance in predicting physically based rendering (PBR) properties such as albedo, roughness, and metallicity. However, transferring these 2D material maps onto reconstructed 3D geometry remains a significant challenge. We propose a framework for fusing 2D material data into 3D geometry using a combination of novel learning-based and projection-based approaches. We begin by reconstructing scene geometry via Gaussian Splatting. From the input images, a diffusion model generates 2D maps for albedo, roughness, and metallic parameters. Any existing diffusion model that can convert images or videos to PBR materials can be applied. The predictions are further integrated into the 3D representation either by optimizing an image-based loss or by directly projecting the material parameters onto the Gaussians using Gaussian ray tracing. To enhance fine-scale accuracy and multi-view consistency, we further introduce a light-weight neural refinement step (Neural Merger), which takes ray-traced material features as input and produces detailed adjustments. Our results demonstrate that the proposed methods outperform existing techniques in both quantitative metrics and perceived visual realism. This enables more accurate, relightable, and photorealistic renderings from reconstructed scenes, significantly improving the realism and efficiency of asset creation workflows in content production pipelines.",
            "upvotes": 2,
            "discussionId": "694a5666335742716e9323ef",
            "projectPage": "https://matspray.jdihlmann.com/",
            "githubRepo": "https://github.com/cgtuebingen/MatSpray",
            "githubRepoAddedBy": "user",
            "ai_summary": "The proposed framework integrates 2D material maps into 3D geometry using diffusion models and Gaussian Splatting, enhancing relighting and photorealism in reconstructed scenes.",
            "ai_keywords": [
                "Gaussian Splatting",
                "diffusion models",
                "PBR",
                "albedo",
                "roughness",
                "metallicity",
                "Gaussian ray tracing",
                "Neural Merger"
            ],
            "githubStars": 15,
            "organization": {
                "_id": "653785a9e5ad8b46d5461196",
                "name": "CGTuebingen",
                "fullname": "CG Tbingen",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/631b6dbcbf1351ed2bd05be1/Yfj6lz9UGYfaBOiTPARed.jpeg"
            }
        },
        "publishedAt": "2025-12-20T05:58:45.000Z",
        "title": "MatSpray: Fusing 2D Material World Knowledge on 3D Geometry",
        "summary": "Manual modeling of material parameters and 3D geometry is a time consuming yet essential task in the gaming and film industries. While recent advances in 3D reconstruction have enabled accurate approximations of scene geometry and appearance, these methods often fall short in relighting scenarios due to the lack of precise, spatially varying material parameters. At the same time, diffusion models operating on 2D images have shown strong performance in predicting physically based rendering (PBR) properties such as albedo, roughness, and metallicity. However, transferring these 2D material maps onto reconstructed 3D geometry remains a significant challenge. We propose a framework for fusing 2D material data into 3D geometry using a combination of novel learning-based and projection-based approaches. We begin by reconstructing scene geometry via Gaussian Splatting. From the input images, a diffusion model generates 2D maps for albedo, roughness, and metallic parameters. Any existing diffusion model that can convert images or videos to PBR materials can be applied. The predictions are further integrated into the 3D representation either by optimizing an image-based loss or by directly projecting the material parameters onto the Gaussians using Gaussian ray tracing. To enhance fine-scale accuracy and multi-view consistency, we further introduce a light-weight neural refinement step (Neural Merger), which takes ray-traced material features as input and produces detailed adjustments. Our results demonstrate that the proposed methods outperform existing techniques in both quantitative metrics and perceived visual realism. This enables more accurate, relightable, and photorealistic renderings from reconstructed scenes, significantly improving the realism and efficiency of asset creation workflows in content production pipelines.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/631b6dbcbf1351ed2bd05be1/3Gu_-_iUsbd0nBbeyprVC.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.18314.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "631b6dbcbf1351ed2bd05be1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631b6dbcbf1351ed2bd05be1/_kzcDD4qCifsb-651_LmY.png",
            "fullname": "Jan-Niklas Dihlmann",
            "name": "JDihlmann",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "653785a9e5ad8b46d5461196",
            "name": "CGTuebingen",
            "fullname": "CG Tbingen",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/631b6dbcbf1351ed2bd05be1/Yfj6lz9UGYfaBOiTPARed.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.12620",
            "authors": [
                {
                    "_id": "694a4167335742716e93239a",
                    "name": "Aheli Poddar",
                    "hidden": false
                },
                {
                    "_id": "694a4167335742716e93239b",
                    "name": "Saptarshi Sahoo",
                    "hidden": false
                },
                {
                    "_id": "694a4167335742716e93239c",
                    "name": "Sujata Ghosh",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/683bd3b1c15aff506f630819/eTh5ugj-Po5rRAt29XPZn.png"
            ],
            "publishedAt": "2025-12-14T09:50:10.000Z",
            "submittedOnDailyAt": "2025-12-23T04:50:37.841Z",
            "title": "Understanding Syllogistic Reasoning in LLMs from Formal and Natural Language Perspectives",
            "submittedOnDailyBy": {
                "_id": "683bd3b1c15aff506f630819",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/683bd3b1c15aff506f630819/Yv-wIKP1jw-bIsrXrikQ1.jpeg",
                "isPro": true,
                "fullname": "Aheli Poddar",
                "user": "xaheli",
                "type": "user"
            },
            "summary": "We study syllogistic reasoning in LLMs from the logical and natural language perspectives. In process, we explore fundamental reasoning capabilities of the LLMs and the direction this research is moving forward. To aid in our studies, we use 14 large language models and investigate their syllogistic reasoning capabilities in terms of symbolic inferences as well as natural language understanding. Even though this reasoning mechanism is not a uniform emergent property across LLMs, the perfect symbolic performances in certain models make us wonder whether LLMs are becoming more and more formal reasoning mechanisms, rather than making explicit the nuances of human reasoning.",
            "upvotes": 2,
            "discussionId": "694a4167335742716e93239d",
            "githubRepo": "https://github.com/XAheli/Logic-in-LLMs",
            "githubRepoAddedBy": "user",
            "ai_summary": "Research explores syllogistic reasoning in LLMs, examining their symbolic inference and natural language understanding capabilities, with some models showing perfect symbolic performance.",
            "ai_keywords": [
                "syllogistic reasoning",
                "large language models",
                "symbolic inferences",
                "natural language understanding"
            ],
            "githubStars": 0
        },
        "publishedAt": "2025-12-14T04:50:10.000Z",
        "title": "Understanding Syllogistic Reasoning in LLMs from Formal and Natural Language Perspectives",
        "summary": "We study syllogistic reasoning in LLMs from the logical and natural language perspectives. In process, we explore fundamental reasoning capabilities of the LLMs and the direction this research is moving forward. To aid in our studies, we use 14 large language models and investigate their syllogistic reasoning capabilities in terms of symbolic inferences as well as natural language understanding. Even though this reasoning mechanism is not a uniform emergent property across LLMs, the perfect symbolic performances in certain models make us wonder whether LLMs are becoming more and more formal reasoning mechanisms, rather than making explicit the nuances of human reasoning.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/683bd3b1c15aff506f630819/eTh5ugj-Po5rRAt29XPZn.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.12620.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "683bd3b1c15aff506f630819",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/683bd3b1c15aff506f630819/Yv-wIKP1jw-bIsrXrikQ1.jpeg",
            "fullname": "Aheli Poddar",
            "name": "xaheli",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.19661",
            "authors": [
                {
                    "_id": "694b1f3d746a34b55dd53bd0",
                    "name": "Luchao Qi",
                    "hidden": false
                },
                {
                    "_id": "694b1f3d746a34b55dd53bd1",
                    "name": "Jiaye Wu",
                    "hidden": false
                },
                {
                    "_id": "694b1f3d746a34b55dd53bd2",
                    "name": "Jun Myeong Choi",
                    "hidden": false
                },
                {
                    "_id": "694b1f3d746a34b55dd53bd3",
                    "name": "Cary Phillips",
                    "hidden": false
                },
                {
                    "_id": "694b1f3d746a34b55dd53bd4",
                    "name": "Roni Sengupta",
                    "hidden": false
                },
                {
                    "_id": "694b1f3d746a34b55dd53bd5",
                    "name": "Dan B Goldman",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-22T18:39:58.000Z",
            "submittedOnDailyAt": "2025-12-23T20:32:58.897Z",
            "title": "Over++: Generative Video Compositing for Layer Interaction Effects",
            "submittedOnDailyBy": {
                "_id": "635734a5fe4ffe942db442b2",
                "avatarUrl": "/avatars/9da256875e6cfc780c6420e465c49099.svg",
                "isPro": false,
                "fullname": "Luchao Qi",
                "user": "luchaoqi",
                "type": "user"
            },
            "summary": "In professional video compositing workflows, artists must manually create environmental interactions-such as shadows, reflections, dust, and splashes-between foreground subjects and background layers. Existing video generative models struggle to preserve the input video while adding such effects, and current video inpainting methods either require costly per-frame masks or yield implausible results. We introduce augmented compositing, a new task that synthesizes realistic, semi-transparent environmental effects conditioned on text prompts and input video layers, while preserving the original scene. To address this task, we present Over++, a video effect generation framework that makes no assumptions about camera pose, scene stationarity, or depth supervision. We construct a paired effect dataset tailored for this task and introduce an unpaired augmentation strategy that preserves text-driven editability. Our method also supports optional mask control and keyframe guidance without requiring dense annotations. Despite training on limited data, Over++ produces diverse and realistic environmental effects and outperforms existing baselines in both effect generation and scene preservation.",
            "upvotes": 1,
            "discussionId": "694b1f3d746a34b55dd53bd6",
            "projectPage": "https://overplusplus.github.io/",
            "ai_summary": "Over++, a video effect generation framework, synthesizes realistic environmental effects conditioned on text prompts while preserving the original video scene without requiring camera pose or dense annotations.",
            "ai_keywords": [
                "augmented compositing",
                "video generative models",
                "video inpainting",
                "text prompts",
                "input video layers",
                "Over++",
                "environmental effects",
                "semi-transparent effects",
                "unpaired augmentation",
                "mask control",
                "keyframe guidance",
                "effect generation",
                "scene preservation"
            ]
        },
        "publishedAt": "2025-12-22T13:39:58.000Z",
        "title": "Over++: Generative Video Compositing for Layer Interaction Effects",
        "summary": "In professional video compositing workflows, artists must manually create environmental interactions-such as shadows, reflections, dust, and splashes-between foreground subjects and background layers. Existing video generative models struggle to preserve the input video while adding such effects, and current video inpainting methods either require costly per-frame masks or yield implausible results. We introduce augmented compositing, a new task that synthesizes realistic, semi-transparent environmental effects conditioned on text prompts and input video layers, while preserving the original scene. To address this task, we present Over++, a video effect generation framework that makes no assumptions about camera pose, scene stationarity, or depth supervision. We construct a paired effect dataset tailored for this task and introduce an unpaired augmentation strategy that preserves text-driven editability. Our method also supports optional mask control and keyframe guidance without requiring dense annotations. Despite training on limited data, Over++ produces diverse and realistic environmental effects and outperforms existing baselines in both effect generation and scene preservation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.19661.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "635734a5fe4ffe942db442b2",
            "avatarUrl": "/avatars/9da256875e6cfc780c6420e465c49099.svg",
            "fullname": "Luchao Qi",
            "name": "luchaoqi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.19399",
            "authors": [
                {
                    "_id": "694a32b1335742716e932374",
                    "name": "Sandro Andric",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-22T13:51:03.000Z",
            "submittedOnDailyAt": "2025-12-23T04:06:45.652Z",
            "title": "Brain-Grounded Axes for Reading and Steering LLM States",
            "submittedOnDailyBy": {
                "_id": "687cbff1bf3c46e6e6d55ab8",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/687cbff1bf3c46e6e6d55ab8/oIxnj9x7plYwbd4egLbwb.png",
                "isPro": true,
                "fullname": "Sandro Andric",
                "user": "AI-nthusiast",
                "type": "user"
            },
            "summary": "Interpretability methods for large language models (LLMs) typically derive directions from textual supervision, which can lack external grounding. We propose using human brain activity not as a training signal but as a coordinate system for reading and steering LLM states. Using the SMN4Lang MEG dataset, we construct a word-level brain atlas of phase-locking value (PLV) patterns and extract latent axes via ICA. We validate axes with independent lexica and NER-based labels (POS/log-frequency used as sanity checks), then train lightweight adapters that map LLM hidden states to these brain axes without fine-tuning the LLM. Steering along the resulting brain-derived directions yields a robust lexical (frequency-linked) axis in a mid TinyLlama layer, surviving perplexity-matched controls, and a brain-vs-text probe comparison shows larger log-frequency shifts (relative to the text probe) with lower perplexity for the brain axis. A function/content axis (axis 13) shows consistent steering in TinyLlama, Qwen2-0.5B, and GPT-2, with PPL-matched text-level corroboration. Layer-4 effects in TinyLlama are large but inconsistent, so we treat them as secondary (Appendix). Axis structure is stable when the atlas is rebuilt without GPT embedding-change features or with word2vec embeddings (|r|=0.64-0.95 across matched axes), reducing circularity concerns. Exploratory fMRI anchoring suggests potential alignment for embedding change and log frequency, but effects are sensitive to hemodynamic modeling assumptions and are treated as population-level evidence only. These results support a new interface: neurophysiology-grounded axes provide interpretable and controllable handles for LLM behavior.",
            "upvotes": 1,
            "discussionId": "694a32b1335742716e932375",
            "githubRepo": "https://github.com/sandroandric/Brain-Grounded-Axes-for-Reading-and-Steering-LLM-States",
            "githubRepoAddedBy": "user",
            "ai_summary": "Neurophysiological brain activity is used to create interpretable axes for large language models, enhancing their controllability and interpretability.",
            "ai_keywords": [
                "large language models",
                "human brain activity",
                "SMN4Lang MEG dataset",
                "phase-locking value",
                "ICA",
                "latent axes",
                "NER-based labels",
                "lightweight adapters",
                "perplexity-matched controls",
                "brain-vs-text probe",
                "function/content axis",
                "embedding change",
                "log frequency",
                "fMRI anchoring"
            ],
            "githubStars": 0
        },
        "publishedAt": "2025-12-22T08:51:03.000Z",
        "title": "Brain-Grounded Axes for Reading and Steering LLM States",
        "summary": "Interpretability methods for large language models (LLMs) typically derive directions from textual supervision, which can lack external grounding. We propose using human brain activity not as a training signal but as a coordinate system for reading and steering LLM states. Using the SMN4Lang MEG dataset, we construct a word-level brain atlas of phase-locking value (PLV) patterns and extract latent axes via ICA. We validate axes with independent lexica and NER-based labels (POS/log-frequency used as sanity checks), then train lightweight adapters that map LLM hidden states to these brain axes without fine-tuning the LLM. Steering along the resulting brain-derived directions yields a robust lexical (frequency-linked) axis in a mid TinyLlama layer, surviving perplexity-matched controls, and a brain-vs-text probe comparison shows larger log-frequency shifts (relative to the text probe) with lower perplexity for the brain axis. A function/content axis (axis 13) shows consistent steering in TinyLlama, Qwen2-0.5B, and GPT-2, with PPL-matched text-level corroboration. Layer-4 effects in TinyLlama are large but inconsistent, so we treat them as secondary (Appendix). Axis structure is stable when the atlas is rebuilt without GPT embedding-change features or with word2vec embeddings (|r|=0.64-0.95 across matched axes), reducing circularity concerns. Exploratory fMRI anchoring suggests potential alignment for embedding change and log frequency, but effects are sensitive to hemodynamic modeling assumptions and are treated as population-level evidence only. These results support a new interface: neurophysiology-grounded axes provide interpretable and controllable handles for LLM behavior.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.19399.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "687cbff1bf3c46e6e6d55ab8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/687cbff1bf3c46e6e6d55ab8/oIxnj9x7plYwbd4egLbwb.png",
            "fullname": "Sandro Andric",
            "name": "AI-nthusiast",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.18542",
            "authors": [
                {
                    "_id": "694b1b48746a34b55dd53bcd",
                    "name": "Scott Thornton",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-20T23:52:12.000Z",
            "submittedOnDailyAt": "2025-12-23T20:18:11.294Z",
            "title": "SecureCode v2.0: A Production-Grade Dataset for Training Security-Aware Code Generation Models",
            "submittedOnDailyBy": {
                "_id": "6871c39c903301306f20af58",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6871c39c903301306f20af58/va0IhpEIP3MTSkqzQ_PIA.jpeg",
                "isPro": true,
                "fullname": "Scott Thornton",
                "user": "scthornton",
                "type": "user"
            },
            "summary": "AI assistants produce vulnerable code in 45% of security-relevant scenarios, introducing flaws into production systems at scale. Yet existing secure coding datasets fall short. They lack incident grounding, don't provide the scale modern training requires, and miss the operational security context developers need for production deployments. We present SecureCode v2.0, a production-grade dataset of 1,215 security-focused coding examples that passed structural validation and expert security review. Every example ties to actual documented security incidents with CVE references, provides vulnerable and secure implementations, demonstrates concrete attacks, and includes defense-in-depth operational guidance. The dataset covers 11 vulnerability categories (complete OWASP Top 10:2025 plus AI/ML Security Threats) across 11 languages (Python, JavaScript, Java, Go, PHP, C#, TypeScript, Ruby, Rust, Kotlin, and YAML for infrastructure-as-code).\n  Our quality assurance framework ensures complete incident grounding. Each example includes SIEM integration strategies, infrastructure hardening recommendations (Docker, AppArmor, WAF configurations), and testing approaches using language-appropriate frameworks. The dataset uses a 4-turn conversational structure mirroring actual developer-AI interactions, escalating from basic implementations to advanced security considerations and defense-in-depth guidance.\n  Our contributions: (1) 1,215 rigorously validated examples split into 989 training, 122 validation, and 104 test sets, (2) an automated validation framework ensuring dataset consistency, (3) a 4-turn conversational structure capturing realistic security workflows, (4) comprehensive operational security guidance with SIEM integration strategies, (5) complete language-specific implementation fidelity, and (6) open-source release of data, validation tools, and benchmarking protocols.",
            "upvotes": 1,
            "discussionId": "694b1b49746a34b55dd53bce",
            "projectPage": "https://perfecxion.ai/",
            "githubRepo": "https://github.com/scthornton/securecode-v2",
            "githubRepoAddedBy": "user",
            "ai_summary": "SecureCode v2.0 is a comprehensive dataset providing security-focused coding examples grounded in real incidents, with vulnerable and secure implementations, attacks, defense guidance, and operational security context across multiple languages.",
            "ai_keywords": [
                "SecureCode v2.0",
                "structural validation",
                "expert security review",
                "CVE references",
                "defense-in-depth",
                "OWASP Top 10:2025",
                "AI/ML Security Threats",
                "SIEM integration",
                "infrastructure hardening",
                "Docker",
                "AppArmor",
                "WAF configurations",
                "language-appropriate frameworks",
                "conversational structure"
            ],
            "githubStars": 1
        },
        "publishedAt": "2025-12-20T18:52:12.000Z",
        "title": "SecureCode v2.0: A Production-Grade Dataset for Training Security-Aware Code Generation Models",
        "summary": "AI assistants produce vulnerable code in 45% of security-relevant scenarios, introducing flaws into production systems at scale. Yet existing secure coding datasets fall short. They lack incident grounding, don't provide the scale modern training requires, and miss the operational security context developers need for production deployments. We present SecureCode v2.0, a production-grade dataset of 1,215 security-focused coding examples that passed structural validation and expert security review. Every example ties to actual documented security incidents with CVE references, provides vulnerable and secure implementations, demonstrates concrete attacks, and includes defense-in-depth operational guidance. The dataset covers 11 vulnerability categories (complete OWASP Top 10:2025 plus AI/ML Security Threats) across 11 languages (Python, JavaScript, Java, Go, PHP, C#, TypeScript, Ruby, Rust, Kotlin, and YAML for infrastructure-as-code).\n  Our quality assurance framework ensures complete incident grounding. Each example includes SIEM integration strategies, infrastructure hardening recommendations (Docker, AppArmor, WAF configurations), and testing approaches using language-appropriate frameworks. The dataset uses a 4-turn conversational structure mirroring actual developer-AI interactions, escalating from basic implementations to advanced security considerations and defense-in-depth guidance.\n  Our contributions: (1) 1,215 rigorously validated examples split into 989 training, 122 validation, and 104 test sets, (2) an automated validation framework ensuring dataset consistency, (3) a 4-turn conversational structure capturing realistic security workflows, (4) comprehensive operational security guidance with SIEM integration strategies, (5) complete language-specific implementation fidelity, and (6) open-source release of data, validation tools, and benchmarking protocols.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.18542.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6871c39c903301306f20af58",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6871c39c903301306f20af58/va0IhpEIP3MTSkqzQ_PIA.jpeg",
            "fullname": "Scott Thornton",
            "name": "scthornton",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    }
]