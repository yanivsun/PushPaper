[
    {
        "paper": {
            "id": "2512.20619",
            "authors": [
                {
                    "_id": "694b614d746a34b55dd53d1a",
                    "name": "Jianhong Bai",
                    "hidden": false
                },
                {
                    "_id": "694b614d746a34b55dd53d1b",
                    "name": "Xiaoshi Wu",
                    "hidden": false
                },
                {
                    "_id": "694b614d746a34b55dd53d1c",
                    "name": "Xintao Wang",
                    "hidden": false
                },
                {
                    "_id": "694b614d746a34b55dd53d1d",
                    "name": "Fu Xiao",
                    "hidden": false
                },
                {
                    "_id": "694b614d746a34b55dd53d1e",
                    "name": "Yuanxing Zhang",
                    "hidden": false
                },
                {
                    "_id": "694b614d746a34b55dd53d1f",
                    "name": "Qinghe Wang",
                    "hidden": false
                },
                {
                    "_id": "694b614d746a34b55dd53d20",
                    "name": "Xiaoyu Shi",
                    "hidden": false
                },
                {
                    "_id": "694b614d746a34b55dd53d21",
                    "name": "Menghan Xia",
                    "hidden": false
                },
                {
                    "_id": "694b614d746a34b55dd53d22",
                    "name": "Zuozhu Liu",
                    "hidden": false
                },
                {
                    "_id": "694b614d746a34b55dd53d23",
                    "name": "Haoji Hu",
                    "hidden": false
                },
                {
                    "_id": "694b614d746a34b55dd53d24",
                    "name": "Pengfei Wan",
                    "hidden": false
                },
                {
                    "_id": "694b614d746a34b55dd53d25",
                    "name": "Kun Gai",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6530bf50f145530101ec03a2/amGfUgsGwtKhlryqSDYnU.png"
            ],
            "publishedAt": "2025-12-23T18:59:56.000Z",
            "submittedOnDailyAt": "2025-12-24T01:20:51.117Z",
            "title": "SemanticGen: Video Generation in Semantic Space",
            "submittedOnDailyBy": {
                "_id": "6530bf50f145530101ec03a2",
                "avatarUrl": "/avatars/c61c00c314cf202b64968e51e855694d.svg",
                "isPro": false,
                "fullname": "Jianhong Bai",
                "user": "jianhongbai",
                "type": "user"
            },
            "summary": "State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, the generation process should begin in a compact, high-level semantic space for global planning, followed by the addition of high-frequency details, rather than directly modeling a vast set of low-level video tokens using bi-directional attention. SemanticGen adopts a two-stage generation process. In the first stage, a diffusion model generates compact semantic video features, which define the global layout of the video. In the second stage, another diffusion model generates VAE latents conditioned on these semantic features to produce the final output. We observe that generation in the semantic space leads to faster convergence compared to the VAE latent space. Our method is also effective and computationally efficient when extended to long video generation. Extensive experiments demonstrate that SemanticGen produces high-quality videos and outperforms state-of-the-art approaches and strong baselines.",
            "upvotes": 78,
            "discussionId": "694b614d746a34b55dd53d26",
            "projectPage": "https://jianhongbai.github.io/SemanticGen/",
            "ai_summary": "SemanticGen addresses slow convergence and computational costs in video generation by using a two-stage diffusion model approach that first generates semantic features and then VAE latents, leading to faster convergence and high-quality results.",
            "ai_keywords": [
                "VAE space",
                "VAE decoder",
                "semantic space",
                "diffusion model",
                "semantic video features",
                "bi-directional attention"
            ],
            "organization": {
                "_id": "662c559b322afcbae51b3c8b",
                "name": "KlingTeam",
                "fullname": "Kling Team",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"
            }
        },
        "publishedAt": "2025-12-23T13:59:56.000Z",
        "title": "SemanticGen: Video Generation in Semantic Space",
        "summary": "State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, the generation process should begin in a compact, high-level semantic space for global planning, followed by the addition of high-frequency details, rather than directly modeling a vast set of low-level video tokens using bi-directional attention. SemanticGen adopts a two-stage generation process. In the first stage, a diffusion model generates compact semantic video features, which define the global layout of the video. In the second stage, another diffusion model generates VAE latents conditioned on these semantic features to produce the final output. We observe that generation in the semantic space leads to faster convergence compared to the VAE latent space. Our method is also effective and computationally efficient when extended to long video generation. Extensive experiments demonstrate that SemanticGen produces high-quality videos and outperforms state-of-the-art approaches and strong baselines.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6530bf50f145530101ec03a2/amGfUgsGwtKhlryqSDYnU.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20619.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6530bf50f145530101ec03a2",
            "avatarUrl": "/avatars/c61c00c314cf202b64968e51e855694d.svg",
            "fullname": "Jianhong Bai",
            "name": "jianhongbai",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 14
        },
        "organization": {
            "_id": "662c559b322afcbae51b3c8b",
            "name": "KlingTeam",
            "fullname": "Kling Team",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.19673",
            "authors": [
                {
                    "_id": "694ac3ad746a34b55dd53b6c",
                    "name": "Yuqiao Tan",
                    "hidden": false
                },
                {
                    "_id": "694ac3ad746a34b55dd53b6d",
                    "name": "Minzheng Wang",
                    "hidden": false
                },
                {
                    "_id": "694ac3ad746a34b55dd53b6e",
                    "name": "Shizhu He",
                    "hidden": false
                },
                {
                    "_id": "694ac3ad746a34b55dd53b6f",
                    "name": "Huanxuan Liao",
                    "hidden": false
                },
                {
                    "_id": "694ac3ad746a34b55dd53b70",
                    "name": "Chengfeng Zhao",
                    "hidden": false
                },
                {
                    "_id": "694ac3ad746a34b55dd53b71",
                    "name": "Qiunan Lu",
                    "hidden": false
                },
                {
                    "_id": "694ac3ad746a34b55dd53b72",
                    "name": "Tian Liang",
                    "hidden": false
                },
                {
                    "_id": "694ac3ad746a34b55dd53b73",
                    "name": "Jun Zhao",
                    "hidden": false
                },
                {
                    "_id": "694ac3ad746a34b55dd53b74",
                    "name": "Kang Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-22T18:51:48.000Z",
            "submittedOnDailyAt": "2025-12-24T00:28:45.252Z",
            "title": "Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies",
            "submittedOnDailyBy": {
                "_id": "64bcc373ef8c0e42bf16acc5",
                "avatarUrl": "/avatars/873308203d28115ae1a9e4d0e26508f4.svg",
                "isPro": false,
                "fullname": "mz.w",
                "user": "iiiiwis",
                "type": "user"
            },
            "summary": "Existing reinforcement learning (RL) approaches treat large language models (LLMs) as a single unified policy, overlooking their internal mechanisms. Understanding how policy evolves across layers and modules is therefore crucial for enabling more targeted optimization and raveling out complex reasoning mechanisms. In this paper, we decompose the language model policy by leveraging the intrinsic split of the Transformer residual stream and the equivalence between the composition of hidden states with the unembedding matrix and the resulting samplable policy. This decomposition reveals Internal Layer Policies, corresponding to contributions from individual layers, and Internal Modular Policies, which align with the self-attention and feed-forward network (FFN) components within each layer. By analyzing the entropy of internal policy, we find that: (a) Early layers keep high entropy for exploration, top layers converge to near-zero entropy for refinement, with convergence patterns varying across model series. (b) LLama's prediction space rapidly converges in the final layer, whereas Qwen-series models, especially Qwen3, exhibit a more human-like, progressively structured reasoning pattern. Motivated by these findings, we propose Bottom-up Policy Optimization (BuPO), a novel RL paradigm that directly optimizes the internal layer policy during early training. By aligning training objective at lower layer, BuPO reconstructs foundational reasoning capabilities and achieves superior performance. Extensive experiments on complex reasoning benchmarks demonstrates the effectiveness of our method. Our code is available at https://github.com/Trae1ounG/BuPO.",
            "upvotes": 49,
            "discussionId": "694ac3ad746a34b55dd53b75",
            "githubRepo": "https://github.com/Trae1ounG/BuPO",
            "githubRepoAddedBy": "user",
            "ai_summary": "The paper decomposes the policy of large language models into internal layer and modular policies, revealing distinct reasoning patterns across layers and proposing Bottom-up Policy Optimization to enhance performance on complex reasoning tasks.",
            "ai_keywords": [
                "reinforcement learning",
                "large language models",
                "Transformer residual stream",
                "unembedding matrix",
                "Internal Layer Policies",
                "Internal Modular Policies",
                "self-attention",
                "feed-forward network",
                "entropy",
                "Bottom-up Policy Optimization"
            ],
            "githubStars": 22,
            "organization": {
                "_id": "66543b6e420092799d2f625c",
                "name": "tencent",
                "fullname": "Tencent",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
            }
        },
        "publishedAt": "2025-12-22T13:51:48.000Z",
        "title": "Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies",
        "summary": "Existing reinforcement learning (RL) approaches treat large language models (LLMs) as a single unified policy, overlooking their internal mechanisms. Understanding how policy evolves across layers and modules is therefore crucial for enabling more targeted optimization and raveling out complex reasoning mechanisms. In this paper, we decompose the language model policy by leveraging the intrinsic split of the Transformer residual stream and the equivalence between the composition of hidden states with the unembedding matrix and the resulting samplable policy. This decomposition reveals Internal Layer Policies, corresponding to contributions from individual layers, and Internal Modular Policies, which align with the self-attention and feed-forward network (FFN) components within each layer. By analyzing the entropy of internal policy, we find that: (a) Early layers keep high entropy for exploration, top layers converge to near-zero entropy for refinement, with convergence patterns varying across model series. (b) LLama's prediction space rapidly converges in the final layer, whereas Qwen-series models, especially Qwen3, exhibit a more human-like, progressively structured reasoning pattern. Motivated by these findings, we propose Bottom-up Policy Optimization (BuPO), a novel RL paradigm that directly optimizes the internal layer policy during early training. By aligning training objective at lower layer, BuPO reconstructs foundational reasoning capabilities and achieves superior performance. Extensive experiments on complex reasoning benchmarks demonstrates the effectiveness of our method. Our code is available at https://github.com/Trae1ounG/BuPO.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.19673.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "64bcc373ef8c0e42bf16acc5",
            "avatarUrl": "/avatars/873308203d28115ae1a9e4d0e26508f4.svg",
            "fullname": "mz.w",
            "name": "iiiiwis",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "66543b6e420092799d2f625c",
            "name": "tencent",
            "fullname": "Tencent",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.20618",
            "authors": [
                {
                    "_id": "694ba02a746a34b55dd53e8b",
                    "name": "Runtao Liu",
                    "hidden": false
                },
                {
                    "_id": "694ba02a746a34b55dd53e8c",
                    "name": "Ziyi Liu",
                    "hidden": false
                },
                {
                    "_id": "694ba02a746a34b55dd53e8d",
                    "name": "Jiaqi Tang",
                    "hidden": false
                },
                {
                    "_id": "694ba02a746a34b55dd53e8e",
                    "name": "Yue Ma",
                    "hidden": false
                },
                {
                    "_id": "694ba02a746a34b55dd53e8f",
                    "name": "Renjie Pi",
                    "hidden": false
                },
                {
                    "_id": "694ba02a746a34b55dd53e90",
                    "name": "Jipeng Zhang",
                    "hidden": false
                },
                {
                    "_id": "694ba02a746a34b55dd53e91",
                    "name": "Qifeng Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-23T18:59:49.000Z",
            "submittedOnDailyAt": "2025-12-24T05:57:23.776Z",
            "title": "LongVideoAgent: Multi-Agent Reasoning with Long Videos",
            "submittedOnDailyBy": {
                "_id": "642e7a12ccdcf5da7f9657a0",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642e7a12ccdcf5da7f9657a0/w8jW5BagTuTp6EvC6KEyR.png",
                "isPro": true,
                "fullname": "Jiaqi Tang",
                "user": "Jiaqi-hkust",
                "type": "user"
            },
            "summary": "Recent advances in multimodal LLMs and systems that use tools for long-video QA point to the promise of reasoning over hour-long episodes. However, many methods still compress content into lossy summaries or rely on limited toolsets, weakening temporal grounding and missing fine-grained cues. We propose a multi-agent framework in which a master LLM coordinates a grounding agent to localize question-relevant segments and a vision agent to extract targeted textual observations. The master agent plans with a step limit, and is trained with reinforcement learning to encourage concise, correct, and efficient multi-agent cooperation. This design helps the master agent focus on relevant clips via grounding, complements subtitles with visual detail, and yields interpretable trajectories. On our proposed LongTVQA and LongTVQA+ which are episode-level datasets aggregated from TVQA/TVQA+, our multi-agent system significantly outperforms strong non-agent baselines. Experiments also show reinforcement learning further strengthens reasoning and planning for the trained agent. Code and data will be shared at https://longvideoagent.github.io/.",
            "upvotes": 38,
            "discussionId": "694ba02a746a34b55dd53e92",
            "ai_summary": "A multi-agent framework, involving a master LLM, grounding agent, and vision agent, enhances long-video QA by improving temporal grounding and leveraging visual and textual data.",
            "ai_keywords": [
                "multimodal LLMs",
                "long-video QA",
                "multi-agent framework",
                "grounding agent",
                "vision agent",
                "reinforcement learning",
                "temporal grounding",
                "LongTVQA",
                "LongTVQA+"
            ]
        },
        "publishedAt": "2025-12-23T13:59:49.000Z",
        "title": "LongVideoAgent: Multi-Agent Reasoning with Long Videos",
        "summary": "Recent advances in multimodal LLMs and systems that use tools for long-video QA point to the promise of reasoning over hour-long episodes. However, many methods still compress content into lossy summaries or rely on limited toolsets, weakening temporal grounding and missing fine-grained cues. We propose a multi-agent framework in which a master LLM coordinates a grounding agent to localize question-relevant segments and a vision agent to extract targeted textual observations. The master agent plans with a step limit, and is trained with reinforcement learning to encourage concise, correct, and efficient multi-agent cooperation. This design helps the master agent focus on relevant clips via grounding, complements subtitles with visual detail, and yields interpretable trajectories. On our proposed LongTVQA and LongTVQA+ which are episode-level datasets aggregated from TVQA/TVQA+, our multi-agent system significantly outperforms strong non-agent baselines. Experiments also show reinforcement learning further strengthens reasoning and planning for the trained agent. Code and data will be shared at https://longvideoagent.github.io/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20618.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "642e7a12ccdcf5da7f9657a0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642e7a12ccdcf5da7f9657a0/w8jW5BagTuTp6EvC6KEyR.png",
            "fullname": "Jiaqi Tang",
            "name": "Jiaqi-hkust",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.20617",
            "authors": [
                {
                    "_id": "694b58e3746a34b55dd53cff",
                    "name": "Yuxi Xiao",
                    "hidden": false
                },
                {
                    "_id": "694b58e3746a34b55dd53d00",
                    "name": "Longfei Li",
                    "hidden": false
                },
                {
                    "_id": "694b58e3746a34b55dd53d01",
                    "name": "Shen Yan",
                    "hidden": false
                },
                {
                    "_id": "694b58e3746a34b55dd53d02",
                    "name": "Xinhang Liu",
                    "hidden": false
                },
                {
                    "_id": "694b58e3746a34b55dd53d03",
                    "name": "Sida Peng",
                    "hidden": false
                },
                {
                    "_id": "694b58e3746a34b55dd53d04",
                    "name": "Yunchao Wei",
                    "hidden": false
                },
                {
                    "_id": "694b58e3746a34b55dd53d05",
                    "name": "Xiaowei Zhou",
                    "hidden": false
                },
                {
                    "_id": "694b58e3746a34b55dd53d06",
                    "name": "Bingyi Kang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/hWsrMM0K13mB2Ej9Zwgbp.mp4"
            ],
            "publishedAt": "2025-12-23T18:59:46.000Z",
            "submittedOnDailyAt": "2025-12-24T00:38:28.003Z",
            "title": "SpatialTree: How Spatial Abilities Branch Out in MLLMs",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Cognitive science suggests that spatial ability develops progressively-from perception to reasoning and interaction. Yet in multimodal LLMs (MLLMs), this hierarchy remains poorly understood, as most studies focus on a narrow set of tasks. We introduce SpatialTree, a cognitive-science-inspired hierarchy that organizes spatial abilities into four levels: low-level perception (L1), mental mapping (L2), simulation (L3), and agentic competence (L4). Based on this taxonomy, we construct the first capability-centric hierarchical benchmark, thoroughly evaluating mainstream MLLMs across 27 sub-abilities. The evaluation results reveal a clear structure: L1 skills are largely orthogonal, whereas higher-level skills are strongly correlated, indicating increasing interdependency. Through targeted supervised fine-tuning, we uncover a surprising transfer dynamic-negative transfer within L1, but strong cross-level transfer from low- to high-level abilities with notable synergy. Finally, we explore how to improve the entire hierarchy. We find that naive RL that encourages extensive \"thinking\" is unreliable: it helps complex reasoning but hurts intuitive perception. We propose a simple auto-think strategy that suppresses unnecessary deliberation, enabling RL to consistently improve performance across all levels. By building SpatialTree, we provide a proof-of-concept framework for understanding and systematically scaling spatial abilities in MLLMs.",
            "upvotes": 34,
            "discussionId": "694b58e4746a34b55dd53d07",
            "projectPage": "https://spatialtree.github.io/",
            "ai_summary": "SpatialTree, a cognitive-science-inspired hierarchy, evaluates and improves spatial abilities in MLLMs across multiple levels, revealing transfer dynamics and proposing an auto-think strategy for consistent performance enhancement.",
            "ai_keywords": [
                "SpatialTree",
                "low-level perception",
                "mental mapping",
                "simulation",
                "agentic competence",
                "capability-centric hierarchical benchmark",
                "targeted supervised fine-tuning",
                "negative transfer",
                "cross-level transfer",
                "naive RL",
                "auto-think strategy"
            ],
            "organization": {
                "_id": "67d1140985ea0644e2f14b99",
                "name": "ByteDance-Seed",
                "fullname": "ByteDance Seed",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
            }
        },
        "publishedAt": "2025-12-23T13:59:46.000Z",
        "title": "SpatialTree: How Spatial Abilities Branch Out in MLLMs",
        "summary": "Cognitive science suggests that spatial ability develops progressively-from perception to reasoning and interaction. Yet in multimodal LLMs (MLLMs), this hierarchy remains poorly understood, as most studies focus on a narrow set of tasks. We introduce SpatialTree, a cognitive-science-inspired hierarchy that organizes spatial abilities into four levels: low-level perception (L1), mental mapping (L2), simulation (L3), and agentic competence (L4). Based on this taxonomy, we construct the first capability-centric hierarchical benchmark, thoroughly evaluating mainstream MLLMs across 27 sub-abilities. The evaluation results reveal a clear structure: L1 skills are largely orthogonal, whereas higher-level skills are strongly correlated, indicating increasing interdependency. Through targeted supervised fine-tuning, we uncover a surprising transfer dynamic-negative transfer within L1, but strong cross-level transfer from low- to high-level abilities with notable synergy. Finally, we explore how to improve the entire hierarchy. We find that naive RL that encourages extensive \"thinking\" is unreliable: it helps complex reasoning but hurts intuitive perception. We propose a simple auto-think strategy that suppresses unnecessary deliberation, enabling RL to consistently improve performance across all levels. By building SpatialTree, we provide a proof-of-concept framework for understanding and systematically scaling spatial abilities in MLLMs.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/hWsrMM0K13mB2Ej9Zwgbp.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20617.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 189
        },
        "organization": {
            "_id": "67d1140985ea0644e2f14b99",
            "name": "ByteDance-Seed",
            "fullname": "ByteDance Seed",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.18746",
            "authors": [
                {
                    "_id": "694b7f3a746a34b55dd53e09",
                    "name": "Guibin Zhang",
                    "hidden": false
                },
                {
                    "_id": "694b7f3a746a34b55dd53e0a",
                    "name": "Haotian Ren",
                    "hidden": false
                },
                {
                    "_id": "694b7f3a746a34b55dd53e0b",
                    "name": "Chong Zhan",
                    "hidden": false
                },
                {
                    "_id": "694b7f3a746a34b55dd53e0c",
                    "name": "Zhenhong Zhou",
                    "hidden": false
                },
                {
                    "_id": "694b7f3a746a34b55dd53e0d",
                    "name": "Junhao Wang",
                    "hidden": false
                },
                {
                    "_id": "694b7f3a746a34b55dd53e0e",
                    "name": "He Zhu",
                    "hidden": false
                },
                {
                    "_id": "694b7f3a746a34b55dd53e0f",
                    "name": "Wangchunshu Zhou",
                    "hidden": false
                },
                {
                    "_id": "694b7f3a746a34b55dd53e10",
                    "name": "Shuicheng Yan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-21T14:26:14.000Z",
            "submittedOnDailyAt": "2025-12-24T03:54:35.992Z",
            "title": "MemEvolve: Meta-Evolution of Agent Memory Systems",
            "submittedOnDailyBy": {
                "_id": "660d17d6c9be0dcd31a30b3d",
                "avatarUrl": "/avatars/3743fe9b695c488ebe33f0d8fd607a8a.svg",
                "isPro": false,
                "fullname": "Zhou Heng",
                "user": "henggg",
                "type": "user"
            },
            "summary": "Self-evolving memory systems are unprecedentedly reshaping the evolutionary paradigm of large language model (LLM)-based agents. Prior work has predominantly relied on manually engineered memory architectures to store trajectories, distill experience, and synthesize reusable tools, enabling agents to evolve on the fly within environment interactions. However, this paradigm is fundamentally constrained by the staticity of the memory system itself: while memory facilitates agent-level evolving, the underlying memory architecture cannot be meta-adapted to diverse task contexts. To address this gap, we propose MemEvolve, a meta-evolutionary framework that jointly evolves agents' experiential knowledge and their memory architecture, allowing agent systems not only to accumulate experience but also to progressively refine how they learn from it. To ground MemEvolve in prior research and foster openness in future self-evolving systems, we introduce EvolveLab, a unified self-evolving memory codebase that distills twelve representative memory systems into a modular design space (encode, store, retrieve, manage), providing both a standardized implementation substrate and a fair experimental arena. Extensive evaluations on four challenging agentic benchmarks demonstrate that MemEvolve achieves (I) substantial performance gains, improving frameworks such as SmolAgent and Flash-Searcher by up to 17.06%; and (II) strong cross-task and cross-LLM generalization, designing memory architectures that transfer effectively across diverse benchmarks and backbone models.",
            "upvotes": 19,
            "discussionId": "694b7f3a746a34b55dd53e11",
            "ai_summary": "MemEvolve, a meta-evolutionary framework, enhances self-evolving memory systems by jointly evolving agents' experiential knowledge and memory architecture, leading to improved performance and generalization.",
            "ai_keywords": [
                "self-evolving memory systems",
                "large language model",
                "LLM-based agents",
                "memory architectures",
                "meta-evolutionary framework",
                "MemEvolve",
                "EvolveLab",
                "modular design space",
                "encode",
                "store",
                "retrieve",
                "manage",
                "SmolAgent",
                "Flash-Searcher"
            ]
        },
        "publishedAt": "2025-12-21T09:26:14.000Z",
        "title": "MemEvolve: Meta-Evolution of Agent Memory Systems",
        "summary": "Self-evolving memory systems are unprecedentedly reshaping the evolutionary paradigm of large language model (LLM)-based agents. Prior work has predominantly relied on manually engineered memory architectures to store trajectories, distill experience, and synthesize reusable tools, enabling agents to evolve on the fly within environment interactions. However, this paradigm is fundamentally constrained by the staticity of the memory system itself: while memory facilitates agent-level evolving, the underlying memory architecture cannot be meta-adapted to diverse task contexts. To address this gap, we propose MemEvolve, a meta-evolutionary framework that jointly evolves agents' experiential knowledge and their memory architecture, allowing agent systems not only to accumulate experience but also to progressively refine how they learn from it. To ground MemEvolve in prior research and foster openness in future self-evolving systems, we introduce EvolveLab, a unified self-evolving memory codebase that distills twelve representative memory systems into a modular design space (encode, store, retrieve, manage), providing both a standardized implementation substrate and a fair experimental arena. Extensive evaluations on four challenging agentic benchmarks demonstrate that MemEvolve achieves (I) substantial performance gains, improving frameworks such as SmolAgent and Flash-Searcher by up to 17.06%; and (II) strong cross-task and cross-LLM generalization, designing memory architectures that transfer effectively across diverse benchmarks and backbone models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.18746.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "660d17d6c9be0dcd31a30b3d",
            "avatarUrl": "/avatars/3743fe9b695c488ebe33f0d8fd607a8a.svg",
            "fullname": "Zhou Heng",
            "name": "henggg",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.20491",
            "authors": [
                {
                    "_id": "694b5132746a34b55dd53c4e",
                    "name": "Chen Hu",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c4f",
                    "name": "Haikuo Du",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c50",
                    "name": "Heng Wang",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c51",
                    "name": "Lin Lin",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c52",
                    "name": "Mingrui Chen",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c53",
                    "name": "Peng Liu",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c54",
                    "name": "Ruihang Miao",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c55",
                    "name": "Tianchi Yue",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c56",
                    "name": "Wang You",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c57",
                    "name": "Wei Ji",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c58",
                    "name": "Wei Yuan",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c59",
                    "name": "Wenjin Deng",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c5a",
                    "name": "Xiaojian Yuan",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c5b",
                    "name": "Xiaoyun Zhang",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c5c",
                    "name": "Xiangyu Liu",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c5d",
                    "name": "Xikai Liu",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c5e",
                    "name": "Yanming Xu",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c5f",
                    "name": "Yicheng Cao",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c60",
                    "name": "Yifei Zhang",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c61",
                    "name": "Yongyao Wang",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c62",
                    "name": "Yubo Shu",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c63",
                    "name": "Yurong Zhang",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c64",
                    "name": "Yuxiang Zhang",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c65",
                    "name": "Zheng Gong",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c66",
                    "name": "Zhichao Chang",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c67",
                    "name": "Binyan Li",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c68",
                    "name": "Dan Ma",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c69",
                    "name": "Furong Jia",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c6a",
                    "name": "Hongyuan Wang",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c6b",
                    "name": "Jiayu Liu",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c6c",
                    "name": "Jing Bai",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c6d",
                    "name": "Junlan Liu",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c6e",
                    "name": "Manjiao Liu",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c6f",
                    "name": "Na Wang",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c70",
                    "name": "Qiuping Wu",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c71",
                    "name": "Qinxin Du",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c72",
                    "name": "Shiwei Li",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c73",
                    "name": "Wen Sun",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c74",
                    "name": "Yifeng Gong",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c75",
                    "name": "Yonglin Chen",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c76",
                    "name": "Yuling Zhao",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c77",
                    "name": "Yuxuan Lin",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c78",
                    "name": "Ziqi Ren",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c79",
                    "name": "Zixuan Wang",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c7a",
                    "name": "Aihu Zhang",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c7b",
                    "name": "Brian Li",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c7c",
                    "name": "Buyun Ma",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c7d",
                    "name": "Kang An",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c7e",
                    "name": "Li Xie",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c7f",
                    "name": "Mingliang Li",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c80",
                    "name": "Pan Li",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c81",
                    "name": "Shidong Yang",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c82",
                    "name": "Xi Chen",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c83",
                    "name": "Xiaojia Liu",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c84",
                    "name": "Yuchu Luo",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c85",
                    "name": "Yuan Song",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c86",
                    "name": "YuanHao Ding",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c87",
                    "name": "Yuanwei Liang",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c88",
                    "name": "Zexi Li",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c89",
                    "name": "Zhaoning Zhang",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c8a",
                    "name": "Zixin Zhang",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c8b",
                    "name": "Binxing Jiao",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c8c",
                    "name": "Daxin Jiang",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c8d",
                    "name": "Jiansheng Chen",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c8e",
                    "name": "Jing Li",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c8f",
                    "name": "Xiangyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "694b5132746a34b55dd53c90",
                    "name": "Yibo Zhu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-23T16:32:27.000Z",
            "submittedOnDailyAt": "2025-12-24T00:04:36.581Z",
            "title": "Step-DeepResearch Technical Report",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "As LLMs shift toward autonomous agents, Deep Research has emerged as a pivotal metric. However, existing academic benchmarks like BrowseComp often fail to meet real-world demands for open-ended research, which requires robust skills in intent recognition, long-horizon decision-making, and cross-source verification. To address this, we introduce Step-DeepResearch, a cost-effective, end-to-end agent. We propose a Data Synthesis Strategy Based on Atomic Capabilities to reinforce planning and report writing, combined with a progressive training path from agentic mid-training to SFT and RL. Enhanced by a Checklist-style Judger, this approach significantly improves robustness. Furthermore, to bridge the evaluation gap in the Chinese domain, we establish ADR-Bench for realistic deep research scenarios. Experimental results show that Step-DeepResearch (32B) scores 61.4% on Scale AI Research Rubrics. On ADR-Bench, it significantly outperforms comparable models and rivals SOTA closed-source models like OpenAI and Gemini DeepResearch. These findings prove that refined training enables medium-sized models to achieve expert-level capabilities at industry-leading cost-efficiency.",
            "upvotes": 12,
            "discussionId": "694b5132746a34b55dd53c91",
            "ai_summary": "Step-DeepResearch, an end-to-end agent enhanced with a data synthesis strategy and progressive training, achieves expert-level capabilities in deep research scenarios, outperforming established models.",
            "ai_keywords": [
                "Deep Research",
                "BrowseComp",
                "Step-DeepResearch",
                "Data Synthesis Strategy",
                "Atomic Capabilities",
                "agentic mid-training",
                "SFT",
                "RL",
                "Checklist-style Judger",
                "ADR-Bench",
                "Scale AI Research Rubrics",
                "OpenAI",
                "Gemini DeepResearch"
            ],
            "organization": {
                "_id": "66e43eae9d477f566f937935",
                "name": "stepfun-ai",
                "fullname": "StepFun",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"
            }
        },
        "publishedAt": "2025-12-23T11:32:27.000Z",
        "title": "Step-DeepResearch Technical Report",
        "summary": "As LLMs shift toward autonomous agents, Deep Research has emerged as a pivotal metric. However, existing academic benchmarks like BrowseComp often fail to meet real-world demands for open-ended research, which requires robust skills in intent recognition, long-horizon decision-making, and cross-source verification. To address this, we introduce Step-DeepResearch, a cost-effective, end-to-end agent. We propose a Data Synthesis Strategy Based on Atomic Capabilities to reinforce planning and report writing, combined with a progressive training path from agentic mid-training to SFT and RL. Enhanced by a Checklist-style Judger, this approach significantly improves robustness. Furthermore, to bridge the evaluation gap in the Chinese domain, we establish ADR-Bench for realistic deep research scenarios. Experimental results show that Step-DeepResearch (32B) scores 61.4% on Scale AI Research Rubrics. On ADR-Bench, it significantly outperforms comparable models and rivals SOTA closed-source models like OpenAI and Gemini DeepResearch. These findings prove that refined training enables medium-sized models to achieve expert-level capabilities at industry-leading cost-efficiency.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20491.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 189
        },
        "organization": {
            "_id": "66e43eae9d477f566f937935",
            "name": "stepfun-ai",
            "fullname": "StepFun",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.17102",
            "authors": [
                {
                    "_id": "694b5c01746a34b55dd53d0f",
                    "name": "Jiongxiao Wang",
                    "hidden": false
                },
                {
                    "_id": "694b5c01746a34b55dd53d10",
                    "name": "Qiaojing Yan",
                    "hidden": false
                },
                {
                    "_id": "694b5c01746a34b55dd53d11",
                    "name": "Yawei Wang",
                    "hidden": false
                },
                {
                    "_id": "694b5c01746a34b55dd53d12",
                    "name": "Yijun Tian",
                    "hidden": false
                },
                {
                    "_id": "694b5c01746a34b55dd53d13",
                    "name": "Soumya Smruti Mishra",
                    "hidden": false
                },
                {
                    "_id": "694b5c01746a34b55dd53d14",
                    "name": "Zhichao Xu",
                    "hidden": false
                },
                {
                    "_id": "694b5c01746a34b55dd53d15",
                    "name": "Megha Gandhi",
                    "hidden": false
                },
                {
                    "_id": "694b5c01746a34b55dd53d16",
                    "name": "Panpan Xu",
                    "hidden": false
                },
                {
                    "_id": "694b5c01746a34b55dd53d17",
                    "name": "Lin Lee Cheong",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-18T21:58:19.000Z",
            "submittedOnDailyAt": "2025-12-24T00:52:22.721Z",
            "title": "Reinforcement Learning for Self-Improving Agent with Skill Library",
            "submittedOnDailyBy": {
                "_id": "638a968ac432da48c7139ccf",
                "avatarUrl": "/avatars/29d08fc35074654b7f6418667847b206.svg",
                "isPro": false,
                "fullname": "Jiongxiao Wang",
                "user": "Jayfeather1024",
                "type": "user"
            },
            "summary": "Large Language Model (LLM)-based agents have demonstrated remarkable capabilities in complex reasoning and multi-turn interactions but struggle to continuously improve and adapt when deployed in new environments. One promising approach is implementing skill libraries that allow agents to learn, validate, and apply new skills. However, current skill library approaches rely primarily on LLM prompting, making consistent skill library implementation challenging. To overcome these challenges, we propose a Reinforcement Learning (RL)-based approach to enhance agents' self-improvement capabilities with a skill library. Specifically, we introduce Skill Augmented GRPO for self-Evolution (SAGE), a novel RL framework that systematically incorporates skills into learning. The framework's key component, Sequential Rollout, iteratively deploys agents across a chain of similar tasks for each rollout. As agents navigate through the task chain, skills generated from previous tasks accumulate in the library and become available for subsequent tasks. Additionally, the framework enhances skill generation and utilization through a Skill-integrated Reward that complements the original outcome-based rewards. Experimental results on AppWorld demonstrate that SAGE, when applied to supervised-finetuned model with expert experience, achieves 8.9% higher Scenario Goal Completion while requiring 26% fewer interaction steps and generating 59% fewer tokens, substantially outperforming existing approaches in both accuracy and efficiency.",
            "upvotes": 12,
            "discussionId": "694b5c01746a34b55dd53d18",
            "ai_summary": "A novel RL framework, SAGE, enhances LLM-based agents' self-improvement capabilities by systematically incorporating skills from a skill library, leading to better performance and efficiency in new environments.",
            "ai_keywords": [
                "Reinforcement Learning",
                "RL",
                "Skill Augmented GRPO for self-Evolution",
                "SAGE",
                "Sequential Rollout",
                "Skill-integrated Reward",
                "Scenario Goal Completion",
                "interaction steps",
                "tokens"
            ]
        },
        "publishedAt": "2025-12-18T16:58:19.000Z",
        "title": "Reinforcement Learning for Self-Improving Agent with Skill Library",
        "summary": "Large Language Model (LLM)-based agents have demonstrated remarkable capabilities in complex reasoning and multi-turn interactions but struggle to continuously improve and adapt when deployed in new environments. One promising approach is implementing skill libraries that allow agents to learn, validate, and apply new skills. However, current skill library approaches rely primarily on LLM prompting, making consistent skill library implementation challenging. To overcome these challenges, we propose a Reinforcement Learning (RL)-based approach to enhance agents' self-improvement capabilities with a skill library. Specifically, we introduce Skill Augmented GRPO for self-Evolution (SAGE), a novel RL framework that systematically incorporates skills into learning. The framework's key component, Sequential Rollout, iteratively deploys agents across a chain of similar tasks for each rollout. As agents navigate through the task chain, skills generated from previous tasks accumulate in the library and become available for subsequent tasks. Additionally, the framework enhances skill generation and utilization through a Skill-integrated Reward that complements the original outcome-based rewards. Experimental results on AppWorld demonstrate that SAGE, when applied to supervised-finetuned model with expert experience, achieves 8.9% higher Scenario Goal Completion while requiring 26% fewer interaction steps and generating 59% fewer tokens, substantially outperforming existing approaches in both accuracy and efficiency.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.17102.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "638a968ac432da48c7139ccf",
            "avatarUrl": "/avatars/29d08fc35074654b7f6418667847b206.svg",
            "fullname": "Jiongxiao Wang",
            "name": "Jayfeather1024",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.18099",
            "authors": [
                {
                    "_id": "694aa9bcd8d7445f420c171d",
                    "name": "Bowen Shi",
                    "hidden": false
                },
                {
                    "_id": "694aa9bcd8d7445f420c171e",
                    "name": "Andros Tjandra",
                    "hidden": false
                },
                {
                    "_id": "694aa9bcd8d7445f420c171f",
                    "name": "John Hoffman",
                    "hidden": false
                },
                {
                    "_id": "694aa9bcd8d7445f420c1720",
                    "name": "Helin Wang",
                    "hidden": false
                },
                {
                    "_id": "694aa9bcd8d7445f420c1721",
                    "name": "Yi-Chiao Wu",
                    "hidden": false
                },
                {
                    "_id": "694aa9bcd8d7445f420c1722",
                    "name": "Luya Gao",
                    "hidden": false
                },
                {
                    "_id": "694aa9bcd8d7445f420c1723",
                    "name": "Julius Richter",
                    "hidden": false
                },
                {
                    "_id": "694aa9bcd8d7445f420c1724",
                    "name": "Matt Le",
                    "hidden": false
                },
                {
                    "_id": "694aa9bcd8d7445f420c1725",
                    "name": "Apoorv Vyas",
                    "hidden": false
                },
                {
                    "_id": "694aa9bcd8d7445f420c1726",
                    "name": "Sanyuan Chen",
                    "hidden": false
                },
                {
                    "_id": "694aa9bcd8d7445f420c1727",
                    "name": "Christoph Feichtenhofer",
                    "hidden": false
                },
                {
                    "_id": "694aa9bcd8d7445f420c1728",
                    "name": "Piotr Dollr",
                    "hidden": false
                },
                {
                    "_id": "694aa9bcd8d7445f420c1729",
                    "name": "Wei-Ning Hsu",
                    "hidden": false
                },
                {
                    "_id": "694aa9bcd8d7445f420c172a",
                    "name": "Ann Lee",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-19T22:14:23.000Z",
            "submittedOnDailyAt": "2025-12-24T00:06:36.953Z",
            "title": "SAM Audio: Segment Anything in Audio",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "General audio source separation is a key capability for multimodal AI systems that can perceive and reason about sound. Despite substantial progress in recent years, existing separation models are either domain-specific, designed for fixed categories such as speech or music, or limited in controllability, supporting only a single prompting modality such as text. In this work, we present SAM Audio, a foundation model for general audio separation that unifies text, visual, and temporal span prompting within a single framework. Built on a diffusion transformer architecture, SAM Audio is trained with flow matching on large-scale audio data spanning speech, music, and general sounds, and can flexibly separate target sources described by language, visual masks, or temporal spans. The model achieves state-of-the-art performance across a diverse suite of benchmarks, including general sound, speech, music, and musical instrument separation in both in-the-wild and professionally produced audios, substantially outperforming prior general-purpose and specialized systems. Furthermore, we introduce a new real-world separation benchmark with human-labeled multimodal prompts and a reference-free evaluation model that correlates strongly with human judgment.",
            "upvotes": 10,
            "discussionId": "694aa9bdd8d7445f420c172b",
            "projectPage": "https://ai.meta.com/samaudio/",
            "githubRepo": "https://github.com/facebookresearch/sam-audio",
            "githubRepoAddedBy": "user",
            "ai_summary": "SAM Audio, a diffusion transformer-based foundation model, achieves superior performance in general audio separation using unified text, visual, and temporal span prompts across various audio types.",
            "ai_keywords": [
                "diffusion transformer architecture",
                "flow matching",
                "audio separation",
                "general sound",
                "speech",
                "music",
                "musical instrument separation",
                "in-the-wild audio",
                "professionally produced audio",
                "real-world separation benchmark",
                "human-labeled multimodal prompts",
                "reference-free evaluation model"
            ],
            "githubStars": 2494,
            "organization": {
                "_id": "5e63d8713071d5be688861b8",
                "name": "facebook",
                "fullname": "AI at Meta",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
            }
        },
        "publishedAt": "2025-12-19T17:14:23.000Z",
        "title": "SAM Audio: Segment Anything in Audio",
        "summary": "General audio source separation is a key capability for multimodal AI systems that can perceive and reason about sound. Despite substantial progress in recent years, existing separation models are either domain-specific, designed for fixed categories such as speech or music, or limited in controllability, supporting only a single prompting modality such as text. In this work, we present SAM Audio, a foundation model for general audio separation that unifies text, visual, and temporal span prompting within a single framework. Built on a diffusion transformer architecture, SAM Audio is trained with flow matching on large-scale audio data spanning speech, music, and general sounds, and can flexibly separate target sources described by language, visual masks, or temporal spans. The model achieves state-of-the-art performance across a diverse suite of benchmarks, including general sound, speech, music, and musical instrument separation in both in-the-wild and professionally produced audios, substantially outperforming prior general-purpose and specialized systems. Furthermore, we introduce a new real-world separation benchmark with human-labeled multimodal prompts and a reference-free evaluation model that correlates strongly with human judgment.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.18099.png",
        "numComments": 0,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 189
        },
        "organization": {
            "_id": "5e63d8713071d5be688861b8",
            "name": "facebook",
            "fullname": "AI at Meta",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.16144",
            "authors": [
                {
                    "_id": "694b7f94746a34b55dd53e1d",
                    "name": "Prime Intellect Team",
                    "hidden": false
                },
                {
                    "_id": "694b7f94746a34b55dd53e1e",
                    "name": "Mika Senghaas",
                    "hidden": false
                },
                {
                    "_id": "694b7f94746a34b55dd53e1f",
                    "name": "Fares Obeid",
                    "hidden": false
                },
                {
                    "_id": "694b7f94746a34b55dd53e20",
                    "name": "Sami Jaghouar",
                    "hidden": false
                },
                {
                    "_id": "694b7f94746a34b55dd53e21",
                    "name": "William Brown",
                    "hidden": false
                },
                {
                    "_id": "694b7f94746a34b55dd53e22",
                    "name": "Jack Min Ong",
                    "hidden": false
                },
                {
                    "_id": "694b7f94746a34b55dd53e23",
                    "name": "Daniel Auras",
                    "hidden": false
                },
                {
                    "_id": "694b7f94746a34b55dd53e24",
                    "name": "Matej Sirovatka",
                    "hidden": false
                },
                {
                    "_id": "694b7f94746a34b55dd53e25",
                    "name": "Jannik Straube",
                    "hidden": false
                },
                {
                    "_id": "694b7f94746a34b55dd53e26",
                    "name": "Andrew Baker",
                    "hidden": false
                },
                {
                    "_id": "694b7f94746a34b55dd53e27",
                    "name": "Sebastian Mller",
                    "hidden": false
                },
                {
                    "_id": "694b7f94746a34b55dd53e28",
                    "name": "Justus Mattern",
                    "hidden": false
                },
                {
                    "_id": "694b7f94746a34b55dd53e29",
                    "name": "Manveer Basra",
                    "hidden": false
                },
                {
                    "_id": "694b7f94746a34b55dd53e2a",
                    "name": "Aiman Ismail",
                    "hidden": false
                },
                {
                    "_id": "694b7f94746a34b55dd53e2b",
                    "name": "Dominik Scherm",
                    "hidden": false
                },
                {
                    "_id": "694b7f94746a34b55dd53e2c",
                    "name": "Cooper Miller",
                    "hidden": false
                },
                {
                    "_id": "694b7f94746a34b55dd53e2d",
                    "name": "Ameen Patel",
                    "hidden": false
                },
                {
                    "_id": "694b7f94746a34b55dd53e2e",
                    "name": "Simon Kirsten",
                    "hidden": false
                },
                {
                    "_id": "694b7f94746a34b55dd53e2f",
                    "name": "Mario Sieg",
                    "hidden": false
                },
                {
                    "_id": "694b7f94746a34b55dd53e30",
                    "name": "Christian Reetz",
                    "hidden": false
                },
                {
                    "_id": "694b7f94746a34b55dd53e31",
                    "name": "Kemal Erdem",
                    "hidden": false
                },
                {
                    "_id": "694b7f94746a34b55dd53e32",
                    "name": "Vincent Weisser",
                    "hidden": false
                },
                {
                    "_id": "694b7f94746a34b55dd53e33",
                    "name": "Johannes Hagemann",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-18T03:57:01.000Z",
            "submittedOnDailyAt": "2025-12-24T03:23:51.710Z",
            "title": "INTELLECT-3: Technical Report",
            "submittedOnDailyBy": {
                "_id": "631ce4b244503b72277fc89f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677431596830-631ce4b244503b72277fc89f.jpeg",
                "isPro": true,
                "fullname": "Quentin Galloudec",
                "user": "qgallouedec",
                "type": "user"
            },
            "summary": "We present INTELLECT-3, a 106B-parameter Mixture-of-Experts model (12B active) trained with large-scale reinforcement learning on our end-to-end RL infrastructure stack. INTELLECT-3 achieves state of the art performance for its size across math, code, science and reasoning benchmarks, outperforming many larger frontier models. We open-source the model together with the full infrastructure stack used to create it, including RL frameworks, complete recipe, and a wide collection of environments, built with the verifiers library, for training and evaluation from our Environments Hub community platform. Built for this effort, we introduce prime-rl, an open framework for large-scale asynchronous reinforcement learning, which scales seamlessly from a single node to thousands of GPUs, and is tailored for agentic RL with first-class support for multi-turn interactions and tool use. Using this stack, we run both SFT and RL training on top of the GLM-4.5-Air-Base model, scaling RL training up to 512 H200s with high training efficiency.",
            "upvotes": 8,
            "discussionId": "694b7f94746a34b55dd53e34",
            "ai_summary": "INTELLECT-3, a large Mixture-of-Experts model trained with reinforcement learning, achieves top performance across various benchmarks and is supported by an open-source RL infrastructure framework.",
            "ai_keywords": [
                "Mixture-of-Experts",
                "reinforcement learning",
                "RL infrastructure",
                "prime-rl",
                "asynchronous reinforcement learning",
                "GLM-4.5-Air-Base",
                "SFT",
                "H200s"
            ],
            "organization": {
                "_id": "656ec1d908bd4deb79a0ba70",
                "name": "PrimeIntellect",
                "fullname": "Prime Intellect",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61e020e4a343274bb132e138/H2mcdPRWtl4iKLd-OYYBc.jpeg"
            }
        },
        "publishedAt": "2025-12-17T22:57:01.000Z",
        "title": "INTELLECT-3: Technical Report",
        "summary": "We present INTELLECT-3, a 106B-parameter Mixture-of-Experts model (12B active) trained with large-scale reinforcement learning on our end-to-end RL infrastructure stack. INTELLECT-3 achieves state of the art performance for its size across math, code, science and reasoning benchmarks, outperforming many larger frontier models. We open-source the model together with the full infrastructure stack used to create it, including RL frameworks, complete recipe, and a wide collection of environments, built with the verifiers library, for training and evaluation from our Environments Hub community platform. Built for this effort, we introduce prime-rl, an open framework for large-scale asynchronous reinforcement learning, which scales seamlessly from a single node to thousands of GPUs, and is tailored for agentic RL with first-class support for multi-turn interactions and tool use. Using this stack, we run both SFT and RL training on top of the GLM-4.5-Air-Base model, scaling RL training up to 512 H200s with high training efficiency.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16144.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "631ce4b244503b72277fc89f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677431596830-631ce4b244503b72277fc89f.jpeg",
            "fullname": "Quentin Galloudec",
            "name": "qgallouedec",
            "type": "user",
            "isPro": true,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 560
        },
        "organization": {
            "_id": "656ec1d908bd4deb79a0ba70",
            "name": "PrimeIntellect",
            "fullname": "Prime Intellect",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61e020e4a343274bb132e138/H2mcdPRWtl4iKLd-OYYBc.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.20182",
            "authors": [
                {
                    "_id": "694b7923746a34b55dd53df4",
                    "name": "Shuzheng Si",
                    "hidden": false
                },
                {
                    "_id": "694b7923746a34b55dd53df5",
                    "name": "Qingyi Wang",
                    "hidden": false
                },
                {
                    "_id": "694b7923746a34b55dd53df6",
                    "name": "Haozhe Zhao",
                    "hidden": false
                },
                {
                    "_id": "694b7923746a34b55dd53df7",
                    "name": "Yuzhuo Bai",
                    "hidden": false
                },
                {
                    "_id": "694b7923746a34b55dd53df8",
                    "name": "Guanqiao Chen",
                    "hidden": false
                },
                {
                    "_id": "694b7923746a34b55dd53df9",
                    "name": "Kangyang Luo",
                    "hidden": false
                },
                {
                    "_id": "694b7923746a34b55dd53dfa",
                    "name": "Gang Chen",
                    "hidden": false
                },
                {
                    "_id": "694b7923746a34b55dd53dfb",
                    "name": "Fanchao Qi",
                    "hidden": false
                },
                {
                    "_id": "694b7923746a34b55dd53dfc",
                    "name": "Minjia Zhang",
                    "hidden": false
                },
                {
                    "_id": "694b7923746a34b55dd53dfd",
                    "name": "Baobao Chang",
                    "hidden": false
                },
                {
                    "_id": "694b7923746a34b55dd53dfe",
                    "name": "Maosong Sun",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-23T09:20:32.000Z",
            "submittedOnDailyAt": "2025-12-24T02:56:50.413Z",
            "title": "FaithLens: Detecting and Explaining Faithfulness Hallucination",
            "submittedOnDailyBy": {
                "_id": "637c99bbfe115289cfedfb44",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c99bbfe115289cfedfb44/p4uSY0TKufJfcHpvEb_ZQ.jpeg",
                "isPro": false,
                "fullname": "ssz",
                "user": "ssz1111",
                "type": "user"
            },
            "summary": "Recognizing whether outputs from large language models (LLMs) contain faithfulness hallucination is crucial for real-world applications, e.g., retrieval-augmented generation and summarization. In this paper, we introduce FaithLens, a cost-efficient and effective faithfulness hallucination detection model that can jointly provide binary predictions and corresponding explanations to improve trustworthiness. To achieve this, we first synthesize training data with explanations via advanced LLMs and apply a well-defined data filtering strategy to ensure label correctness, explanation quality, and data diversity. Subsequently, we fine-tune the model on these well-curated training data as a cold start and further optimize it with rule-based reinforcement learning, using rewards for both prediction correctness and explanation quality. Results on 12 diverse tasks show that the 8B-parameter FaithLens outperforms advanced models such as GPT-4.1 and o3. Also, FaithLens can produce high-quality explanations, delivering a distinctive balance of trustworthiness, efficiency, and effectiveness.",
            "upvotes": 5,
            "discussionId": "694b7923746a34b55dd53dff",
            "githubRepo": "https://github.com/S1s-Z/FaithLens",
            "githubRepoAddedBy": "user",
            "ai_summary": "FaithLens, a cost-efficient faithfulness hallucination detection model using advanced LLMs for training data synthesis and rule-based reinforcement learning, outperforms models like GPT-4.1 and o3 on 12 tasks with high-quality explanations.",
            "ai_keywords": [
                "faithfulness hallucination",
                "FaithLens",
                "large language models (LLMs)",
                "retrieval-augmented generation",
                "summarization",
                "binary predictions",
                "explanations",
                "data filtering strategy",
                "cold start",
                "rule-based reinforcement learning",
                "prediction correctness",
                "explanation quality",
                "data diversity",
                "trustworthiness",
                "efficiency",
                "effectiveness"
            ],
            "githubStars": 11,
            "organization": {
                "_id": "6904703d1c4c3a34bf765739",
                "name": "TsinghuaNLP",
                "fullname": "Tsinghua NLP Group",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60965a5d206218bf2b0e00ae/4_JDQKsQDyLK0oGegQCl7.png"
            }
        },
        "publishedAt": "2025-12-23T04:20:32.000Z",
        "title": "FaithLens: Detecting and Explaining Faithfulness Hallucination",
        "summary": "Recognizing whether outputs from large language models (LLMs) contain faithfulness hallucination is crucial for real-world applications, e.g., retrieval-augmented generation and summarization. In this paper, we introduce FaithLens, a cost-efficient and effective faithfulness hallucination detection model that can jointly provide binary predictions and corresponding explanations to improve trustworthiness. To achieve this, we first synthesize training data with explanations via advanced LLMs and apply a well-defined data filtering strategy to ensure label correctness, explanation quality, and data diversity. Subsequently, we fine-tune the model on these well-curated training data as a cold start and further optimize it with rule-based reinforcement learning, using rewards for both prediction correctness and explanation quality. Results on 12 diverse tasks show that the 8B-parameter FaithLens outperforms advanced models such as GPT-4.1 and o3. Also, FaithLens can produce high-quality explanations, delivering a distinctive balance of trustworthiness, efficiency, and effectiveness.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20182.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "637c99bbfe115289cfedfb44",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c99bbfe115289cfedfb44/p4uSY0TKufJfcHpvEb_ZQ.jpeg",
            "fullname": "ssz",
            "name": "ssz1111",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "6904703d1c4c3a34bf765739",
            "name": "TsinghuaNLP",
            "fullname": "Tsinghua NLP Group",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60965a5d206218bf2b0e00ae/4_JDQKsQDyLK0oGegQCl7.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.13472",
            "authors": [
                {
                    "_id": "694be9e1746a34b55dd53f86",
                    "name": "Jian Yang",
                    "hidden": false
                },
                {
                    "_id": "694be9e1746a34b55dd53f87",
                    "name": "Shawn Guo",
                    "hidden": false
                },
                {
                    "_id": "694be9e1746a34b55dd53f88",
                    "name": "Lin Jing",
                    "hidden": false
                },
                {
                    "_id": "694be9e1746a34b55dd53f89",
                    "name": "Wei Zhang",
                    "hidden": false
                },
                {
                    "_id": "694be9e1746a34b55dd53f8a",
                    "name": "Aishan Liu",
                    "hidden": false
                },
                {
                    "_id": "694be9e1746a34b55dd53f8b",
                    "name": "Chuan Hao",
                    "hidden": false
                },
                {
                    "_id": "694be9e1746a34b55dd53f8c",
                    "name": "Zhoujun Li",
                    "hidden": false
                },
                {
                    "_id": "694be9e1746a34b55dd53f8d",
                    "name": "Wayne Xin Zhao",
                    "hidden": false
                },
                {
                    "_id": "694be9e1746a34b55dd53f8e",
                    "name": "Xianglong Liu",
                    "hidden": false
                },
                {
                    "_id": "694be9e1746a34b55dd53f8f",
                    "name": "Weifeng Lv",
                    "hidden": false
                },
                {
                    "_id": "694be9e1746a34b55dd53f90",
                    "name": "Bryan Dai",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-15T16:07:34.000Z",
            "submittedOnDailyAt": "2025-12-24T10:58:25.531Z",
            "title": "Scaling Laws for Code: Every Programming Language Matters",
            "submittedOnDailyBy": {
                "_id": "64ccb9bfead94891d12aef42",
                "avatarUrl": "/avatars/c54809d43d93d3f0766bd2555cecc4e3.svg",
                "isPro": false,
                "fullname": "Yang Jian",
                "user": "CSJianYang",
                "type": "user"
            },
            "summary": "Code large language models (Code LLMs) are powerful but costly to train, with scaling laws predicting performance from model size, data, and compute. However, different programming languages (PLs) have varying impacts during pre-training that significantly affect base model performance, leading to inaccurate performance prediction. Besides, existing works focus on language-agnostic settings, neglecting the inherently multilingual nature of modern software development. Therefore, it is first necessary to investigate the scaling laws of different PLs, and then consider their mutual influences to arrive at the final multilingual scaling law. In this paper, we present the first systematic exploration of scaling laws for multilingual code pre-training, conducting over 1000+ experiments (Equivalent to 336,000+ H800 hours) across multiple PLs, model sizes (0.2B to 14B parameters), and dataset sizes (1T tokens). We establish comprehensive scaling laws for code LLMs across multiple PLs, revealing that interpreted languages (e.g., Python) benefit more from increased model size and data than compiled languages (e.g., Rust). The study demonstrates that multilingual pre-training provides synergistic benefits, particularly between syntactically similar PLs. Further, the pre-training strategy of the parallel pairing (concatenating code snippets with their translations) significantly enhances cross-lingual abilities with favorable scaling properties. Finally, a proportion-dependent multilingual scaling law is proposed to optimally allocate training tokens by prioritizing high-utility PLs (e.g., Python), balancing high-synergy pairs (e.g., JavaScript-TypeScript), and reducing allocation to fast-saturating languages (Rust), achieving superior average performance across all PLs compared to uniform distribution under the same compute budget.",
            "upvotes": 5,
            "discussionId": "694be9e2746a34b55dd53f91",
            "ai_summary": "Systematic exploration of scaling laws for multilingual code pre-training reveals language-specific benefits and proposes a strategy for optimal token allocation across programming languages.",
            "ai_keywords": [
                "Code LLMs",
                "programming languages",
                "scaling laws",
                "pre-training",
                "multilingual pre-training",
                "interpreted languages",
                "compiled languages",
                "parallel pairing",
                "proportion-dependent multilingual scaling law"
            ]
        },
        "publishedAt": "2025-12-15T11:07:34.000Z",
        "title": "Scaling Laws for Code: Every Programming Language Matters",
        "summary": "Code large language models (Code LLMs) are powerful but costly to train, with scaling laws predicting performance from model size, data, and compute. However, different programming languages (PLs) have varying impacts during pre-training that significantly affect base model performance, leading to inaccurate performance prediction. Besides, existing works focus on language-agnostic settings, neglecting the inherently multilingual nature of modern software development. Therefore, it is first necessary to investigate the scaling laws of different PLs, and then consider their mutual influences to arrive at the final multilingual scaling law. In this paper, we present the first systematic exploration of scaling laws for multilingual code pre-training, conducting over 1000+ experiments (Equivalent to 336,000+ H800 hours) across multiple PLs, model sizes (0.2B to 14B parameters), and dataset sizes (1T tokens). We establish comprehensive scaling laws for code LLMs across multiple PLs, revealing that interpreted languages (e.g., Python) benefit more from increased model size and data than compiled languages (e.g., Rust). The study demonstrates that multilingual pre-training provides synergistic benefits, particularly between syntactically similar PLs. Further, the pre-training strategy of the parallel pairing (concatenating code snippets with their translations) significantly enhances cross-lingual abilities with favorable scaling properties. Finally, a proportion-dependent multilingual scaling law is proposed to optimally allocate training tokens by prioritizing high-utility PLs (e.g., Python), balancing high-synergy pairs (e.g., JavaScript-TypeScript), and reducing allocation to fast-saturating languages (Rust), achieving superior average performance across all PLs compared to uniform distribution under the same compute budget.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13472.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64ccb9bfead94891d12aef42",
            "avatarUrl": "/avatars/c54809d43d93d3f0766bd2555cecc4e3.svg",
            "fullname": "Yang Jian",
            "name": "CSJianYang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 22
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.19526",
            "authors": [
                {
                    "_id": "694ac8d6746a34b55dd53b88",
                    "name": "Li Puyin",
                    "hidden": false
                },
                {
                    "_id": "694ac8d6746a34b55dd53b89",
                    "name": "Tiange Xiang",
                    "hidden": false
                },
                {
                    "_id": "694ac8d6746a34b55dd53b8a",
                    "name": "Ella Mao",
                    "hidden": false
                },
                {
                    "_id": "694ac8d6746a34b55dd53b8b",
                    "name": "Shirley Wei",
                    "hidden": false
                },
                {
                    "_id": "694ac8d6746a34b55dd53b8c",
                    "name": "Xinye Chen",
                    "hidden": false
                },
                {
                    "_id": "694ac8d6746a34b55dd53b8d",
                    "name": "Adnan Masood",
                    "hidden": false
                },
                {
                    "_id": "694ac8d6746a34b55dd53b8e",
                    "name": "Li Fei-fei",
                    "hidden": false
                },
                {
                    "_id": "694ac8d6746a34b55dd53b8f",
                    "name": "Ehsan Adeli",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/7KZOmUy4Qk-jnKXmBFZw5.png"
            ],
            "publishedAt": "2025-12-22T16:18:00.000Z",
            "submittedOnDailyAt": "2025-12-24T00:29:59.725Z",
            "title": "QuantiPhy: A Quantitative Benchmark Evaluating Physical Reasoning Abilities of Vision-Language Models",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Understanding the physical world is essential for generalist AI agents. However, it remains unclear whether state-of-the-art vision perception models (e.g., large VLMs) can reason physical properties quantitatively. Existing evaluations are predominantly VQA-based and qualitative, offering limited insight into whether these models can infer the kinematic quantities of moving objects from video observations. To address this, we present QuantiPhy, the first benchmark designed to quantitatively measure a VLM's physical reasoning ability. Comprising more than 3.3K video-text instances with numerical ground truth, QuantiPhy evaluates a VLM's performance on estimating an object's size, velocity, and acceleration at a given timestamp, using one of these properties as an input prior. The benchmark standardizes prompts and scoring to assess numerical accuracy, enabling fair comparisons across models. Our experiments on state-of-the-art VLMs reveal a consistent gap between their qualitative plausibility and actual numerical correctness. We further provide an in-depth analysis of key factors like background noise, counterfactual priors, and strategic prompting and find that state-of-the-art VLMs lean heavily on pre-trained world knowledge rather than faithfully using the provided visual and textual inputs as references when reasoning kinematic properties quantitatively. QuantiPhy offers the first rigorous, scalable testbed to move VLMs beyond mere verbal plausibility toward a numerically grounded physical understanding.",
            "upvotes": 3,
            "discussionId": "694ac8d6746a34b55dd53b90",
            "ai_summary": "QuantiPhy is a benchmark that quantitatively assesses state-of-the-art vision perception models' ability to reason about physical properties such as size, velocity, and acceleration from video observations, revealing gaps between qualitative plausibility and numerical correctness.",
            "ai_keywords": [
                "VLMs",
                "QuantiPhy",
                "physical reasoning",
                "numerical ground truth",
                "size",
                "velocity",
                "acceleration",
                "background noise",
                "counterfactual priors",
                "strategic prompting",
                "numerical accuracy",
                "visual perception",
                "kinematic properties"
            ],
            "organization": {
                "_id": "672c672dcf09d152f4da04c4",
                "name": "StanfordUniversity",
                "fullname": "Stanford University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/vJI0POlzGMXL2878t1vz2.jpeg"
            }
        },
        "publishedAt": "2025-12-22T11:18:00.000Z",
        "title": "QuantiPhy: A Quantitative Benchmark Evaluating Physical Reasoning Abilities of Vision-Language Models",
        "summary": "Understanding the physical world is essential for generalist AI agents. However, it remains unclear whether state-of-the-art vision perception models (e.g., large VLMs) can reason physical properties quantitatively. Existing evaluations are predominantly VQA-based and qualitative, offering limited insight into whether these models can infer the kinematic quantities of moving objects from video observations. To address this, we present QuantiPhy, the first benchmark designed to quantitatively measure a VLM's physical reasoning ability. Comprising more than 3.3K video-text instances with numerical ground truth, QuantiPhy evaluates a VLM's performance on estimating an object's size, velocity, and acceleration at a given timestamp, using one of these properties as an input prior. The benchmark standardizes prompts and scoring to assess numerical accuracy, enabling fair comparisons across models. Our experiments on state-of-the-art VLMs reveal a consistent gap between their qualitative plausibility and actual numerical correctness. We further provide an in-depth analysis of key factors like background noise, counterfactual priors, and strategic prompting and find that state-of-the-art VLMs lean heavily on pre-trained world knowledge rather than faithfully using the provided visual and textual inputs as references when reasoning kinematic properties quantitatively. QuantiPhy offers the first rigorous, scalable testbed to move VLMs beyond mere verbal plausibility toward a numerically grounded physical understanding.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/7KZOmUy4Qk-jnKXmBFZw5.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.19526.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 189
        },
        "organization": {
            "_id": "672c672dcf09d152f4da04c4",
            "name": "StanfordUniversity",
            "fullname": "Stanford University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/vJI0POlzGMXL2878t1vz2.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.17648",
            "authors": [
                {
                    "_id": "69490a3834f46eaf46cbb412",
                    "name": "Marco Gaido",
                    "hidden": false
                },
                {
                    "_id": "69490a3834f46eaf46cbb413",
                    "user": {
                        "_id": "66309b3833ccd9e68c5d5171",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66309b3833ccd9e68c5d5171/UGt7pZazJHhtwjg0iiyBu.jpeg",
                        "isPro": false,
                        "fullname": "Sara Papi",
                        "user": "spapi",
                        "type": "user"
                    },
                    "name": "Sara Papi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-22T10:58:19.636Z",
                    "hidden": false
                },
                {
                    "_id": "69490a3834f46eaf46cbb414",
                    "name": "Mauro Cettolo",
                    "hidden": false
                },
                {
                    "_id": "69490a3834f46eaf46cbb415",
                    "name": "Matteo Negri",
                    "hidden": false
                },
                {
                    "_id": "69490a3834f46eaf46cbb416",
                    "name": "Luisa Bentivogli",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-19T14:48:59.000Z",
            "submittedOnDailyAt": "2025-12-24T08:28:58.581Z",
            "title": "Simulstream: Open-Source Toolkit for Evaluation and Demonstration of Streaming Speech-to-Text Translation Systems",
            "submittedOnDailyBy": {
                "_id": "66309b3833ccd9e68c5d5171",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66309b3833ccd9e68c5d5171/UGt7pZazJHhtwjg0iiyBu.jpeg",
                "isPro": false,
                "fullname": "Sara Papi",
                "user": "spapi",
                "type": "user"
            },
            "summary": "Streaming Speech-to-Text Translation (StreamST) requires producing translations concurrently with incoming speech, imposing strict latency constraints and demanding models that balance partial-information decision-making with high translation quality. Research efforts on the topic have so far relied on the SimulEval repository, which is no longer maintained and does not support systems that revise their outputs. In addition, it has been designed for simulating the processing of short segments, rather than long-form audio streams, and it does not provide an easy method to showcase systems in a demo. As a solution, we introduce simulstream, the first open-source framework dedicated to unified evaluation and demonstration of StreamST systems. Designed for long-form speech processing, it supports not only incremental decoding approaches, but also re-translation methods, enabling for their comparison within the same framework both in terms of quality and latency. In addition, it also offers an interactive web interface to demo any system built within the tool.",
            "upvotes": 3,
            "discussionId": "69490a3934f46eaf46cbb417",
            "ai_summary": "A new open-source framework, simulstream, is introduced for evaluating and demonstrating streaming speech-to-text translation systems, supporting both incremental decoding and re-translation methods with a focus on long-form audio processing.",
            "ai_keywords": [
                "streaming speech-to-text translation",
                "SimulEval",
                "simulstream",
                "long-form speech processing",
                "incremental decoding",
                "re-translation methods",
                "web interface"
            ]
        },
        "publishedAt": "2025-12-19T09:48:59.000Z",
        "title": "Simulstream: Open-Source Toolkit for Evaluation and Demonstration of Streaming Speech-to-Text Translation Systems",
        "summary": "Streaming Speech-to-Text Translation (StreamST) requires producing translations concurrently with incoming speech, imposing strict latency constraints and demanding models that balance partial-information decision-making with high translation quality. Research efforts on the topic have so far relied on the SimulEval repository, which is no longer maintained and does not support systems that revise their outputs. In addition, it has been designed for simulating the processing of short segments, rather than long-form audio streams, and it does not provide an easy method to showcase systems in a demo. As a solution, we introduce simulstream, the first open-source framework dedicated to unified evaluation and demonstration of StreamST systems. Designed for long-form speech processing, it supports not only incremental decoding approaches, but also re-translation methods, enabling for their comparison within the same framework both in terms of quality and latency. In addition, it also offers an interactive web interface to demo any system built within the tool.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.17648.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "66309b3833ccd9e68c5d5171",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66309b3833ccd9e68c5d5171/UGt7pZazJHhtwjg0iiyBu.jpeg",
            "fullname": "Sara Papi",
            "name": "spapi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 13
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.20615",
            "authors": [
                {
                    "_id": "694b6889746a34b55dd53dd2",
                    "name": "Xuanhua He",
                    "hidden": false
                },
                {
                    "_id": "694b6889746a34b55dd53dd3",
                    "name": "Tianyu Yang",
                    "hidden": false
                },
                {
                    "_id": "694b6889746a34b55dd53dd4",
                    "name": "Ke Cao",
                    "hidden": false
                },
                {
                    "_id": "694b6889746a34b55dd53dd5",
                    "name": "Ruiqi Wu",
                    "hidden": false
                },
                {
                    "_id": "694b6889746a34b55dd53dd6",
                    "name": "Cheng Meng",
                    "hidden": false
                },
                {
                    "_id": "694b6889746a34b55dd53dd7",
                    "name": "Yong Zhang",
                    "hidden": false
                },
                {
                    "_id": "694b6889746a34b55dd53dd8",
                    "name": "Zhuoliang Kang",
                    "hidden": false
                },
                {
                    "_id": "694b6889746a34b55dd53dd9",
                    "name": "Xiaoming Wei",
                    "hidden": false
                },
                {
                    "_id": "694b6889746a34b55dd53dda",
                    "name": "Qifeng Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-23T18:59:16.000Z",
            "submittedOnDailyAt": "2025-12-24T01:45:05.088Z",
            "title": "Active Intelligence in Video Avatars via Closed-loop World Modeling",
            "submittedOnDailyBy": {
                "_id": "64375035946fb080c6fc4551",
                "avatarUrl": "/avatars/8dba2c8911726656d4088862a2b8fe7c.svg",
                "isPro": false,
                "fullname": "Xuanhua He",
                "user": "Alexhe101",
                "type": "user"
            },
            "summary": "Current video avatar generation methods excel at identity preservation and motion alignment but lack genuine agency, they cannot autonomously pursue long-term goals through adaptive environmental interaction. We address this by introducing L-IVA (Long-horizon Interactive Visual Avatar), a task and benchmark for evaluating goal-directed planning in stochastic generative environments, and ORCA (Online Reasoning and Cognitive Architecture), the first framework enabling active intelligence in video avatars. ORCA embodies Internal World Model (IWM) capabilities through two key innovations: (1) a closed-loop OTAR cycle (Observe-Think-Act-Reflect) that maintains robust state tracking under generative uncertainty by continuously verifying predicted outcomes against actual generations, and (2) a hierarchical dual-system architecture where System 2 performs strategic reasoning with state prediction while System 1 translates abstract plans into precise, model-specific action captions. By formulating avatar control as a POMDP and implementing continuous belief updating with outcome verification, ORCA enables autonomous multi-step task completion in open-domain scenarios. Extensive experiments demonstrate that ORCA significantly outperforms open-loop and non-reflective baselines in task success rate and behavioral coherence, validating our IWM-inspired design for advancing video avatar intelligence from passive animation to active, goal-oriented behavior.",
            "upvotes": 2,
            "discussionId": "694b6889746a34b55dd53ddb",
            "projectPage": "https://xuanhuahe.github.io/ORCA/",
            "ai_summary": "ORCA, a framework for goal-directed planning in video avatars, uses an internal world model and dual-system architecture to enable autonomous task completion in stochastic environments.",
            "ai_keywords": [
                "L-IVA",
                "ORCA",
                "Internal World Model",
                "IWM",
                "closed-loop OTAR cycle",
                "Observe-Think-Act-Reflect",
                "hierarchical dual-system architecture",
                "POMDP",
                "belief updating",
                "outcome verification"
            ]
        },
        "publishedAt": "2025-12-23T13:59:16.000Z",
        "title": "Active Intelligence in Video Avatars via Closed-loop World Modeling",
        "summary": "Current video avatar generation methods excel at identity preservation and motion alignment but lack genuine agency, they cannot autonomously pursue long-term goals through adaptive environmental interaction. We address this by introducing L-IVA (Long-horizon Interactive Visual Avatar), a task and benchmark for evaluating goal-directed planning in stochastic generative environments, and ORCA (Online Reasoning and Cognitive Architecture), the first framework enabling active intelligence in video avatars. ORCA embodies Internal World Model (IWM) capabilities through two key innovations: (1) a closed-loop OTAR cycle (Observe-Think-Act-Reflect) that maintains robust state tracking under generative uncertainty by continuously verifying predicted outcomes against actual generations, and (2) a hierarchical dual-system architecture where System 2 performs strategic reasoning with state prediction while System 1 translates abstract plans into precise, model-specific action captions. By formulating avatar control as a POMDP and implementing continuous belief updating with outcome verification, ORCA enables autonomous multi-step task completion in open-domain scenarios. Extensive experiments demonstrate that ORCA significantly outperforms open-loop and non-reflective baselines in task success rate and behavioral coherence, validating our IWM-inspired design for advancing video avatar intelligence from passive animation to active, goal-oriented behavior.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20615.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64375035946fb080c6fc4551",
            "avatarUrl": "/avatars/8dba2c8911726656d4088862a2b8fe7c.svg",
            "fullname": "Xuanhua He",
            "name": "Alexhe101",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.20352",
            "authors": [
                {
                    "_id": "694b7f40746a34b55dd53e13",
                    "name": "Nilesh Jain",
                    "hidden": false
                },
                {
                    "_id": "694b7f40746a34b55dd53e14",
                    "name": "Seyi Adeyinka",
                    "hidden": false
                },
                {
                    "_id": "694b7f40746a34b55dd53e15",
                    "name": "Leor Roseman",
                    "hidden": false
                },
                {
                    "_id": "694b7f40746a34b55dd53e16",
                    "name": "Aza Allsop",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-23T13:32:43.000Z",
            "submittedOnDailyAt": "2025-12-24T03:23:09.742Z",
            "title": "Multi-LLM Thematic Analysis with Dual Reliability Metrics: Combining Cohen's Kappa and Semantic Similarity for Qualitative Research Validation",
            "submittedOnDailyBy": {
                "_id": "62d28f0c8d8206fd4d84ceca",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62d28f0c8d8206fd4d84ceca/E9k4-RUCnoAgYinh6Ax2W.png",
                "isPro": false,
                "fullname": "Nilesh Jain - Founder of Bibby AI",
                "user": "BibbyResearch",
                "type": "user"
            },
            "summary": "Qualitative research faces a critical reliability challenge: traditional inter-rater agreement methods require multiple human coders, are time-intensive, and often yield moderate consistency. We present a multi-perspective validation framework for LLM-based thematic analysis that combines ensemble validation with dual reliability metrics: Cohen's Kappa () for inter-rater agreement and cosine similarity for semantic consistency. Our framework enables configurable analysis parameters (1-6 seeds, temperature 0.0-2.0), supports custom prompt structures with variable substitution, and provides consensus theme extraction across any JSON format. As proof-of-concept, we evaluate three leading LLMs (Gemini 2.5 Pro, GPT-4o, Claude 3.5 Sonnet) on a psychedelic art therapy interview transcript, conducting six independent runs per model. Results demonstrate Gemini achieves highest reliability (= 0.907, cosine=95.3%), followed by GPT-4o (= 0.853, cosine=92.6%) and Claude (= 0.842, cosine=92.1%). All three models achieve a high agreement (> 0.80), validating the multi-run ensemble approach. The framework successfully extracts consensus themes across runs, with Gemini identifying 6 consensus themes (50-83% consistency), GPT-4o identifying 5 themes, and Claude 4 themes. Our open-source implementation provides researchers with transparent reliability metrics, flexible configuration, and structure-agnostic consensus extraction, establishing methodological foundations for reliable AI-assisted qualitative research.",
            "upvotes": 2,
            "discussionId": "694b7f40746a34b55dd53e17",
            "projectPage": "https://azalab-llm-tool.vercel.app/",
            "githubRepo": "https://github.com/NileshArnaiya/LLM-Thematic-Analysis-Tool",
            "githubRepoAddedBy": "user",
            "ai_summary": "A multi-perspective validation framework using LLMs for thematic analysis combines ensemble validation with Cohen's Kappa and cosine similarity to enhance reliability and extract consensus themes from qualitative data.",
            "ai_keywords": [
                "Cohen's Kappa",
                "cosine similarity",
                "ensemble validation",
                "LLM-based thematic analysis",
                "multi-perspective validation framework",
                "consensus theme extraction",
                "JSON format",
                "psychedelic art therapy",
                "Gemini 2.5 Pro",
                "GPT-4o",
                "Claude 3.5 Sonnet"
            ],
            "githubStars": 0,
            "organization": {
                "_id": "6756014dd83c390221a3815c",
                "name": "YaleUniversity",
                "fullname": "Yale University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6755ff00d3ff7f20aad244d2/xk0r-87S0AVfUA1XX8CG3.png"
            }
        },
        "publishedAt": "2025-12-23T08:32:43.000Z",
        "title": "Multi-LLM Thematic Analysis with Dual Reliability Metrics: Combining Cohen's Kappa and Semantic Similarity for Qualitative Research Validation",
        "summary": "Qualitative research faces a critical reliability challenge: traditional inter-rater agreement methods require multiple human coders, are time-intensive, and often yield moderate consistency. We present a multi-perspective validation framework for LLM-based thematic analysis that combines ensemble validation with dual reliability metrics: Cohen's Kappa () for inter-rater agreement and cosine similarity for semantic consistency. Our framework enables configurable analysis parameters (1-6 seeds, temperature 0.0-2.0), supports custom prompt structures with variable substitution, and provides consensus theme extraction across any JSON format. As proof-of-concept, we evaluate three leading LLMs (Gemini 2.5 Pro, GPT-4o, Claude 3.5 Sonnet) on a psychedelic art therapy interview transcript, conducting six independent runs per model. Results demonstrate Gemini achieves highest reliability (= 0.907, cosine=95.3%), followed by GPT-4o (= 0.853, cosine=92.6%) and Claude (= 0.842, cosine=92.1%). All three models achieve a high agreement (> 0.80), validating the multi-run ensemble approach. The framework successfully extracts consensus themes across runs, with Gemini identifying 6 consensus themes (50-83% consistency), GPT-4o identifying 5 themes, and Claude 4 themes. Our open-source implementation provides researchers with transparent reliability metrics, flexible configuration, and structure-agnostic consensus extraction, establishing methodological foundations for reliable AI-assisted qualitative research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20352.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "62d28f0c8d8206fd4d84ceca",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62d28f0c8d8206fd4d84ceca/E9k4-RUCnoAgYinh6Ax2W.png",
            "fullname": "Nilesh Jain - Founder of Bibby AI",
            "name": "BibbyResearch",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 17
        },
        "organization": {
            "_id": "6756014dd83c390221a3815c",
            "name": "YaleUniversity",
            "fullname": "Yale University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6755ff00d3ff7f20aad244d2/xk0r-87S0AVfUA1XX8CG3.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.20092",
            "authors": [
                {
                    "_id": "694b8232746a34b55dd53e36",
                    "name": "Yiming Du",
                    "hidden": false
                },
                {
                    "_id": "694b8232746a34b55dd53e37",
                    "name": "Baojun Wang",
                    "hidden": false
                },
                {
                    "_id": "694b8232746a34b55dd53e38",
                    "name": "Yifan Xiang",
                    "hidden": false
                },
                {
                    "_id": "694b8232746a34b55dd53e39",
                    "name": "Zhaowei Wang",
                    "hidden": false
                },
                {
                    "_id": "694b8232746a34b55dd53e3a",
                    "name": "Wenyu Huang",
                    "hidden": false
                },
                {
                    "_id": "694b8232746a34b55dd53e3b",
                    "name": "Boyang Xue",
                    "hidden": false
                },
                {
                    "_id": "694b8232746a34b55dd53e3c",
                    "name": "Bin Liang",
                    "hidden": false
                },
                {
                    "_id": "694b8232746a34b55dd53e3d",
                    "name": "Xingshan Zeng",
                    "hidden": false
                },
                {
                    "_id": "694b8232746a34b55dd53e3e",
                    "name": "Fei Mi",
                    "hidden": false
                },
                {
                    "_id": "694b8232746a34b55dd53e3f",
                    "name": "Haoli Bai",
                    "hidden": false
                },
                {
                    "_id": "694b8232746a34b55dd53e40",
                    "name": "Lifeng Shang",
                    "hidden": false
                },
                {
                    "_id": "694b8232746a34b55dd53e41",
                    "name": "Jeff Z. Pan",
                    "hidden": false
                },
                {
                    "_id": "694b8232746a34b55dd53e42",
                    "name": "Yuxin Jiang",
                    "hidden": false
                },
                {
                    "_id": "694b8232746a34b55dd53e43",
                    "name": "Kam-Fai Wong",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-23T06:37:29.000Z",
            "submittedOnDailyAt": "2025-12-24T13:33:07.382Z",
            "title": "Memory-T1: Reinforcement Learning for Temporal Reasoning in Multi-session Agents",
            "submittedOnDailyBy": {
                "_id": "62281c11236b7b2eefa7f198",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62281c11236b7b2eefa7f198/O-LoLaDkIoWcP19mzkNgS.jpeg",
                "isPro": true,
                "fullname": "Zhaowei Wang",
                "user": "ZhaoweiWang",
                "type": "user"
            },
            "summary": "Temporal reasoning over long, multi-session dialogues is a critical capability for conversational agents. However, existing works and our pilot study have shown that as dialogue histories grow in length and accumulate noise, current long-context models struggle to accurately identify temporally pertinent information, significantly impairing reasoning performance. To address this, we introduce Memory-T1, a framework that learns a time-aware memory selection policy using reinforcement learning (RL). It employs a coarse-to-fine strategy, first pruning the dialogue history into a candidate set using temporal and relevance filters, followed by an RL agent that selects the precise evidence sessions. The RL training is guided by a multi-level reward function optimizing (i) answer accuracy, (ii) evidence grounding, and (iii) temporal consistency. In particular, the temporal consistency reward provides a dense signal by evaluating alignment with the query time scope at both the session-level (chronological proximity) and the utterance-level (chronological fidelity), enabling the agent to resolve subtle chronological ambiguities. On the Time-Dialog benchmark, Memory-T1 boosts a 7B model to an overall score of 67.0\\%, establishing a new state-of-the-art performance for open-source models and outperforming a 14B baseline by 10.2\\%. Ablation studies show temporal consistency and evidence grounding rewards jointly contribute to a 15.0\\% performance gain. Moreover, Memory-T1 maintains robustness up to 128k tokens, where baseline models collapse, proving effectiveness against noise in extensive dialogue histories. The code and datasets are publicly available at https://github.com/Elvin-Yiming-Du/Memory-T1/",
            "upvotes": 2,
            "discussionId": "694b8232746a34b55dd53e44",
            "ai_summary": "Memory-T1, a framework using reinforcement learning, enhances temporal reasoning in long dialogues by selecting relevant sessions and ensuring temporal consistency, achieving state-of-the-art performance on the Time-Dialog benchmark.",
            "ai_keywords": [
                "reinforcement learning",
                "RL",
                "temporal reasoning",
                "dialogue history",
                "time-aware memory",
                "coarse-to-fine strategy",
                "temporal filters",
                "evidence grounding",
                "temporal consistency",
                "reward function",
                "session-level",
                "utterance-level",
                "chronological proximity",
                "chronological fidelity",
                "Time-Dialog benchmark",
                "performance gain",
                "robustness"
            ]
        },
        "publishedAt": "2025-12-23T01:37:29.000Z",
        "title": "Memory-T1: Reinforcement Learning for Temporal Reasoning in Multi-session Agents",
        "summary": "Temporal reasoning over long, multi-session dialogues is a critical capability for conversational agents. However, existing works and our pilot study have shown that as dialogue histories grow in length and accumulate noise, current long-context models struggle to accurately identify temporally pertinent information, significantly impairing reasoning performance. To address this, we introduce Memory-T1, a framework that learns a time-aware memory selection policy using reinforcement learning (RL). It employs a coarse-to-fine strategy, first pruning the dialogue history into a candidate set using temporal and relevance filters, followed by an RL agent that selects the precise evidence sessions. The RL training is guided by a multi-level reward function optimizing (i) answer accuracy, (ii) evidence grounding, and (iii) temporal consistency. In particular, the temporal consistency reward provides a dense signal by evaluating alignment with the query time scope at both the session-level (chronological proximity) and the utterance-level (chronological fidelity), enabling the agent to resolve subtle chronological ambiguities. On the Time-Dialog benchmark, Memory-T1 boosts a 7B model to an overall score of 67.0\\%, establishing a new state-of-the-art performance for open-source models and outperforming a 14B baseline by 10.2\\%. Ablation studies show temporal consistency and evidence grounding rewards jointly contribute to a 15.0\\% performance gain. Moreover, Memory-T1 maintains robustness up to 128k tokens, where baseline models collapse, proving effectiveness against noise in extensive dialogue histories. The code and datasets are publicly available at https://github.com/Elvin-Yiming-Du/Memory-T1/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20092.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "62281c11236b7b2eefa7f198",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62281c11236b7b2eefa7f198/O-LoLaDkIoWcP19mzkNgS.jpeg",
            "fullname": "Zhaowei Wang",
            "name": "ZhaoweiWang",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.15031",
            "authors": [
                {
                    "_id": "6945c9b9316ca45c457282d1",
                    "user": {
                        "_id": "6331c3f618711776b468e9ec",
                        "avatarUrl": "/avatars/af2c4bba031e474bf4fd2ea19e415aaf.svg",
                        "isPro": false,
                        "fullname": "Mia Mohammad Imran",
                        "user": "imranraad",
                        "type": "user"
                    },
                    "name": "Mia Mohammad Imran",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-22T10:59:38.597Z",
                    "hidden": false
                },
                {
                    "_id": "6945c9b9316ca45c457282d2",
                    "name": "Robert Zita",
                    "hidden": false
                },
                {
                    "_id": "6945c9b9316ca45c457282d3",
                    "name": "Rahat Rizvi Rahman",
                    "hidden": false
                },
                {
                    "_id": "6945c9b9316ca45c457282d4",
                    "name": "Preetha Chatterjee",
                    "hidden": false
                },
                {
                    "_id": "6945c9b9316ca45c457282d5",
                    "name": "Kostadin Damevski",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-17T02:45:12.000Z",
            "submittedOnDailyAt": "2025-12-24T07:26:01.776Z",
            "title": "Toxicity Ahead: Forecasting Conversational Derailment on GitHub",
            "submittedOnDailyBy": {
                "_id": "6331c3f618711776b468e9ec",
                "avatarUrl": "/avatars/af2c4bba031e474bf4fd2ea19e415aaf.svg",
                "isPro": false,
                "fullname": "Mia Mohammad Imran",
                "user": "imranraad",
                "type": "user"
            },
            "summary": "Toxic interactions in Open Source Software (OSS) communities reduce contributor engagement and threaten project sustainability. Preventing such toxicity before it emerges requires a clear understanding of how harmful conversations unfold. However, most proactive moderation strategies are manual, requiring significant time and effort from community maintainers. To support more scalable approaches, we curate a dataset of 159 derailed toxic threads and 207 non-toxic threads from GitHub discussions. Our analysis reveals that toxicity can be forecast by tension triggers, sentiment shifts, and specific conversational patterns.\n  We present a novel Large Language Model (LLM)-based framework for predicting conversational derailment on GitHub using a two-step prompting pipeline. First, we generate Summaries of Conversation Dynamics (SCDs) via Least-to-Most (LtM) prompting; then we use these summaries to estimate the likelihood of derailment. Evaluated on Qwen and Llama models, our LtM strategy achieves F1-scores of 0.901 and 0.852, respectively, at a decision threshold of 0.3, outperforming established NLP baselines on conversation derailment. External validation on a dataset of 308 GitHub issue threads (65 toxic, 243 non-toxic) yields an F1-score up to 0.797. Our findings demonstrate the effectiveness of structured LLM prompting for early detection of conversational derailment in OSS, enabling proactive and explainable moderation.",
            "upvotes": 1,
            "discussionId": "6945c9b9316ca45c457282d6",
            "ai_summary": "A novel framework using Large Language Models and a two-step prompting pipeline effectively predicts and prevents conversational derailment in open source software communities on GitHub.",
            "ai_keywords": [
                "Large Language Model",
                "LLM",
                "Summaries of Conversation Dynamics",
                "SCD",
                "Least-to-Most",
                "LtM prompting",
                "F1-scores",
                "conversation derailment",
                "proactive moderation",
                "explainable moderation"
            ]
        },
        "publishedAt": "2025-12-16T21:45:12.000Z",
        "title": "Toxicity Ahead: Forecasting Conversational Derailment on GitHub",
        "summary": "Toxic interactions in Open Source Software (OSS) communities reduce contributor engagement and threaten project sustainability. Preventing such toxicity before it emerges requires a clear understanding of how harmful conversations unfold. However, most proactive moderation strategies are manual, requiring significant time and effort from community maintainers. To support more scalable approaches, we curate a dataset of 159 derailed toxic threads and 207 non-toxic threads from GitHub discussions. Our analysis reveals that toxicity can be forecast by tension triggers, sentiment shifts, and specific conversational patterns.\n  We present a novel Large Language Model (LLM)-based framework for predicting conversational derailment on GitHub using a two-step prompting pipeline. First, we generate Summaries of Conversation Dynamics (SCDs) via Least-to-Most (LtM) prompting; then we use these summaries to estimate the likelihood of derailment. Evaluated on Qwen and Llama models, our LtM strategy achieves F1-scores of 0.901 and 0.852, respectively, at a decision threshold of 0.3, outperforming established NLP baselines on conversation derailment. External validation on a dataset of 308 GitHub issue threads (65 toxic, 243 non-toxic) yields an F1-score up to 0.797. Our findings demonstrate the effectiveness of structured LLM prompting for early detection of conversational derailment in OSS, enabling proactive and explainable moderation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.15031.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6331c3f618711776b468e9ec",
            "avatarUrl": "/avatars/af2c4bba031e474bf4fd2ea19e415aaf.svg",
            "fullname": "Mia Mohammad Imran",
            "name": "imranraad",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.19823",
            "authors": [
                {
                    "_id": "694c3517746a34b55dd53fb8",
                    "name": "SaiKiran Tedla",
                    "hidden": false
                },
                {
                    "_id": "694c3517746a34b55dd53fb9",
                    "name": "Zhoutong Zhang",
                    "hidden": false
                },
                {
                    "_id": "694c3517746a34b55dd53fba",
                    "name": "Xuaner Zhang",
                    "hidden": false
                },
                {
                    "_id": "694c3517746a34b55dd53fbb",
                    "name": "Shumian Xin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-22T19:29:57.000Z",
            "submittedOnDailyAt": "2025-12-24T16:17:17.479Z",
            "title": "Learning to Refocus with Video Diffusion Models",
            "submittedOnDailyBy": {
                "_id": "64fb54957021123688c08959",
                "avatarUrl": "/avatars/e0fe41e3701bc6eeba7429dbd05a686f.svg",
                "isPro": false,
                "fullname": "SaiKiran Tedla",
                "user": "tedlasai",
                "type": "user"
            },
            "summary": "Focus is a cornerstone of photography, yet autofocus systems often fail to capture the intended subject, and users frequently wish to adjust focus after capture. We introduce a novel method for realistic post-capture refocusing using video diffusion models. From a single defocused image, our approach generates a perceptually accurate focal stack, represented as a video sequence, enabling interactive refocusing and unlocking a range of downstream applications. We release a large-scale focal stack dataset acquired under diverse real-world smartphone conditions to support this work and future research. Our method consistently outperforms existing approaches in both perceptual quality and robustness across challenging scenarios, paving the way for more advanced focus-editing capabilities in everyday photography. Code and data are available at www.learn2refocus.github.io",
            "upvotes": 0,
            "discussionId": "694c3518746a34b55dd53fbc",
            "projectPage": "https://learn2refocus.github.io/",
            "githubRepo": "https://github.com/tedlasai/learn2refocus",
            "githubRepoAddedBy": "user",
            "githubStars": 5
        },
        "publishedAt": "2025-12-22T14:29:57.000Z",
        "title": "Learning to Refocus with Video Diffusion Models",
        "summary": "Focus is a cornerstone of photography, yet autofocus systems often fail to capture the intended subject, and users frequently wish to adjust focus after capture. We introduce a novel method for realistic post-capture refocusing using video diffusion models. From a single defocused image, our approach generates a perceptually accurate focal stack, represented as a video sequence, enabling interactive refocusing and unlocking a range of downstream applications. We release a large-scale focal stack dataset acquired under diverse real-world smartphone conditions to support this work and future research. Our method consistently outperforms existing approaches in both perceptual quality and robustness across challenging scenarios, paving the way for more advanced focus-editing capabilities in everyday photography. Code and data are available at www.learn2refocus.github.io",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.19823.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64fb54957021123688c08959",
            "avatarUrl": "/avatars/e0fe41e3701bc6eeba7429dbd05a686f.svg",
            "fullname": "SaiKiran Tedla",
            "name": "tedlasai",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    }
]