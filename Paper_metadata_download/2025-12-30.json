[
    {
        "paper": {
            "id": "2512.23447",
            "authors": [
                {
                    "_id": "69534a9589916ff627aa3f5c",
                    "name": "Ang Lv",
                    "hidden": false
                },
                {
                    "_id": "69534a9589916ff627aa3f5d",
                    "name": "Jin Ma",
                    "hidden": false
                },
                {
                    "_id": "69534a9589916ff627aa3f5e",
                    "name": "Yiyuan Ma",
                    "hidden": false
                },
                {
                    "_id": "69534a9589916ff627aa3f5f",
                    "name": "Siyuan Qiao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-29T13:03:18.000Z",
            "submittedOnDailyAt": "2025-12-30T01:18:40.635Z",
            "title": "Coupling Experts and Routers in Mixture-of-Experts via an Auxiliary Loss",
            "submittedOnDailyBy": {
                "_id": "64b8ca3c5067873176d4b436",
                "avatarUrl": "/avatars/b659d147b2454b47c9a7e89bbed525fc.svg",
                "isPro": false,
                "fullname": "AngLv",
                "user": "AngLv",
                "type": "user"
            },
            "summary": "Mixture-of-Experts (MoE) models lack explicit constraints to ensure the router's decisions align well with the experts' capabilities, which ultimately limits model performance. To address this, we propose expert-router coupling (ERC) loss, a lightweight auxiliary loss that tightly couples the router's decisions with expert capabilities. Our approach treats each expert's router embedding as a proxy token for the tokens assigned to that expert, and feeds perturbed router embeddings through the experts to obtain internal activations. The ERC loss enforces two constraints on these activations: (1) Each expert must exhibit higher activation for its own proxy token than for the proxy tokens of any other expert. (2) Each proxy token must elicit stronger activation from its corresponding expert than from any other expert. These constraints jointly ensure that each router embedding faithfully represents its corresponding expert's capability, while each expert specializes in processing the tokens actually routed to it. The ERC loss is computationally efficient, operating only on n^2 activations, where n is the number of experts. This represents a fixed cost independent of batch size, unlike prior coupling methods that scale with the number of tokens (often millions per batch). Through pre-training MoE-LLMs ranging from 3B to 15B parameters and extensive analysis on trillions of tokens, we demonstrate the effectiveness of the ERC loss. Moreover, the ERC loss offers flexible control and quantitative tracking of expert specialization levels during training, providing valuable insights into MoEs.",
            "upvotes": 71,
            "discussionId": "69534a9589916ff627aa3f60",
            "ai_summary": "An expert-router coupling (ERC) loss aligns router decisions with expert capabilities in Mixture-of-Experts (MoE) models by enforcing constraints on internal activations, improving performance and computational efficiency.",
            "ai_keywords": [
                "Mixture-of-Experts (MoE)",
                "expert-router coupling (ERC) loss",
                "router embeddings",
                "proxy tokens",
                "internal activations",
                "MoE-LLMs",
                "expert specialization levels",
                "n² activations"
            ],
            "organization": {
                "_id": "67d1140985ea0644e2f14b99",
                "name": "ByteDance-Seed",
                "fullname": "ByteDance Seed",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
            }
        },
        "publishedAt": "2025-12-29T08:03:18.000Z",
        "title": "Coupling Experts and Routers in Mixture-of-Experts via an Auxiliary Loss",
        "summary": "Mixture-of-Experts (MoE) models lack explicit constraints to ensure the router's decisions align well with the experts' capabilities, which ultimately limits model performance. To address this, we propose expert-router coupling (ERC) loss, a lightweight auxiliary loss that tightly couples the router's decisions with expert capabilities. Our approach treats each expert's router embedding as a proxy token for the tokens assigned to that expert, and feeds perturbed router embeddings through the experts to obtain internal activations. The ERC loss enforces two constraints on these activations: (1) Each expert must exhibit higher activation for its own proxy token than for the proxy tokens of any other expert. (2) Each proxy token must elicit stronger activation from its corresponding expert than from any other expert. These constraints jointly ensure that each router embedding faithfully represents its corresponding expert's capability, while each expert specializes in processing the tokens actually routed to it. The ERC loss is computationally efficient, operating only on n^2 activations, where n is the number of experts. This represents a fixed cost independent of batch size, unlike prior coupling methods that scale with the number of tokens (often millions per batch). Through pre-training MoE-LLMs ranging from 3B to 15B parameters and extensive analysis on trillions of tokens, we demonstrate the effectiveness of the ERC loss. Moreover, the ERC loss offers flexible control and quantitative tracking of expert specialization levels during training, providing valuable insights into MoEs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23447.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64b8ca3c5067873176d4b436",
            "avatarUrl": "/avatars/b659d147b2454b47c9a7e89bbed525fc.svg",
            "fullname": "AngLv",
            "name": "AngLv",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 10
        },
        "organization": {
            "_id": "67d1140985ea0644e2f14b99",
            "name": "ByteDance-Seed",
            "fullname": "ByteDance Seed",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.23576",
            "authors": [
                {
                    "_id": "69534f1e89916ff627aa3fe3",
                    "name": "Ethan Chern",
                    "hidden": false
                },
                {
                    "_id": "69534f1e89916ff627aa3fe4",
                    "name": "Zhulin Hu",
                    "hidden": false
                },
                {
                    "_id": "69534f1e89916ff627aa3fe5",
                    "name": "Bohao Tang",
                    "hidden": false
                },
                {
                    "_id": "69534f1e89916ff627aa3fe6",
                    "name": "Jiadi Su",
                    "hidden": false
                },
                {
                    "_id": "69534f1e89916ff627aa3fe7",
                    "name": "Steffi Chern",
                    "hidden": false
                },
                {
                    "_id": "69534f1e89916ff627aa3fe8",
                    "name": "Zhijie Deng",
                    "hidden": false
                },
                {
                    "_id": "69534f1e89916ff627aa3fe9",
                    "name": "Pengfei Liu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64bb5f9d8e051085bace4d1e/skNa_3Ly0Bg7F6aL0mk92.mp4"
            ],
            "publishedAt": "2025-12-29T16:17:36.000Z",
            "submittedOnDailyAt": "2025-12-30T02:36:23.479Z",
            "title": "LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation",
            "submittedOnDailyBy": {
                "_id": "64bb5f9d8e051085bace4d1e",
                "avatarUrl": "/avatars/15ccbb78c6131dfe46b7a9d8e7d1a31f.svg",
                "isPro": false,
                "fullname": "Ethan Chern",
                "user": "ethanchern",
                "type": "user"
            },
            "summary": "Real-time video generation via diffusion is essential for building general-purpose multimodal interactive AI systems. However, the simultaneous denoising of all video frames with bidirectional attention via an iterative process in diffusion models prevents real-time interaction. While existing distillation methods can make the model autoregressive and reduce sampling steps to mitigate this, they focus primarily on text-to-video generation, leaving the human-AI interaction unnatural and less efficient. This paper targets real-time interactive video diffusion conditioned on a multimodal context, including text, image, and audio, to bridge the gap. Given the observation that the leading on-policy distillation approach Self Forcing encounters challenges (visual artifacts like flickering, black frames, and quality degradation) with multimodal conditioning, we investigate an improved distillation recipe with emphasis on the quality of condition inputs as well as the initialization and schedule for the on-policy optimization. On benchmarks for multimodal-conditioned (audio, image, and text) avatar video generation including HDTF, AVSpeech, and CelebV-HQ, our distilled model matches the visual quality of the full-step, bidirectional baselines of similar or larger size with 20x less inference cost and latency. Further, we integrate our model with audio language models and long-form video inference technique Anchor-Heavy Identity Sinks to build LiveTalk, a real-time multimodal interactive avatar system. System-level evaluation on our curated multi-turn interaction benchmark shows LiveTalk outperforms state-of-the-art models (Sora2, Veo3) in multi-turn video coherence and content quality, while reducing response latency from 1 to 2 minutes to real-time generation, enabling seamless human-AI multimodal interaction.",
            "upvotes": 51,
            "discussionId": "69534f1e89916ff627aa3fea",
            "githubRepo": "https://github.com/GAIR-NLP/LiveTalk",
            "githubRepoAddedBy": "user",
            "ai_summary": "Real-time multimodal video generation via diffusion is enabled by an improved distillation approach with multimodal conditioning and optimized scheduling, reducing inference latency while maintaining quality for interactive systems.",
            "ai_keywords": [
                "diffusion models",
                "bidirectional attention",
                "distillation methods",
                "on-policy distillation",
                "Self Forcing",
                "audio language models",
                "Anchor-Heavy Identity Sinks",
                "multimodal conditioning",
                "autoregressive",
                "on-policy optimization"
            ],
            "githubStars": 81
        },
        "publishedAt": "2025-12-29T11:17:36.000Z",
        "title": "LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation",
        "summary": "Real-time video generation via diffusion is essential for building general-purpose multimodal interactive AI systems. However, the simultaneous denoising of all video frames with bidirectional attention via an iterative process in diffusion models prevents real-time interaction. While existing distillation methods can make the model autoregressive and reduce sampling steps to mitigate this, they focus primarily on text-to-video generation, leaving the human-AI interaction unnatural and less efficient. This paper targets real-time interactive video diffusion conditioned on a multimodal context, including text, image, and audio, to bridge the gap. Given the observation that the leading on-policy distillation approach Self Forcing encounters challenges (visual artifacts like flickering, black frames, and quality degradation) with multimodal conditioning, we investigate an improved distillation recipe with emphasis on the quality of condition inputs as well as the initialization and schedule for the on-policy optimization. On benchmarks for multimodal-conditioned (audio, image, and text) avatar video generation including HDTF, AVSpeech, and CelebV-HQ, our distilled model matches the visual quality of the full-step, bidirectional baselines of similar or larger size with 20x less inference cost and latency. Further, we integrate our model with audio language models and long-form video inference technique Anchor-Heavy Identity Sinks to build LiveTalk, a real-time multimodal interactive avatar system. System-level evaluation on our curated multi-turn interaction benchmark shows LiveTalk outperforms state-of-the-art models (Sora2, Veo3) in multi-turn video coherence and content quality, while reducing response latency from 1 to 2 minutes to real-time generation, enabling seamless human-AI multimodal interaction.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64bb5f9d8e051085bace4d1e/skNa_3Ly0Bg7F6aL0mk92.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23576.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64bb5f9d8e051085bace4d1e",
            "avatarUrl": "/avatars/15ccbb78c6131dfe46b7a9d8e7d1a31f.svg",
            "fullname": "Ethan Chern",
            "name": "ethanchern",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.22096",
            "authors": [
                {
                    "_id": "695206a8746a34b55dd548dd",
                    "name": "Xiaofeng Mao",
                    "hidden": false
                },
                {
                    "_id": "695206a8746a34b55dd548de",
                    "name": "Zhen Li",
                    "hidden": false
                },
                {
                    "_id": "695206a8746a34b55dd548df",
                    "name": "Chuanhao Li",
                    "hidden": false
                },
                {
                    "_id": "695206a8746a34b55dd548e0",
                    "name": "Xiaojie Xu",
                    "hidden": false
                },
                {
                    "_id": "695206a8746a34b55dd548e1",
                    "name": "Kaining Ying",
                    "hidden": false
                },
                {
                    "_id": "695206a8746a34b55dd548e2",
                    "name": "Tong He",
                    "hidden": false
                },
                {
                    "_id": "695206a8746a34b55dd548e3",
                    "name": "Jiangmiao Pang",
                    "hidden": false
                },
                {
                    "_id": "695206a8746a34b55dd548e4",
                    "name": "Yu Qiao",
                    "hidden": false
                },
                {
                    "_id": "695206a8746a34b55dd548e5",
                    "name": "Kaipeng Zhang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/65f1713552c38a91e0a445e8/NMttpBTOZdYJorkqyoD67.mp4"
            ],
            "publishedAt": "2025-12-26T17:52:49.000Z",
            "submittedOnDailyAt": "2025-12-30T01:50:23.447Z",
            "title": "Yume-1.5: A Text-Controlled Interactive World Generation Model",
            "submittedOnDailyBy": {
                "_id": "65f1713552c38a91e0a445e8",
                "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
                "isPro": false,
                "fullname": "kaipeng",
                "user": "kpzhang996",
                "type": "user"
            },
            "summary": "Recent approaches have demonstrated the promise of using diffusion models to generate interactive and explorable worlds. However, most of these methods face critical challenges such as excessively large parameter sizes, reliance on lengthy inference steps, and rapidly growing historical context, which severely limit real-time performance and lack text-controlled generation capabilities. To address these challenges, we propose \\method, a novel framework designed to generate realistic, interactive, and continuous worlds from a single image or text prompt. \\method achieves this through a carefully designed framework that supports keyboard-based exploration of the generated worlds. The framework comprises three core components: (1) a long-video generation framework integrating unified context compression with linear attention; (2) a real-time streaming acceleration strategy powered by bidirectional attention distillation and an enhanced text embedding scheme; (3) a text-controlled method for generating world events. We have provided the codebase in the supplementary material.",
            "upvotes": 50,
            "discussionId": "695206a8746a34b55dd548e6",
            "projectPage": "https://stdstu12.github.io/YUME-Project/",
            "githubRepo": "https://github.com/stdstu12/YUME",
            "githubRepoAddedBy": "user",
            "githubStars": 426
        },
        "publishedAt": "2025-12-26T12:52:49.000Z",
        "title": "Yume-1.5: A Text-Controlled Interactive World Generation Model",
        "summary": "Recent approaches have demonstrated the promise of using diffusion models to generate interactive and explorable worlds. However, most of these methods face critical challenges such as excessively large parameter sizes, reliance on lengthy inference steps, and rapidly growing historical context, which severely limit real-time performance and lack text-controlled generation capabilities. To address these challenges, we propose \\method, a novel framework designed to generate realistic, interactive, and continuous worlds from a single image or text prompt. \\method achieves this through a carefully designed framework that supports keyboard-based exploration of the generated worlds. The framework comprises three core components: (1) a long-video generation framework integrating unified context compression with linear attention; (2) a real-time streaming acceleration strategy powered by bidirectional attention distillation and an enhanced text embedding scheme; (3) a text-controlled method for generating world events. We have provided the codebase in the supplementary material.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/65f1713552c38a91e0a445e8/NMttpBTOZdYJorkqyoD67.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.22096.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65f1713552c38a91e0a445e8",
            "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
            "fullname": "kaipeng",
            "name": "kpzhang996",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.22322",
            "authors": [
                {
                    "_id": "69533fb889916ff627aa3ecb",
                    "name": "Shaofei Cai",
                    "hidden": false
                },
                {
                    "_id": "69533fb889916ff627aa3ecc",
                    "name": "Yulei Qin",
                    "hidden": false
                },
                {
                    "_id": "69533fb889916ff627aa3ecd",
                    "name": "Haojia Lin",
                    "hidden": false
                },
                {
                    "_id": "69533fb889916ff627aa3ece",
                    "name": "Zihan Xu",
                    "hidden": false
                },
                {
                    "_id": "69533fb889916ff627aa3ecf",
                    "name": "Gang Li",
                    "hidden": false
                },
                {
                    "_id": "69533fb889916ff627aa3ed0",
                    "name": "Yuchen Shi",
                    "hidden": false
                },
                {
                    "_id": "69533fb889916ff627aa3ed1",
                    "name": "Zongyi Li",
                    "hidden": false
                },
                {
                    "_id": "69533fb889916ff627aa3ed2",
                    "name": "Yong Mao",
                    "hidden": false
                },
                {
                    "_id": "69533fb889916ff627aa3ed3",
                    "name": "Siqi Cai",
                    "hidden": false
                },
                {
                    "_id": "69533fb889916ff627aa3ed4",
                    "name": "Xiaoyu Tan",
                    "hidden": false
                },
                {
                    "_id": "69533fb889916ff627aa3ed5",
                    "name": "Yitao Liang",
                    "hidden": false
                },
                {
                    "_id": "69533fb889916ff627aa3ed6",
                    "name": "Ke Li",
                    "hidden": false
                },
                {
                    "_id": "69533fb889916ff627aa3ed7",
                    "name": "Xing Sun",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6390525c00fb8ec4a424e0ff/h0k49_chVHOUTywBgnJR6.gif"
            ],
            "publishedAt": "2025-12-26T14:51:39.000Z",
            "submittedOnDailyAt": "2025-12-30T01:07:21.942Z",
            "title": "SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents",
            "submittedOnDailyBy": {
                "_id": "6390525c00fb8ec4a424e0ff",
                "avatarUrl": "/avatars/4302571e2ef4a9875491221aa630a329.svg",
                "isPro": false,
                "fullname": "Yulei Qin",
                "user": "yolay",
                "type": "user"
            },
            "summary": "Agentic reinforcement learning (RL) holds great promise for the development of autonomous agents under complex GUI tasks, but its scalability remains severely hampered by the verification of task completion. Existing task verification is treated as a passive, post-hoc process: a verifier (i.e., rule-based scoring script, reward or critic model, and LLM-as-a-Judge) analyzes the agent's entire interaction trajectory to determine if the agent succeeds. Such processing of verbose context that contains irrelevant, noisy history poses challenges to the verification protocols and therefore leads to prohibitive cost and low reliability. To overcome this bottleneck, we propose SmartSnap, a paradigm shift from this passive, post-hoc verification to proactive, in-situ self-verification by the agent itself. We introduce the Self-Verifying Agent, a new type of agent designed with dual missions: to not only complete a task but also to prove its accomplishment with curated snapshot evidences. Guided by our proposed 3C Principles (Completeness, Conciseness, and Creativity), the agent leverages its accessibility to the online environment to perform self-verification on a minimal, decisive set of snapshots. Such evidences are provided as the sole materials for a general LLM-as-a-Judge verifier to determine their validity and relevance. Experiments on mobile tasks across model families and scales demonstrate that our SmartSnap paradigm allows training LLM-driven agents in a scalable manner, bringing performance gains up to 26.08% and 16.66% respectively to 8B and 30B models. The synergizing between solution finding and evidence seeking facilitates the cultivation of efficient, self-verifying agents with competitive performance against DeepSeek V3.1 and Qwen3-235B-A22B.",
            "upvotes": 33,
            "discussionId": "69533fb889916ff627aa3ed8",
            "projectPage": "https://huggingface.co/collections/yolay/smartsnap",
            "organization": {
                "_id": "66543b6e420092799d2f625c",
                "name": "tencent",
                "fullname": "Tencent",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
            }
        },
        "publishedAt": "2025-12-26T09:51:39.000Z",
        "title": "SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents",
        "summary": "Agentic reinforcement learning (RL) holds great promise for the development of autonomous agents under complex GUI tasks, but its scalability remains severely hampered by the verification of task completion. Existing task verification is treated as a passive, post-hoc process: a verifier (i.e., rule-based scoring script, reward or critic model, and LLM-as-a-Judge) analyzes the agent's entire interaction trajectory to determine if the agent succeeds. Such processing of verbose context that contains irrelevant, noisy history poses challenges to the verification protocols and therefore leads to prohibitive cost and low reliability. To overcome this bottleneck, we propose SmartSnap, a paradigm shift from this passive, post-hoc verification to proactive, in-situ self-verification by the agent itself. We introduce the Self-Verifying Agent, a new type of agent designed with dual missions: to not only complete a task but also to prove its accomplishment with curated snapshot evidences. Guided by our proposed 3C Principles (Completeness, Conciseness, and Creativity), the agent leverages its accessibility to the online environment to perform self-verification on a minimal, decisive set of snapshots. Such evidences are provided as the sole materials for a general LLM-as-a-Judge verifier to determine their validity and relevance. Experiments on mobile tasks across model families and scales demonstrate that our SmartSnap paradigm allows training LLM-driven agents in a scalable manner, bringing performance gains up to 26.08% and 16.66% respectively to 8B and 30B models. The synergizing between solution finding and evidence seeking facilitates the cultivation of efficient, self-verifying agents with competitive performance against DeepSeek V3.1 and Qwen3-235B-A22B.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6390525c00fb8ec4a424e0ff/h0k49_chVHOUTywBgnJR6.gif"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.22322.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6390525c00fb8ec4a424e0ff",
            "avatarUrl": "/avatars/4302571e2ef4a9875491221aa630a329.svg",
            "fullname": "Yulei Qin",
            "name": "yolay",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "organization": {
            "_id": "66543b6e420092799d2f625c",
            "name": "tencent",
            "fullname": "Tencent",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.23705",
            "authors": [
                {
                    "_id": "6953546989916ff627aa4002",
                    "name": "Shaocong Xu",
                    "hidden": false
                },
                {
                    "_id": "6953546989916ff627aa4003",
                    "name": "Songlin Wei",
                    "hidden": false
                },
                {
                    "_id": "6953546989916ff627aa4004",
                    "name": "Qizhe Wei",
                    "hidden": false
                },
                {
                    "_id": "6953546989916ff627aa4005",
                    "name": "Zheng Geng",
                    "hidden": false
                },
                {
                    "_id": "6953546989916ff627aa4006",
                    "name": "Hong Li",
                    "hidden": false
                },
                {
                    "_id": "6953546989916ff627aa4007",
                    "name": "Licheng Shen",
                    "hidden": false
                },
                {
                    "_id": "6953546989916ff627aa4008",
                    "name": "Qianpu Sun",
                    "hidden": false
                },
                {
                    "_id": "6953546989916ff627aa4009",
                    "name": "Shu Han",
                    "hidden": false
                },
                {
                    "_id": "6953546989916ff627aa400a",
                    "name": "Bin Ma",
                    "hidden": false
                },
                {
                    "_id": "6953546989916ff627aa400b",
                    "name": "Bohan Li",
                    "hidden": false
                },
                {
                    "_id": "6953546989916ff627aa400c",
                    "name": "Chongjie Ye",
                    "hidden": false
                },
                {
                    "_id": "6953546989916ff627aa400d",
                    "name": "Yuhang Zheng",
                    "hidden": false
                },
                {
                    "_id": "6953546989916ff627aa400e",
                    "name": "Nan Wang",
                    "hidden": false
                },
                {
                    "_id": "6953546989916ff627aa400f",
                    "name": "Saining Zhang",
                    "hidden": false
                },
                {
                    "_id": "6953546989916ff627aa4010",
                    "name": "Hao Zhao",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/652bd2493a416e1f21beb01a/6NM5vLS_B3DlYtmX1N4A_.gif"
            ],
            "publishedAt": "2025-12-29T18:59:24.000Z",
            "submittedOnDailyAt": "2025-12-30T01:56:18.708Z",
            "title": "Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation",
            "submittedOnDailyBy": {
                "_id": "652bd2493a416e1f21beb01a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652bd2493a416e1f21beb01a/tKijq1pbjmBZuRm82dNEV.jpeg",
                "isPro": true,
                "fullname": "Shaocong.Xu",
                "user": "Daniellesry",
                "type": "user"
            },
            "summary": "Transparent objects remain notoriously hard for perception systems: refraction, reflection and transmission break the assumptions behind stereo, ToF and purely discriminative monocular depth, causing holes and temporally unstable estimates. Our key observation is that modern video diffusion models already synthesize convincing transparent phenomena, suggesting they have internalized the optical rules. We build TransPhy3D, a synthetic video corpus of transparent/reflective scenes: 11k sequences rendered with Blender/Cycles. Scenes are assembled from a curated bank of category-rich static assets and shape-rich procedural assets paired with glass/plastic/metal materials. We render RGB + depth + normals with physically based ray tracing and OptiX denoising. Starting from a large video diffusion model, we learn a video-to-video translator for depth (and normals) via lightweight LoRA adapters. During training we concatenate RGB and (noisy) depth latents in the DiT backbone and co-train on TransPhy3D and existing frame-wise synthetic datasets, yielding temporally consistent predictions for arbitrary-length input videos. The resulting model, DKT, achieves zero-shot SOTA on real and synthetic video benchmarks involving transparency: ClearPose, DREDS (CatKnown/CatNovel), and TransPhy3D-Test. It improves accuracy and temporal consistency over strong image/video baselines, and a normal variant sets the best video normal estimation results on ClearPose. A compact 1.3B version runs at ~0.17 s/frame. Integrated into a grasping stack, DKT's depth boosts success rates across translucent, reflective and diffuse surfaces, outperforming prior estimators. Together, these results support a broader claim: \"Diffusion knows transparency.\" Generative video priors can be repurposed, efficiently and label-free, into robust, temporally coherent perception for challenging real-world manipulation.",
            "upvotes": 32,
            "discussionId": "6953546a89916ff627aa4011",
            "projectPage": "https://daniellli.github.io/projects/DKT/",
            "githubRepo": "https://github.com/Daniellli/DKT",
            "githubRepoAddedBy": "user",
            "githubStars": 94,
            "organization": {
                "_id": "61be9739d2f9358e24ca0a4f",
                "name": "BAAI",
                "fullname": "Beijing Academy of Artificial Intelligence",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"
            }
        },
        "publishedAt": "2025-12-29T13:59:24.000Z",
        "title": "Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation",
        "summary": "Transparent objects remain notoriously hard for perception systems: refraction, reflection and transmission break the assumptions behind stereo, ToF and purely discriminative monocular depth, causing holes and temporally unstable estimates. Our key observation is that modern video diffusion models already synthesize convincing transparent phenomena, suggesting they have internalized the optical rules. We build TransPhy3D, a synthetic video corpus of transparent/reflective scenes: 11k sequences rendered with Blender/Cycles. Scenes are assembled from a curated bank of category-rich static assets and shape-rich procedural assets paired with glass/plastic/metal materials. We render RGB + depth + normals with physically based ray tracing and OptiX denoising. Starting from a large video diffusion model, we learn a video-to-video translator for depth (and normals) via lightweight LoRA adapters. During training we concatenate RGB and (noisy) depth latents in the DiT backbone and co-train on TransPhy3D and existing frame-wise synthetic datasets, yielding temporally consistent predictions for arbitrary-length input videos. The resulting model, DKT, achieves zero-shot SOTA on real and synthetic video benchmarks involving transparency: ClearPose, DREDS (CatKnown/CatNovel), and TransPhy3D-Test. It improves accuracy and temporal consistency over strong image/video baselines, and a normal variant sets the best video normal estimation results on ClearPose. A compact 1.3B version runs at ~0.17 s/frame. Integrated into a grasping stack, DKT's depth boosts success rates across translucent, reflective and diffuse surfaces, outperforming prior estimators. Together, these results support a broader claim: \"Diffusion knows transparency.\" Generative video priors can be repurposed, efficiently and label-free, into robust, temporally coherent perception for challenging real-world manipulation.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/652bd2493a416e1f21beb01a/6NM5vLS_B3DlYtmX1N4A_.gif"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23705.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "652bd2493a416e1f21beb01a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652bd2493a416e1f21beb01a/tKijq1pbjmBZuRm82dNEV.jpeg",
            "fullname": "Shaocong.Xu",
            "name": "Daniellesry",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "61be9739d2f9358e24ca0a4f",
            "name": "BAAI",
            "fullname": "Beijing Academy of Artificial Intelligence",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.23709",
            "authors": [
                {
                    "_id": "69537f4189916ff627aa40c0",
                    "name": "Hau-Shiang Shiu",
                    "hidden": false
                },
                {
                    "_id": "69537f4189916ff627aa40c1",
                    "name": "Chin-Yang Lin",
                    "hidden": false
                },
                {
                    "_id": "69537f4189916ff627aa40c2",
                    "name": "Zhixiang Wang",
                    "hidden": false
                },
                {
                    "_id": "69537f4189916ff627aa40c3",
                    "name": "Chi-Wei Hsiao",
                    "hidden": false
                },
                {
                    "_id": "69537f4189916ff627aa40c4",
                    "name": "Po-Fan Yu",
                    "hidden": false
                },
                {
                    "_id": "69537f4189916ff627aa40c5",
                    "name": "Yu-Chih Chen",
                    "hidden": false
                },
                {
                    "_id": "69537f4189916ff627aa40c6",
                    "name": "Yu-Lun Liu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/4Urho_F4h3YB3NjJxTgCc.mp4"
            ],
            "publishedAt": "2025-12-29T18:59:57.000Z",
            "submittedOnDailyAt": "2025-12-30T05:04:09.292Z",
            "title": "Stream-DiffVSR: Low-Latency Streamable Video Super-Resolution via Auto-Regressive Diffusion",
            "submittedOnDailyBy": {
                "_id": "6459d5da3b6fafd9664807ab",
                "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
                "isPro": false,
                "fullname": "Yu-Lun Liu",
                "user": "yulunliu",
                "type": "user"
            },
            "summary": "Diffusion-based video super-resolution (VSR) methods achieve strong perceptual quality but remain impractical for latency-sensitive settings due to reliance on future frames and expensive multi-step denoising. We propose Stream-DiffVSR, a causally conditioned diffusion framework for efficient online VSR. Operating strictly on past frames, it combines a four-step distilled denoiser for fast inference, an Auto-regressive Temporal Guidance (ARTG) module that injects motion-aligned cues during latent denoising, and a lightweight temporal-aware decoder with a Temporal Processor Module (TPM) that enhances detail and temporal coherence. Stream-DiffVSR processes 720p frames in 0.328 seconds on an RTX4090 GPU and significantly outperforms prior diffusion-based methods. Compared with the online SOTA TMP, it boosts perceptual quality (LPIPS +0.095) while reducing latency by over 130x. Stream-DiffVSR achieves the lowest latency reported for diffusion-based VSR, reducing initial delay from over 4600 seconds to 0.328 seconds, thereby making it the first diffusion VSR method suitable for low-latency online deployment. Project page: https://jamichss.github.io/stream-diffvsr-project-page/",
            "upvotes": 29,
            "discussionId": "69537f4289916ff627aa40c7",
            "projectPage": "https://jamichss.github.io/stream-diffvsr-project-page/"
        },
        "publishedAt": "2025-12-29T13:59:57.000Z",
        "title": "Stream-DiffVSR: Low-Latency Streamable Video Super-Resolution via Auto-Regressive Diffusion",
        "summary": "Diffusion-based video super-resolution (VSR) methods achieve strong perceptual quality but remain impractical for latency-sensitive settings due to reliance on future frames and expensive multi-step denoising. We propose Stream-DiffVSR, a causally conditioned diffusion framework for efficient online VSR. Operating strictly on past frames, it combines a four-step distilled denoiser for fast inference, an Auto-regressive Temporal Guidance (ARTG) module that injects motion-aligned cues during latent denoising, and a lightweight temporal-aware decoder with a Temporal Processor Module (TPM) that enhances detail and temporal coherence. Stream-DiffVSR processes 720p frames in 0.328 seconds on an RTX4090 GPU and significantly outperforms prior diffusion-based methods. Compared with the online SOTA TMP, it boosts perceptual quality (LPIPS +0.095) while reducing latency by over 130x. Stream-DiffVSR achieves the lowest latency reported for diffusion-based VSR, reducing initial delay from over 4600 seconds to 0.328 seconds, thereby making it the first diffusion VSR method suitable for low-latency online deployment. Project page: https://jamichss.github.io/stream-diffvsr-project-page/",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/4Urho_F4h3YB3NjJxTgCc.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23709.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6459d5da3b6fafd9664807ab",
            "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
            "fullname": "Yu-Lun Liu",
            "name": "yulunliu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.22615",
            "authors": [
                {
                    "_id": "6953489889916ff627aa3f25",
                    "name": "Jiacheng Ye",
                    "hidden": false
                },
                {
                    "_id": "6953489889916ff627aa3f26",
                    "name": "Shansan Gong",
                    "hidden": false
                },
                {
                    "_id": "6953489889916ff627aa3f27",
                    "name": "Jiahui Gao",
                    "hidden": false
                },
                {
                    "_id": "6953489889916ff627aa3f28",
                    "name": "Junming Fan",
                    "hidden": false
                },
                {
                    "_id": "6953489889916ff627aa3f29",
                    "name": "Shuang Wu",
                    "hidden": false
                },
                {
                    "_id": "6953489889916ff627aa3f2a",
                    "name": "Wei Bi",
                    "hidden": false
                },
                {
                    "_id": "6953489889916ff627aa3f2b",
                    "name": "Haoli Bai",
                    "hidden": false
                },
                {
                    "_id": "6953489889916ff627aa3f2c",
                    "name": "Lifeng Shang",
                    "hidden": false
                },
                {
                    "_id": "6953489889916ff627aa3f2d",
                    "name": "Lingpeng Kong",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-27T14:46:24.000Z",
            "submittedOnDailyAt": "2025-12-30T03:42:33.237Z",
            "title": "Dream-VL & Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone",
            "submittedOnDailyBy": {
                "_id": "628c83d186fc004b14e1ed48",
                "avatarUrl": "/avatars/05ff943a9b89b5f67c5bc254bf45b8f5.svg",
                "isPro": false,
                "fullname": "Shansan Gong",
                "user": "Sansa",
                "type": "user"
            },
            "summary": "While autoregressive Large Vision-Language Models (VLMs) have achieved remarkable success, their sequential generation often limits their efficacy in complex visual planning and dynamic robotic control. In this work, we investigate the potential of constructing Vision-Language Models upon diffusion-based large language models (dLLMs) to overcome these limitations. We introduce Dream-VL, an open diffusion-based VLM (dVLM) that achieves state-of-the-art performance among previous dVLMs. Dream-VL is comparable to top-tier AR-based VLMs trained on open data on various benchmarks but exhibits superior potential when applied to visual planning tasks. Building upon Dream-VL, we introduce Dream-VLA, a dLLM-based Vision-Language-Action model (dVLA) developed through continuous pre-training on open robotic datasets. We demonstrate that the natively bidirectional nature of this diffusion backbone serves as a superior foundation for VLA tasks, inherently suited for action chunking and parallel generation, leading to significantly faster convergence in downstream fine-tuning. Dream-VLA achieves top-tier performance of 97.2% average success rate on LIBERO, 71.4% overall average on SimplerEnv-Bridge, and 60.5% overall average on SimplerEnv-Fractal, surpassing leading models such as π_0 and GR00T-N1. We also validate that dVLMs surpass AR baselines on downstream tasks across different training objectives. We release both Dream-VL and Dream-VLA to facilitate further research in the community.",
            "upvotes": 27,
            "discussionId": "6953489889916ff627aa3f2e",
            "projectPage": "https://hkunlp.github.io/blog/2025/dream-vlx/",
            "githubRepo": "https://github.com/DreamLM/Dream-VLX",
            "githubRepoAddedBy": "user",
            "ai_summary": "Diffusion-based vision-language models and action frameworks demonstrate superior performance in visual planning and robotic control tasks compared to autoregressive baselines.",
            "ai_keywords": [
                "diffusion-based large language models (dLLMs)",
                "Vision-Language Models (VLMs)",
                "Dream-VL",
                "Vision-Language-Action model (dVLA)",
                "Dream-VLA",
                "action chunking",
                "parallel generation",
                "LIBERO",
                "SimplerEnv-Bridge",
                "SimplerEnv-Fractal",
                "continuous pre-training"
            ],
            "githubStars": 41,
            "organization": {
                "_id": "67ea9ecfc234715db8dbf339",
                "name": "hkuhk",
                "fullname": "The University of Hong Kong",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67ea9e8d2d95c10a0da11b0c/FNnR4M7YqKRuG43N5771B.png"
            }
        },
        "publishedAt": "2025-12-27T09:46:24.000Z",
        "title": "Dream-VL & Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone",
        "summary": "While autoregressive Large Vision-Language Models (VLMs) have achieved remarkable success, their sequential generation often limits their efficacy in complex visual planning and dynamic robotic control. In this work, we investigate the potential of constructing Vision-Language Models upon diffusion-based large language models (dLLMs) to overcome these limitations. We introduce Dream-VL, an open diffusion-based VLM (dVLM) that achieves state-of-the-art performance among previous dVLMs. Dream-VL is comparable to top-tier AR-based VLMs trained on open data on various benchmarks but exhibits superior potential when applied to visual planning tasks. Building upon Dream-VL, we introduce Dream-VLA, a dLLM-based Vision-Language-Action model (dVLA) developed through continuous pre-training on open robotic datasets. We demonstrate that the natively bidirectional nature of this diffusion backbone serves as a superior foundation for VLA tasks, inherently suited for action chunking and parallel generation, leading to significantly faster convergence in downstream fine-tuning. Dream-VLA achieves top-tier performance of 97.2% average success rate on LIBERO, 71.4% overall average on SimplerEnv-Bridge, and 60.5% overall average on SimplerEnv-Fractal, surpassing leading models such as π_0 and GR00T-N1. We also validate that dVLMs surpass AR baselines on downstream tasks across different training objectives. We release both Dream-VL and Dream-VLA to facilitate further research in the community.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.22615.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "628c83d186fc004b14e1ed48",
            "avatarUrl": "/avatars/05ff943a9b89b5f67c5bc254bf45b8f5.svg",
            "fullname": "Shansan Gong",
            "name": "Sansa",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "organization": {
            "_id": "67ea9ecfc234715db8dbf339",
            "name": "hkuhk",
            "fullname": "The University of Hong Kong",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67ea9e8d2d95c10a0da11b0c/FNnR4M7YqKRuG43N5771B.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.22323",
            "authors": [
                {
                    "_id": "6953692989916ff627aa4065",
                    "name": "Zhibin Qin",
                    "hidden": false
                },
                {
                    "_id": "6953692989916ff627aa4066",
                    "name": "Zhenxiong Tan",
                    "hidden": false
                },
                {
                    "_id": "6953692989916ff627aa4067",
                    "name": "Zeqing Wang",
                    "hidden": false
                },
                {
                    "_id": "6953692989916ff627aa4068",
                    "name": "Songhua Liu",
                    "hidden": false
                },
                {
                    "_id": "6953692989916ff627aa4069",
                    "name": "Xinchao Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-26T14:59:41.000Z",
            "submittedOnDailyAt": "2025-12-30T03:43:24.884Z",
            "title": "SpotEdit: Selective Region Editing in Diffusion Transformers",
            "submittedOnDailyBy": {
                "_id": "640ebdfefdeaae139086f4d8",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640ebdfefdeaae139086f4d8/2N94gbHubplYD8njmUTPf.jpeg",
                "isPro": true,
                "fullname": "Zhenxiong Tan",
                "user": "Yuanshi",
                "type": "user"
            },
            "summary": "Diffusion Transformer models have significantly advanced image editing by encoding conditional images and integrating them into transformer layers. However, most edits involve modifying only small regions, while current methods uniformly process and denoise all tokens at every timestep, causing redundant computation and potentially degrading unchanged areas. This raises a fundamental question: Is it truly necessary to regenerate every region during editing? To address this, we propose SpotEdit, a training-free diffusion editing framework that selectively updates only the modified regions. SpotEdit comprises two key components: SpotSelector identifies stable regions via perceptual similarity and skips their computation by reusing conditional image features; SpotFusion adaptively blends these features with edited tokens through a dynamic fusion mechanism, preserving contextual coherence and editing quality. By reducing unnecessary computation and maintaining high fidelity in unmodified areas, SpotEdit achieves efficient and precise image editing.",
            "upvotes": 27,
            "discussionId": "6953692989916ff627aa406a",
            "projectPage": "https://biangbiang0321.github.io/SpotEdit.github.io",
            "githubRepo": "https://github.com/Biangbiang0321/SpotEdit",
            "githubRepoAddedBy": "user",
            "githubStars": 48,
            "organization": {
                "_id": "6508ab2b349930913196378b",
                "name": "NationalUniversityofSingapore",
                "fullname": "National University of Singapore",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZYUmpSMsa5Whihw3me2Bw.png"
            }
        },
        "publishedAt": "2025-12-26T09:59:41.000Z",
        "title": "SpotEdit: Selective Region Editing in Diffusion Transformers",
        "summary": "Diffusion Transformer models have significantly advanced image editing by encoding conditional images and integrating them into transformer layers. However, most edits involve modifying only small regions, while current methods uniformly process and denoise all tokens at every timestep, causing redundant computation and potentially degrading unchanged areas. This raises a fundamental question: Is it truly necessary to regenerate every region during editing? To address this, we propose SpotEdit, a training-free diffusion editing framework that selectively updates only the modified regions. SpotEdit comprises two key components: SpotSelector identifies stable regions via perceptual similarity and skips their computation by reusing conditional image features; SpotFusion adaptively blends these features with edited tokens through a dynamic fusion mechanism, preserving contextual coherence and editing quality. By reducing unnecessary computation and maintaining high fidelity in unmodified areas, SpotEdit achieves efficient and precise image editing.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.22323.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "640ebdfefdeaae139086f4d8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640ebdfefdeaae139086f4d8/2N94gbHubplYD8njmUTPf.jpeg",
            "fullname": "Zhenxiong Tan",
            "name": "Yuanshi",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 170
        },
        "organization": {
            "_id": "6508ab2b349930913196378b",
            "name": "NationalUniversityofSingapore",
            "fullname": "National University of Singapore",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZYUmpSMsa5Whihw3me2Bw.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.15560",
            "authors": [
                {
                    "_id": "6953703689916ff627aa407c",
                    "name": "Bozhou Li",
                    "hidden": false
                },
                {
                    "_id": "6953703689916ff627aa407d",
                    "name": "Sihan Yang",
                    "hidden": false
                },
                {
                    "_id": "6953703689916ff627aa407e",
                    "name": "Yushuo Guan",
                    "hidden": false
                },
                {
                    "_id": "6953703689916ff627aa407f",
                    "name": "Ruichuan An",
                    "hidden": false
                },
                {
                    "_id": "6953703689916ff627aa4080",
                    "name": "Xinlong Chen",
                    "hidden": false
                },
                {
                    "_id": "6953703689916ff627aa4081",
                    "name": "Yang Shi",
                    "hidden": false
                },
                {
                    "_id": "6953703689916ff627aa4082",
                    "name": "Pengfei Wan",
                    "hidden": false
                },
                {
                    "_id": "6953703689916ff627aa4083",
                    "name": "Wentao Zhang",
                    "hidden": false
                },
                {
                    "_id": "6953703689916ff627aa4084",
                    "name": "Yuanxing zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-17T16:09:43.000Z",
            "submittedOnDailyAt": "2025-12-30T04:19:17.454Z",
            "title": "GRAN-TED: Generating Robust, Aligned, and Nuanced Text Embedding for Diffusion Models",
            "submittedOnDailyBy": {
                "_id": "661e62c6bac5d981f886f77b",
                "avatarUrl": "/avatars/f1eb51ed4499ca434c8939573dfbd5e2.svg",
                "isPro": false,
                "fullname": "Bozhou Li",
                "user": "zooblastlbz",
                "type": "user"
            },
            "summary": "The text encoder is a critical component of text-to-image and text-to-video diffusion models, fundamentally determining the semantic fidelity of the generated content. However, its development has been hindered by two major challenges: the lack of an efficient evaluation framework that reliably predicts downstream generation performance, and the difficulty of effectively adapting pretrained language models for visual synthesis. To address these issues, we introduce GRAN-TED, a paradigm to Generate Robust, Aligned, and Nuanced Text Embeddings for Diffusion models. Our contribution is twofold. First, we propose TED-6K, a novel text-only benchmark that enables efficient and robust assessment of an encoder's representational quality without requiring costly end-to-end model training. We demonstrate that performance on TED-6K, standardized via a lightweight, unified adapter, strongly correlates with an encoder's effectiveness in downstream generation tasks. Notably, under our experimental setup, compared with training a diffusion model from scratch, evaluating with TED-6K is about 750times faster. Second, guided by this validated framework, we develop a superior text encoder using a novel two-stage training paradigm. This process involves an initial fine-tuning stage on a Multimodal Large Language Model for better visual representation, followed by a layer-wise weighting method to extract more nuanced and potent text features. Our experiments show that the resulting GRAN-TED encoder not only achieves state-of-the-art performance on TED-6K but also leads to demonstrable performance gains in text-to-image and text-to-video generation. Our TED-6K dataset and evaluation code are available at the following link: https://anonymous.4open.science/r/GRAN-TED-4FCC/.",
            "upvotes": 21,
            "discussionId": "6953703689916ff627aa4085",
            "organization": {
                "_id": "662c559b322afcbae51b3c8b",
                "name": "KlingTeam",
                "fullname": "Kling Team",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"
            }
        },
        "publishedAt": "2025-12-17T11:09:43.000Z",
        "title": "GRAN-TED: Generating Robust, Aligned, and Nuanced Text Embedding for Diffusion Models",
        "summary": "The text encoder is a critical component of text-to-image and text-to-video diffusion models, fundamentally determining the semantic fidelity of the generated content. However, its development has been hindered by two major challenges: the lack of an efficient evaluation framework that reliably predicts downstream generation performance, and the difficulty of effectively adapting pretrained language models for visual synthesis. To address these issues, we introduce GRAN-TED, a paradigm to Generate Robust, Aligned, and Nuanced Text Embeddings for Diffusion models. Our contribution is twofold. First, we propose TED-6K, a novel text-only benchmark that enables efficient and robust assessment of an encoder's representational quality without requiring costly end-to-end model training. We demonstrate that performance on TED-6K, standardized via a lightweight, unified adapter, strongly correlates with an encoder's effectiveness in downstream generation tasks. Notably, under our experimental setup, compared with training a diffusion model from scratch, evaluating with TED-6K is about 750times faster. Second, guided by this validated framework, we develop a superior text encoder using a novel two-stage training paradigm. This process involves an initial fine-tuning stage on a Multimodal Large Language Model for better visual representation, followed by a layer-wise weighting method to extract more nuanced and potent text features. Our experiments show that the resulting GRAN-TED encoder not only achieves state-of-the-art performance on TED-6K but also leads to demonstrable performance gains in text-to-image and text-to-video generation. Our TED-6K dataset and evaluation code are available at the following link: https://anonymous.4open.science/r/GRAN-TED-4FCC/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.15560.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "661e62c6bac5d981f886f77b",
            "avatarUrl": "/avatars/f1eb51ed4499ca434c8939573dfbd5e2.svg",
            "fullname": "Bozhou Li",
            "name": "zooblastlbz",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "662c559b322afcbae51b3c8b",
            "name": "KlingTeam",
            "fullname": "Kling Team",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.23541",
            "authors": [
                {
                    "_id": "69534cef89916ff627aa3f77",
                    "name": "Pengfei Zhou",
                    "hidden": false
                },
                {
                    "_id": "69534cef89916ff627aa3f78",
                    "name": "Liliang Chen",
                    "hidden": false
                },
                {
                    "_id": "69534cef89916ff627aa3f79",
                    "name": "Shengcong Chen",
                    "hidden": false
                },
                {
                    "_id": "69534cef89916ff627aa3f7a",
                    "name": "Di Chen",
                    "hidden": false
                },
                {
                    "_id": "69534cef89916ff627aa3f7b",
                    "name": "Wenzhi Zhao",
                    "hidden": false
                },
                {
                    "_id": "69534cef89916ff627aa3f7c",
                    "name": "Rongjun Jin",
                    "hidden": false
                },
                {
                    "_id": "69534cef89916ff627aa3f7d",
                    "name": "Guanghui Ren",
                    "hidden": false
                },
                {
                    "_id": "69534cef89916ff627aa3f7e",
                    "name": "Jianlan Luo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-29T15:28:42.000Z",
            "submittedOnDailyAt": "2025-12-30T05:09:42.689Z",
            "title": "Act2Goal: From World Model To General Goal-conditioned Policy",
            "submittedOnDailyBy": {
                "_id": "646ec9b135f55eb49e405faa",
                "avatarUrl": "/avatars/a17194be585d20e2a021e77a5a20e213.svg",
                "isPro": false,
                "fullname": "Guanghui Ren",
                "user": "sundrops",
                "type": "user"
            },
            "summary": "Specifying robotic manipulation tasks in a manner that is both expressive and precise remains a central challenge. While visual goals provide a compact and unambiguous task specification, existing goal-conditioned policies often struggle with long-horizon manipulation due to their reliance on single-step action prediction without explicit modeling of task progress. We propose Act2Goal, a general goal-conditioned manipulation policy that integrates a goal-conditioned visual world model with multi-scale temporal control. Given a current observation and a target visual goal, the world model generates a plausible sequence of intermediate visual states that captures long-horizon structure. To translate this visual plan into robust execution, we introduce Multi-Scale Temporal Hashing (MSTH), which decomposes the imagined trajectory into dense proximal frames for fine-grained closed-loop control and sparse distal frames that anchor global task consistency. The policy couples these representations with motor control through end-to-end cross-attention, enabling coherent long-horizon behavior while remaining reactive to local disturbances. Act2Goal achieves strong zero-shot generalization to novel objects, spatial layouts, and environments. We further enable reward-free online adaptation through hindsight goal relabeling with LoRA-based finetuning, allowing rapid autonomous improvement without external supervision. Real-robot experiments demonstrate that Act2Goal improves success rates from 30% to 90% on challenging out-of-distribution tasks within minutes of autonomous interaction, validating that goal-conditioned world models with multi-scale temporal control provide structured guidance necessary for robust long-horizon manipulation. Project page: https://act2goal.github.io/",
            "upvotes": 19,
            "discussionId": "69534cf089916ff627aa3f7f",
            "projectPage": "https://act2goal.github.io/",
            "ai_summary": "Act2Goal employs a goal-conditioned visual world model with multi-scale temporal control and cross-attention to achieve robust long-horizon robotic manipulation through structured planning and adaptive execution.",
            "ai_keywords": [
                "goal-conditioned visual world model",
                "Multi-Scale Temporal Hashing (MSTH)",
                "cross-attention",
                "end-to-end cross-attention",
                "LoRA-based finetuning",
                "hindsight goal relabeling"
            ],
            "organization": {
                "_id": "676fc7c31c48eff17fac3135",
                "name": "agibot-world",
                "fullname": "AgiBot World",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64e57309b78bc92221ce3b70/ewI1QvFVMDgSsShQeSvlX.png"
            }
        },
        "publishedAt": "2025-12-29T10:28:42.000Z",
        "title": "Act2Goal: From World Model To General Goal-conditioned Policy",
        "summary": "Specifying robotic manipulation tasks in a manner that is both expressive and precise remains a central challenge. While visual goals provide a compact and unambiguous task specification, existing goal-conditioned policies often struggle with long-horizon manipulation due to their reliance on single-step action prediction without explicit modeling of task progress. We propose Act2Goal, a general goal-conditioned manipulation policy that integrates a goal-conditioned visual world model with multi-scale temporal control. Given a current observation and a target visual goal, the world model generates a plausible sequence of intermediate visual states that captures long-horizon structure. To translate this visual plan into robust execution, we introduce Multi-Scale Temporal Hashing (MSTH), which decomposes the imagined trajectory into dense proximal frames for fine-grained closed-loop control and sparse distal frames that anchor global task consistency. The policy couples these representations with motor control through end-to-end cross-attention, enabling coherent long-horizon behavior while remaining reactive to local disturbances. Act2Goal achieves strong zero-shot generalization to novel objects, spatial layouts, and environments. We further enable reward-free online adaptation through hindsight goal relabeling with LoRA-based finetuning, allowing rapid autonomous improvement without external supervision. Real-robot experiments demonstrate that Act2Goal improves success rates from 30% to 90% on challenging out-of-distribution tasks within minutes of autonomous interaction, validating that goal-conditioned world models with multi-scale temporal control provide structured guidance necessary for robust long-horizon manipulation. Project page: https://act2goal.github.io/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23541.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "646ec9b135f55eb49e405faa",
            "avatarUrl": "/avatars/a17194be585d20e2a021e77a5a20e213.svg",
            "fullname": "Guanghui Ren",
            "name": "sundrops",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "organization": {
            "_id": "676fc7c31c48eff17fac3135",
            "name": "agibot-world",
            "fullname": "AgiBot World",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64e57309b78bc92221ce3b70/ewI1QvFVMDgSsShQeSvlX.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.23676",
            "authors": [
                {
                    "_id": "695344be89916ff627aa3ee9",
                    "name": "Jichen Feng",
                    "hidden": false
                },
                {
                    "_id": "695344be89916ff627aa3eea",
                    "name": "Yifan Zhang",
                    "hidden": false
                },
                {
                    "_id": "695344be89916ff627aa3eeb",
                    "name": "Chenggong Zhang",
                    "hidden": false
                },
                {
                    "_id": "695344be89916ff627aa3eec",
                    "name": "Yifu Lu",
                    "hidden": false
                },
                {
                    "_id": "695344be89916ff627aa3eed",
                    "name": "Shilong Liu",
                    "hidden": false
                },
                {
                    "_id": "695344be89916ff627aa3eee",
                    "name": "Mengdi Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-29T18:31:45.000Z",
            "submittedOnDailyAt": "2025-12-30T01:11:00.554Z",
            "title": "Web World Models",
            "submittedOnDailyBy": {
                "_id": "647bf082aba7062fe5c51ca9",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647bf082aba7062fe5c51ca9/VvKAhQC_LxBcBuy3XROSX.jpeg",
                "isPro": false,
                "fullname": "Yifan Zhang",
                "user": "yifAI",
                "type": "user"
            },
            "summary": "Language agents increasingly require persistent worlds in which they can act, remember, and learn. Existing approaches sit at two extremes: conventional web frameworks provide reliable but fixed contexts backed by databases, while fully generative world models aim for unlimited environments at the expense of controllability and practical engineering. In this work, we introduce the Web World Model (WWM), a middle ground where world state and ``physics'' are implemented in ordinary web code to ensure logical consistency, while large language models generate context, narratives, and high-level decisions on top of this structured latent state. We build a suite of WWMs on a realistic web stack, including an infinite travel atlas grounded in real geography, fictional galaxy explorers, web-scale encyclopedic and narrative worlds, and simulation- and game-like environments. Across these systems, we identify practical design principles for WWMs: separating code-defined rules from model-driven imagination, representing latent state as typed web interfaces, and utilizing deterministic generation to achieve unlimited but structured exploration. Our results suggest that web stacks themselves can serve as a scalable substrate for world models, enabling controllable yet open-ended environments. Project Page: https://github.com/Princeton-AI2-Lab/Web-World-Models.",
            "upvotes": 16,
            "discussionId": "695344be89916ff627aa3eef",
            "projectPage": "https://github.com/Princeton-AI2-Lab/Web-World-Models",
            "githubRepo": "https://github.com/Princeton-AI2-Lab/Web-World-Models",
            "githubRepoAddedBy": "user",
            "ai_summary": "Web World Models (WWMs) combine web frameworks with large language models to create controllable, open-ended persistent environments by structuring world state in web code and leveraging model-driven imagination for narratives and decisions.",
            "ai_keywords": [
                "large language models",
                "latent state",
                "web-based world models"
            ],
            "githubStars": 17,
            "organization": {
                "_id": "69081a9c8b3b900d6e63602f",
                "name": "princeton-ai",
                "fullname": "Princeton AI Lab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/647bf082aba7062fe5c51ca9/Xh9rZKOFsWasVQXJwjmVt.jpeg"
            }
        },
        "publishedAt": "2025-12-29T13:31:45.000Z",
        "title": "Web World Models",
        "summary": "Language agents increasingly require persistent worlds in which they can act, remember, and learn. Existing approaches sit at two extremes: conventional web frameworks provide reliable but fixed contexts backed by databases, while fully generative world models aim for unlimited environments at the expense of controllability and practical engineering. In this work, we introduce the Web World Model (WWM), a middle ground where world state and ``physics'' are implemented in ordinary web code to ensure logical consistency, while large language models generate context, narratives, and high-level decisions on top of this structured latent state. We build a suite of WWMs on a realistic web stack, including an infinite travel atlas grounded in real geography, fictional galaxy explorers, web-scale encyclopedic and narrative worlds, and simulation- and game-like environments. Across these systems, we identify practical design principles for WWMs: separating code-defined rules from model-driven imagination, representing latent state as typed web interfaces, and utilizing deterministic generation to achieve unlimited but structured exploration. Our results suggest that web stacks themselves can serve as a scalable substrate for world models, enabling controllable yet open-ended environments. Project Page: https://github.com/Princeton-AI2-Lab/Web-World-Models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23676.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "647bf082aba7062fe5c51ca9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647bf082aba7062fe5c51ca9/VvKAhQC_LxBcBuy3XROSX.jpeg",
            "fullname": "Yifan Zhang",
            "name": "yifAI",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 17
        },
        "organization": {
            "_id": "69081a9c8b3b900d6e63602f",
            "name": "princeton-ai",
            "fullname": "Princeton AI Lab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/647bf082aba7062fe5c51ca9/Xh9rZKOFsWasVQXJwjmVt.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.22234",
            "authors": [
                {
                    "_id": "695364c389916ff627aa404f",
                    "name": "Ying Zhu",
                    "hidden": false
                },
                {
                    "_id": "695364c389916ff627aa4050",
                    "name": "Jiaxin Wan",
                    "hidden": false
                },
                {
                    "_id": "695364c389916ff627aa4051",
                    "name": "Xiaoran Liu",
                    "hidden": false
                },
                {
                    "_id": "695364c389916ff627aa4052",
                    "name": "Siyanag He",
                    "hidden": false
                },
                {
                    "_id": "695364c389916ff627aa4053",
                    "name": "Qiqi Wang",
                    "hidden": false
                },
                {
                    "_id": "695364c389916ff627aa4054",
                    "name": "Xu Guo",
                    "hidden": false
                },
                {
                    "_id": "695364c389916ff627aa4055",
                    "name": "Tianyi Liang",
                    "hidden": false
                },
                {
                    "_id": "695364c389916ff627aa4056",
                    "name": "Zengfeng Huang",
                    "hidden": false
                },
                {
                    "_id": "695364c389916ff627aa4057",
                    "name": "Ziwei He",
                    "hidden": false
                },
                {
                    "_id": "695364c389916ff627aa4058",
                    "name": "Xipeng Qiu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-23T08:33:19.000Z",
            "submittedOnDailyAt": "2025-12-30T03:37:55.619Z",
            "title": "DiRL: An Efficient Post-Training Framework for Diffusion Language Models",
            "submittedOnDailyBy": {
                "_id": "64f033ef82c6eea604c4da8b",
                "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg",
                "isPro": false,
                "fullname": "Xiaoran Liu (SII)",
                "user": "SII-xrliu",
                "type": "user"
            },
            "summary": "Diffusion Language Models (dLLMs) have emerged as promising alternatives to Auto-Regressive (AR) models. While recent efforts have validated their pre-training potential and accelerated inference speeds, the post-training landscape for dLLMs remains underdeveloped. Existing methods suffer from computational inefficiency and objective mismatches between training and inference, severely limiting performance on complex reasoning tasks such as mathematics. To address this, we introduce DiRL, an efficient post-training framework that tightly integrates FlexAttention-accelerated blockwise training with LMDeploy-optimized inference. This architecture enables a streamlined online model update loop, facilitating efficient two-stage post-training (Supervised Fine-Tuning followed by Reinforcement Learning). Building on this framework, we propose DiPO, the first unbiased Group Relative Policy Optimization (GRPO) implementation tailored for dLLMs. We validate our approach by training DiRL-8B-Instruct on high-quality math data. Our model achieves state-of-the-art math performance among dLLMs and surpasses comparable models in the Qwen2.5 series on several benchmarks.",
            "upvotes": 16,
            "discussionId": "695364c389916ff627aa4059",
            "githubRepo": "https://github.com/OpenMOSS/DiRL",
            "githubRepoAddedBy": "user",
            "githubStars": 113,
            "organization": {
                "_id": "613b0dee83ec35d460684607",
                "name": "OpenMOSS-Team",
                "fullname": "OpenMOSS",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"
            }
        },
        "publishedAt": "2025-12-23T03:33:19.000Z",
        "title": "DiRL: An Efficient Post-Training Framework for Diffusion Language Models",
        "summary": "Diffusion Language Models (dLLMs) have emerged as promising alternatives to Auto-Regressive (AR) models. While recent efforts have validated their pre-training potential and accelerated inference speeds, the post-training landscape for dLLMs remains underdeveloped. Existing methods suffer from computational inefficiency and objective mismatches between training and inference, severely limiting performance on complex reasoning tasks such as mathematics. To address this, we introduce DiRL, an efficient post-training framework that tightly integrates FlexAttention-accelerated blockwise training with LMDeploy-optimized inference. This architecture enables a streamlined online model update loop, facilitating efficient two-stage post-training (Supervised Fine-Tuning followed by Reinforcement Learning). Building on this framework, we propose DiPO, the first unbiased Group Relative Policy Optimization (GRPO) implementation tailored for dLLMs. We validate our approach by training DiRL-8B-Instruct on high-quality math data. Our model achieves state-of-the-art math performance among dLLMs and surpasses comparable models in the Qwen2.5 series on several benchmarks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.22234.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64f033ef82c6eea604c4da8b",
            "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg",
            "fullname": "Xiaoran Liu (SII)",
            "name": "SII-xrliu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 12
        },
        "organization": {
            "_id": "613b0dee83ec35d460684607",
            "name": "OpenMOSS-Team",
            "fullname": "OpenMOSS",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.23707",
            "authors": [
                {
                    "_id": "6953527889916ff627aa3ff0",
                    "name": "Shashwat Goel",
                    "hidden": false
                },
                {
                    "_id": "6953527889916ff627aa3ff1",
                    "name": "Rishi Hazra",
                    "hidden": false
                },
                {
                    "_id": "6953527889916ff627aa3ff2",
                    "name": "Dulhan Jayalath",
                    "hidden": false
                },
                {
                    "_id": "6953527889916ff627aa3ff3",
                    "name": "Timon Willi",
                    "hidden": false
                },
                {
                    "_id": "6953527889916ff627aa3ff4",
                    "name": "Parag Jain",
                    "hidden": false
                },
                {
                    "_id": "6953527889916ff627aa3ff5",
                    "name": "William F. Shen",
                    "hidden": false
                },
                {
                    "_id": "6953527889916ff627aa3ff6",
                    "name": "Ilias Leontiadis",
                    "hidden": false
                },
                {
                    "_id": "6953527889916ff627aa3ff7",
                    "name": "Francesco Barbieri",
                    "hidden": false
                },
                {
                    "_id": "6953527889916ff627aa3ff8",
                    "name": "Yoram Bachrach",
                    "hidden": false
                },
                {
                    "_id": "6953527889916ff627aa3ff9",
                    "name": "Jonas Geiping",
                    "hidden": false
                },
                {
                    "_id": "6953527889916ff627aa3ffa",
                    "name": "Chenxi Whitehouse",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-29T18:59:33.000Z",
            "submittedOnDailyAt": "2025-12-30T01:50:32.619Z",
            "title": "Training AI Co-Scientists Using Rubric Rewards",
            "submittedOnDailyBy": {
                "_id": "6506832221ac448013f94995",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6506832221ac448013f94995/sVUI1JV4Dxan5l-MqNze4.jpeg",
                "isPro": false,
                "fullname": "Shashwat Goel",
                "user": "shash42",
                "type": "user"
            },
            "summary": "AI co-scientists are emerging as a tool to assist human researchers in achieving their research goals. A crucial feature of these AI co-scientists is the ability to generate a research plan given a set of aims and constraints. The plan may be used by researchers for brainstorming, or may even be implemented after further refinement. However, language models currently struggle to generate research plans that follow all constraints and implicit requirements. In this work, we study how to leverage the vast corpus of existing research papers to train language models that generate better research plans. We build a scalable, diverse training corpus by automatically extracting research goals and goal-specific grading rubrics from papers across several domains. We then train models for research plan generation via reinforcement learning with self-grading. A frozen copy of the initial policy acts as the grader during training, with the rubrics creating a generator-verifier gap that enables improvements without external human supervision. To validate this approach, we conduct a study with human experts for machine learning research goals, spanning 225 hours. The experts prefer plans generated by our finetuned Qwen3-30B-A3B model over the initial model for 70% of research goals, and approve 84% of the automatically extracted goal-specific grading rubrics. To assess generality, we also extend our approach to research goals from medical papers, and new arXiv preprints, evaluating with a jury of frontier models. Our finetuning yields 12-22% relative improvements and significant cross-domain generalization, proving effective even in problem settings like medical research where execution feedback is infeasible. Together, these findings demonstrate the potential of a scalable, automated training recipe as a step towards improving general AI co-scientists.",
            "upvotes": 12,
            "discussionId": "6953527989916ff627aa3ffb",
            "organization": {
                "_id": "5e63d8713071d5be688861b8",
                "name": "facebook",
                "fullname": "AI at Meta",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
            }
        },
        "publishedAt": "2025-12-29T13:59:33.000Z",
        "title": "Training AI Co-Scientists Using Rubric Rewards",
        "summary": "AI co-scientists are emerging as a tool to assist human researchers in achieving their research goals. A crucial feature of these AI co-scientists is the ability to generate a research plan given a set of aims and constraints. The plan may be used by researchers for brainstorming, or may even be implemented after further refinement. However, language models currently struggle to generate research plans that follow all constraints and implicit requirements. In this work, we study how to leverage the vast corpus of existing research papers to train language models that generate better research plans. We build a scalable, diverse training corpus by automatically extracting research goals and goal-specific grading rubrics from papers across several domains. We then train models for research plan generation via reinforcement learning with self-grading. A frozen copy of the initial policy acts as the grader during training, with the rubrics creating a generator-verifier gap that enables improvements without external human supervision. To validate this approach, we conduct a study with human experts for machine learning research goals, spanning 225 hours. The experts prefer plans generated by our finetuned Qwen3-30B-A3B model over the initial model for 70% of research goals, and approve 84% of the automatically extracted goal-specific grading rubrics. To assess generality, we also extend our approach to research goals from medical papers, and new arXiv preprints, evaluating with a jury of frontier models. Our finetuning yields 12-22% relative improvements and significant cross-domain generalization, proving effective even in problem settings like medical research where execution feedback is infeasible. Together, these findings demonstrate the potential of a scalable, automated training recipe as a step towards improving general AI co-scientists.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23707.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6506832221ac448013f94995",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6506832221ac448013f94995/sVUI1JV4Dxan5l-MqNze4.jpeg",
            "fullname": "Shashwat Goel",
            "name": "shash42",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "organization": {
            "_id": "5e63d8713071d5be688861b8",
            "name": "facebook",
            "fullname": "AI at Meta",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.23044",
            "authors": [
                {
                    "_id": "695354a689916ff627aa4013",
                    "name": "Zhengyang Liang",
                    "hidden": false
                },
                {
                    "_id": "695354a689916ff627aa4014",
                    "name": "Yan Shu",
                    "hidden": false
                },
                {
                    "_id": "695354a689916ff627aa4015",
                    "name": "Xiangrui Liu",
                    "hidden": false
                },
                {
                    "_id": "695354a689916ff627aa4016",
                    "name": "Minghao Qin",
                    "hidden": false
                },
                {
                    "_id": "695354a689916ff627aa4017",
                    "name": "Kaixin Liang",
                    "hidden": false
                },
                {
                    "_id": "695354a689916ff627aa4018",
                    "name": "Paolo Rota",
                    "hidden": false
                },
                {
                    "_id": "695354a689916ff627aa4019",
                    "name": "Nicu Sebe",
                    "hidden": false
                },
                {
                    "_id": "695354a689916ff627aa401a",
                    "name": "Zheng Liu",
                    "hidden": false
                },
                {
                    "_id": "695354a689916ff627aa401b",
                    "name": "Lizi Liao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-28T19:08:27.000Z",
            "submittedOnDailyAt": "2025-12-30T01:57:28.016Z",
            "title": "Video-BrowseComp: Benchmarking Agentic Video Research on Open Web",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "The evolution of autonomous agents is redefining information seeking, transitioning from passive retrieval to proactive, open-ended web research. However, while textual and static multimodal agents have seen rapid progress, a significant modality gap remains in processing the web's most dynamic modality: video. Existing video benchmarks predominantly focus on passive perception, feeding curated clips to models without requiring external retrieval. They fail to evaluate agentic video research, which necessitates actively interrogating video timelines, cross-referencing dispersed evidence, and verifying claims against the open web. To bridge this gap, we present Video-BrowseComp, a challenging benchmark comprising 210 questions tailored for open-web agentic video reasoning. Unlike prior benchmarks, Video-BrowseComp enforces a mandatory dependency on temporal visual evidence, ensuring that answers cannot be derived solely through text search but require navigating video timelines to verify external claims. Our evaluation of state-of-the-art models reveals a critical bottleneck: even advanced search-augmented models like GPT-5.1 (w/ Search) achieve only 15.24\\% accuracy. Our analysis reveals that these models largely rely on textual proxies, excelling in metadata-rich domains (e.g., TV shows with plot summaries) but collapsing in metadata-sparse, dynamic environments (e.g., sports, gameplay) where visual grounding is essential. As the first open-web video research benchmark, Video-BrowseComp advances the field beyond passive perception toward proactive video reasoning.",
            "upvotes": 9,
            "discussionId": "695354a789916ff627aa401c",
            "projectPage": "https://liang-zhengyang.github.io/video-browsecomp/",
            "ai_summary": "The paper addresses the modality gap in autonomous agents for video processing by introducing a benchmark requiring proactive, open-web video reasoning, revealing limitations of current models in metadata-sparse, dynamic video domains.",
            "ai_keywords": [
                "autonomous agents",
                "temporal visual evidence",
                "Video-BrowseComp",
                "search-augmented models",
                "metadata-rich domains",
                "metadata-sparse environments",
                "visual grounding",
                "proactive video reasoning",
                "passive perception",
                "GPT-5.1 (w/ Search)"
            ]
        },
        "publishedAt": "2025-12-28T14:08:27.000Z",
        "title": "Video-BrowseComp: Benchmarking Agentic Video Research on Open Web",
        "summary": "The evolution of autonomous agents is redefining information seeking, transitioning from passive retrieval to proactive, open-ended web research. However, while textual and static multimodal agents have seen rapid progress, a significant modality gap remains in processing the web's most dynamic modality: video. Existing video benchmarks predominantly focus on passive perception, feeding curated clips to models without requiring external retrieval. They fail to evaluate agentic video research, which necessitates actively interrogating video timelines, cross-referencing dispersed evidence, and verifying claims against the open web. To bridge this gap, we present Video-BrowseComp, a challenging benchmark comprising 210 questions tailored for open-web agentic video reasoning. Unlike prior benchmarks, Video-BrowseComp enforces a mandatory dependency on temporal visual evidence, ensuring that answers cannot be derived solely through text search but require navigating video timelines to verify external claims. Our evaluation of state-of-the-art models reveals a critical bottleneck: even advanced search-augmented models like GPT-5.1 (w/ Search) achieve only 15.24\\% accuracy. Our analysis reveals that these models largely rely on textual proxies, excelling in metadata-rich domains (e.g., TV shows with plot summaries) but collapsing in metadata-sparse, dynamic environments (e.g., sports, gameplay) where visual grounding is essential. As the first open-web video research benchmark, Video-BrowseComp advances the field beyond passive perception toward proactive video reasoning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23044.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 197
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.23646",
            "authors": [
                {
                    "_id": "695349a689916ff627aa3f47",
                    "name": "Keda Tao",
                    "hidden": false
                },
                {
                    "_id": "695349a689916ff627aa3f48",
                    "name": "Wenjie Du",
                    "hidden": false
                },
                {
                    "_id": "695349a689916ff627aa3f49",
                    "name": "Bohan Yu",
                    "hidden": false
                },
                {
                    "_id": "695349a689916ff627aa3f4a",
                    "name": "Weiqiang Wang",
                    "hidden": false
                },
                {
                    "_id": "695349a689916ff627aa3f4b",
                    "name": "Jian Liu",
                    "hidden": false
                },
                {
                    "_id": "695349a689916ff627aa3f4c",
                    "name": "Huan Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-29T17:59:05.000Z",
            "submittedOnDailyAt": "2025-12-30T01:10:55.248Z",
            "title": "OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding",
            "submittedOnDailyBy": {
                "_id": "65ce28c6340c3e914285aa58",
                "avatarUrl": "/avatars/ffaa6d6ce92274bff960f8ea229a37f8.svg",
                "isPro": false,
                "fullname": "Keda TAO",
                "user": "KD-TAO",
                "type": "user"
            },
            "summary": "Omnimodal large language models have made significant strides in unifying audio and visual modalities; however, they often lack the fine-grained cross-modal understanding and have difficulty with multimodal alignment. To address these limitations, we introduce OmniAgent, a fully audio-guided active perception agent that dynamically orchestrates specialized tools to achieve more fine-grained audio-visual reasoning. Unlike previous works that rely on rigid, static workflows and dense frame-captioning, this paper demonstrates a paradigm shift from passive response generation to active multimodal inquiry. OmniAgent employs dynamic planning to autonomously orchestrate tool invocation on demand, strategically concentrating perceptual attention on task-relevant cues. Central to our approach is a novel coarse-to-fine audio-guided perception paradigm, which leverages audio cues to localize temporal events and guide subsequent reasoning. Extensive empirical evaluations on three audio-video understanding benchmarks demonstrate that OmniAgent achieves state-of-the-art performance, surpassing leading open-source and proprietary models by substantial margins of 10% - 20% accuracy.",
            "upvotes": 8,
            "discussionId": "695349a689916ff627aa3f4d",
            "projectPage": "https://kd-tao.github.io/OmniAgent/"
        },
        "publishedAt": "2025-12-29T12:59:05.000Z",
        "title": "OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding",
        "summary": "Omnimodal large language models have made significant strides in unifying audio and visual modalities; however, they often lack the fine-grained cross-modal understanding and have difficulty with multimodal alignment. To address these limitations, we introduce OmniAgent, a fully audio-guided active perception agent that dynamically orchestrates specialized tools to achieve more fine-grained audio-visual reasoning. Unlike previous works that rely on rigid, static workflows and dense frame-captioning, this paper demonstrates a paradigm shift from passive response generation to active multimodal inquiry. OmniAgent employs dynamic planning to autonomously orchestrate tool invocation on demand, strategically concentrating perceptual attention on task-relevant cues. Central to our approach is a novel coarse-to-fine audio-guided perception paradigm, which leverages audio cues to localize temporal events and guide subsequent reasoning. Extensive empirical evaluations on three audio-video understanding benchmarks demonstrate that OmniAgent achieves state-of-the-art performance, surpassing leading open-source and proprietary models by substantial margins of 10% - 20% accuracy.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23646.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65ce28c6340c3e914285aa58",
            "avatarUrl": "/avatars/ffaa6d6ce92274bff960f8ea229a37f8.svg",
            "fullname": "Keda TAO",
            "name": "KD-TAO",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.23273",
            "authors": [
                {
                    "_id": "69537e0089916ff627aa40b9",
                    "name": "Xu Lin",
                    "hidden": false
                },
                {
                    "_id": "69537e0089916ff627aa40ba",
                    "name": "Jinlong Peng",
                    "hidden": false
                },
                {
                    "_id": "69537e0089916ff627aa40bb",
                    "name": "Zhenye Gan",
                    "hidden": false
                },
                {
                    "_id": "69537e0089916ff627aa40bc",
                    "name": "Jiawen Zhu",
                    "hidden": false
                },
                {
                    "_id": "69537e0089916ff627aa40bd",
                    "name": "Jun Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-29T07:54:49.000Z",
            "submittedOnDailyAt": "2025-12-30T08:51:07.828Z",
            "title": "YOLO-Master: MOE-Accelerated with Specialized Transformers for Enhanced Real-time Detection",
            "submittedOnDailyBy": {
                "_id": "64faed2e5ca946a010857aec",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64faed2e5ca946a010857aec/eR8Hx0Dyy-DrPd1rmww8_.png",
                "isPro": false,
                "fullname": "Xu Lin",
                "user": "gatilin",
                "type": "user"
            },
            "summary": "Existing Real-Time Object Detection (RTOD) methods commonly adopt YOLO-like architectures for their favorable trade-off between accuracy and speed. However, these models rely on static dense computation that applies uniform processing to all inputs, misallocating representational capacity and computational resources such as over-allocating on trivial scenes while under-serving complex ones. This mismatch results in both computational redundancy and suboptimal detection performance. To overcome this limitation, we propose YOLO-Master, a novel YOLO-like framework that introduces instance-conditional adaptive computation for RTOD. This is achieved through a Efficient Sparse Mixture-of-Experts (ES-MoE) block that dynamically allocates computational resources to each input according to its scene complexity. At its core, a lightweight dynamic routing network guides expert specialization during training through a diversity enhancing objective, encouraging complementary expertise among experts. Additionally, the routing network adaptively learns to activate only the most relevant experts, thereby improving detection performance while minimizing computational overhead during inference. Comprehensive experiments on five large-scale benchmarks demonstrate the superiority of YOLO-Master. On MS COCO, our model achieves 42.4% AP with 1.62ms latency, outperforming YOLOv13-N by +0.8% mAP and 17.8% faster inference. Notably, the gains are most pronounced on challenging dense scenes, while the model preserves efficiency on typical inputs and maintains real-time inference speed. Code will be available.",
            "upvotes": 8,
            "discussionId": "69537e0089916ff627aa40be",
            "organization": {
                "_id": "66543b6e420092799d2f625c",
                "name": "tencent",
                "fullname": "Tencent",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
            }
        },
        "publishedAt": "2025-12-29T02:54:49.000Z",
        "title": "YOLO-Master: MOE-Accelerated with Specialized Transformers for Enhanced Real-time Detection",
        "summary": "Existing Real-Time Object Detection (RTOD) methods commonly adopt YOLO-like architectures for their favorable trade-off between accuracy and speed. However, these models rely on static dense computation that applies uniform processing to all inputs, misallocating representational capacity and computational resources such as over-allocating on trivial scenes while under-serving complex ones. This mismatch results in both computational redundancy and suboptimal detection performance. To overcome this limitation, we propose YOLO-Master, a novel YOLO-like framework that introduces instance-conditional adaptive computation for RTOD. This is achieved through a Efficient Sparse Mixture-of-Experts (ES-MoE) block that dynamically allocates computational resources to each input according to its scene complexity. At its core, a lightweight dynamic routing network guides expert specialization during training through a diversity enhancing objective, encouraging complementary expertise among experts. Additionally, the routing network adaptively learns to activate only the most relevant experts, thereby improving detection performance while minimizing computational overhead during inference. Comprehensive experiments on five large-scale benchmarks demonstrate the superiority of YOLO-Master. On MS COCO, our model achieves 42.4% AP with 1.62ms latency, outperforming YOLOv13-N by +0.8% mAP and 17.8% faster inference. Notably, the gains are most pronounced on challenging dense scenes, while the model preserves efficiency on typical inputs and maintains real-time inference speed. Code will be available.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23273.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64faed2e5ca946a010857aec",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64faed2e5ca946a010857aec/eR8Hx0Dyy-DrPd1rmww8_.png",
            "fullname": "Xu Lin",
            "name": "gatilin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 12
        },
        "organization": {
            "_id": "66543b6e420092799d2f625c",
            "name": "tencent",
            "fullname": "Tencent",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.22342",
            "authors": [
                {
                    "_id": "695384ce89916ff627aa40f3",
                    "name": "Wensi Huang",
                    "hidden": false
                },
                {
                    "_id": "695384ce89916ff627aa40f4",
                    "name": "Shaohao Zhu",
                    "hidden": false
                },
                {
                    "_id": "695384ce89916ff627aa40f5",
                    "name": "Meng Wei",
                    "hidden": false
                },
                {
                    "_id": "695384ce89916ff627aa40f6",
                    "name": "Jinming Xu",
                    "hidden": false
                },
                {
                    "_id": "695384ce89916ff627aa40f7",
                    "name": "Xihui Liu",
                    "hidden": false
                },
                {
                    "_id": "695384ce89916ff627aa40f8",
                    "name": "Hanqing Wang",
                    "hidden": false
                },
                {
                    "_id": "695384ce89916ff627aa40f9",
                    "name": "Tai Wang",
                    "hidden": false
                },
                {
                    "_id": "695384ce89916ff627aa40fa",
                    "name": "Feng Zhao",
                    "hidden": false
                },
                {
                    "_id": "695384ce89916ff627aa40fb",
                    "name": "Jiangmiao Pang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64e6d9d229a548f66aff6e5b/iNUGm-VHM_GpWJKR5w9pn.mp4"
            ],
            "publishedAt": "2025-12-26T19:00:12.000Z",
            "submittedOnDailyAt": "2025-12-30T06:59:25.447Z",
            "title": "VL-LN Bench: Towards Long-horizon Goal-oriented Navigation with Active Dialogs",
            "submittedOnDailyBy": {
                "_id": "64e6d9d229a548f66aff6e5b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6d9d229a548f66aff6e5b/yQ9E2TyzM4CfSjMPigcey.jpeg",
                "isPro": false,
                "fullname": "Tai Wang",
                "user": "taiwang",
                "type": "user"
            },
            "summary": "In most existing embodied navigation tasks, instructions are well-defined and unambiguous, such as instruction following and object searching. Under this idealized setting, agents are required solely to produce effective navigation outputs conditioned on vision and language inputs. However, real-world navigation instructions are often vague and ambiguous, requiring the agent to resolve uncertainty and infer user intent through active dialog. To address this gap, we propose Interactive Instance Object Navigation (IION), a task that requires agents not only to generate navigation actions but also to produce language outputs via active dialog, thereby aligning more closely with practical settings. IION extends Instance Object Navigation (ION) by allowing agents to freely consult an oracle in natural language while navigating. Building on this task, we present the Vision Language-Language Navigation (VL-LN) benchmark, which provides a large-scale, automatically generated dataset and a comprehensive evaluation protocol for training and assessing dialog-enabled navigation models. VL-LN comprises over 41k long-horizon dialog-augmented trajectories for training and an automatic evaluation protocol with an oracle capable of responding to agent queries. Using this benchmark, we train a navigation model equipped with dialog capabilities and show that it achieves significant improvements over the baselines. Extensive experiments and analyses further demonstrate the effectiveness and reliability of VL-LN for advancing research on dialog-enabled embodied navigation. Code and dataset: https://0309hws.github.io/VL-LN.github.io/",
            "upvotes": 8,
            "discussionId": "695384ce89916ff627aa40fc",
            "ai_summary": "A new benchmark for dialog-enabled navigation tasks introduces interactive learning to resolve ambiguous instructions through active dialog, enhancing real-world applicability of embodied agents.",
            "ai_keywords": [
                "Interactive Instance Object Navigation (IION)",
                "Instance Object Navigation (ION)",
                "Vision Language-Language Navigation (VL-LN)",
                "dialog-enabled navigation models",
                "dialog-augmented trajectories",
                "oracle"
            ]
        },
        "publishedAt": "2025-12-26T14:00:12.000Z",
        "title": "VL-LN Bench: Towards Long-horizon Goal-oriented Navigation with Active Dialogs",
        "summary": "In most existing embodied navigation tasks, instructions are well-defined and unambiguous, such as instruction following and object searching. Under this idealized setting, agents are required solely to produce effective navigation outputs conditioned on vision and language inputs. However, real-world navigation instructions are often vague and ambiguous, requiring the agent to resolve uncertainty and infer user intent through active dialog. To address this gap, we propose Interactive Instance Object Navigation (IION), a task that requires agents not only to generate navigation actions but also to produce language outputs via active dialog, thereby aligning more closely with practical settings. IION extends Instance Object Navigation (ION) by allowing agents to freely consult an oracle in natural language while navigating. Building on this task, we present the Vision Language-Language Navigation (VL-LN) benchmark, which provides a large-scale, automatically generated dataset and a comprehensive evaluation protocol for training and assessing dialog-enabled navigation models. VL-LN comprises over 41k long-horizon dialog-augmented trajectories for training and an automatic evaluation protocol with an oracle capable of responding to agent queries. Using this benchmark, we train a navigation model equipped with dialog capabilities and show that it achieves significant improvements over the baselines. Extensive experiments and analyses further demonstrate the effectiveness and reliability of VL-LN for advancing research on dialog-enabled embodied navigation. Code and dataset: https://0309hws.github.io/VL-LN.github.io/",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64e6d9d229a548f66aff6e5b/iNUGm-VHM_GpWJKR5w9pn.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.22342.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64e6d9d229a548f66aff6e5b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6d9d229a548f66aff6e5b/yQ9E2TyzM4CfSjMPigcey.jpeg",
            "fullname": "Tai Wang",
            "name": "taiwang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.23647",
            "authors": [
                {
                    "_id": "69534a6d89916ff627aa3f4f",
                    "name": "Baixuan Li",
                    "hidden": false
                },
                {
                    "_id": "69534a6d89916ff627aa3f50",
                    "name": "Jialong Wu",
                    "hidden": false
                },
                {
                    "_id": "69534a6d89916ff627aa3f51",
                    "name": "Wenbiao Yin",
                    "hidden": false
                },
                {
                    "_id": "69534a6d89916ff627aa3f52",
                    "name": "Kuan Li",
                    "hidden": false
                },
                {
                    "_id": "69534a6d89916ff627aa3f53",
                    "name": "Zhongwang Zhang",
                    "hidden": false
                },
                {
                    "_id": "69534a6d89916ff627aa3f54",
                    "name": "Huifeng Yin",
                    "hidden": false
                },
                {
                    "_id": "69534a6d89916ff627aa3f55",
                    "name": "Zhengwei Tao",
                    "hidden": false
                },
                {
                    "_id": "69534a6d89916ff627aa3f56",
                    "name": "Liwen Zhang",
                    "hidden": false
                },
                {
                    "_id": "69534a6d89916ff627aa3f57",
                    "name": "Pengjun Xie",
                    "hidden": false
                },
                {
                    "_id": "69534a6d89916ff627aa3f58",
                    "name": "Jingren Zhou",
                    "hidden": false
                },
                {
                    "_id": "69534a6d89916ff627aa3f59",
                    "name": "Yong Jiang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-29T17:59:14.000Z",
            "submittedOnDailyAt": "2025-12-30T01:15:07.579Z",
            "title": "Nested Browser-Use Learning for Agentic Information Seeking",
            "submittedOnDailyBy": {
                "_id": "644a4fbc2166258fccc664bc",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
                "isPro": false,
                "fullname": "Jialong Wu",
                "user": "callanwu",
                "type": "user"
            },
            "summary": "Information-seeking (IS) agents have achieved strong performance across a range of wide and deep search tasks, yet their tool use remains largely restricted to API-level snippet retrieval and URL-based page fetching, limiting access to the richer information available through real browsing. While full browser interaction could unlock deeper capabilities, its fine-grained control and verbose page content returns introduce substantial complexity for ReAct-style function-calling agents. To bridge this gap, we propose Nested Browser-Use Learning (NestBrowse), which introduces a minimal and complete browser-action framework that decouples interaction control from page exploration through a nested structure. This design simplifies agentic reasoning while enabling effective deep-web information acquisition. Empirical results on challenging deep IS benchmarks demonstrate that NestBrowse offers clear benefits in practice. Further in-depth analyses underscore its efficiency and flexibility.",
            "upvotes": 7,
            "discussionId": "69534a6e89916ff627aa3f5a",
            "organization": {
                "_id": "67d15cca6e2cf0e062dbfb54",
                "name": "AlibabaTongyiLab",
                "fullname": "TongyiLab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
            }
        },
        "publishedAt": "2025-12-29T12:59:14.000Z",
        "title": "Nested Browser-Use Learning for Agentic Information Seeking",
        "summary": "Information-seeking (IS) agents have achieved strong performance across a range of wide and deep search tasks, yet their tool use remains largely restricted to API-level snippet retrieval and URL-based page fetching, limiting access to the richer information available through real browsing. While full browser interaction could unlock deeper capabilities, its fine-grained control and verbose page content returns introduce substantial complexity for ReAct-style function-calling agents. To bridge this gap, we propose Nested Browser-Use Learning (NestBrowse), which introduces a minimal and complete browser-action framework that decouples interaction control from page exploration through a nested structure. This design simplifies agentic reasoning while enabling effective deep-web information acquisition. Empirical results on challenging deep IS benchmarks demonstrate that NestBrowse offers clear benefits in practice. Further in-depth analyses underscore its efficiency and flexibility.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23647.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "644a4fbc2166258fccc664bc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
            "fullname": "Jialong Wu",
            "name": "callanwu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 31
        },
        "organization": {
            "_id": "67d15cca6e2cf0e062dbfb54",
            "name": "AlibabaTongyiLab",
            "fullname": "TongyiLab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.23162",
            "authors": [
                {
                    "_id": "6953568c89916ff627aa401e",
                    "name": "Yufan He",
                    "hidden": false
                },
                {
                    "_id": "6953568c89916ff627aa401f",
                    "name": "Pengfei Guo",
                    "hidden": false
                },
                {
                    "_id": "6953568c89916ff627aa4020",
                    "name": "Mengya Xu",
                    "hidden": false
                },
                {
                    "_id": "6953568c89916ff627aa4021",
                    "name": "Zhaoshuo Li",
                    "hidden": false
                },
                {
                    "_id": "6953568c89916ff627aa4022",
                    "name": "Andriy Myronenko",
                    "hidden": false
                },
                {
                    "_id": "6953568c89916ff627aa4023",
                    "name": "Dillan Imans",
                    "hidden": false
                },
                {
                    "_id": "6953568c89916ff627aa4024",
                    "name": "Bingjie Liu",
                    "hidden": false
                },
                {
                    "_id": "6953568c89916ff627aa4025",
                    "name": "Dongren Yang",
                    "hidden": false
                },
                {
                    "_id": "6953568c89916ff627aa4026",
                    "name": "Mingxue Gu",
                    "hidden": false
                },
                {
                    "_id": "6953568c89916ff627aa4027",
                    "name": "Yongnan Ji",
                    "hidden": false
                },
                {
                    "_id": "6953568c89916ff627aa4028",
                    "name": "Yueming Jin",
                    "hidden": false
                },
                {
                    "_id": "6953568c89916ff627aa4029",
                    "name": "Ren Zhao",
                    "hidden": false
                },
                {
                    "_id": "6953568c89916ff627aa402a",
                    "name": "Baiyong Shen",
                    "hidden": false
                },
                {
                    "_id": "6953568c89916ff627aa402b",
                    "name": "Daguang Xu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-29T03:03:00.000Z",
            "submittedOnDailyAt": "2025-12-30T02:05:36.073Z",
            "title": "SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Data scarcity remains a fundamental barrier to achieving fully autonomous surgical robots. While large scale vision language action (VLA) models have shown impressive generalization in household and industrial manipulation by leveraging paired video action data from diverse domains, surgical robotics suffers from the paucity of datasets that include both visual observations and accurate robot kinematics. In contrast, vast corpora of surgical videos exist, but they lack corresponding action labels, preventing direct application of imitation learning or VLA training. In this work, we aim to alleviate this problem by learning policy models from SurgWorld, a world model designed for surgical physical AI. We curated the Surgical Action Text Alignment (SATA) dataset with detailed action description specifically for surgical robots. Then we built SurgeWorld based on the most advanced physical AI world model and SATA. It's able to generate diverse, generalizable and realistic surgery videos. We are also the first to use an inverse dynamics model to infer pseudokinematics from synthetic surgical videos, producing synthetic paired video action data. We demonstrate that a surgical VLA policy trained with these augmented data significantly outperforms models trained only on real demonstrations on a real surgical robot platform. Our approach offers a scalable path toward autonomous surgical skill acquisition by leveraging the abundance of unlabeled surgical video and generative world modeling, thus opening the door to generalizable and data efficient surgical robot policies.",
            "upvotes": 7,
            "discussionId": "6953568d89916ff627aa402c",
            "organization": {
                "_id": "60262b67268c201cdc8b7d43",
                "name": "nvidia",
                "fullname": "NVIDIA",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
            }
        },
        "publishedAt": "2025-12-28T22:03:00.000Z",
        "title": "SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling",
        "summary": "Data scarcity remains a fundamental barrier to achieving fully autonomous surgical robots. While large scale vision language action (VLA) models have shown impressive generalization in household and industrial manipulation by leveraging paired video action data from diverse domains, surgical robotics suffers from the paucity of datasets that include both visual observations and accurate robot kinematics. In contrast, vast corpora of surgical videos exist, but they lack corresponding action labels, preventing direct application of imitation learning or VLA training. In this work, we aim to alleviate this problem by learning policy models from SurgWorld, a world model designed for surgical physical AI. We curated the Surgical Action Text Alignment (SATA) dataset with detailed action description specifically for surgical robots. Then we built SurgeWorld based on the most advanced physical AI world model and SATA. It's able to generate diverse, generalizable and realistic surgery videos. We are also the first to use an inverse dynamics model to infer pseudokinematics from synthetic surgical videos, producing synthetic paired video action data. We demonstrate that a surgical VLA policy trained with these augmented data significantly outperforms models trained only on real demonstrations on a real surgical robot platform. Our approach offers a scalable path toward autonomous surgical skill acquisition by leveraging the abundance of unlabeled surgical video and generative world modeling, thus opening the door to generalizable and data efficient surgical robot policies.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23162.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 197
        },
        "organization": {
            "_id": "60262b67268c201cdc8b7d43",
            "name": "nvidia",
            "fullname": "NVIDIA",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.22431",
            "authors": [
                {
                    "_id": "695350e289916ff627aa3fec",
                    "name": "Yifan Zhang",
                    "hidden": false
                },
                {
                    "_id": "695350e289916ff627aa3fed",
                    "name": "Mengdi Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-27T01:52:06.000Z",
            "submittedOnDailyAt": "2025-12-30T01:41:17.578Z",
            "title": "Monadic Context Engineering",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "The proliferation of Large Language Models (LLMs) has catalyzed a shift towards autonomous agents capable of complex reasoning and tool use. However, current agent architectures are frequently constructed using imperative, ad hoc patterns. This results in brittle systems plagued by difficulties in state management, error handling, and concurrency. This paper introduces Monadic Context Engineering (MCE), a novel architectural paradigm leveraging the algebraic structures of Functors, Applicative Functors, and Monads to provide a formal foundation for agent design. MCE treats agent workflows as computational contexts where cross-cutting concerns, such as state propagation, short-circuiting error handling, and asynchronous execution, are managed intrinsically by the algebraic properties of the abstraction. We demonstrate how Monads enable robust sequential composition, how Applicatives provide a principled structure for parallel execution, and crucially, how Monad Transformers allow for the systematic composition of these capabilities. This layered approach enables developers to construct complex, resilient, and efficient AI agents from simple, independently verifiable components. We further extend this framework to describe Meta-Agents, which leverage MCE for generative orchestration, dynamically creating and managing sub-agent workflows through metaprogramming. Project Page: https://github.com/yifanzhang-pro/monadic-context-engineering.",
            "upvotes": 7,
            "discussionId": "695350e289916ff627aa3fee",
            "projectPage": "https://yifanzhang-pro.github.io/monadic-context-engineering/",
            "githubRepo": "https://github.com/yifanzhang-pro/monadic-context-engineering",
            "githubRepoAddedBy": "user",
            "githubStars": 6,
            "organization": {
                "_id": "64374111a701a7e744c02b0e",
                "name": "princetonu",
                "fullname": "Princeton University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/b3xXusq8Zz3ej8Z6fRTSZ.png"
            }
        },
        "publishedAt": "2025-12-26T20:52:06.000Z",
        "title": "Monadic Context Engineering",
        "summary": "The proliferation of Large Language Models (LLMs) has catalyzed a shift towards autonomous agents capable of complex reasoning and tool use. However, current agent architectures are frequently constructed using imperative, ad hoc patterns. This results in brittle systems plagued by difficulties in state management, error handling, and concurrency. This paper introduces Monadic Context Engineering (MCE), a novel architectural paradigm leveraging the algebraic structures of Functors, Applicative Functors, and Monads to provide a formal foundation for agent design. MCE treats agent workflows as computational contexts where cross-cutting concerns, such as state propagation, short-circuiting error handling, and asynchronous execution, are managed intrinsically by the algebraic properties of the abstraction. We demonstrate how Monads enable robust sequential composition, how Applicatives provide a principled structure for parallel execution, and crucially, how Monad Transformers allow for the systematic composition of these capabilities. This layered approach enables developers to construct complex, resilient, and efficient AI agents from simple, independently verifiable components. We further extend this framework to describe Meta-Agents, which leverage MCE for generative orchestration, dynamically creating and managing sub-agent workflows through metaprogramming. Project Page: https://github.com/yifanzhang-pro/monadic-context-engineering.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.22431.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 197
        },
        "organization": {
            "_id": "64374111a701a7e744c02b0e",
            "name": "princetonu",
            "fullname": "Princeton University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/b3xXusq8Zz3ej8Z6fRTSZ.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.21720",
            "authors": [
                {
                    "_id": "69534ed489916ff627aa3fdb",
                    "name": "Shizhe He",
                    "hidden": false
                },
                {
                    "_id": "69534ed489916ff627aa3fdc",
                    "name": "Avanika Narayan",
                    "hidden": false
                },
                {
                    "_id": "69534ed489916ff627aa3fdd",
                    "name": "Ishan S. Khare",
                    "hidden": false
                },
                {
                    "_id": "69534ed489916ff627aa3fde",
                    "name": "Scott W. Linderman",
                    "hidden": false
                },
                {
                    "_id": "69534ed489916ff627aa3fdf",
                    "name": "Christopher Ré",
                    "hidden": false
                },
                {
                    "_id": "69534ed489916ff627aa3fe0",
                    "name": "Dan Biderman",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-25T15:45:31.000Z",
            "submittedOnDailyAt": "2025-12-30T01:32:34.100Z",
            "title": "An Information Theoretic Perspective on Agentic System Design",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Agentic language model (LM) systems power modern applications like \"Deep Research\" and \"Claude Code,\" and leverage multi-LM architectures to overcome context limitations. Beneath their apparent diversity lies a recurring pattern: smaller \"compressor\" LMs (that can even run locally) distill raw context into compact text that is then consumed by larger \"predictor\" LMs. Despite their popularity, the design of compressor-predictor systems remains largely ad hoc, with little guidance on how compressor and predictor choices shape downstream performance. In practice, attributing gains to compression versus prediction requires costly, task-specific pairwise sweeps. We argue that these agentic system design questions are, at root, information-theoretic. Viewing the compressor LM as a noisy channel, we introduce a simple estimator of mutual information between the context and its compression to quantify compression quality in a task-independent way. We show that mutual information strongly predicts downstream performance, independent of any specific task. Through an information-theoretic framework, we perform a comprehensive empirical analysis across five datasets and three model families. Results reveal that larger compressors not only are more accurate, but also more token-efficient, conveying more bits of information per token. A 7B Qwen-2.5 compressor, for instance, is 1.6times more accurate, 4.6times more concise, and conveys 5.5times more bits of mutual information per token than its 1.5B sibling. Across datasets, scaling compressors is substantially more effective than scaling predictors, enabling larger on-device compressors to pair with smaller cloud predictors. Applied to a Deep Research system, these principles enable local compressors as small as 3B parameters to recover 99% of frontier-LM accuracy at 26% of API costs.",
            "upvotes": 6,
            "discussionId": "69534ed589916ff627aa3fe1",
            "projectPage": "https://hazyresearch.stanford.edu/blog/2025-12-29-agentic-it",
            "organization": {
                "_id": "672c672dcf09d152f4da04c4",
                "name": "StanfordUniversity",
                "fullname": "Stanford University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/vJI0POlzGMXL2878t1vz2.jpeg"
            }
        },
        "publishedAt": "2025-12-25T10:45:31.000Z",
        "title": "An Information Theoretic Perspective on Agentic System Design",
        "summary": "Agentic language model (LM) systems power modern applications like \"Deep Research\" and \"Claude Code,\" and leverage multi-LM architectures to overcome context limitations. Beneath their apparent diversity lies a recurring pattern: smaller \"compressor\" LMs (that can even run locally) distill raw context into compact text that is then consumed by larger \"predictor\" LMs. Despite their popularity, the design of compressor-predictor systems remains largely ad hoc, with little guidance on how compressor and predictor choices shape downstream performance. In practice, attributing gains to compression versus prediction requires costly, task-specific pairwise sweeps. We argue that these agentic system design questions are, at root, information-theoretic. Viewing the compressor LM as a noisy channel, we introduce a simple estimator of mutual information between the context and its compression to quantify compression quality in a task-independent way. We show that mutual information strongly predicts downstream performance, independent of any specific task. Through an information-theoretic framework, we perform a comprehensive empirical analysis across five datasets and three model families. Results reveal that larger compressors not only are more accurate, but also more token-efficient, conveying more bits of information per token. A 7B Qwen-2.5 compressor, for instance, is 1.6times more accurate, 4.6times more concise, and conveys 5.5times more bits of mutual information per token than its 1.5B sibling. Across datasets, scaling compressors is substantially more effective than scaling predictors, enabling larger on-device compressors to pair with smaller cloud predictors. Applied to a Deep Research system, these principles enable local compressors as small as 3B parameters to recover 99% of frontier-LM accuracy at 26% of API costs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.21720.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 197
        },
        "organization": {
            "_id": "672c672dcf09d152f4da04c4",
            "name": "StanfordUniversity",
            "fullname": "Stanford University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/vJI0POlzGMXL2878t1vz2.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.20927",
            "authors": [
                {
                    "_id": "694e0d9b746a34b55dd5451c",
                    "name": "Yoonwoo Jeong",
                    "hidden": false
                },
                {
                    "_id": "694e0d9b746a34b55dd5451d",
                    "name": "Cheng Sun",
                    "hidden": false
                },
                {
                    "_id": "694e0d9b746a34b55dd5451e",
                    "name": "Frank Wang",
                    "hidden": false
                },
                {
                    "_id": "694e0d9b746a34b55dd5451f",
                    "name": "Minsu Cho",
                    "hidden": false
                },
                {
                    "_id": "694e0d9b746a34b55dd54520",
                    "user": {
                        "_id": "65e83d337c10574cc311d379",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65e83d337c10574cc311d379/lB9xr44ApLZ4tAfomMDeB.png",
                        "isPro": false,
                        "fullname": "JaesungChoe",
                        "user": "jchoe",
                        "type": "user"
                    },
                    "name": "Jaesung Choe",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-12-29T14:24:06.553Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-24T04:16:18.000Z",
            "submittedOnDailyAt": "2025-12-30T05:58:50.938Z",
            "title": "Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting",
            "submittedOnDailyBy": {
                "_id": "65e83d337c10574cc311d379",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65e83d337c10574cc311d379/lB9xr44ApLZ4tAfomMDeB.png",
                "isPro": false,
                "fullname": "JaesungChoe",
                "user": "jchoe",
                "type": "user"
            },
            "summary": "Recent advancements in computer vision have successfully extended Open-vocabulary segmentation (OVS) to the 3D domain by leveraging 3D Gaussian Splatting (3D-GS). Despite this progress, efficiently rendering the high-dimensional features required for open-vocabulary queries poses a significant challenge. Existing methods employ codebooks or feature compression, causing information loss, thereby degrading segmentation quality. To address this limitation, we introduce Quantile Rendering (Q-Render), a novel rendering strategy for 3D Gaussians that efficiently handles high-dimensional features while maintaining high fidelity. Unlike conventional volume rendering, which densely samples all 3D Gaussians intersecting each ray, Q-Render sparsely samples only those with dominant influence along the ray. By integrating Q-Render into a generalizable 3D neural network, we also propose Gaussian Splatting Network (GS-Net), which predicts Gaussian features in a generalizable manner. Extensive experiments on ScanNet and LeRF demonstrate that our framework outperforms state-of-the-art methods, while enabling real-time rendering with an approximate ~43.7x speedup on 512-D feature maps. Code will be made publicly available.",
            "upvotes": 6,
            "discussionId": "694e0d9b746a34b55dd54521",
            "projectPage": "https://jaesung-choe.github.io/qrender/index.html",
            "organization": {
                "_id": "60262b67268c201cdc8b7d43",
                "name": "nvidia",
                "fullname": "NVIDIA",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
            }
        },
        "publishedAt": "2025-12-23T23:16:18.000Z",
        "title": "Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting",
        "summary": "Recent advancements in computer vision have successfully extended Open-vocabulary segmentation (OVS) to the 3D domain by leveraging 3D Gaussian Splatting (3D-GS). Despite this progress, efficiently rendering the high-dimensional features required for open-vocabulary queries poses a significant challenge. Existing methods employ codebooks or feature compression, causing information loss, thereby degrading segmentation quality. To address this limitation, we introduce Quantile Rendering (Q-Render), a novel rendering strategy for 3D Gaussians that efficiently handles high-dimensional features while maintaining high fidelity. Unlike conventional volume rendering, which densely samples all 3D Gaussians intersecting each ray, Q-Render sparsely samples only those with dominant influence along the ray. By integrating Q-Render into a generalizable 3D neural network, we also propose Gaussian Splatting Network (GS-Net), which predicts Gaussian features in a generalizable manner. Extensive experiments on ScanNet and LeRF demonstrate that our framework outperforms state-of-the-art methods, while enabling real-time rendering with an approximate ~43.7x speedup on 512-D feature maps. Code will be made publicly available.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20927.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65e83d337c10574cc311d379",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65e83d337c10574cc311d379/lB9xr44ApLZ4tAfomMDeB.png",
            "fullname": "JaesungChoe",
            "name": "jchoe",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "60262b67268c201cdc8b7d43",
            "name": "nvidia",
            "fullname": "NVIDIA",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.23703",
            "authors": [
                {
                    "_id": "6953cdc209c8c0a5381814e1",
                    "name": "Huajie Tan",
                    "hidden": false
                },
                {
                    "_id": "6953cdc209c8c0a5381814e2",
                    "name": "Sixiang Chen",
                    "hidden": false
                },
                {
                    "_id": "6953cdc209c8c0a5381814e3",
                    "name": "Yijie Xu",
                    "hidden": false
                },
                {
                    "_id": "6953cdc209c8c0a5381814e4",
                    "name": "Zixiao Wang",
                    "hidden": false
                },
                {
                    "_id": "6953cdc209c8c0a5381814e5",
                    "name": "Yuheng Ji",
                    "hidden": false
                },
                {
                    "_id": "6953cdc209c8c0a5381814e6",
                    "name": "Cheng Chi",
                    "hidden": false
                },
                {
                    "_id": "6953cdc209c8c0a5381814e7",
                    "name": "Yaoxu Lyu",
                    "hidden": false
                },
                {
                    "_id": "6953cdc209c8c0a5381814e8",
                    "name": "Zhongxia Zhao",
                    "hidden": false
                },
                {
                    "_id": "6953cdc209c8c0a5381814e9",
                    "name": "Xiansheng Chen",
                    "hidden": false
                },
                {
                    "_id": "6953cdc209c8c0a5381814ea",
                    "name": "Peterson Co",
                    "hidden": false
                },
                {
                    "_id": "6953cdc209c8c0a5381814eb",
                    "name": "Shaoxuan Xie",
                    "hidden": false
                },
                {
                    "_id": "6953cdc209c8c0a5381814ec",
                    "name": "Guocai Yao",
                    "hidden": false
                },
                {
                    "_id": "6953cdc209c8c0a5381814ed",
                    "name": "Pengwei Wang",
                    "hidden": false
                },
                {
                    "_id": "6953cdc209c8c0a5381814ee",
                    "name": "Zhongyuan Wang",
                    "hidden": false
                },
                {
                    "_id": "6953cdc209c8c0a5381814ef",
                    "name": "Shanghang Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-29T18:57:44.000Z",
            "submittedOnDailyAt": "2025-12-30T10:35:25.428Z",
            "title": "Robo-Dopamine: General Process Reward Modeling for High-Precision Robotic Manipulation",
            "submittedOnDailyBy": {
                "_id": "66b5dc0b854ad316cf835ab4",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66b5dc0b854ad316cf835ab4/8gOWw81rV5la7mzjw_qRv.jpeg",
                "isPro": false,
                "fullname": "tanhuajie2001",
                "user": "tanhuajie2001",
                "type": "user"
            },
            "summary": "The primary obstacle for applying reinforcement learning (RL) to real-world robotics is the design of effective reward functions. While recently learning-based Process Reward Models (PRMs) are a promising direction, they are often hindered by two fundamental limitations: their reward models lack step-aware understanding and rely on single-view perception, leading to unreliable assessments of fine-grained manipulation progress; and their reward shaping procedures are theoretically unsound, often inducing a semantic trap that misguides policy optimization. To address these, we introduce Dopamine-Reward, a novel reward modeling method for learning a general-purpose, step-aware process reward model from multi-view inputs. At its core is our General Reward Model (GRM), trained on a vast 3,400+ hour dataset, which leverages Step-wise Reward Discretization for structural understanding and Multi-Perspective Reward Fusion to overcome perceptual limitations. Building upon Dopamine-Reward, we propose Dopamine-RL, a robust policy learning framework that employs a theoretically-sound Policy-Invariant Reward Shaping method, which enables the agent to leverage dense rewards for efficient self-improvement without altering the optimal policy, thereby fundamentally avoiding the semantic trap. Extensive experiments across diverse simulated and real-world tasks validate our approach. GRM achieves state-of-the-art accuracy in reward assessment, and Dopamine-RL built on GRM significantly improves policy learning efficiency. For instance, after GRM is adapted to a new task in a one-shot manner from a single expert trajectory, the resulting reward model enables Dopamine-RL to improve the policy from near-zero to 95% success with only 150 online rollouts (approximately 1 hour of real robot interaction), while retaining strong generalization across tasks. Project website: https://robo-dopamine.github.io",
            "upvotes": 4,
            "discussionId": "6953cdc209c8c0a5381814f0",
            "projectPage": "https://robo-dopamine.github.io/",
            "githubRepo": "https://github.com/FlagOpen/Robo-Dopamine",
            "githubRepoAddedBy": "user",
            "githubStars": 21
        },
        "publishedAt": "2025-12-29T13:57:44.000Z",
        "title": "Robo-Dopamine: General Process Reward Modeling for High-Precision Robotic Manipulation",
        "summary": "The primary obstacle for applying reinforcement learning (RL) to real-world robotics is the design of effective reward functions. While recently learning-based Process Reward Models (PRMs) are a promising direction, they are often hindered by two fundamental limitations: their reward models lack step-aware understanding and rely on single-view perception, leading to unreliable assessments of fine-grained manipulation progress; and their reward shaping procedures are theoretically unsound, often inducing a semantic trap that misguides policy optimization. To address these, we introduce Dopamine-Reward, a novel reward modeling method for learning a general-purpose, step-aware process reward model from multi-view inputs. At its core is our General Reward Model (GRM), trained on a vast 3,400+ hour dataset, which leverages Step-wise Reward Discretization for structural understanding and Multi-Perspective Reward Fusion to overcome perceptual limitations. Building upon Dopamine-Reward, we propose Dopamine-RL, a robust policy learning framework that employs a theoretically-sound Policy-Invariant Reward Shaping method, which enables the agent to leverage dense rewards for efficient self-improvement without altering the optimal policy, thereby fundamentally avoiding the semantic trap. Extensive experiments across diverse simulated and real-world tasks validate our approach. GRM achieves state-of-the-art accuracy in reward assessment, and Dopamine-RL built on GRM significantly improves policy learning efficiency. For instance, after GRM is adapted to a new task in a one-shot manner from a single expert trajectory, the resulting reward model enables Dopamine-RL to improve the policy from near-zero to 95% success with only 150 online rollouts (approximately 1 hour of real robot interaction), while retaining strong generalization across tasks. Project website: https://robo-dopamine.github.io",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23703.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66b5dc0b854ad316cf835ab4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66b5dc0b854ad316cf835ab4/8gOWw81rV5la7mzjw_qRv.jpeg",
            "fullname": "tanhuajie2001",
            "name": "tanhuajie2001",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 10
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.23573",
            "authors": [
                {
                    "_id": "69535cbc89916ff627aa4040",
                    "name": "Shaohan Yu",
                    "hidden": false
                },
                {
                    "_id": "69535cbc89916ff627aa4041",
                    "name": "Lijun Li",
                    "hidden": false
                },
                {
                    "_id": "69535cbc89916ff627aa4042",
                    "name": "Chenyang Si",
                    "hidden": false
                },
                {
                    "_id": "69535cbc89916ff627aa4043",
                    "name": "Lu Sheng",
                    "hidden": false
                },
                {
                    "_id": "69535cbc89916ff627aa4044",
                    "name": "Jing Shao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-29T16:13:23.000Z",
            "submittedOnDailyAt": "2025-12-30T07:00:58.910Z",
            "title": "ProGuard: Towards Proactive Multimodal Safeguard",
            "submittedOnDailyBy": {
                "_id": "635f8ed47c05eb9f59963d3a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635f8ed47c05eb9f59963d3a/uQf4p9N9pSaFy87Wg9v4k.jpeg",
                "isPro": false,
                "fullname": "ChenyangSi",
                "user": "ChenyangSi",
                "type": "user"
            },
            "summary": "The rapid evolution of generative models has led to a continuous emergence of multimodal safety risks, exposing the limitations of existing defense methods. To address these challenges, we propose ProGuard, a vision-language proactive guard that identifies and describes out-of-distribution (OOD) safety risks without the need for model adjustments required by traditional reactive approaches. We first construct a modality-balanced dataset of 87K samples, each annotated with both binary safety labels and risk categories under a hierarchical multimodal safety taxonomy, effectively mitigating modality bias and ensuring consistent moderation across text, image, and text-image inputs. Based on this dataset, we train our vision-language base model purely through reinforcement learning (RL) to achieve efficient and concise reasoning. To approximate proactive safety scenarios in a controlled setting, we further introduce an OOD safety category inference task and augment the RL objective with a synonym-bank-based similarity reward that encourages the model to generate concise descriptions for unseen unsafe categories. Experimental results show that ProGuard achieves performance comparable to closed-source large models on binary safety classification, substantially outperforms existing open-source guard models on unsafe content categorization. Most notably, ProGuard delivers a strong proactive moderation ability, improving OOD risk detection by 52.6% and OOD risk description by 64.8%.",
            "upvotes": 4,
            "discussionId": "69535cbc89916ff627aa4045",
            "projectPage": "https://yushaohan.github.io/ProGuard/"
        },
        "publishedAt": "2025-12-29T11:13:23.000Z",
        "title": "ProGuard: Towards Proactive Multimodal Safeguard",
        "summary": "The rapid evolution of generative models has led to a continuous emergence of multimodal safety risks, exposing the limitations of existing defense methods. To address these challenges, we propose ProGuard, a vision-language proactive guard that identifies and describes out-of-distribution (OOD) safety risks without the need for model adjustments required by traditional reactive approaches. We first construct a modality-balanced dataset of 87K samples, each annotated with both binary safety labels and risk categories under a hierarchical multimodal safety taxonomy, effectively mitigating modality bias and ensuring consistent moderation across text, image, and text-image inputs. Based on this dataset, we train our vision-language base model purely through reinforcement learning (RL) to achieve efficient and concise reasoning. To approximate proactive safety scenarios in a controlled setting, we further introduce an OOD safety category inference task and augment the RL objective with a synonym-bank-based similarity reward that encourages the model to generate concise descriptions for unseen unsafe categories. Experimental results show that ProGuard achieves performance comparable to closed-source large models on binary safety classification, substantially outperforms existing open-source guard models on unsafe content categorization. Most notably, ProGuard delivers a strong proactive moderation ability, improving OOD risk detection by 52.6% and OOD risk description by 64.8%.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23573.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "635f8ed47c05eb9f59963d3a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635f8ed47c05eb9f59963d3a/uQf4p9N9pSaFy87Wg9v4k.jpeg",
            "fullname": "ChenyangSi",
            "name": "ChenyangSi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 25
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.23222",
            "authors": [
                {
                    "_id": "69535b9989916ff627aa4031",
                    "name": "Jiaxu Zhang",
                    "hidden": false
                },
                {
                    "_id": "69535b9989916ff627aa4032",
                    "name": "Tianshu Hu",
                    "hidden": false
                },
                {
                    "_id": "69535b9989916ff627aa4033",
                    "name": "Yuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "69535b9989916ff627aa4034",
                    "name": "Zenan Li",
                    "hidden": false
                },
                {
                    "_id": "69535b9989916ff627aa4035",
                    "name": "Linjie Luo",
                    "hidden": false
                },
                {
                    "_id": "69535b9989916ff627aa4036",
                    "name": "Guosheng Lin",
                    "hidden": false
                },
                {
                    "_id": "69535b9989916ff627aa4037",
                    "name": "Xin Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-29T05:56:22.000Z",
            "submittedOnDailyAt": "2025-12-30T02:27:05.875Z",
            "title": "Bridging Your Imagination with Audio-Video Generation via a Unified Director",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Existing AI-driven video creation systems typically treat script drafting and key-shot design as two disjoint tasks: the former relies on large language models, while the latter depends on image generation models. We argue that these two tasks should be unified within a single framework, as logical reasoning and imaginative thinking are both fundamental qualities of a film director. In this work, we propose UniMAGE, a unified director model that bridges user prompts with well-structured scripts, thereby empowering non-experts to produce long-context, multi-shot films by leveraging existing audio-video generation models. To achieve this, we employ the Mixture-of-Transformers architecture that unifies text and image generation. To further enhance narrative logic and keyframe consistency, we introduce a ``first interleaving, then disentangling'' training paradigm. Specifically, we first perform Interleaved Concept Learning, which utilizes interleaved text-image data to foster the model's deeper understanding and imaginative interpretation of scripts. We then conduct Disentangled Expert Learning, which decouples script writing from keyframe generation, enabling greater flexibility and creativity in storytelling. Extensive experiments demonstrate that UniMAGE achieves state-of-the-art performance among open-source models, generating logically coherent video scripts and visually consistent keyframe images.",
            "upvotes": 3,
            "discussionId": "69535b9a89916ff627aa4038",
            "projectPage": "https://kebii.github.io/UniMAGE/",
            "ai_summary": "A unified director model leveraging a Mixture-of-Transformers architecture with interleaved and disentangled learning generates coherent video scripts and consistent keyframes through a single framework.",
            "ai_keywords": [
                "UniMAGE",
                "Mixture-of-Transformers",
                "Interleaved Concept Learning",
                "Disentangled Expert Learning"
            ],
            "organization": {
                "_id": "653b817d32c97d0655575872",
                "name": "ByteDance",
                "fullname": "ByteDance",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
            }
        },
        "publishedAt": "2025-12-29T00:56:22.000Z",
        "title": "Bridging Your Imagination with Audio-Video Generation via a Unified Director",
        "summary": "Existing AI-driven video creation systems typically treat script drafting and key-shot design as two disjoint tasks: the former relies on large language models, while the latter depends on image generation models. We argue that these two tasks should be unified within a single framework, as logical reasoning and imaginative thinking are both fundamental qualities of a film director. In this work, we propose UniMAGE, a unified director model that bridges user prompts with well-structured scripts, thereby empowering non-experts to produce long-context, multi-shot films by leveraging existing audio-video generation models. To achieve this, we employ the Mixture-of-Transformers architecture that unifies text and image generation. To further enhance narrative logic and keyframe consistency, we introduce a ``first interleaving, then disentangling'' training paradigm. Specifically, we first perform Interleaved Concept Learning, which utilizes interleaved text-image data to foster the model's deeper understanding and imaginative interpretation of scripts. We then conduct Disentangled Expert Learning, which decouples script writing from keyframe generation, enabling greater flexibility and creativity in storytelling. Extensive experiments demonstrate that UniMAGE achieves state-of-the-art performance among open-source models, generating logically coherent video scripts and visually consistent keyframe images.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23222.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 197
        },
        "organization": {
            "_id": "653b817d32c97d0655575872",
            "name": "ByteDance",
            "fullname": "ByteDance",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.21734",
            "authors": [
                {
                    "_id": "6953898589916ff627aa40fe",
                    "name": "Steven Xiao",
                    "hidden": false
                },
                {
                    "_id": "6953898589916ff627aa40ff",
                    "name": "Xindi Zhang",
                    "hidden": false
                },
                {
                    "_id": "6953898589916ff627aa4100",
                    "name": "Dechao Meng",
                    "hidden": false
                },
                {
                    "_id": "6953898589916ff627aa4101",
                    "name": "Qi Wang",
                    "hidden": false
                },
                {
                    "_id": "6953898589916ff627aa4102",
                    "name": "Peng Zhang",
                    "hidden": false
                },
                {
                    "_id": "6953898589916ff627aa4103",
                    "name": "Bang Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-25T16:34:56.000Z",
            "submittedOnDailyAt": "2025-12-30T05:42:55.798Z",
            "title": "Knot Forcing: Taming Autoregressive Video Diffusion Models for Real-time Infinite Interactive Portrait Animation",
            "submittedOnDailyBy": {
                "_id": "640ad946b871d7117d5eead7",
                "avatarUrl": "/avatars/4cd90f6926a24c58485ef364eae50ef1.svg",
                "isPro": false,
                "fullname": "Zihan Wang",
                "user": "ZihanWang99",
                "type": "user"
            },
            "summary": "Real-time portrait animation is essential for interactive applications such as virtual assistants and live avatars, requiring high visual fidelity, temporal coherence, ultra-low latency, and responsive control from dynamic inputs like reference images and driving signals. While diffusion-based models achieve strong quality, their non-causal nature hinders streaming deployment. Causal autoregressive video generation approaches enable efficient frame-by-frame generation but suffer from error accumulation, motion discontinuities at chunk boundaries, and degraded long-term consistency. In this work, we present a novel streaming framework named Knot Forcing for real-time portrait animation that addresses these challenges through three key designs: (1) a chunk-wise generation strategy with global identity preservation via cached KV states of the reference image and local temporal modeling using sliding window attention; (2) a temporal knot module that overlaps adjacent chunks and propagates spatio-temporal cues via image-to-video conditioning to smooth inter-chunk motion transitions; and (3) A \"running ahead\" mechanism that dynamically updates the reference frame's temporal coordinate during inference, keeping its semantic context ahead of the current rollout frame to support long-term coherence. Knot Forcing enables high-fidelity, temporally consistent, and interactive portrait animation over infinite sequences, achieving real-time performance with strong visual stability on consumer-grade GPUs.",
            "upvotes": 3,
            "discussionId": "6953898589916ff627aa4104",
            "organization": {
                "_id": "67d15cca6e2cf0e062dbfb54",
                "name": "AlibabaTongyiLab",
                "fullname": "TongyiLab",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
            }
        },
        "publishedAt": "2025-12-25T11:34:56.000Z",
        "title": "Knot Forcing: Taming Autoregressive Video Diffusion Models for Real-time Infinite Interactive Portrait Animation",
        "summary": "Real-time portrait animation is essential for interactive applications such as virtual assistants and live avatars, requiring high visual fidelity, temporal coherence, ultra-low latency, and responsive control from dynamic inputs like reference images and driving signals. While diffusion-based models achieve strong quality, their non-causal nature hinders streaming deployment. Causal autoregressive video generation approaches enable efficient frame-by-frame generation but suffer from error accumulation, motion discontinuities at chunk boundaries, and degraded long-term consistency. In this work, we present a novel streaming framework named Knot Forcing for real-time portrait animation that addresses these challenges through three key designs: (1) a chunk-wise generation strategy with global identity preservation via cached KV states of the reference image and local temporal modeling using sliding window attention; (2) a temporal knot module that overlaps adjacent chunks and propagates spatio-temporal cues via image-to-video conditioning to smooth inter-chunk motion transitions; and (3) A \"running ahead\" mechanism that dynamically updates the reference frame's temporal coordinate during inference, keeping its semantic context ahead of the current rollout frame to support long-term coherence. Knot Forcing enables high-fidelity, temporally consistent, and interactive portrait animation over infinite sequences, achieving real-time performance with strong visual stability on consumer-grade GPUs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.21734.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "640ad946b871d7117d5eead7",
            "avatarUrl": "/avatars/4cd90f6926a24c58485ef364eae50ef1.svg",
            "fullname": "Zihan Wang",
            "name": "ZihanWang99",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "organization": {
            "_id": "67d15cca6e2cf0e062dbfb54",
            "name": "AlibabaTongyiLab",
            "fullname": "TongyiLab",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.23236",
            "authors": [
                {
                    "_id": "6953f16b09c8c0a53818152e",
                    "name": "Gang Liao",
                    "hidden": false
                },
                {
                    "_id": "6953f16b09c8c0a53818152f",
                    "name": "Hongsen Qin",
                    "hidden": false
                },
                {
                    "_id": "6953f16b09c8c0a538181530",
                    "name": "Ying Wang",
                    "hidden": false
                },
                {
                    "_id": "6953f16b09c8c0a538181531",
                    "name": "Alicia Golden",
                    "hidden": false
                },
                {
                    "_id": "6953f16b09c8c0a538181532",
                    "name": "Michael Kuchnik",
                    "hidden": false
                },
                {
                    "_id": "6953f16b09c8c0a538181533",
                    "name": "Yavuz Yetim",
                    "hidden": false
                },
                {
                    "_id": "6953f16b09c8c0a538181534",
                    "name": "Jia Jiunn Ang",
                    "hidden": false
                },
                {
                    "_id": "6953f16b09c8c0a538181535",
                    "name": "Chunli Fu",
                    "hidden": false
                },
                {
                    "_id": "6953f16b09c8c0a538181536",
                    "name": "Yihan He",
                    "hidden": false
                },
                {
                    "_id": "6953f16b09c8c0a538181537",
                    "name": "Samuel Hsia",
                    "hidden": false
                },
                {
                    "_id": "6953f16b09c8c0a538181538",
                    "name": "Zewei Jiang",
                    "hidden": false
                },
                {
                    "_id": "6953f16b09c8c0a538181539",
                    "name": "Dianshi Li",
                    "hidden": false
                },
                {
                    "_id": "6953f16b09c8c0a53818153a",
                    "name": "Uladzimir Pashkevich",
                    "hidden": false
                },
                {
                    "_id": "6953f16b09c8c0a53818153b",
                    "name": "Varna Puvvada",
                    "hidden": false
                },
                {
                    "_id": "6953f16b09c8c0a53818153c",
                    "name": "Feng Shi",
                    "hidden": false
                },
                {
                    "_id": "6953f16b09c8c0a53818153d",
                    "name": "Matt Steiner",
                    "hidden": false
                },
                {
                    "_id": "6953f16b09c8c0a53818153e",
                    "name": "Ruichao Xiao",
                    "hidden": false
                },
                {
                    "_id": "6953f16b09c8c0a53818153f",
                    "name": "Nathan Yan",
                    "hidden": false
                },
                {
                    "_id": "6953f16b09c8c0a538181540",
                    "name": "Xiayu Yu",
                    "hidden": false
                },
                {
                    "_id": "6953f16b09c8c0a538181541",
                    "name": "Zhou Fang",
                    "hidden": false
                },
                {
                    "_id": "6953f16b09c8c0a538181542",
                    "name": "Abdul Zainul-Abedin",
                    "hidden": false
                },
                {
                    "_id": "6953f16b09c8c0a538181543",
                    "name": "Ketan Singh",
                    "hidden": false
                },
                {
                    "_id": "6953f16b09c8c0a538181544",
                    "name": "Hongtao Yu",
                    "hidden": false
                },
                {
                    "_id": "6953f16b09c8c0a538181545",
                    "name": "Wenyuan Chi",
                    "hidden": false
                },
                {
                    "_id": "6953f16b09c8c0a538181546",
                    "name": "Barney Huang",
                    "hidden": false
                },
                {
                    "_id": "6953f16b09c8c0a538181547",
                    "name": "Sean Zhang",
                    "hidden": false
                },
                {
                    "_id": "6953f16b09c8c0a538181548",
                    "name": "Noah Weller",
                    "hidden": false
                },
                {
                    "_id": "6953f16b09c8c0a538181549",
                    "name": "Zach Marine",
                    "hidden": false
                },
                {
                    "_id": "6953f16b09c8c0a53818154a",
                    "name": "Wyatt Cook",
                    "hidden": false
                },
                {
                    "_id": "6953f16b09c8c0a53818154b",
                    "name": "Carole-Jean Wu",
                    "hidden": false
                },
                {
                    "_id": "6953f16b09c8c0a53818154c",
                    "name": "Gaoxiang Liu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/654d2d7a86f53e80527dd32d/G3KxDkXZhXouV-N8X8GqD.png",
                "https://cdn-uploads.huggingface.co/production/uploads/654d2d7a86f53e80527dd32d/VlJwrzgxDV_MXCX58izRz.png",
                "https://cdn-uploads.huggingface.co/production/uploads/654d2d7a86f53e80527dd32d/5KItwY0yFk0pnaqj54B3L.png"
            ],
            "publishedAt": "2025-12-29T06:31:55.000Z",
            "submittedOnDailyAt": "2025-12-30T13:10:32.991Z",
            "title": "KernelEvolve: Scaling Agentic Kernel Coding for Heterogeneous AI Accelerators at Meta",
            "submittedOnDailyBy": {
                "_id": "654d2d7a86f53e80527dd32d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654d2d7a86f53e80527dd32d/9XIl3O9iFh_0q9Ly_1LdI.jpeg",
                "isPro": false,
                "fullname": "Gang Liao",
                "user": "gangliao",
                "type": "user"
            },
            "summary": "Making deep learning recommendation model (DLRM) training and inference fast and efficient is important. However, this presents three key system challenges - model architecture diversity, kernel primitive diversity, and hardware generation and architecture heterogeneity. This paper presents KernelEvolve-an agentic kernel coding framework-to tackle heterogeneity at-scale for DLRM. KernelEvolve is designed to take kernel specifications as input and automate the process of kernel generation and optimization for recommendation model across heterogeneous hardware architectures. KernelEvolve does so by operating at multiple programming abstractions, from Triton and CuTe DSL to low-level hardware agnostic languages, spanning the full hardware-software optimization stack. The kernel optimization process is described as graph-based search with selection policy, universal operator, fitness function, and termination rule, dynamically adapts to runtime execution context through retrieval-augmented prompt synthesis. We designed, implemented, and deployed KernelEvolve to optimize a wide variety of production recommendation models across generations of NVIDIA and AMD GPUs, as well as Meta's AI accelerators. We validate KernelEvolve on the publicly-available KernelBench suite, achieving 100% pass rate on all 250 problems across three difficulty levels, and 160 PyTorch ATen operators across three heterogeneous hardware platforms, demonstrating 100% correctness. KernelEvolve reduces development time from weeks to hours and achieves substantial performance improvements over PyTorch baselines across diverse production use cases and for heterogeneous AI systems at-scale. Beyond performance efficiency improvements, KernelEvolve significantly mitigates the programmability barrier for new AI hardware by enabling automated kernel generation for in-house developed AI hardware.",
            "upvotes": 2,
            "discussionId": "6953f16b09c8c0a53818154d",
            "organization": {
                "_id": "66b54027408752ae16404b05",
                "name": "metaresearch",
                "fullname": "Meta Research",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66b25f3f58babfaeb76112dc/2GmiaF075AZ7BcE538oPk.png"
            }
        },
        "publishedAt": "2025-12-29T01:31:55.000Z",
        "title": "KernelEvolve: Scaling Agentic Kernel Coding for Heterogeneous AI Accelerators at Meta",
        "summary": "Making deep learning recommendation model (DLRM) training and inference fast and efficient is important. However, this presents three key system challenges - model architecture diversity, kernel primitive diversity, and hardware generation and architecture heterogeneity. This paper presents KernelEvolve-an agentic kernel coding framework-to tackle heterogeneity at-scale for DLRM. KernelEvolve is designed to take kernel specifications as input and automate the process of kernel generation and optimization for recommendation model across heterogeneous hardware architectures. KernelEvolve does so by operating at multiple programming abstractions, from Triton and CuTe DSL to low-level hardware agnostic languages, spanning the full hardware-software optimization stack. The kernel optimization process is described as graph-based search with selection policy, universal operator, fitness function, and termination rule, dynamically adapts to runtime execution context through retrieval-augmented prompt synthesis. We designed, implemented, and deployed KernelEvolve to optimize a wide variety of production recommendation models across generations of NVIDIA and AMD GPUs, as well as Meta's AI accelerators. We validate KernelEvolve on the publicly-available KernelBench suite, achieving 100% pass rate on all 250 problems across three difficulty levels, and 160 PyTorch ATen operators across three heterogeneous hardware platforms, demonstrating 100% correctness. KernelEvolve reduces development time from weeks to hours and achieves substantial performance improvements over PyTorch baselines across diverse production use cases and for heterogeneous AI systems at-scale. Beyond performance efficiency improvements, KernelEvolve significantly mitigates the programmability barrier for new AI hardware by enabling automated kernel generation for in-house developed AI hardware.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/654d2d7a86f53e80527dd32d/G3KxDkXZhXouV-N8X8GqD.png",
            "https://cdn-uploads.huggingface.co/production/uploads/654d2d7a86f53e80527dd32d/VlJwrzgxDV_MXCX58izRz.png",
            "https://cdn-uploads.huggingface.co/production/uploads/654d2d7a86f53e80527dd32d/5KItwY0yFk0pnaqj54B3L.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23236.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "654d2d7a86f53e80527dd32d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654d2d7a86f53e80527dd32d/9XIl3O9iFh_0q9Ly_1LdI.jpeg",
            "fullname": "Gang Liao",
            "name": "gangliao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "organization": {
            "_id": "66b54027408752ae16404b05",
            "name": "metaresearch",
            "fullname": "Meta Research",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66b25f3f58babfaeb76112dc/2GmiaF075AZ7BcE538oPk.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.22100",
            "authors": [
                {
                    "_id": "695270ff746a34b55dd5492f",
                    "name": "Duygu Altinok",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/635c4736177df3f16e99db63/Vq55RftsJIDVCqICPyaJq.png"
            ],
            "publishedAt": "2025-12-26T18:02:09.000Z",
            "submittedOnDailyAt": "2025-12-30T10:14:13.974Z",
            "title": "Introducing TrGLUE and SentiTurca: A Comprehensive Benchmark for Turkish General Language Understanding and Sentiment Analysis",
            "submittedOnDailyBy": {
                "_id": "635c4736177df3f16e99db63",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635c4736177df3f16e99db63/wP8zIiCLNLyVEJCwPr51-.jpeg",
                "isPro": true,
                "fullname": "Duygu Altinok",
                "user": "BayanDuygu",
                "type": "user"
            },
            "summary": "Evaluating the performance of various model architectures, such as transformers, large language models (LLMs), and other NLP systems, requires comprehensive benchmarks that measure performance across multiple dimensions. Among these, the evaluation of natural language understanding (NLU) is particularly critical as it serves as a fundamental criterion for assessing model capabilities. Thus, it is essential to establish benchmarks that enable thorough evaluation and analysis of NLU abilities from diverse perspectives. While the GLUE benchmark has set a standard for evaluating English NLU, similar benchmarks have been developed for other languages, such as CLUE for Chinese, FLUE for French, and JGLUE for Japanese. However, no comparable benchmark currently exists for the Turkish language. To address this gap, we introduce TrGLUE, a comprehensive benchmark encompassing a variety of NLU tasks for Turkish. In addition, we present SentiTurca, a specialized benchmark for sentiment analysis. To support researchers, we also provide fine-tuning and evaluation code for transformer-based models, facilitating the effective use of these benchmarks. TrGLUE comprises Turkish-native corpora curated to mirror the domains and task formulations of GLUE-style evaluations, with labels obtained through a semi-automated pipeline that combines strong LLM-based annotation, cross-model agreement checks, and subsequent human validation. This design prioritizes linguistic naturalness, minimizes direct translation artifacts, and yields a scalable, reproducible workflow. With TrGLUE, our goal is to establish a robust evaluation framework for Turkish NLU, empower researchers with valuable resources, and provide insights into generating high-quality semi-automated datasets.",
            "upvotes": 2,
            "discussionId": "695270ff746a34b55dd54930",
            "organization": {
                "_id": "635e8862398ff343c4f60ff3",
                "name": "turkish-nlp-suite",
                "fullname": "Turkish NLP Suite",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1675973012506-635c4736177df3f16e99db63.png"
            }
        },
        "publishedAt": "2025-12-26T13:02:09.000Z",
        "title": "Introducing TrGLUE and SentiTurca: A Comprehensive Benchmark for Turkish General Language Understanding and Sentiment Analysis",
        "summary": "Evaluating the performance of various model architectures, such as transformers, large language models (LLMs), and other NLP systems, requires comprehensive benchmarks that measure performance across multiple dimensions. Among these, the evaluation of natural language understanding (NLU) is particularly critical as it serves as a fundamental criterion for assessing model capabilities. Thus, it is essential to establish benchmarks that enable thorough evaluation and analysis of NLU abilities from diverse perspectives. While the GLUE benchmark has set a standard for evaluating English NLU, similar benchmarks have been developed for other languages, such as CLUE for Chinese, FLUE for French, and JGLUE for Japanese. However, no comparable benchmark currently exists for the Turkish language. To address this gap, we introduce TrGLUE, a comprehensive benchmark encompassing a variety of NLU tasks for Turkish. In addition, we present SentiTurca, a specialized benchmark for sentiment analysis. To support researchers, we also provide fine-tuning and evaluation code for transformer-based models, facilitating the effective use of these benchmarks. TrGLUE comprises Turkish-native corpora curated to mirror the domains and task formulations of GLUE-style evaluations, with labels obtained through a semi-automated pipeline that combines strong LLM-based annotation, cross-model agreement checks, and subsequent human validation. This design prioritizes linguistic naturalness, minimizes direct translation artifacts, and yields a scalable, reproducible workflow. With TrGLUE, our goal is to establish a robust evaluation framework for Turkish NLU, empower researchers with valuable resources, and provide insights into generating high-quality semi-automated datasets.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/635c4736177df3f16e99db63/Vq55RftsJIDVCqICPyaJq.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.22100.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "635c4736177df3f16e99db63",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635c4736177df3f16e99db63/wP8zIiCLNLyVEJCwPr51-.jpeg",
            "fullname": "Duygu Altinok",
            "name": "BayanDuygu",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 41
        },
        "organization": {
            "_id": "635e8862398ff343c4f60ff3",
            "name": "turkish-nlp-suite",
            "fullname": "Turkish NLP Suite",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1675973012506-635c4736177df3f16e99db63.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.22374",
            "authors": [
                {
                    "_id": "69541fd4869a8627b452c8ca",
                    "name": "Xin Yu",
                    "hidden": false
                },
                {
                    "_id": "69541fd4869a8627b452c8cb",
                    "name": "Xiaojuan Qi",
                    "hidden": false
                },
                {
                    "_id": "69541fd4869a8627b452c8cc",
                    "name": "Zhengqi Li",
                    "hidden": false
                },
                {
                    "_id": "69541fd4869a8627b452c8cd",
                    "name": "Kai Zhang",
                    "hidden": false
                },
                {
                    "_id": "69541fd4869a8627b452c8ce",
                    "name": "Richard Zhang",
                    "hidden": false
                },
                {
                    "_id": "69541fd4869a8627b452c8cf",
                    "name": "Zhe Lin",
                    "hidden": false
                },
                {
                    "_id": "69541fd4869a8627b452c8d0",
                    "name": "Eli Shechtman",
                    "hidden": false
                },
                {
                    "_id": "69541fd4869a8627b452c8d1",
                    "name": "Tianyu Wang",
                    "hidden": false
                },
                {
                    "_id": "69541fd4869a8627b452c8d2",
                    "name": "Yotam Nitzan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-26T20:42:11.000Z",
            "submittedOnDailyAt": "2025-12-30T16:26:54.054Z",
            "title": "Self-Evaluation Unlocks Any-Step Text-to-Image Generation",
            "submittedOnDailyBy": {
                "_id": "62d1a71ac40bde8b986c3748",
                "avatarUrl": "/avatars/6dbfba55563c71ed0db0d59c2629d525.svg",
                "isPro": false,
                "fullname": "Xin Yu",
                "user": "Andyx",
                "type": "user"
            },
            "summary": "We introduce the Self-Evaluating Model (Self-E), a novel, from-scratch training approach for text-to-image generation that supports any-step inference. Self-E learns from data similarly to a Flow Matching model, while simultaneously employing a novel self-evaluation mechanism: it evaluates its own generated samples using its current score estimates, effectively serving as a dynamic self-teacher. Unlike traditional diffusion or flow models, it does not rely solely on local supervision, which typically necessitates many inference steps. Unlike distillation-based approaches, it does not require a pretrained teacher. This combination of instantaneous local learning and self-driven global matching bridges the gap between the two paradigms, enabling the training of a high-quality text-to-image model from scratch that excels even at very low step counts. Extensive experiments on large-scale text-to-image benchmarks show that Self-E not only excels in few-step generation, but is also competitive with state-of-the-art Flow Matching models at 50 steps. We further find that its performance improves monotonically as inference steps increase, enabling both ultra-fast few-step generation and high-quality long-trajectory sampling within a single unified model. To our knowledge, Self-E is the first from-scratch, any-step text-to-image model, offering a unified framework for efficient and scalable generation.",
            "upvotes": 1,
            "discussionId": "69541fd4869a8627b452c8d3",
            "projectPage": "https://xinyu-andy.github.io/SelfE-project/",
            "ai_summary": "Self-E is a novel self-evaluating text-to-image model trained from scratch that supports any-step generation and combines local learning with self-driven global matching to achieve high quality even at low step counts.",
            "ai_keywords": [
                "Self-Evaluating Model (Self-E)",
                "Flow Matching model",
                "self-evaluation mechanism",
                "score estimates",
                "dynamic self-teacher",
                "diffusion or flow models",
                "local supervision",
                "distillation-based approaches",
                "pretrained teacher",
                "instantaneous local learning",
                "self-driven global matching",
                "few-step generation",
                "any-step text-to-image model",
                "unified framework"
            ],
            "organization": {
                "_id": "637b318856db0404b7c5a0c2",
                "name": "adobe-research",
                "fullname": "Adobe Research",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1669033410364-624bebf604abc7ebb01789af.png"
            }
        },
        "publishedAt": "2025-12-26T15:42:11.000Z",
        "title": "Self-Evaluation Unlocks Any-Step Text-to-Image Generation",
        "summary": "We introduce the Self-Evaluating Model (Self-E), a novel, from-scratch training approach for text-to-image generation that supports any-step inference. Self-E learns from data similarly to a Flow Matching model, while simultaneously employing a novel self-evaluation mechanism: it evaluates its own generated samples using its current score estimates, effectively serving as a dynamic self-teacher. Unlike traditional diffusion or flow models, it does not rely solely on local supervision, which typically necessitates many inference steps. Unlike distillation-based approaches, it does not require a pretrained teacher. This combination of instantaneous local learning and self-driven global matching bridges the gap between the two paradigms, enabling the training of a high-quality text-to-image model from scratch that excels even at very low step counts. Extensive experiments on large-scale text-to-image benchmarks show that Self-E not only excels in few-step generation, but is also competitive with state-of-the-art Flow Matching models at 50 steps. We further find that its performance improves monotonically as inference steps increase, enabling both ultra-fast few-step generation and high-quality long-trajectory sampling within a single unified model. To our knowledge, Self-E is the first from-scratch, any-step text-to-image model, offering a unified framework for efficient and scalable generation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.22374.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62d1a71ac40bde8b986c3748",
            "avatarUrl": "/avatars/6dbfba55563c71ed0db0d59c2629d525.svg",
            "fullname": "Xin Yu",
            "name": "Andyx",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "organization": {
            "_id": "637b318856db0404b7c5a0c2",
            "name": "adobe-research",
            "fullname": "Adobe Research",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1669033410364-624bebf604abc7ebb01789af.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.22255",
            "authors": [
                {
                    "_id": "6953ec8309c8c0a53818151f",
                    "name": "Abhranil Chandra",
                    "hidden": false
                },
                {
                    "_id": "6953ec8309c8c0a538181520",
                    "name": "Ayush Agrawal",
                    "hidden": false
                },
                {
                    "_id": "6953ec8309c8c0a538181521",
                    "name": "Arian Hosseini",
                    "hidden": false
                },
                {
                    "_id": "6953ec8309c8c0a538181522",
                    "name": "Sebastian Fischmeister",
                    "hidden": false
                },
                {
                    "_id": "6953ec8309c8c0a538181523",
                    "name": "Rishabh Agarwal",
                    "hidden": false
                },
                {
                    "_id": "6953ec8309c8c0a538181524",
                    "name": "Navin Goyal",
                    "hidden": false
                },
                {
                    "_id": "6953ec8309c8c0a538181525",
                    "name": "Aaron Courville",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-24T07:35:55.000Z",
            "submittedOnDailyAt": "2025-12-30T12:49:31.495Z",
            "title": "Shape of Thought: When Distribution Matters More than Correctness in Reasoning Tasks",
            "submittedOnDailyBy": {
                "_id": "6154ee656181394cc00cb990",
                "avatarUrl": "/avatars/25d3b6911c5991b1869f5d76ca2c4069.svg",
                "isPro": true,
                "fullname": "Abhranil Chandra",
                "user": "abhranil14",
                "type": "user"
            },
            "summary": "We present the surprising finding that a language model's reasoning capabilities can be improved by training on synthetic datasets of chain-of-thought (CoT) traces from more capable models, even when all of those traces lead to an incorrect final answer. Our experiments show this approach can yield better performance on reasoning tasks than training on human-annotated datasets. We hypothesize that two key factors explain this phenomenon: first, the distribution of synthetic data is inherently closer to the language model's own distribution, making it more amenable to learning. Second, these `incorrect' traces are often only partially flawed and contain valid reasoning steps from which the model can learn. To further test the first hypothesis, we use a language model to paraphrase human-annotated traces -- shifting their distribution closer to the model's own distribution -- and show that this improves performance. For the second hypothesis, we introduce increasingly flawed CoT traces and study to what extent models are tolerant to these flaws. We demonstrate our findings across various reasoning domains like math, algorithmic reasoning and code generation using MATH, GSM8K, Countdown and MBPP datasets on various language models ranging from 1.5B to 9B across Qwen, Llama, and Gemma models. Our study shows that curating datasets that are closer to the model's distribution is a critical aspect to consider. We also show that a correct final answer is not always a reliable indicator of a faithful reasoning process.",
            "upvotes": 1,
            "discussionId": "6953ec8409c8c0a538181526",
            "organization": {
                "_id": "6230c3ced93e84e2338765f3",
                "name": "UWaterloo",
                "fullname": "University of Waterloo",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1647363004040-6230c34bacb94a81eec91ed5.png"
            }
        },
        "publishedAt": "2025-12-24T02:35:55.000Z",
        "title": "Shape of Thought: When Distribution Matters More than Correctness in Reasoning Tasks",
        "summary": "We present the surprising finding that a language model's reasoning capabilities can be improved by training on synthetic datasets of chain-of-thought (CoT) traces from more capable models, even when all of those traces lead to an incorrect final answer. Our experiments show this approach can yield better performance on reasoning tasks than training on human-annotated datasets. We hypothesize that two key factors explain this phenomenon: first, the distribution of synthetic data is inherently closer to the language model's own distribution, making it more amenable to learning. Second, these `incorrect' traces are often only partially flawed and contain valid reasoning steps from which the model can learn. To further test the first hypothesis, we use a language model to paraphrase human-annotated traces -- shifting their distribution closer to the model's own distribution -- and show that this improves performance. For the second hypothesis, we introduce increasingly flawed CoT traces and study to what extent models are tolerant to these flaws. We demonstrate our findings across various reasoning domains like math, algorithmic reasoning and code generation using MATH, GSM8K, Countdown and MBPP datasets on various language models ranging from 1.5B to 9B across Qwen, Llama, and Gemma models. Our study shows that curating datasets that are closer to the model's distribution is a critical aspect to consider. We also show that a correct final answer is not always a reliable indicator of a faithful reasoning process.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.22255.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6154ee656181394cc00cb990",
            "avatarUrl": "/avatars/25d3b6911c5991b1869f5d76ca2c4069.svg",
            "fullname": "Abhranil Chandra",
            "name": "abhranil14",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "organization": {
            "_id": "6230c3ced93e84e2338765f3",
            "name": "UWaterloo",
            "fullname": "University of Waterloo",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1647363004040-6230c34bacb94a81eec91ed5.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.22984",
            "authors": [
                {
                    "_id": "6953959489916ff627aa4124",
                    "name": "Han-Wei Kung",
                    "hidden": false
                },
                {
                    "_id": "6953959489916ff627aa4125",
                    "name": "Tuomas Varanka",
                    "hidden": false
                },
                {
                    "_id": "6953959489916ff627aa4126",
                    "name": "Nicu Sebe",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-28T16:06:55.000Z",
            "submittedOnDailyAt": "2025-12-30T06:36:55.196Z",
            "title": "Reverse Personalization",
            "submittedOnDailyBy": {
                "_id": "64a3eb280111d5ff6c4849fd",
                "avatarUrl": "/avatars/3a9000393b8d200418bae5fe7d902e4d.svg",
                "isPro": false,
                "fullname": "Han-Wei Kung",
                "user": "hkung",
                "type": "user"
            },
            "summary": "Recent text-to-image diffusion models have demonstrated remarkable generation of realistic facial images conditioned on textual prompts and human identities, enabling creating personalized facial imagery. However, existing prompt-based methods for removing or modifying identity-specific features rely either on the subject being well-represented in the pre-trained model or require model fine-tuning for specific identities. In this work, we analyze the identity generation process and introduce a reverse personalization framework for face anonymization. Our approach leverages conditional diffusion inversion, allowing direct manipulation of images without using text prompts. To generalize beyond subjects in the model's training data, we incorporate an identity-guided conditioning branch. Unlike prior anonymization methods, which lack control over facial attributes, our framework supports attribute-controllable anonymization. We demonstrate that our method achieves a state-of-the-art balance between identity removal, attribute preservation, and image quality. Source code and data are available at https://github.com/hanweikung/reverse-personalization .",
            "upvotes": 0,
            "discussionId": "6953959489916ff627aa4127",
            "githubRepo": "https://github.com/hanweikung/reverse-personalization",
            "githubRepoAddedBy": "user",
            "ai_summary": "A reverse personalization framework using conditional diffusion inversion enables attribute-controllable face anonymization, balancing identity removal and image quality.",
            "ai_keywords": [
                "text-to-image diffusion models",
                "conditional diffusion inversion",
                "identity-guided conditioning branch",
                "reverse personalization framework",
                "attribute-controllable anonymization"
            ],
            "githubStars": 0
        },
        "publishedAt": "2025-12-28T11:06:55.000Z",
        "title": "Reverse Personalization",
        "summary": "Recent text-to-image diffusion models have demonstrated remarkable generation of realistic facial images conditioned on textual prompts and human identities, enabling creating personalized facial imagery. However, existing prompt-based methods for removing or modifying identity-specific features rely either on the subject being well-represented in the pre-trained model or require model fine-tuning for specific identities. In this work, we analyze the identity generation process and introduce a reverse personalization framework for face anonymization. Our approach leverages conditional diffusion inversion, allowing direct manipulation of images without using text prompts. To generalize beyond subjects in the model's training data, we incorporate an identity-guided conditioning branch. Unlike prior anonymization methods, which lack control over facial attributes, our framework supports attribute-controllable anonymization. We demonstrate that our method achieves a state-of-the-art balance between identity removal, attribute preservation, and image quality. Source code and data are available at https://github.com/hanweikung/reverse-personalization .",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.22984.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64a3eb280111d5ff6c4849fd",
            "avatarUrl": "/avatars/3a9000393b8d200418bae5fe7d902e4d.svg",
            "fullname": "Han-Wei Kung",
            "name": "hkung",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    }
]