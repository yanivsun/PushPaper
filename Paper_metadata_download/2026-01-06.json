[
    {
        "paper": {
            "id": "2512.20578",
            "authors": [
                {
                    "_id": "69534e6789916ff627aa3fbb",
                    "user": {
                        "_id": "6830c4be7f72826192827659",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0hTKkO_zytpy9W8_0s291.png",
                        "isPro": false,
                        "fullname": "Amirhosein Ghasemabadi",
                        "user": "AmirhoseinGH",
                        "type": "user"
                    },
                    "name": "Amirhosein Ghasemabadi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-06T10:03:00.600Z",
                    "hidden": false
                },
                {
                    "_id": "69534e6789916ff627aa3fbc",
                    "name": "Di Niu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6830c4be7f72826192827659/qufIktW_mZHzzI_iR1NCR.gif",
                "https://cdn-uploads.huggingface.co/production/uploads/6830c4be7f72826192827659/KVw6GK90RxzX2Jv9aF7v2.jpeg"
            ],
            "publishedAt": "2025-12-23T18:21:32.000Z",
            "submittedOnDailyAt": "2026-01-06T00:39:40.820Z",
            "title": "Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits",
            "submittedOnDailyBy": {
                "_id": "6830c4be7f72826192827659",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0hTKkO_zytpy9W8_0s291.png",
                "isPro": false,
                "fullname": "Amirhosein Ghasemabadi",
                "user": "AmirhoseinGH",
                "type": "user"
            },
            "summary": "Large language models (LLMs) generate fluent and complex outputs but often fail to recognize their own mistakes and hallucinations. Existing approaches typically rely on external judges, multi-sample consistency, or text-based self-critique, which incur additional compute or correlate weakly with true correctness. We ask: can LLMs predict their own failures by inspecting internal states during inference? We introduce Gnosis, a lightweight self-awareness mechanism that enables frozen LLMs to perform intrinsic self-verification by decoding signals from hidden states and attention patterns. Gnosis passively observes internal traces, compresses them into fixed-budget descriptors, and predicts correctness with negligible inference cost, adding only ~5M parameters and operating independently of sequence length. Across math reasoning, open-domain question answering, and academic knowledge benchmarks, and over frozen backbones ranging from 1.7B to 20B parameters, Gnosis consistently outperforms strong internal baselines and large external judges in both accuracy and calibration. Moreover, it generalizes zero-shot to partial generations, enabling early detection of failing trajectories and compute-aware control. These results show that reliable correctness cues are intrinsic to generation process and can be extracted efficiently without external supervision.",
            "upvotes": 47,
            "discussionId": "69534e6889916ff627aa3fc3",
            "githubRepo": "https://github.com/Amirhosein-gh98/Gnosis",
            "githubRepoAddedBy": "auto",
            "githubStars": 6,
            "organization": {
                "_id": "64237839df71531a9c440b06",
                "name": "UAlberta",
                "fullname": "University of Alberta",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/tdtBlXSCOd6XG48TORSBZ.png"
            }
        },
        "publishedAt": "2025-12-23T13:21:32.000Z",
        "title": "Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits",
        "summary": "Large language models (LLMs) generate fluent and complex outputs but often fail to recognize their own mistakes and hallucinations. Existing approaches typically rely on external judges, multi-sample consistency, or text-based self-critique, which incur additional compute or correlate weakly with true correctness. We ask: can LLMs predict their own failures by inspecting internal states during inference? We introduce Gnosis, a lightweight self-awareness mechanism that enables frozen LLMs to perform intrinsic self-verification by decoding signals from hidden states and attention patterns. Gnosis passively observes internal traces, compresses them into fixed-budget descriptors, and predicts correctness with negligible inference cost, adding only ~5M parameters and operating independently of sequence length. Across math reasoning, open-domain question answering, and academic knowledge benchmarks, and over frozen backbones ranging from 1.7B to 20B parameters, Gnosis consistently outperforms strong internal baselines and large external judges in both accuracy and calibration. Moreover, it generalizes zero-shot to partial generations, enabling early detection of failing trajectories and compute-aware control. These results show that reliable correctness cues are intrinsic to generation process and can be extracted efficiently without external supervision.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6830c4be7f72826192827659/qufIktW_mZHzzI_iR1NCR.gif",
            "https://cdn-uploads.huggingface.co/production/uploads/6830c4be7f72826192827659/KVw6GK90RxzX2Jv9aF7v2.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20578.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6830c4be7f72826192827659",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0hTKkO_zytpy9W8_0s291.png",
            "fullname": "Amirhosein Ghasemabadi",
            "name": "AmirhoseinGH",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2,
            "isUserFollowing": false
        },
        "organization": {
            "_id": "64237839df71531a9c440b06",
            "name": "UAlberta",
            "fullname": "University of Alberta",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/tdtBlXSCOd6XG48TORSBZ.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2601.02204",
            "authors": [
                {
                    "_id": "695c7d0d6aa73bc11f091433",
                    "name": "Huichao Zhang",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f091434",
                    "user": {
                        "_id": "64b796079ebb7e6c7ddcdabf",
                        "avatarUrl": "/avatars/51af43cab078705b8745b4f942f542e5.svg",
                        "isPro": false,
                        "fullname": "Liao Qu",
                        "user": "leo1117",
                        "type": "user"
                    },
                    "name": "Liao Qu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-06T09:57:57.686Z",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f091435",
                    "name": "Yiheng Liu",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f091436",
                    "name": "Hang Chen",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f091437",
                    "name": "Yangyang Song",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f091438",
                    "name": "Yongsheng Dong",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f091439",
                    "name": "Shikun Sun",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f09143a",
                    "name": "Xian Li",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f09143b",
                    "name": "Xu Wang",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f09143c",
                    "user": {
                        "_id": "6344dcb1cd37e44d9ed46508",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6344dcb1cd37e44d9ed46508/J92UKSxKR3iziD2WJfih4.jpeg",
                        "isPro": false,
                        "fullname": "Yi Jiang",
                        "user": "JiangYi",
                        "type": "user"
                    },
                    "name": "Yi Jiang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-06T09:57:55.158Z",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f09143d",
                    "name": "Hu Ye",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f09143e",
                    "name": "Bo Chen",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f09143f",
                    "name": "Yiming Gao",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f091440",
                    "name": "Peng Liu",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f091441",
                    "name": "Akide Liu",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f091442",
                    "name": "Zhipeng Yang",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f091443",
                    "name": "Qili Deng",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f091444",
                    "name": "Linjie Xing",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f091445",
                    "name": "Jiyang Liu",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f091446",
                    "name": "Zhao Wang",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f091447",
                    "name": "Yang Zhou",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f091448",
                    "name": "Mingcong Liu",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f091449",
                    "name": "Yi Zhang",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f09144a",
                    "name": "Qian He",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f09144b",
                    "name": "Xiwei Hu",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f09144c",
                    "name": "Zhongqi Qi",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f09144d",
                    "name": "Jie Shao",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f09144e",
                    "name": "Zhiye Fu",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f09144f",
                    "name": "Shuai Wang",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f091450",
                    "name": "Fangmin Chen",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f091451",
                    "name": "Xuezhi Chai",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f091452",
                    "name": "Zhihua Wu",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f091453",
                    "name": "Yitong Wang",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f091454",
                    "name": "Zehuan Yuan",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f091455",
                    "name": "Daniel K. Du",
                    "hidden": false
                },
                {
                    "_id": "695c7d0d6aa73bc11f091456",
                    "name": "Xinglong Wu",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-05T15:27:04.000Z",
            "submittedOnDailyAt": "2026-01-06T00:52:35.953Z",
            "title": "NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation",
            "submittedOnDailyBy": {
                "_id": "64b796079ebb7e6c7ddcdabf",
                "avatarUrl": "/avatars/51af43cab078705b8745b4f942f542e5.svg",
                "isPro": false,
                "fullname": "Liao Qu",
                "user": "leo1117",
                "type": "user"
            },
            "summary": "We present NextFlow, a unified decoder-only autoregressive transformer trained on 6 trillion interleaved text-image discrete tokens. By leveraging a unified vision representation within a unified autoregressive architecture, NextFlow natively activates multimodal understanding and generation capabilities, unlocking abilities of image editing, interleaved content and video generation. Motivated by the distinct nature of modalities - where text is strictly sequential and images are inherently hierarchical - we retain next-token prediction for text but adopt next-scale prediction for visual generation. This departs from traditional raster-scan methods, enabling the generation of 1024x1024 images in just 5 seconds - orders of magnitude faster than comparable AR models. We address the instabilities of multi-scale generation through a robust training recipe. Furthermore, we introduce a prefix-tuning strategy for reinforcement learning. Experiments demonstrate that NextFlow achieves state-of-the-art performance among unified models and rivals specialized diffusion baselines in visual quality.",
            "upvotes": 45,
            "discussionId": "695c7d0d6aa73bc11f091457",
            "githubRepo": "https://github.com/ByteVisionLab/NextFlow",
            "githubRepoAddedBy": "user",
            "ai_summary": "NextFlow is a unified decoder-only autoregressive transformer that processes interleaved text-image tokens, enabling fast multimodal generation through novel next-token and next-scale prediction strategies.",
            "ai_keywords": [
                "decoder-only autoregressive transformer",
                "interleaved text-image discrete tokens",
                "unified vision representation",
                "multimodal understanding",
                "multimodal generation",
                "next-token prediction",
                "next-scale prediction",
                "raster-scan methods",
                "visual generation",
                "prefix-tuning strategy",
                "reinforcement learning",
                "diffusion baselines"
            ],
            "githubStars": 60,
            "organization": {
                "_id": "653b817d32c97d0655575872",
                "name": "ByteDance",
                "fullname": "ByteDance",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
            }
        },
        "publishedAt": "2026-01-05T10:27:04.000Z",
        "title": "NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation",
        "summary": "We present NextFlow, a unified decoder-only autoregressive transformer trained on 6 trillion interleaved text-image discrete tokens. By leveraging a unified vision representation within a unified autoregressive architecture, NextFlow natively activates multimodal understanding and generation capabilities, unlocking abilities of image editing, interleaved content and video generation. Motivated by the distinct nature of modalities - where text is strictly sequential and images are inherently hierarchical - we retain next-token prediction for text but adopt next-scale prediction for visual generation. This departs from traditional raster-scan methods, enabling the generation of 1024x1024 images in just 5 seconds - orders of magnitude faster than comparable AR models. We address the instabilities of multi-scale generation through a robust training recipe. Furthermore, we introduce a prefix-tuning strategy for reinforcement learning. Experiments demonstrate that NextFlow achieves state-of-the-art performance among unified models and rivals specialized diffusion baselines in visual quality.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02204.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64b796079ebb7e6c7ddcdabf",
            "avatarUrl": "/avatars/51af43cab078705b8745b4f942f542e5.svg",
            "fullname": "Liao Qu",
            "name": "leo1117",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2,
            "isUserFollowing": false
        },
        "organization": {
            "_id": "653b817d32c97d0655575872",
            "name": "ByteDance",
            "fullname": "ByteDance",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2601.01739",
            "authors": [
                {
                    "_id": "695c72346aa73bc11f0913bf",
                    "user": {
                        "_id": "6044fd39e6aa3e130cb92867",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6044fd39e6aa3e130cb92867/L5hb8vpHY6SKMEL-Xacma.jpeg",
                        "isPro": false,
                        "fullname": "Eunbi Choi",
                        "user": "unbiarirang",
                        "type": "user"
                    },
                    "name": "Eunbi Choi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-06T09:58:17.472Z",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913c0",
                    "user": {
                        "_id": "64d31ca9465b6039259838df",
                        "avatarUrl": "/avatars/b3bde5067ed3fcd908d3d91c00680bfb.svg",
                        "isPro": false,
                        "fullname": "kibong choi",
                        "user": "bongchoi",
                        "type": "user"
                    },
                    "name": "Kibong Choi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-06T09:58:11.322Z",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913c1",
                    "name": "Seokhee Hong",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913c2",
                    "user": {
                        "_id": "63c50e590c24c8b53958f75e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673858632881-noauth.png",
                        "isPro": false,
                        "fullname": "Junwon Hwang",
                        "user": "nuxlear",
                        "type": "user"
                    },
                    "name": "Junwon Hwang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-06T09:58:09.333Z",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913c3",
                    "name": "Hyojin Jeon",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913c4",
                    "user": {
                        "_id": "66a9e066a203add977948988",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66a9e066a203add977948988/mwVS-vt-8p-DFC5T9H9H3.jpeg",
                        "isPro": false,
                        "fullname": "hyunjik.jo",
                        "user": "switiz87",
                        "type": "user"
                    },
                    "name": "Hyunjik Jo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-06T09:58:13.551Z",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913c5",
                    "name": "Joonkee Kim",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913c6",
                    "name": "Seonghwan Kim",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913c7",
                    "name": "Soyeon Kim",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913c8",
                    "name": "Sunkyoung Kim",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913c9",
                    "name": "Yireun Kim",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913ca",
                    "name": "Yongil Kim",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913cb",
                    "name": "Haeju Lee",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913cc",
                    "name": "Jinsik Lee",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913cd",
                    "name": "Kyungmin Lee",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913ce",
                    "name": "Sangha Park",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913cf",
                    "name": "Heuiyeen Yeen",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913d0",
                    "name": "Hwan Chang",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913d1",
                    "name": "Stanley Jungkyu Choi",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913d2",
                    "name": "Yejin Choi",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913d3",
                    "name": "Jiwon Ham",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913d4",
                    "name": "Kijeong Jeon",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913d5",
                    "name": "Geunyeong Jeong",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913d6",
                    "name": "Gerrard Jeongwon Jo",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913d7",
                    "name": "Yonghwan Jo",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913d8",
                    "name": "Jiyeon Jung",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913d9",
                    "name": "Naeun Kang",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913da",
                    "name": "Dohoon Kim",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913db",
                    "name": "Euisoon Kim",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913dc",
                    "name": "Hayeon Kim",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913dd",
                    "name": "Hyosang Kim",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913de",
                    "name": "Hyunseo Kim",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913df",
                    "name": "Jieun Kim",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913e0",
                    "name": "Minu Kim",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913e1",
                    "name": "Myoungshin Kim",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913e2",
                    "name": "Unsol Kim",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913e3",
                    "name": "Youchul Kim",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913e4",
                    "name": "YoungJin Kim",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913e5",
                    "name": "Chaeeun Lee",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913e6",
                    "name": "Chaeyoon Lee",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913e7",
                    "user": {
                        "_id": "6399ab9e92e12136b99ef60e",
                        "avatarUrl": "/avatars/b76895c53f0f046586555c20292c78a1.svg",
                        "isPro": false,
                        "fullname": "Changhun Lee",
                        "user": "xvyaward",
                        "type": "user"
                    },
                    "name": "Changhun Lee",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-06T09:58:15.420Z",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913e8",
                    "name": "Dahm Lee",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913e9",
                    "name": "Edward Hwayoung Lee",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913ea",
                    "name": "Honglak Lee",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913eb",
                    "name": "Jinsang Lee",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913ec",
                    "name": "Jiyoung Lee",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913ed",
                    "name": "Sangeun Lee",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913ee",
                    "name": "Seungwon Lim",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913ef",
                    "name": "Solji Lim",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913f0",
                    "name": "Woohyung Lim",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913f1",
                    "name": "Chanwoo Moon",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913f2",
                    "name": "Jaewoo Park",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913f3",
                    "name": "Jinho Park",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913f4",
                    "name": "Yongmin Park",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913f5",
                    "name": "Hyerin Seo",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913f6",
                    "name": "Wooseok Seo",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913f7",
                    "name": "Yongwoo Song",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913f8",
                    "name": "Sejong Yang",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913f9",
                    "name": "Sihoon Yang",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913fa",
                    "name": "Chang En Yea",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913fb",
                    "name": "Sihyuk Yi",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913fc",
                    "name": "Chansik Yoon",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913fd",
                    "name": "Dongkeun Yoon",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913fe",
                    "name": "Sangyeon Yoon",
                    "hidden": false
                },
                {
                    "_id": "695c72346aa73bc11f0913ff",
                    "name": "Hyeongu Yun",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-05T02:30:59.000Z",
            "submittedOnDailyAt": "2026-01-06T01:03:14.011Z",
            "title": "K-EXAONE Technical Report",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "This technical report presents K-EXAONE, a large-scale multilingual language model developed by LG AI Research. K-EXAONE is built on a Mixture-of-Experts architecture with 236B total parameters, activating 23B parameters during inference. It supports a 256K-token context window and covers six languages: Korean, English, Spanish, German, Japanese, and Vietnamese. We evaluate K-EXAONE on a comprehensive benchmark suite spanning reasoning, agentic, general, Korean, and multilingual abilities. Across these evaluations, K-EXAONE demonstrates performance comparable to open-weight models of similar size. K-EXAONE, designed to advance AI for a better life, is positioned as a powerful proprietary AI foundation model for a wide range of industrial and research applications.",
            "upvotes": 44,
            "discussionId": "695c72356aa73bc11f091400",
            "githubRepo": "https://github.com/LG-AI-EXAONE/K-EXAONE",
            "githubRepoAddedBy": "user",
            "ai_summary": "K-EXAONE is a multilingual language model with a Mixture-of-Experts architecture that achieves competitive performance on various benchmarks while supporting multiple languages and long-context windows.",
            "ai_keywords": [
                "Mixture-of-Experts",
                "256K-token context window",
                "multilingual language model",
                "parameter-efficient fine-tuning"
            ],
            "githubStars": 39,
            "organization": {
                "_id": "66a89bc1d96a5adbccbe85d4",
                "name": "LGAI-EXAONE",
                "fullname": "LG AI Research",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66a899a72f11aaf66001a8dc/UfdrP3GMo9pNT62BaMnhw.png"
            }
        },
        "publishedAt": "2026-01-04T21:30:59.000Z",
        "title": "K-EXAONE Technical Report",
        "summary": "This technical report presents K-EXAONE, a large-scale multilingual language model developed by LG AI Research. K-EXAONE is built on a Mixture-of-Experts architecture with 236B total parameters, activating 23B parameters during inference. It supports a 256K-token context window and covers six languages: Korean, English, Spanish, German, Japanese, and Vietnamese. We evaluate K-EXAONE on a comprehensive benchmark suite spanning reasoning, agentic, general, Korean, and multilingual abilities. Across these evaluations, K-EXAONE demonstrates performance comparable to open-weight models of similar size. K-EXAONE, designed to advance AI for a better life, is positioned as a powerful proprietary AI foundation model for a wide range of industrial and research applications.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.01739.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 200,
            "isUserFollowing": false
        },
        "organization": {
            "_id": "66a89bc1d96a5adbccbe85d4",
            "name": "LGAI-EXAONE",
            "fullname": "LG AI Research",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66a899a72f11aaf66001a8dc/UfdrP3GMo9pNT62BaMnhw.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2601.01425",
            "authors": [
                {
                    "_id": "695c765d6aa73bc11f091402",
                    "name": "Xu Guo",
                    "hidden": false
                },
                {
                    "_id": "695c765d6aa73bc11f091403",
                    "name": "Fulong Ye",
                    "hidden": false
                },
                {
                    "_id": "695c765d6aa73bc11f091404",
                    "name": "Xinghui Li",
                    "hidden": false
                },
                {
                    "_id": "695c765d6aa73bc11f091405",
                    "name": "Pengqi Tu",
                    "hidden": false
                },
                {
                    "_id": "695c765d6aa73bc11f091406",
                    "name": "Pengze Zhang",
                    "hidden": false
                },
                {
                    "_id": "695c765d6aa73bc11f091407",
                    "user": {
                        "_id": "674566cb79d6f3a9da7be0de",
                        "avatarUrl": "/avatars/b6a5384820e150405039aa2b9badac29.svg",
                        "isPro": false,
                        "fullname": "Qichao Sun",
                        "user": "Simons212",
                        "type": "user"
                    },
                    "name": "Qichao Sun",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-06T09:58:07.336Z",
                    "hidden": false
                },
                {
                    "_id": "695c765d6aa73bc11f091408",
                    "name": "Songtao Zhao",
                    "hidden": false
                },
                {
                    "_id": "695c765d6aa73bc11f091409",
                    "name": "Xiangwang Hou",
                    "hidden": false
                },
                {
                    "_id": "695c765d6aa73bc11f09140a",
                    "name": "Qian He",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/67d50738fed7787297d737d6/0nECkDL67gltfx3htZ82l.mp4"
            ],
            "publishedAt": "2026-01-04T08:07:11.000Z",
            "submittedOnDailyAt": "2026-01-06T00:30:06.666Z",
            "title": "DreamID-V:Bridging the Image-to-Video Gap for High-Fidelity Face Swapping via Diffusion Transformer",
            "submittedOnDailyBy": {
                "_id": "67d50738fed7787297d737d6",
                "avatarUrl": "/avatars/30d7126f7c3feda732c5783ea3db9c7f.svg",
                "isPro": false,
                "fullname": "xuguo",
                "user": "XuGuo699",
                "type": "user"
            },
            "summary": "Video Face Swapping (VFS) requires seamlessly injecting a source identity into a target video while meticulously preserving the original pose, expression, lighting, background, and dynamic information. Existing methods struggle to maintain identity similarity and attribute preservation while preserving temporal consistency. To address the challenge, we propose a comprehensive framework to seamlessly transfer the superiority of Image Face Swapping (IFS) to the video domain. We first introduce a novel data pipeline SyncID-Pipe that pre-trains an Identity-Anchored Video Synthesizer and combines it with IFS models to construct bidirectional ID quadruplets for explicit supervision. Building upon paired data, we propose the first Diffusion Transformer-based framework DreamID-V, employing a core Modality-Aware Conditioning module to discriminatively inject multi-model conditions. Meanwhile, we propose a Synthetic-to-Real Curriculum mechanism and an Identity-Coherence Reinforcement Learning strategy to enhance visual realism and identity consistency under challenging scenarios. To address the issue of limited benchmarks, we introduce IDBench-V, a comprehensive benchmark encompassing diverse scenes. Extensive experiments demonstrate DreamID-V outperforms state-of-the-art methods and further exhibits exceptional versatility, which can be seamlessly adapted to various swap-related tasks.",
            "upvotes": 34,
            "discussionId": "695c765d6aa73bc11f09140b",
            "projectPage": "https://guoxu1233.github.io/DreamID-V/",
            "githubRepo": "https://github.com/bytedance/DreamID-V",
            "githubRepoAddedBy": "user",
            "ai_summary": "A novel video face swapping framework combines image face swapping techniques with diffusion transformers and curriculum learning to achieve superior identity preservation and visual realism.",
            "ai_keywords": [
                "Video Face Swapping",
                "Image Face Swapping",
                "diffusion transformer",
                "Modality-Aware Conditioning",
                "Synthetic-to-Real Curriculum",
                "Identity-Coherence Reinforcement Learning",
                "IDBench-V",
                "Identity-Anchored Video Synthesizer",
                "bidirectional ID quadruplets",
                "multi-model conditions"
            ],
            "githubStars": 86,
            "organization": {
                "_id": "653b817d32c97d0655575872",
                "name": "ByteDance",
                "fullname": "ByteDance",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
            }
        },
        "publishedAt": "2026-01-04T03:07:11.000Z",
        "title": "DreamID-V:Bridging the Image-to-Video Gap for High-Fidelity Face Swapping via Diffusion Transformer",
        "summary": "Video Face Swapping (VFS) requires seamlessly injecting a source identity into a target video while meticulously preserving the original pose, expression, lighting, background, and dynamic information. Existing methods struggle to maintain identity similarity and attribute preservation while preserving temporal consistency. To address the challenge, we propose a comprehensive framework to seamlessly transfer the superiority of Image Face Swapping (IFS) to the video domain. We first introduce a novel data pipeline SyncID-Pipe that pre-trains an Identity-Anchored Video Synthesizer and combines it with IFS models to construct bidirectional ID quadruplets for explicit supervision. Building upon paired data, we propose the first Diffusion Transformer-based framework DreamID-V, employing a core Modality-Aware Conditioning module to discriminatively inject multi-model conditions. Meanwhile, we propose a Synthetic-to-Real Curriculum mechanism and an Identity-Coherence Reinforcement Learning strategy to enhance visual realism and identity consistency under challenging scenarios. To address the issue of limited benchmarks, we introduce IDBench-V, a comprehensive benchmark encompassing diverse scenes. Extensive experiments demonstrate DreamID-V outperforms state-of-the-art methods and further exhibits exceptional versatility, which can be seamlessly adapted to various swap-related tasks.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/67d50738fed7787297d737d6/0nECkDL67gltfx3htZ82l.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.01425.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "67d50738fed7787297d737d6",
            "avatarUrl": "/avatars/30d7126f7c3feda732c5783ea3db9c7f.svg",
            "fullname": "xuguo",
            "name": "XuGuo699",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2,
            "isUserFollowing": false
        },
        "organization": {
            "_id": "653b817d32c97d0655575872",
            "name": "ByteDance",
            "fullname": "ByteDance",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2601.02256",
            "authors": [
                {
                    "_id": "695c7d256aa73bc11f091459",
                    "name": "Shikun Sun",
                    "hidden": false
                },
                {
                    "_id": "695c7d256aa73bc11f09145a",
                    "user": {
                        "_id": "64b796079ebb7e6c7ddcdabf",
                        "avatarUrl": "/avatars/51af43cab078705b8745b4f942f542e5.svg",
                        "isPro": false,
                        "fullname": "Liao Qu",
                        "user": "leo1117",
                        "type": "user"
                    },
                    "name": "Liao Qu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-06T09:57:53.171Z",
                    "hidden": false
                },
                {
                    "_id": "695c7d256aa73bc11f09145b",
                    "name": "Huichao Zhang",
                    "hidden": false
                },
                {
                    "_id": "695c7d256aa73bc11f09145c",
                    "name": "Yiheng Liu",
                    "hidden": false
                },
                {
                    "_id": "695c7d256aa73bc11f09145d",
                    "name": "Yangyang Song",
                    "hidden": false
                },
                {
                    "_id": "695c7d256aa73bc11f09145e",
                    "name": "Xian Li",
                    "hidden": false
                },
                {
                    "_id": "695c7d256aa73bc11f09145f",
                    "name": "Xu Wang",
                    "hidden": false
                },
                {
                    "_id": "695c7d256aa73bc11f091460",
                    "name": "Yi Jiang",
                    "hidden": false
                },
                {
                    "_id": "695c7d256aa73bc11f091461",
                    "name": "Daniel K. Du",
                    "hidden": false
                },
                {
                    "_id": "695c7d256aa73bc11f091462",
                    "name": "Xinglong Wu",
                    "hidden": false
                },
                {
                    "_id": "695c7d256aa73bc11f091463",
                    "name": "Jia Jia",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-05T16:36:40.000Z",
            "submittedOnDailyAt": "2026-01-06T00:51:20.686Z",
            "title": "VAR RL Done Right: Tackling Asynchronous Policy Conflicts in Visual Autoregressive Generation",
            "submittedOnDailyBy": {
                "_id": "64b796079ebb7e6c7ddcdabf",
                "avatarUrl": "/avatars/51af43cab078705b8745b4f942f542e5.svg",
                "isPro": false,
                "fullname": "Liao Qu",
                "user": "leo1117",
                "type": "user"
            },
            "summary": "Visual generation is dominated by three paradigms: AutoRegressive (AR), diffusion, and Visual AutoRegressive (VAR) models. Unlike AR and diffusion, VARs operate on heterogeneous input structures across their generation steps, which creates severe asynchronous policy conflicts. This issue becomes particularly acute in reinforcement learning (RL) scenarios, leading to unstable training and suboptimal alignment. To resolve this, we propose a novel framework to enhance Group Relative Policy Optimization (GRPO) by explicitly managing these conflicts. Our method integrates three synergistic components: 1) a stabilizing intermediate reward to guide early-stage generation; 2) a dynamic time-step reweighting scheme for precise credit assignment; and 3) a novel mask propagation algorithm, derived from principles of Reward Feedback Learning (ReFL), designed to isolate optimization effects both spatially and temporally. Our approach demonstrates significant improvements in sample quality and objective alignment over the vanilla GRPO baseline, enabling robust and effective optimization for VAR models.",
            "upvotes": 28,
            "discussionId": "695c7d266aa73bc11f091464",
            "ai_summary": "Visual autoregressive models face training instability due to asynchronous policy conflicts, which are addressed through a novel framework enhancing group relative policy optimization with intermediate rewards, dynamic time-step reweighting, and mask propagation algorithms.",
            "ai_keywords": [
                "AutoRegressive",
                "diffusion",
                "Visual AutoRegressive",
                "reinforcement learning",
                "Group Relative Policy Optimization",
                "intermediate reward",
                "dynamic time-step reweighting",
                "mask propagation",
                "Reward Feedback Learning",
                "visual generation"
            ],
            "organization": {
                "_id": "653b817d32c97d0655575872",
                "name": "ByteDance",
                "fullname": "ByteDance",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
            }
        },
        "publishedAt": "2026-01-05T11:36:40.000Z",
        "title": "VAR RL Done Right: Tackling Asynchronous Policy Conflicts in Visual Autoregressive Generation",
        "summary": "Visual generation is dominated by three paradigms: AutoRegressive (AR), diffusion, and Visual AutoRegressive (VAR) models. Unlike AR and diffusion, VARs operate on heterogeneous input structures across their generation steps, which creates severe asynchronous policy conflicts. This issue becomes particularly acute in reinforcement learning (RL) scenarios, leading to unstable training and suboptimal alignment. To resolve this, we propose a novel framework to enhance Group Relative Policy Optimization (GRPO) by explicitly managing these conflicts. Our method integrates three synergistic components: 1) a stabilizing intermediate reward to guide early-stage generation; 2) a dynamic time-step reweighting scheme for precise credit assignment; and 3) a novel mask propagation algorithm, derived from principles of Reward Feedback Learning (ReFL), designed to isolate optimization effects both spatially and temporally. Our approach demonstrates significant improvements in sample quality and objective alignment over the vanilla GRPO baseline, enabling robust and effective optimization for VAR models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02256.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64b796079ebb7e6c7ddcdabf",
            "avatarUrl": "/avatars/51af43cab078705b8745b4f942f542e5.svg",
            "fullname": "Liao Qu",
            "name": "leo1117",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2,
            "isUserFollowing": false
        },
        "organization": {
            "_id": "653b817d32c97d0655575872",
            "name": "ByteDance",
            "fullname": "ByteDance",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.24138",
            "authors": [
                {
                    "_id": "695c9b756aa73bc11f0914fb",
                    "name": "Haoran He",
                    "hidden": false
                },
                {
                    "_id": "695c9b756aa73bc11f0914fc",
                    "name": "Yuxiao Ye",
                    "hidden": false
                },
                {
                    "_id": "695c9b756aa73bc11f0914fd",
                    "name": "Jie Liu",
                    "hidden": false
                },
                {
                    "_id": "695c9b756aa73bc11f0914fe",
                    "name": "Jiajun Liang",
                    "hidden": false
                },
                {
                    "_id": "695c9b756aa73bc11f0914ff",
                    "name": "Zhiyong Wang",
                    "hidden": false
                },
                {
                    "_id": "695c9b756aa73bc11f091500",
                    "name": "Ziyang Yuan",
                    "hidden": false
                },
                {
                    "_id": "695c9b756aa73bc11f091501",
                    "name": "Xintao Wang",
                    "hidden": false
                },
                {
                    "_id": "695c9b756aa73bc11f091502",
                    "name": "Hangyu Mao",
                    "hidden": false
                },
                {
                    "_id": "695c9b756aa73bc11f091503",
                    "name": "Pengfei Wan",
                    "hidden": false
                },
                {
                    "_id": "695c9b756aa73bc11f091504",
                    "name": "Ling Pan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-30T10:55:45.000Z",
            "submittedOnDailyAt": "2026-01-06T02:53:45.651Z",
            "title": "GARDO: Reinforcing Diffusion Models without Reward Hacking",
            "submittedOnDailyBy": {
                "_id": "6672937ceac0fb1b9e516595",
                "avatarUrl": "/avatars/5eea5657016572f60b0ecd0fa9a7dae4.svg",
                "isPro": false,
                "fullname": "haoran he",
                "user": "haoranhe",
                "type": "user"
            },
            "summary": "Fine-tuning diffusion models via online reinforcement learning (RL) has shown great potential for enhancing text-to-image alignment. However, since precisely specifying a ground-truth objective for visual tasks remains challenging, the models are often optimized using a proxy reward that only partially captures the true goal. This mismatch often leads to reward hacking, where proxy scores increase while real image quality deteriorates and generation diversity collapses. While common solutions add regularization against the reference policy to prevent reward hacking, they compromise sample efficiency and impede the exploration of novel, high-reward regions, as the reference policy is usually sub-optimal. To address the competing demands of sample efficiency, effective exploration, and mitigation of reward hacking, we propose Gated and Adaptive Regularization with Diversity-aware Optimization (GARDO), a versatile framework compatible with various RL algorithms. Our key insight is that regularization need not be applied universally; instead, it is highly effective to selectively penalize a subset of samples that exhibit high uncertainty. To address the exploration challenge, GARDO introduces an adaptive regularization mechanism wherein the reference model is periodically updated to match the capabilities of the online policy, ensuring a relevant regularization target. To address the mode collapse issue in RL, GARDO amplifies the rewards for high-quality samples that also exhibit high diversity, encouraging mode coverage without destabilizing the optimization process. Extensive experiments across diverse proxy rewards and hold-out unseen metrics consistently show that GARDO mitigates reward hacking and enhances generation diversity without sacrificing sample efficiency or exploration, highlighting its effectiveness and robustness.",
            "upvotes": 23,
            "discussionId": "695c9b766aa73bc11f091505",
            "projectPage": "https://tinnerhrhe.github.io/gardo_project/",
            "githubRepo": "https://github.com/tinnerhrhe/GARDO",
            "githubRepoAddedBy": "user",
            "ai_summary": "Online reinforcement learning for diffusion model fine-tuning suffers from reward hacking due to proxy reward mismatches, which GARDO addresses through selective regularization, adaptive reference updates, and diversity-aware reward amplification.",
            "ai_keywords": [
                "diffusion models",
                "reinforcement learning",
                "reward hacking",
                "proxy reward",
                "regularization",
                "online policy",
                "reference policy",
                "mode collapse",
                "diversity-aware optimization",
                "adaptive regularization",
                "sample efficiency",
                "exploration"
            ],
            "githubStars": 18
        },
        "publishedAt": "2025-12-30T05:55:45.000Z",
        "title": "GARDO: Reinforcing Diffusion Models without Reward Hacking",
        "summary": "Fine-tuning diffusion models via online reinforcement learning (RL) has shown great potential for enhancing text-to-image alignment. However, since precisely specifying a ground-truth objective for visual tasks remains challenging, the models are often optimized using a proxy reward that only partially captures the true goal. This mismatch often leads to reward hacking, where proxy scores increase while real image quality deteriorates and generation diversity collapses. While common solutions add regularization against the reference policy to prevent reward hacking, they compromise sample efficiency and impede the exploration of novel, high-reward regions, as the reference policy is usually sub-optimal. To address the competing demands of sample efficiency, effective exploration, and mitigation of reward hacking, we propose Gated and Adaptive Regularization with Diversity-aware Optimization (GARDO), a versatile framework compatible with various RL algorithms. Our key insight is that regularization need not be applied universally; instead, it is highly effective to selectively penalize a subset of samples that exhibit high uncertainty. To address the exploration challenge, GARDO introduces an adaptive regularization mechanism wherein the reference model is periodically updated to match the capabilities of the online policy, ensuring a relevant regularization target. To address the mode collapse issue in RL, GARDO amplifies the rewards for high-quality samples that also exhibit high diversity, encouraging mode coverage without destabilizing the optimization process. Extensive experiments across diverse proxy rewards and hold-out unseen metrics consistently show that GARDO mitigates reward hacking and enhances generation diversity without sacrificing sample efficiency or exploration, highlighting its effectiveness and robustness.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24138.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6672937ceac0fb1b9e516595",
            "avatarUrl": "/avatars/5eea5657016572f60b0ecd0fa9a7dae4.svg",
            "fullname": "haoran he",
            "name": "haoranhe",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2,
            "isUserFollowing": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2601.02358",
            "authors": [
                {
                    "_id": "695c80146aa73bc11f09146a",
                    "name": "Junyi Chen",
                    "hidden": false
                },
                {
                    "_id": "695c80146aa73bc11f09146b",
                    "name": "Tong He",
                    "hidden": false
                },
                {
                    "_id": "695c80146aa73bc11f09146c",
                    "name": "Zhoujie Fu",
                    "hidden": false
                },
                {
                    "_id": "695c80146aa73bc11f09146d",
                    "name": "Pengfei Wan",
                    "hidden": false
                },
                {
                    "_id": "695c80146aa73bc11f09146e",
                    "name": "Kun Gai",
                    "hidden": false
                },
                {
                    "_id": "695c80146aa73bc11f09146f",
                    "name": "Weicai Ye",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-05T18:56:34.000Z",
            "submittedOnDailyAt": "2026-01-06T00:53:36.146Z",
            "title": "VINO: A Unified Visual Generator with Interleaved OmniModal Context",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We present VINO, a unified visual generator that performs image and video generation and editing within a single framework. Instead of relying on task-specific models or independent modules for each modality, VINO uses a shared diffusion backbone that conditions on text, images and videos, enabling a broad range of visual creation and editing tasks under one model. Specifically, VINO couples a vision-language model (VLM) with a Multimodal Diffusion Transformer (MMDiT), where multimodal inputs are encoded as interleaved conditioning tokens, and then used to guide the diffusion process. This design supports multi-reference grounding, long-form instruction following, and coherent identity preservation across static and dynamic content, while avoiding modality-specific architectural components. To train such a unified system, we introduce a multi-stage training pipeline that progressively expands a video generation base model into a unified, multi-task generator capable of both image and video input and output. Across diverse generation and editing benchmarks, VINO demonstrates strong visual quality, faithful instruction following, improved reference and attribute preservation, and more controllable multi-identity edits. Our results highlight a practical path toward scalable unified visual generation, and the promise of interleaved, in-context computation as a foundation for general-purpose visual creation.",
            "upvotes": 21,
            "discussionId": "695c80146aa73bc11f091470",
            "projectPage": "https://sotamak1r.github.io/VINO-web/",
            "githubRepo": "https://github.com/SOTAMak1r/VINO-code",
            "githubRepoAddedBy": "user",
            "ai_summary": "VINO is a unified visual generator that uses a shared diffusion backbone with multimodal inputs to perform image and video generation and editing tasks.",
            "ai_keywords": [
                "visual generator",
                "diffusion backbone",
                "vision-language model",
                "Multimodal Diffusion Transformer",
                "conditioning tokens",
                "multimodal inputs",
                "video generation",
                "image generation",
                "visual editing",
                "multi-stage training pipeline",
                "unified system",
                "in-context computation"
            ],
            "githubStars": 42
        },
        "publishedAt": "2026-01-05T13:56:34.000Z",
        "title": "VINO: A Unified Visual Generator with Interleaved OmniModal Context",
        "summary": "We present VINO, a unified visual generator that performs image and video generation and editing within a single framework. Instead of relying on task-specific models or independent modules for each modality, VINO uses a shared diffusion backbone that conditions on text, images and videos, enabling a broad range of visual creation and editing tasks under one model. Specifically, VINO couples a vision-language model (VLM) with a Multimodal Diffusion Transformer (MMDiT), where multimodal inputs are encoded as interleaved conditioning tokens, and then used to guide the diffusion process. This design supports multi-reference grounding, long-form instruction following, and coherent identity preservation across static and dynamic content, while avoiding modality-specific architectural components. To train such a unified system, we introduce a multi-stage training pipeline that progressively expands a video generation base model into a unified, multi-task generator capable of both image and video input and output. Across diverse generation and editing benchmarks, VINO demonstrates strong visual quality, faithful instruction following, improved reference and attribute preservation, and more controllable multi-identity edits. Our results highlight a practical path toward scalable unified visual generation, and the promise of interleaved, in-context computation as a foundation for general-purpose visual creation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02358.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 200,
            "isUserFollowing": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2601.02281",
            "authors": [
                {
                    "_id": "695ccd446aa73bc11f0915e3",
                    "name": "Shuai Yuan",
                    "hidden": false
                },
                {
                    "_id": "695ccd446aa73bc11f0915e4",
                    "name": "Yantai Yang",
                    "hidden": false
                },
                {
                    "_id": "695ccd446aa73bc11f0915e5",
                    "name": "Xiaotian Yang",
                    "hidden": false
                },
                {
                    "_id": "695ccd446aa73bc11f0915e6",
                    "name": "Xupeng Zhang",
                    "hidden": false
                },
                {
                    "_id": "695ccd446aa73bc11f0915e7",
                    "name": "Zhonghao Zhao",
                    "hidden": false
                },
                {
                    "_id": "695ccd446aa73bc11f0915e8",
                    "name": "Lingming Zhang",
                    "hidden": false
                },
                {
                    "_id": "695ccd446aa73bc11f0915e9",
                    "name": "Zhipeng Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-05T17:11:00.000Z",
            "submittedOnDailyAt": "2026-01-06T06:27:03.712Z",
            "title": "InfiniteVGGT: Visual Geometry Grounded Transformer for Endless Streams",
            "submittedOnDailyBy": {
                "_id": "65b9f710e7c83813628a5cd0",
                "avatarUrl": "/avatars/47075fb646359211b2abe601fa8156d5.svg",
                "isPro": false,
                "fullname": "Yantai Yang",
                "user": "yantaiyang05",
                "type": "user"
            },
            "summary": "The grand vision of enabling persistent, large-scale 3D visual geometry understanding is shackled by the irreconcilable demands of scalability and long-term stability. While offline models like VGGT achieve inspiring geometry capability, their batch-based nature renders them irrelevant for live systems. Streaming architectures, though the intended solution for live operation, have proven inadequate. Existing methods either fail to support truly infinite-horizon inputs or suffer from catastrophic drift over long sequences. We shatter this long-standing dilemma with InfiniteVGGT, a causal visual geometry transformer that operationalizes the concept of a rolling memory through a bounded yet adaptive and perpetually expressive KV cache. Capitalizing on this, we devise a training-free, attention-agnostic pruning strategy that intelligently discards obsolete information, effectively ``rolling'' the memory forward with each new frame. Fully compatible with FlashAttention, InfiniteVGGT finally alleviates the compromise, enabling infinite-horizon streaming while outperforming existing streaming methods in long-term stability. The ultimate test for such a system is its performance over a truly infinite horizon, a capability that has been impossible to rigorously validate due to the lack of extremely long-term, continuous benchmarks. To address this critical gap, we introduce the Long3D benchmark, which, for the first time, enables a rigorous evaluation of continuous 3D geometry estimation on sequences about 10,000 frames. This provides the definitive evaluation platform for future research in long-term 3D geometry understanding. Code is available at: https://github.com/AutoLab-SAI-SJTU/InfiniteVGGT",
            "upvotes": 20,
            "discussionId": "695ccd446aa73bc11f0915ea",
            "githubRepo": "https://github.com/AutoLab-SAI-SJTU/InfiniteVGGT",
            "githubRepoAddedBy": "user",
            "ai_summary": "InfiniteVGGT enables continuous 3D visual geometry understanding through a causal transformer with adaptive memory management, outperforming existing streaming methods in long-term stability while introducing a new benchmark for extended evaluation.",
            "ai_keywords": [
                "visual geometry transformer",
                "causal visual geometry transformer",
                "rolling memory",
                "KV cache",
                "attention-agnostic pruning",
                "FlashAttention",
                "streaming architectures",
                "long-term stability",
                "infinite-horizon inputs",
                "Long3D benchmark"
            ],
            "githubStars": 76,
            "organization": {
                "_id": "68ee0edd23dc954f7744ac27",
                "name": "AutoLab-SJTU",
                "fullname": "AutoLab"
            }
        },
        "publishedAt": "2026-01-05T12:11:00.000Z",
        "title": "InfiniteVGGT: Visual Geometry Grounded Transformer for Endless Streams",
        "summary": "The grand vision of enabling persistent, large-scale 3D visual geometry understanding is shackled by the irreconcilable demands of scalability and long-term stability. While offline models like VGGT achieve inspiring geometry capability, their batch-based nature renders them irrelevant for live systems. Streaming architectures, though the intended solution for live operation, have proven inadequate. Existing methods either fail to support truly infinite-horizon inputs or suffer from catastrophic drift over long sequences. We shatter this long-standing dilemma with InfiniteVGGT, a causal visual geometry transformer that operationalizes the concept of a rolling memory through a bounded yet adaptive and perpetually expressive KV cache. Capitalizing on this, we devise a training-free, attention-agnostic pruning strategy that intelligently discards obsolete information, effectively ``rolling'' the memory forward with each new frame. Fully compatible with FlashAttention, InfiniteVGGT finally alleviates the compromise, enabling infinite-horizon streaming while outperforming existing streaming methods in long-term stability. The ultimate test for such a system is its performance over a truly infinite horizon, a capability that has been impossible to rigorously validate due to the lack of extremely long-term, continuous benchmarks. To address this critical gap, we introduce the Long3D benchmark, which, for the first time, enables a rigorous evaluation of continuous 3D geometry estimation on sequences about 10,000 frames. This provides the definitive evaluation platform for future research in long-term 3D geometry understanding. Code is available at: https://github.com/AutoLab-SAI-SJTU/InfiniteVGGT",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02281.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65b9f710e7c83813628a5cd0",
            "avatarUrl": "/avatars/47075fb646359211b2abe601fa8156d5.svg",
            "fullname": "Yantai Yang",
            "name": "yantaiyang05",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "isUserFollowing": false
        },
        "organization": {
            "_id": "68ee0edd23dc954f7744ac27",
            "name": "AutoLab-SJTU",
            "fullname": "AutoLab"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.24601",
            "authors": [
                {
                    "_id": "695b6f84832867f253525e5a",
                    "name": "Alex L. Zhang",
                    "hidden": false
                },
                {
                    "_id": "695b6f84832867f253525e5b",
                    "name": "Tim Kraska",
                    "hidden": false
                },
                {
                    "_id": "695b6f84832867f253525e5c",
                    "name": "Omar Khattab",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-31T03:43:41.000Z",
            "submittedOnDailyAt": "2026-01-06T04:56:19.174Z",
            "title": "Recursive Language Models",
            "submittedOnDailyBy": {
                "_id": "64b8e82aa62c52b252c827fa",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b8e82aa62c52b252c827fa/Jyk5PHMXCaRlmWy4mT3Bt.jpeg",
                "isPro": true,
                "fullname": "Rajkumar rawal",
                "user": "rajkumarrawal",
                "type": "user"
            },
            "summary": "We study allowing large language models (LLMs) to process arbitrarily long prompts through the lens of inference-time scaling. We propose Recursive Language Models (RLMs), a general inference strategy that treats long prompts as part of an external environment and allows the LLM to programmatically examine, decompose, and recursively call itself over snippets of the prompt. We find that RLMs successfully handle inputs up to two orders of magnitude beyond model context windows and, even for shorter prompts, dramatically outperform the quality of base LLMs and common long-context scaffolds across four diverse long-context tasks, while having comparable (or cheaper) cost per query.",
            "upvotes": 14,
            "discussionId": "695b6f84832867f253525e5d",
            "projectPage": "https://alexzhang13.github.io/blog/2025/rlm/",
            "githubRepo": "https://github.com/alexzhang13/rlm/tree/main",
            "githubRepoAddedBy": "user",
            "githubStars": 675,
            "organization": {
                "_id": "63728bde14d543d507ae970d",
                "name": "MIT",
                "fullname": "Massachusetts Institute of Technology",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"
            }
        },
        "publishedAt": "2025-12-30T22:43:41.000Z",
        "title": "Recursive Language Models",
        "summary": "We study allowing large language models (LLMs) to process arbitrarily long prompts through the lens of inference-time scaling. We propose Recursive Language Models (RLMs), a general inference strategy that treats long prompts as part of an external environment and allows the LLM to programmatically examine, decompose, and recursively call itself over snippets of the prompt. We find that RLMs successfully handle inputs up to two orders of magnitude beyond model context windows and, even for shorter prompts, dramatically outperform the quality of base LLMs and common long-context scaffolds across four diverse long-context tasks, while having comparable (or cheaper) cost per query.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24601.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "64b8e82aa62c52b252c827fa",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b8e82aa62c52b252c827fa/Jyk5PHMXCaRlmWy4mT3Bt.jpeg",
            "fullname": "Rajkumar rawal",
            "name": "rajkumarrawal",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 46,
            "isUserFollowing": false
        },
        "organization": {
            "_id": "63728bde14d543d507ae970d",
            "name": "MIT",
            "fullname": "Massachusetts Institute of Technology",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2601.02346",
            "authors": [
                {
                    "_id": "695c84156aa73bc11f09149d",
                    "name": "Falcon LLM Team",
                    "hidden": false
                },
                {
                    "_id": "695c84156aa73bc11f09149e",
                    "user": {
                        "_id": "660bd11884eca4537c4aeedd",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/660bd11884eca4537c4aeedd/nQ7qIgNJAK3r0DWNR9ysw.jpeg",
                        "isPro": false,
                        "fullname": "Iheb Chaabane",
                        "user": "Iheb-Chaabane",
                        "type": "user"
                    },
                    "name": "Iheb Chaabane",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-06T09:57:42.636Z",
                    "hidden": false
                },
                {
                    "_id": "695c84156aa73bc11f09149f",
                    "user": {
                        "_id": "66430f96463dc8f36d5bf4a4",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66430f96463dc8f36d5bf4a4/V6dUB7SiHUt83s-AIp6P4.jpeg",
                        "isPro": false,
                        "fullname": "Puneesh Khanna",
                        "user": "puneeshkhanna",
                        "type": "user"
                    },
                    "name": "Puneesh Khanna",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-06T09:57:44.813Z",
                    "hidden": false
                },
                {
                    "_id": "695c84156aa73bc11f0914a0",
                    "name": "Suhail Mohmad",
                    "hidden": false
                },
                {
                    "_id": "695c84156aa73bc11f0914a1",
                    "user": {
                        "_id": "66db041dd04c920ebf3198ff",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66db041dd04c920ebf3198ff/yfkeoBpp6ztKi27GglP1z.jpeg",
                        "isPro": false,
                        "fullname": "Slim Frikha",
                        "user": "slimfrikha",
                        "type": "user"
                    },
                    "name": "Slim Frikha",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-06T09:40:40.378Z",
                    "hidden": false
                },
                {
                    "_id": "695c84156aa73bc11f0914a2",
                    "name": "Shi Hu",
                    "hidden": false
                },
                {
                    "_id": "695c84156aa73bc11f0914a3",
                    "user": {
                        "_id": "650476b6594e61e0c4d27ae8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650476b6594e61e0c4d27ae8/-2aF4BX7KpjJ1rDau1b7H.jpeg",
                        "isPro": false,
                        "fullname": "Abdalgader Abubaker",
                        "user": "abdalgader",
                        "type": "user"
                    },
                    "name": "Abdalgader Abubaker",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-06T09:40:35.808Z",
                    "hidden": false
                },
                {
                    "_id": "695c84156aa73bc11f0914a4",
                    "name": "Reda Alami",
                    "hidden": false
                },
                {
                    "_id": "695c84156aa73bc11f0914a5",
                    "name": "Mikhail Lubinets",
                    "hidden": false
                },
                {
                    "_id": "695c84156aa73bc11f0914a6",
                    "user": {
                        "_id": "6460bb7c455531c6be7997bd",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6460bb7c455531c6be7997bd/l0uuYf0rAPnKfvS90jAAr.jpeg",
                        "isPro": false,
                        "fullname": "Mohamed El Amine Seddik",
                        "user": "melaseddik",
                        "type": "user"
                    },
                    "name": "Mohamed El Amine Seddik",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-06T09:40:37.824Z",
                    "hidden": false
                },
                {
                    "_id": "695c84156aa73bc11f0914a7",
                    "name": "Hakim Hacid",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-05T18:44:27.000Z",
            "submittedOnDailyAt": "2026-01-06T01:10:59.614Z",
            "title": "Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "This work introduces Falcon-H1R, a 7B-parameter reasoning-optimized model that establishes the feasibility of achieving competitive reasoning performance with small language models (SLMs). Falcon-H1R stands out for its parameter efficiency, consistently matching or outperforming SOTA reasoning models that are 2times to 7times larger across a variety of reasoning-intensive benchmarks. These results underscore the importance of careful data curation and targeted training strategies (via both efficient SFT and RL scaling) in delivering significant performance gains without increasing model size. Furthermore, Falcon-H1R advances the 3D limits of reasoning efficiency by combining faster inference (through its hybrid-parallel architecture design), token efficiency, and higher accuracy. This unique blend makes Falcon-H1R-7B a practical backbone for scaling advanced reasoning systems, particularly in scenarios requiring extensive chain-of-thoughts generation and parallel test-time scaling. Leveraging the recently introduced DeepConf approach, Falcon-H1R achieves state-of-the-art test-time scaling efficiency, offering substantial improvements in both accuracy and computational cost. As a result, Falcon-H1R demonstrates that compact models, through targeted model training and architectural choices, can deliver robust and scalable reasoning performance.",
            "upvotes": 13,
            "discussionId": "695c84156aa73bc11f0914a8",
            "ai_summary": "Falcon-H1R is a 7B-parameter language model that achieves competitive reasoning performance through efficient training strategies and architectural design, enabling scalable reasoning capabilities in compact models.",
            "ai_keywords": [
                "language models",
                "parameter efficiency",
                "reasoning-optimized model",
                "small language models",
                "SOTA reasoning models",
                "efficient SFT",
                "RL scaling",
                "hybrid-parallel architecture design",
                "token efficiency",
                "chain-of-thoughts generation",
                "test-time scaling",
                "DeepConf approach"
            ],
            "organization": {
                "_id": "6448cad23adf50d86406b0a3",
                "name": "tiiuae",
                "fullname": "Technology Innovation Institute",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61a8d1aac664736898ffc84f/AT6cAB5ZNwCcqFMal71WD.jpeg"
            }
        },
        "publishedAt": "2026-01-05T13:44:27.000Z",
        "title": "Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling",
        "summary": "This work introduces Falcon-H1R, a 7B-parameter reasoning-optimized model that establishes the feasibility of achieving competitive reasoning performance with small language models (SLMs). Falcon-H1R stands out for its parameter efficiency, consistently matching or outperforming SOTA reasoning models that are 2times to 7times larger across a variety of reasoning-intensive benchmarks. These results underscore the importance of careful data curation and targeted training strategies (via both efficient SFT and RL scaling) in delivering significant performance gains without increasing model size. Furthermore, Falcon-H1R advances the 3D limits of reasoning efficiency by combining faster inference (through its hybrid-parallel architecture design), token efficiency, and higher accuracy. This unique blend makes Falcon-H1R-7B a practical backbone for scaling advanced reasoning systems, particularly in scenarios requiring extensive chain-of-thoughts generation and parallel test-time scaling. Leveraging the recently introduced DeepConf approach, Falcon-H1R achieves state-of-the-art test-time scaling efficiency, offering substantial improvements in both accuracy and computational cost. As a result, Falcon-H1R demonstrates that compact models, through targeted model training and architectural choices, can deliver robust and scalable reasoning performance.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02346.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 200,
            "isUserFollowing": false
        },
        "organization": {
            "_id": "6448cad23adf50d86406b0a3",
            "name": "tiiuae",
            "fullname": "Technology Innovation Institute",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61a8d1aac664736898ffc84f/AT6cAB5ZNwCcqFMal71WD.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2601.02356",
            "authors": [
                {
                    "_id": "695c813c6aa73bc11f091475",
                    "user": {
                        "_id": "65367c40061949598892dbdc",
                        "avatarUrl": "/avatars/4baf27263841471cbd5f629a8b99424d.svg",
                        "isPro": false,
                        "fullname": "Jing Tan",
                        "user": "jingtan",
                        "type": "user"
                    },
                    "name": "Jing Tan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-06T09:57:49.343Z",
                    "hidden": false
                },
                {
                    "_id": "695c813c6aa73bc11f091476",
                    "user": {
                        "_id": "658409ceca19ccf6d9989add",
                        "avatarUrl": "/avatars/3ac1dbd25e52435185babdeb3da28875.svg",
                        "isPro": false,
                        "fullname": "Zhaoyang Zhang",
                        "user": "ZyZcuhk",
                        "type": "user"
                    },
                    "name": "Zhaoyang Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-06T09:57:47.322Z",
                    "hidden": false
                },
                {
                    "_id": "695c813c6aa73bc11f091477",
                    "name": "Yantao Shen",
                    "hidden": false
                },
                {
                    "_id": "695c813c6aa73bc11f091478",
                    "name": "Jiarui Cai",
                    "hidden": false
                },
                {
                    "_id": "695c813c6aa73bc11f091479",
                    "name": "Shuo Yang",
                    "hidden": false
                },
                {
                    "_id": "695c813c6aa73bc11f09147a",
                    "name": "Jiajun Wu",
                    "hidden": false
                },
                {
                    "_id": "695c813c6aa73bc11f09147b",
                    "name": "Wei Xia",
                    "hidden": false
                },
                {
                    "_id": "695c813c6aa73bc11f09147c",
                    "name": "Zhuowen Tu",
                    "hidden": false
                },
                {
                    "_id": "695c813c6aa73bc11f09147d",
                    "name": "Stefano Soatto",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/Aih52OAIypmz2lxbcxOv-.mp4"
            ],
            "publishedAt": "2026-01-05T18:55:32.000Z",
            "submittedOnDailyAt": "2026-01-06T00:57:59.956Z",
            "title": "Talk2Move: Reinforcement Learning for Text-Instructed Object-Level Geometric Transformation in Scenes",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We introduce Talk2Move, a reinforcement learning (RL) based diffusion framework for text-instructed spatial transformation of objects within scenes. Spatially manipulating objects in a scene through natural language poses a challenge for multimodal generation systems. While existing text-based manipulation methods can adjust appearance or style, they struggle to perform object-level geometric transformations-such as translating, rotating, or resizing objects-due to scarce paired supervision and pixel-level optimization limits. Talk2Move employs Group Relative Policy Optimization (GRPO) to explore geometric actions through diverse rollouts generated from input images and lightweight textual variations, removing the need for costly paired data. A spatial reward guided model aligns geometric transformations with linguistic description, while off-policy step evaluation and active step sampling improve learning efficiency by focusing on informative transformation stages. Furthermore, we design object-centric spatial rewards that evaluate displacement, rotation, and scaling behaviors directly, enabling interpretable and coherent transformations. Experiments on curated benchmarks demonstrate that Talk2Move achieves precise, consistent, and semantically faithful object transformations, outperforming existing text-guided editing approaches in both spatial accuracy and scene coherence.",
            "upvotes": 11,
            "discussionId": "695c813c6aa73bc11f09147e",
            "projectPage": "https://sparkstj.github.io/talk2move/",
            "githubRepo": "https://github.com/sparkstj/Talk2Move",
            "githubRepoAddedBy": "user",
            "ai_summary": "Talk2Move presents a reinforcement learning-based diffusion framework that enables precise, semantically faithful spatial transformations of objects in scenes using natural language instructions.",
            "ai_keywords": [
                "reinforcement learning",
                "diffusion framework",
                "text-instructed spatial transformation",
                "Group Relative Policy Optimization",
                "geometric actions",
                "spatial reward guided model",
                "off-policy step evaluation",
                "active step sampling",
                "object-centric spatial rewards",
                "spatial accuracy",
                "scene coherence"
            ],
            "githubStars": 13
        },
        "publishedAt": "2026-01-05T13:55:32.000Z",
        "title": "Talk2Move: Reinforcement Learning for Text-Instructed Object-Level Geometric Transformation in Scenes",
        "summary": "We introduce Talk2Move, a reinforcement learning (RL) based diffusion framework for text-instructed spatial transformation of objects within scenes. Spatially manipulating objects in a scene through natural language poses a challenge for multimodal generation systems. While existing text-based manipulation methods can adjust appearance or style, they struggle to perform object-level geometric transformations-such as translating, rotating, or resizing objects-due to scarce paired supervision and pixel-level optimization limits. Talk2Move employs Group Relative Policy Optimization (GRPO) to explore geometric actions through diverse rollouts generated from input images and lightweight textual variations, removing the need for costly paired data. A spatial reward guided model aligns geometric transformations with linguistic description, while off-policy step evaluation and active step sampling improve learning efficiency by focusing on informative transformation stages. Furthermore, we design object-centric spatial rewards that evaluate displacement, rotation, and scaling behaviors directly, enabling interpretable and coherent transformations. Experiments on curated benchmarks demonstrate that Talk2Move achieves precise, consistent, and semantically faithful object transformations, outperforming existing text-guided editing approaches in both spatial accuracy and scene coherence.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/Aih52OAIypmz2lxbcxOv-.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02356.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 200,
            "isUserFollowing": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2601.02179",
            "authors": [
                {
                    "_id": "695d1b7ac03d6d81e4399d2b",
                    "name": "Caiqi Zhang",
                    "hidden": false
                },
                {
                    "_id": "695d1b7ac03d6d81e4399d2c",
                    "name": "Ruihan Yang",
                    "hidden": false
                },
                {
                    "_id": "695d1b7ac03d6d81e4399d2d",
                    "name": "Xiaochen Zhu",
                    "hidden": false
                },
                {
                    "_id": "695d1b7ac03d6d81e4399d2e",
                    "name": "Chengzu Li",
                    "hidden": false
                },
                {
                    "_id": "695d1b7ac03d6d81e4399d2f",
                    "name": "Tiancheng Hu",
                    "hidden": false
                },
                {
                    "_id": "695d1b7ac03d6d81e4399d30",
                    "name": "Yijiang River Dong",
                    "hidden": false
                },
                {
                    "_id": "695d1b7ac03d6d81e4399d31",
                    "name": "Deqing Yang",
                    "hidden": false
                },
                {
                    "_id": "695d1b7ac03d6d81e4399d32",
                    "name": "Nigel Collier",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-05T14:58:04.000Z",
            "submittedOnDailyAt": "2026-01-06T11:57:24.455Z",
            "title": "Confidence Estimation for LLMs in Multi-turn Interactions",
            "submittedOnDailyBy": {
                "_id": "63920dfac47e36ddeb8f1864",
                "avatarUrl": "/avatars/c36cbf7b084d62368312e5c9292e4260.svg",
                "isPro": false,
                "fullname": "Caiqi Zhang",
                "user": "caiqizh",
                "type": "user"
            },
            "summary": "While confidence estimation is a promising direction for mitigating hallucinations in Large Language Models (LLMs), current research dominantly focuses on single-turn settings. The dynamics of model confidence in multi-turn conversations, where context accumulates and ambiguity is progressively resolved, remain largely unexplored. Reliable confidence estimation in multi-turn settings is critical for many downstream applications, such as autonomous agents and human-in-the-loop systems. This work presents the first systematic study of confidence estimation in multi-turn interactions, establishing a formal evaluation framework grounded in two key desiderata: per-turn calibration and monotonicity of confidence as more information becomes available. To facilitate this, we introduce novel metrics, including a length-normalized Expected Calibration Error (InfoECE), and a new \"Hinter-Guesser\" paradigm for generating controlled evaluation datasets. Our experiments reveal that widely-used confidence techniques struggle with calibration and monotonicity in multi-turn dialogues. We propose P(Sufficient), a logit-based probe that achieves comparatively better performance, although the task remains far from solved. Our work provides a foundational methodology for developing more reliable and trustworthy conversational agents.",
            "upvotes": 7,
            "discussionId": "695d1b7ac03d6d81e4399d33",
            "ai_summary": "Multi-turn conversation confidence estimation lacks systematic evaluation frameworks, prompting the introduction of novel metrics and a \"Hinter-Guesser\" paradigm for controlled dataset generation to improve calibration and monotonicity.",
            "ai_keywords": [
                "confidence estimation",
                "large language models",
                "multi-turn conversations",
                "calibration",
                "monotonicity",
                "Expected Calibration Error",
                "logit-based probe"
            ],
            "organization": {
                "_id": "679c9d2bb741486264125a9a",
                "name": "uni-cambridge",
                "fullname": "University of Cambridge",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/679c98caac2b47d6f4132f9b/5tGEF02r7vvH94c0pQ5_0.png"
            }
        },
        "publishedAt": "2026-01-05T09:58:04.000Z",
        "title": "Confidence Estimation for LLMs in Multi-turn Interactions",
        "summary": "While confidence estimation is a promising direction for mitigating hallucinations in Large Language Models (LLMs), current research dominantly focuses on single-turn settings. The dynamics of model confidence in multi-turn conversations, where context accumulates and ambiguity is progressively resolved, remain largely unexplored. Reliable confidence estimation in multi-turn settings is critical for many downstream applications, such as autonomous agents and human-in-the-loop systems. This work presents the first systematic study of confidence estimation in multi-turn interactions, establishing a formal evaluation framework grounded in two key desiderata: per-turn calibration and monotonicity of confidence as more information becomes available. To facilitate this, we introduce novel metrics, including a length-normalized Expected Calibration Error (InfoECE), and a new \"Hinter-Guesser\" paradigm for generating controlled evaluation datasets. Our experiments reveal that widely-used confidence techniques struggle with calibration and monotonicity in multi-turn dialogues. We propose P(Sufficient), a logit-based probe that achieves comparatively better performance, although the task remains far from solved. Our work provides a foundational methodology for developing more reliable and trustworthy conversational agents.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02179.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63920dfac47e36ddeb8f1864",
            "avatarUrl": "/avatars/c36cbf7b084d62368312e5c9292e4260.svg",
            "fullname": "Caiqi Zhang",
            "name": "caiqizh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "isUserFollowing": false
        },
        "organization": {
            "_id": "679c9d2bb741486264125a9a",
            "name": "uni-cambridge",
            "fullname": "University of Cambridge",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/679c98caac2b47d6f4132f9b/5tGEF02r7vvH94c0pQ5_0.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2601.01046",
            "authors": [
                {
                    "_id": "695c79206aa73bc11f09140d",
                    "name": "Yixuan Tang",
                    "hidden": false
                },
                {
                    "_id": "695c79206aa73bc11f09140e",
                    "name": "Yi Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-03T02:55:43.000Z",
            "submittedOnDailyAt": "2026-01-06T00:46:57.123Z",
            "title": "KV-Embedding: Training-free Text Embedding via Internal KV Re-routing in Decoder-only LLMs",
            "submittedOnDailyBy": {
                "_id": "647d834618274bce03013cc2",
                "avatarUrl": "/avatars/a95c7df96dc4fb6a96193f6dd5068227.svg",
                "isPro": true,
                "fullname": "yixuan",
                "user": "yixuantt",
                "type": "user"
            },
            "summary": "While LLMs are powerful embedding backbones, their application in training-free settings faces two structural challenges: causal attention restricts early tokens from accessing subsequent context, and the next-token prediction objective biases representations toward generation rather than semantic compression. To address these limitations, we propose KV-Embedding, a framework that activates the latent representation power of frozen LLMs. Our method leverages the observation that the key-value (KV) states of the final token at each layer encode a compressed view of the sequence. By re-routing these states as a prepended prefix, we enable all tokens to access sequence-level context within a single forward pass. To ensure model-agnostic applicability, we introduce an automated layer selection strategy based on intrinsic dimensionality. Evaluations on MTEB across Qwen, Mistral, and Llama backbones show that KV-Embedding outperforms existing training-free baselines by up to 10%, while maintaining robust performance on sequences up to 4,096 tokens. These results demonstrate that internal state manipulation offers an efficient alternative to input modification, and we hope this work encourages further exploration of LLM internals for representation learning.",
            "upvotes": 5,
            "discussionId": "695c79216aa73bc11f09140f",
            "ai_summary": "KV-Embedding enables training-free representation learning from frozen LLMs by utilizing key-value states for enhanced context access and automated layer selection.",
            "ai_keywords": [
                "LLMs",
                "causal attention",
                "next-token prediction",
                "latent representation",
                "key-value states",
                "pre-trained models",
                "training-free",
                "intrinsic dimensionality",
                "representation learning"
            ]
        },
        "publishedAt": "2026-01-02T21:55:43.000Z",
        "title": "KV-Embedding: Training-free Text Embedding via Internal KV Re-routing in Decoder-only LLMs",
        "summary": "While LLMs are powerful embedding backbones, their application in training-free settings faces two structural challenges: causal attention restricts early tokens from accessing subsequent context, and the next-token prediction objective biases representations toward generation rather than semantic compression. To address these limitations, we propose KV-Embedding, a framework that activates the latent representation power of frozen LLMs. Our method leverages the observation that the key-value (KV) states of the final token at each layer encode a compressed view of the sequence. By re-routing these states as a prepended prefix, we enable all tokens to access sequence-level context within a single forward pass. To ensure model-agnostic applicability, we introduce an automated layer selection strategy based on intrinsic dimensionality. Evaluations on MTEB across Qwen, Mistral, and Llama backbones show that KV-Embedding outperforms existing training-free baselines by up to 10%, while maintaining robust performance on sequences up to 4,096 tokens. These results demonstrate that internal state manipulation offers an efficient alternative to input modification, and we hope this work encourages further exploration of LLM internals for representation learning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.01046.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "647d834618274bce03013cc2",
            "avatarUrl": "/avatars/a95c7df96dc4fb6a96193f6dd5068227.svg",
            "fullname": "yixuan",
            "name": "yixuantt",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 10,
            "isUserFollowing": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2601.00501",
            "authors": [
                {
                    "_id": "695d52e7c03d6d81e4399dce",
                    "name": "Ahmad Rezaei",
                    "hidden": false
                },
                {
                    "_id": "695d52e7c03d6d81e4399dcf",
                    "name": "Mohsen Gholami",
                    "hidden": false
                },
                {
                    "_id": "695d52e7c03d6d81e4399dd0",
                    "name": "Saeed Ranjbar Alvar",
                    "hidden": false
                },
                {
                    "_id": "695d52e7c03d6d81e4399dd1",
                    "name": "Kevin Cannons",
                    "hidden": false
                },
                {
                    "_id": "695d52e7c03d6d81e4399dd2",
                    "name": "Mohammad Asiful Hossain",
                    "hidden": false
                },
                {
                    "_id": "695d52e7c03d6d81e4399dd3",
                    "name": "Zhou Weimin",
                    "hidden": false
                },
                {
                    "_id": "695d52e7c03d6d81e4399dd4",
                    "name": "Shunbo Zhou",
                    "hidden": false
                },
                {
                    "_id": "695d52e7c03d6d81e4399dd5",
                    "name": "Yong Zhang",
                    "hidden": false
                },
                {
                    "_id": "695d52e7c03d6d81e4399dd6",
                    "name": "Mohammad Akbari",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-01T22:48:26.000Z",
            "submittedOnDailyAt": "2026-01-06T15:57:49.685Z",
            "title": "CPPO: Contrastive Perception for Vision Language Policy Optimization",
            "submittedOnDailyBy": {
                "_id": "6495d9b6e6692d3676406834",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/W7ktMxTNo75rqUtTWJfCw.png",
                "isPro": false,
                "fullname": "Ahmad Rezaei",
                "user": "AhNr",
                "type": "user"
            },
            "summary": "We introduce CPPO, a Contrastive Perception Policy Optimization method for finetuning vision-language models (VLMs). While reinforcement learning (RL) has advanced reasoning in language models, extending it to multimodal reasoning requires improving both the perception and reasoning aspects. Prior works tackle this challenge mainly with explicit perception rewards, but disentangling perception tokens from reasoning tokens is difficult, requiring extra LLMs, ground-truth data, forced separation of perception from reasoning by policy model, or applying rewards indiscriminately to all output tokens. CPPO addresses this problem by detecting perception tokens via entropy shifts in the model outputs under perturbed input images. CPPO then extends the RL objective function with a Contrastive Perception Loss (CPL) that enforces consistency under information-preserving perturbations and sensitivity under information-removing ones. Experiments show that CPPO surpasses previous perception-rewarding methods, while avoiding extra models, making training more efficient and scalable.",
            "upvotes": 5,
            "discussionId": "695d52e7c03d6d81e4399dd7",
            "ai_summary": "CPPO improves vision-language model fine-tuning by detecting perception tokens through entropy shifts and using contrastive perception loss to enhance multimodal reasoning efficiency.",
            "ai_keywords": [
                "Contrastive Perception Policy Optimization",
                "vision-language models",
                "reinforcement learning",
                "perception tokens",
                "reasoning tokens",
                "entropy shifts",
                "Contrastive Perception Loss",
                "information-preserving perturbations",
                "information-removing perturbations"
            ]
        },
        "publishedAt": "2026-01-01T17:48:26.000Z",
        "title": "CPPO: Contrastive Perception for Vision Language Policy Optimization",
        "summary": "We introduce CPPO, a Contrastive Perception Policy Optimization method for finetuning vision-language models (VLMs). While reinforcement learning (RL) has advanced reasoning in language models, extending it to multimodal reasoning requires improving both the perception and reasoning aspects. Prior works tackle this challenge mainly with explicit perception rewards, but disentangling perception tokens from reasoning tokens is difficult, requiring extra LLMs, ground-truth data, forced separation of perception from reasoning by policy model, or applying rewards indiscriminately to all output tokens. CPPO addresses this problem by detecting perception tokens via entropy shifts in the model outputs under perturbed input images. CPPO then extends the RL objective function with a Contrastive Perception Loss (CPL) that enforces consistency under information-preserving perturbations and sensitivity under information-removing ones. Experiments show that CPPO surpasses previous perception-rewarding methods, while avoiding extra models, making training more efficient and scalable.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.00501.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6495d9b6e6692d3676406834",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/W7ktMxTNo75rqUtTWJfCw.png",
            "fullname": "Ahmad Rezaei",
            "name": "AhNr",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "isUserFollowing": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2601.02267",
            "authors": [
                {
                    "_id": "695c7c1f6aa73bc11f09142d",
                    "user": {
                        "_id": "643ceae2b561f17fa467451d",
                        "avatarUrl": "/avatars/3827b922f74a5d6ea6ba326a18ee4892.svg",
                        "isPro": false,
                        "fullname": "dblue",
                        "user": "wrk226",
                        "type": "user"
                    },
                    "name": "Renke Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-06T09:58:03.279Z",
                    "hidden": false
                },
                {
                    "_id": "695c7c1f6aa73bc11f09142e",
                    "name": "Zhenyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "695c7c1f6aa73bc11f09142f",
                    "name": "Ying Tai",
                    "hidden": false
                },
                {
                    "_id": "695c7c1f6aa73bc11f091430",
                    "name": "Jian Yang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/643ceae2b561f17fa467451d/sjJ65uNV0d7C_IMAOKH6S.png",
                "https://cdn-uploads.huggingface.co/production/uploads/643ceae2b561f17fa467451d/4fXkavuqZIXVQ5QThZQkV.mp4",
                "https://cdn-uploads.huggingface.co/production/uploads/643ceae2b561f17fa467451d/cwl5govX08symsUFkZJxr.mp4"
            ],
            "publishedAt": "2026-01-05T16:51:45.000Z",
            "submittedOnDailyAt": "2026-01-06T13:42:25.712Z",
            "title": "DiffProxy: Multi-View Human Mesh Recovery via Diffusion-Generated Dense Proxies",
            "submittedOnDailyBy": {
                "_id": "643ceae2b561f17fa467451d",
                "avatarUrl": "/avatars/3827b922f74a5d6ea6ba326a18ee4892.svg",
                "isPro": false,
                "fullname": "dblue",
                "user": "wrk226",
                "type": "user"
            },
            "summary": "Human mesh recovery from multi-view images faces a fundamental challenge: real-world datasets contain imperfect ground-truth annotations that bias the models' training, while synthetic data with precise supervision suffers from domain gap. In this paper, we propose DiffProxy, a novel framework that generates multi-view consistent human proxies for mesh recovery. Central to DiffProxy is leveraging the diffusion-based generative priors to bridge the synthetic training and real-world generalization. Its key innovations include: (1) a multi-conditional mechanism for generating multi-view consistent, pixel-aligned human proxies; (2) a hand refinement module that incorporates flexible visual prompts to enhance local details; and (3) an uncertainty-aware test-time scaling method that increases robustness to challenging cases during optimization. These designs ensure that the mesh recovery process effectively benefits from the precise synthetic ground truth and generative advantages of the diffusion-based pipeline. Trained entirely on synthetic data, DiffProxy achieves state-of-the-art performance across five real-world benchmarks, demonstrating strong zero-shot generalization particularly on challenging scenarios with occlusions and partial views. Project page: https://wrk226.github.io/DiffProxy.html",
            "upvotes": 4,
            "discussionId": "695c7c1f6aa73bc11f091431",
            "projectPage": "https://wrk226.github.io/DiffProxy.html",
            "githubRepo": "https://github.com/wrk226/DiffProxy",
            "githubRepoAddedBy": "user",
            "ai_summary": "DiffProxy enables human mesh recovery from multi-view images by generating multi-view consistent proxies using diffusion-based generative priors, achieving state-of-the-art performance through synthetic training and robust test-time scaling.",
            "ai_keywords": [
                "diffusion-based generative priors",
                "multi-view consistent human proxies",
                "hand refinement module",
                "uncertainty-aware test-time scaling",
                "zero-shot generalization",
                "multi-conditional mechanism",
                "pixel-aligned human proxies"
            ],
            "githubStars": 1
        },
        "publishedAt": "2026-01-05T11:51:45.000Z",
        "title": "DiffProxy: Multi-View Human Mesh Recovery via Diffusion-Generated Dense Proxies",
        "summary": "Human mesh recovery from multi-view images faces a fundamental challenge: real-world datasets contain imperfect ground-truth annotations that bias the models' training, while synthetic data with precise supervision suffers from domain gap. In this paper, we propose DiffProxy, a novel framework that generates multi-view consistent human proxies for mesh recovery. Central to DiffProxy is leveraging the diffusion-based generative priors to bridge the synthetic training and real-world generalization. Its key innovations include: (1) a multi-conditional mechanism for generating multi-view consistent, pixel-aligned human proxies; (2) a hand refinement module that incorporates flexible visual prompts to enhance local details; and (3) an uncertainty-aware test-time scaling method that increases robustness to challenging cases during optimization. These designs ensure that the mesh recovery process effectively benefits from the precise synthetic ground truth and generative advantages of the diffusion-based pipeline. Trained entirely on synthetic data, DiffProxy achieves state-of-the-art performance across five real-world benchmarks, demonstrating strong zero-shot generalization particularly on challenging scenarios with occlusions and partial views. Project page: https://wrk226.github.io/DiffProxy.html",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/643ceae2b561f17fa467451d/sjJ65uNV0d7C_IMAOKH6S.png",
            "https://cdn-uploads.huggingface.co/production/uploads/643ceae2b561f17fa467451d/4fXkavuqZIXVQ5QThZQkV.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/643ceae2b561f17fa467451d/cwl5govX08symsUFkZJxr.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02267.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "643ceae2b561f17fa467451d",
            "avatarUrl": "/avatars/3827b922f74a5d6ea6ba326a18ee4892.svg",
            "fullname": "dblue",
            "name": "wrk226",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "isUserFollowing": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2601.01836",
            "authors": [
                {
                    "_id": "695c87e66aa73bc11f0914bb",
                    "user": {
                        "_id": "66120647cac232c1507e13da",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66120647cac232c1507e13da/iOBrsyr2mVdVLAFF6pZt6.png",
                        "isPro": false,
                        "fullname": "DasolChoi",
                        "user": "Dasool",
                        "type": "user"
                    },
                    "name": "Dasol Choi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-06T09:40:31.785Z",
                    "hidden": false
                },
                {
                    "_id": "695c87e66aa73bc11f0914bc",
                    "user": {
                        "_id": "6540fbf9cb7fffd683942b43",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6540fbf9cb7fffd683942b43/KSB9DPtpF7yv5Dl5vurAH.png",
                        "isPro": false,
                        "fullname": "DongGeon Lee",
                        "user": "oneonlee",
                        "type": "user"
                    },
                    "name": "DongGeon Lee",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-06T09:40:33.640Z",
                    "hidden": false
                },
                {
                    "_id": "695c87e66aa73bc11f0914bd",
                    "name": "Brigitta Jesica Kartono",
                    "hidden": false
                },
                {
                    "_id": "695c87e66aa73bc11f0914be",
                    "name": "Helena Berndt",
                    "hidden": false
                },
                {
                    "_id": "695c87e66aa73bc11f0914bf",
                    "name": "Taeyoun Kwon",
                    "hidden": false
                },
                {
                    "_id": "695c87e66aa73bc11f0914c0",
                    "name": "Joonwon Jang",
                    "hidden": false
                },
                {
                    "_id": "695c87e66aa73bc11f0914c1",
                    "name": "Haon Park",
                    "hidden": false
                },
                {
                    "_id": "695c87e66aa73bc11f0914c2",
                    "name": "Hwanjo Yu",
                    "hidden": false
                },
                {
                    "_id": "695c87e66aa73bc11f0914c3",
                    "name": "Minsuk Kahng",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-05T06:57:45.000Z",
            "submittedOnDailyAt": "2026-01-06T02:12:10.795Z",
            "title": "COMPASS: A Framework for Evaluating Organization-Specific Policy Alignment in LLMs",
            "submittedOnDailyBy": {
                "_id": "6540fbf9cb7fffd683942b43",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6540fbf9cb7fffd683942b43/KSB9DPtpF7yv5Dl5vurAH.png",
                "isPro": false,
                "fullname": "DongGeon Lee",
                "user": "oneonlee",
                "type": "user"
            },
            "summary": "As large language models are deployed in high-stakes enterprise applications, from healthcare to finance, ensuring adherence to organization-specific policies has become essential. Yet existing safety evaluations focus exclusively on universal harms. We present COMPASS (Company/Organization Policy Alignment Assessment), the first systematic framework for evaluating whether LLMs comply with organizational allowlist and denylist policies. We apply COMPASS to eight diverse industry scenarios, generating and validating 5,920 queries that test both routine compliance and adversarial robustness through strategically designed edge cases. Evaluating seven state-of-the-art models, we uncover a fundamental asymmetry: models reliably handle legitimate requests (>95% accuracy) but catastrophically fail at enforcing prohibitions, refusing only 13-40% of adversarial denylist violations. These results demonstrate that current LLMs lack the robustness required for policy-critical deployments, establishing COMPASS as an essential evaluation framework for organizational AI safety.",
            "upvotes": 4,
            "discussionId": "695c87e66aa73bc11f0914c4",
            "githubRepo": "https://github.com/AIM-Intelligence/COMPASS",
            "githubRepoAddedBy": "user",
            "ai_summary": "COMPASS evaluates large language models' compliance with organizational policies, revealing significant gaps in enforcing prohibitions despite strong performance on legitimate requests.",
            "ai_keywords": [
                "large language models",
                "organizational policies",
                "allowlist",
                "denylist",
                "adversarial robustness",
                "policy-critical deployments",
                "organizational AI safety"
            ],
            "githubStars": 8,
            "organization": {
                "_id": "66dedbfa524118c3638b2370",
                "name": "AIM-Intelligence",
                "fullname": "AIM Intelligence",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6237dfd72d85ea7b3051d5bb/4ziksWDvCH9aF1hnr5SW8.png"
            }
        },
        "publishedAt": "2026-01-05T01:57:45.000Z",
        "title": "COMPASS: A Framework for Evaluating Organization-Specific Policy Alignment in LLMs",
        "summary": "As large language models are deployed in high-stakes enterprise applications, from healthcare to finance, ensuring adherence to organization-specific policies has become essential. Yet existing safety evaluations focus exclusively on universal harms. We present COMPASS (Company/Organization Policy Alignment Assessment), the first systematic framework for evaluating whether LLMs comply with organizational allowlist and denylist policies. We apply COMPASS to eight diverse industry scenarios, generating and validating 5,920 queries that test both routine compliance and adversarial robustness through strategically designed edge cases. Evaluating seven state-of-the-art models, we uncover a fundamental asymmetry: models reliably handle legitimate requests (>95% accuracy) but catastrophically fail at enforcing prohibitions, refusing only 13-40% of adversarial denylist violations. These results demonstrate that current LLMs lack the robustness required for policy-critical deployments, establishing COMPASS as an essential evaluation framework for organizational AI safety.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.01836.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6540fbf9cb7fffd683942b43",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6540fbf9cb7fffd683942b43/KSB9DPtpF7yv5Dl5vurAH.png",
            "fullname": "DongGeon Lee",
            "name": "oneonlee",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5,
            "isUserFollowing": false
        },
        "organization": {
            "_id": "66dedbfa524118c3638b2370",
            "name": "AIM-Intelligence",
            "fullname": "AIM Intelligence",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6237dfd72d85ea7b3051d5bb/4ziksWDvCH9aF1hnr5SW8.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2601.01426",
            "authors": [
                {
                    "_id": "695c7a8a6aa73bc11f091411",
                    "name": "Chaofan Tao",
                    "hidden": false
                },
                {
                    "_id": "695c7a8a6aa73bc11f091412",
                    "name": "Jierun Chen",
                    "hidden": false
                },
                {
                    "_id": "695c7a8a6aa73bc11f091413",
                    "user": {
                        "_id": "63c20105726f62e411fbe882",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c20105726f62e411fbe882/2UsU9O2psbDjJzz-sAmGH.jpeg",
                        "isPro": false,
                        "fullname": "Yuxin Jiang",
                        "user": "YuxinJiang",
                        "type": "user"
                    },
                    "name": "Yuxin Jiang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-06T09:58:05.382Z",
                    "hidden": false
                },
                {
                    "_id": "695c7a8a6aa73bc11f091414",
                    "name": "Kaiqi Kou",
                    "hidden": false
                },
                {
                    "_id": "695c7a8a6aa73bc11f091415",
                    "name": "Shaowei Wang",
                    "hidden": false
                },
                {
                    "_id": "695c7a8a6aa73bc11f091416",
                    "name": "Ruoyu Wang",
                    "hidden": false
                },
                {
                    "_id": "695c7a8a6aa73bc11f091417",
                    "name": "Xiaohui Li",
                    "hidden": false
                },
                {
                    "_id": "695c7a8a6aa73bc11f091418",
                    "name": "Sidi Yang",
                    "hidden": false
                },
                {
                    "_id": "695c7a8a6aa73bc11f091419",
                    "name": "Yiming Du",
                    "hidden": false
                },
                {
                    "_id": "695c7a8a6aa73bc11f09141a",
                    "name": "Jianbo Dai",
                    "hidden": false
                },
                {
                    "_id": "695c7a8a6aa73bc11f09141b",
                    "name": "Zhiming Mao",
                    "hidden": false
                },
                {
                    "_id": "695c7a8a6aa73bc11f09141c",
                    "name": "Xinyu Wang",
                    "hidden": false
                },
                {
                    "_id": "695c7a8a6aa73bc11f09141d",
                    "name": "Lifeng Shang",
                    "hidden": false
                },
                {
                    "_id": "695c7a8a6aa73bc11f09141e",
                    "name": "Haoli Bai",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-04T08:07:27.000Z",
            "submittedOnDailyAt": "2026-01-06T06:29:01.298Z",
            "title": "SWE-Lego: Pushing the Limits of Supervised Fine-tuning for Software Issue Resolving",
            "submittedOnDailyBy": {
                "_id": "63c20105726f62e411fbe882",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c20105726f62e411fbe882/2UsU9O2psbDjJzz-sAmGH.jpeg",
                "isPro": false,
                "fullname": "Yuxin Jiang",
                "user": "YuxinJiang",
                "type": "user"
            },
            "summary": "We present SWE-Lego, a supervised fine-tuning (SFT) recipe designed to achieve state-ofthe-art performance in software engineering (SWE) issue resolving. In contrast to prevalent methods that rely on complex training paradigms (e.g., mid-training, SFT, reinforcement learning, and their combinations), we explore how to push the limits of a lightweight SFT-only approach for SWE tasks. SWE-Lego comprises three core building blocks, with key findings summarized as follows: 1) the SWE-Lego dataset, a collection of 32k highquality task instances and 18k validated trajectories, combining real and synthetic data to complement each other in both quality and quantity; 2) a refined SFT procedure with error masking and a difficulty-based curriculum, which demonstrably improves action quality and overall performance. Empirical results show that with these two building bricks alone,the SFT can push SWE-Lego models to state-of-the-art performance among open-source models of comparable size on SWE-bench Verified: SWE-Lego-Qwen3-8B reaches 42.2%, and SWE-Lego-Qwen3-32B attains 52.6%. 3) We further evaluate and improve test-time scaling (TTS) built upon the SFT foundation. Based on a well-trained verifier, SWE-Lego models can be significantly boosted--for example, 42.2% to 49.6% and 52.6% to 58.8% under TTS@16 for the 8B and 32B models, respectively.",
            "upvotes": 4,
            "discussionId": "695c7a8a6aa73bc11f09141f",
            "projectPage": "https://github.com/SWE-Lego/SWE-Lego",
            "githubRepo": "https://github.com/SWE-Lego/SWE-Lego",
            "githubRepoAddedBy": "user",
            "ai_summary": "SWE-Lego achieves state-of-the-art performance in software engineering task resolution through a lightweight supervised fine-tuning approach combined with a curated dataset and refined training procedures.",
            "ai_keywords": [
                "supervised fine-tuning",
                "SWE-bench Verified",
                "error masking",
                "difficulty-based curriculum",
                "test-time scaling",
                "verifier"
            ],
            "githubStars": 8,
            "organization": {
                "_id": "6951e68428a36c372970db39",
                "name": "SWE-Lego",
                "fullname": "SWE-Lego",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60fc2fcca6bdebbe52dfdaf4/AeuYwUH-CQCt893qnmAGa.png"
            }
        },
        "publishedAt": "2026-01-04T03:07:27.000Z",
        "title": "SWE-Lego: Pushing the Limits of Supervised Fine-tuning for Software Issue Resolving",
        "summary": "We present SWE-Lego, a supervised fine-tuning (SFT) recipe designed to achieve state-ofthe-art performance in software engineering (SWE) issue resolving. In contrast to prevalent methods that rely on complex training paradigms (e.g., mid-training, SFT, reinforcement learning, and their combinations), we explore how to push the limits of a lightweight SFT-only approach for SWE tasks. SWE-Lego comprises three core building blocks, with key findings summarized as follows: 1) the SWE-Lego dataset, a collection of 32k highquality task instances and 18k validated trajectories, combining real and synthetic data to complement each other in both quality and quantity; 2) a refined SFT procedure with error masking and a difficulty-based curriculum, which demonstrably improves action quality and overall performance. Empirical results show that with these two building bricks alone,the SFT can push SWE-Lego models to state-of-the-art performance among open-source models of comparable size on SWE-bench Verified: SWE-Lego-Qwen3-8B reaches 42.2%, and SWE-Lego-Qwen3-32B attains 52.6%. 3) We further evaluate and improve test-time scaling (TTS) built upon the SFT foundation. Based on a well-trained verifier, SWE-Lego models can be significantly boosted--for example, 42.2% to 49.6% and 52.6% to 58.8% under TTS@16 for the 8B and 32B models, respectively.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.01426.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "63c20105726f62e411fbe882",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c20105726f62e411fbe882/2UsU9O2psbDjJzz-sAmGH.jpeg",
            "fullname": "Yuxin Jiang",
            "name": "YuxinJiang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4,
            "isUserFollowing": false
        },
        "organization": {
            "_id": "6951e68428a36c372970db39",
            "name": "SWE-Lego",
            "fullname": "SWE-Lego",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60fc2fcca6bdebbe52dfdaf4/AeuYwUH-CQCt893qnmAGa.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.23035",
            "authors": [
                {
                    "_id": "695ca3916aa73bc11f091522",
                    "name": "Yi Zhou",
                    "hidden": false
                },
                {
                    "_id": "695ca3916aa73bc11f091523",
                    "name": "Xuechao Zou",
                    "hidden": false
                },
                {
                    "_id": "695ca3916aa73bc11f091524",
                    "name": "Shun Zhang",
                    "hidden": false
                },
                {
                    "_id": "695ca3916aa73bc11f091525",
                    "name": "Kai Li",
                    "hidden": false
                },
                {
                    "_id": "695ca3916aa73bc11f091526",
                    "name": "Shiying Wang",
                    "hidden": false
                },
                {
                    "_id": "695ca3916aa73bc11f091527",
                    "name": "Jingming Chen",
                    "hidden": false
                },
                {
                    "_id": "695ca3916aa73bc11f091528",
                    "name": "Congyan Lang",
                    "hidden": false
                },
                {
                    "_id": "695ca3916aa73bc11f091529",
                    "name": "Tengfei Cao",
                    "hidden": false
                },
                {
                    "_id": "695ca3916aa73bc11f09152a",
                    "name": "Pin Tao",
                    "hidden": false
                },
                {
                    "_id": "695ca3916aa73bc11f09152b",
                    "name": "Yuanchun Shi",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6617af2beab5eef6b1e8bb9e/f6PYGb8HrnOgJZMhhQrXU.png",
                "https://cdn-uploads.huggingface.co/production/uploads/6617af2beab5eef6b1e8bb9e/RR1EFPfHSpHXHT09lXfVN.png",
                "https://cdn-uploads.huggingface.co/production/uploads/6617af2beab5eef6b1e8bb9e/-u6q-jRaEuWhdHHNEnpc2.png"
            ],
            "publishedAt": "2025-12-28T18:24:19.000Z",
            "submittedOnDailyAt": "2026-01-06T03:35:49.228Z",
            "title": "Toward Stable Semi-Supervised Remote Sensing Segmentation via Co-Guidance and Co-Fusion",
            "submittedOnDailyBy": {
                "_id": "6617af2beab5eef6b1e8bb9e",
                "avatarUrl": "/avatars/d939c02027916331d4c44119565f2ca6.svg",
                "isPro": false,
                "fullname": "XavierJiezou",
                "user": "XavierJiezou",
                "type": "user"
            },
            "summary": "Semi-supervised remote sensing (RS) image semantic segmentation offers a promising solution to alleviate the burden of exhaustive annotation, yet it fundamentally struggles with pseudo-label drift, a phenomenon where confirmation bias leads to the accumulation of errors during training. In this work, we propose Co2S, a stable semi-supervised RS segmentation framework that synergistically fuses priors from vision-language models and self-supervised models. Specifically, we construct a heterogeneous dual-student architecture comprising two distinct ViT-based vision foundation models initialized with pretrained CLIP and DINOv3 to mitigate error accumulation and pseudo-label drift. To effectively incorporate these distinct priors, an explicit-implicit semantic co-guidance mechanism is introduced that utilizes text embeddings and learnable queries to provide explicit and implicit class-level guidance, respectively, thereby jointly enhancing semantic consistency. Furthermore, a global-local feature collaborative fusion strategy is developed to effectively fuse the global contextual information captured by CLIP with the local details produced by DINOv3, enabling the model to generate highly precise segmentation results. Extensive experiments on six popular datasets demonstrate the superiority of the proposed method, which consistently achieves leading performance across various partition protocols and diverse scenarios. Project page is available at https://xavierjiezou.github.io/Co2S/.",
            "upvotes": 4,
            "discussionId": "695ca3916aa73bc11f09152c",
            "projectPage": "https://xavierjiezou.github.io/Co2S/",
            "githubRepo": "https://github.com/XavierJiezou/Co2S",
            "githubRepoAddedBy": "user",
            "ai_summary": "A semi-supervised remote sensing image segmentation framework combines vision-language and self-supervised models to reduce pseudo-label drift through dual-student architecture and semantic co-guidance mechanisms.",
            "ai_keywords": [
                "semi-supervised learning",
                "remote sensing image semantic segmentation",
                "pseudo-label drift",
                "vision-language models",
                "self-supervised models",
                "ViT-based vision foundation models",
                "CLIP",
                "DINOv3",
                "dual-student architecture",
                "explicit-implicit semantic co-guidance",
                "text embeddings",
                "learnable queries",
                "global-local feature collaborative fusion"
            ],
            "githubStars": 4
        },
        "publishedAt": "2025-12-28T13:24:19.000Z",
        "title": "Toward Stable Semi-Supervised Remote Sensing Segmentation via Co-Guidance and Co-Fusion",
        "summary": "Semi-supervised remote sensing (RS) image semantic segmentation offers a promising solution to alleviate the burden of exhaustive annotation, yet it fundamentally struggles with pseudo-label drift, a phenomenon where confirmation bias leads to the accumulation of errors during training. In this work, we propose Co2S, a stable semi-supervised RS segmentation framework that synergistically fuses priors from vision-language models and self-supervised models. Specifically, we construct a heterogeneous dual-student architecture comprising two distinct ViT-based vision foundation models initialized with pretrained CLIP and DINOv3 to mitigate error accumulation and pseudo-label drift. To effectively incorporate these distinct priors, an explicit-implicit semantic co-guidance mechanism is introduced that utilizes text embeddings and learnable queries to provide explicit and implicit class-level guidance, respectively, thereby jointly enhancing semantic consistency. Furthermore, a global-local feature collaborative fusion strategy is developed to effectively fuse the global contextual information captured by CLIP with the local details produced by DINOv3, enabling the model to generate highly precise segmentation results. Extensive experiments on six popular datasets demonstrate the superiority of the proposed method, which consistently achieves leading performance across various partition protocols and diverse scenarios. Project page is available at https://xavierjiezou.github.io/Co2S/.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6617af2beab5eef6b1e8bb9e/f6PYGb8HrnOgJZMhhQrXU.png",
            "https://cdn-uploads.huggingface.co/production/uploads/6617af2beab5eef6b1e8bb9e/RR1EFPfHSpHXHT09lXfVN.png",
            "https://cdn-uploads.huggingface.co/production/uploads/6617af2beab5eef6b1e8bb9e/-u6q-jRaEuWhdHHNEnpc2.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23035.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6617af2beab5eef6b1e8bb9e",
            "avatarUrl": "/avatars/d939c02027916331d4c44119565f2ca6.svg",
            "fullname": "XavierJiezou",
            "name": "XavierJiezou",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1,
            "isUserFollowing": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2601.01576",
            "authors": [
                {
                    "_id": "695c82ff6aa73bc11f091484",
                    "name": "Ming Zhang",
                    "hidden": false
                },
                {
                    "_id": "695c82ff6aa73bc11f091485",
                    "name": "Kexin Tan",
                    "hidden": false
                },
                {
                    "_id": "695c82ff6aa73bc11f091486",
                    "name": "Yueyuan Huang",
                    "hidden": false
                },
                {
                    "_id": "695c82ff6aa73bc11f091487",
                    "name": "Yujiong Shen",
                    "hidden": false
                },
                {
                    "_id": "695c82ff6aa73bc11f091488",
                    "name": "Chunchun Ma",
                    "hidden": false
                },
                {
                    "_id": "695c82ff6aa73bc11f091489",
                    "name": "Li Ju",
                    "hidden": false
                },
                {
                    "_id": "695c82ff6aa73bc11f09148a",
                    "name": "Xinran Zhang",
                    "hidden": false
                },
                {
                    "_id": "695c82ff6aa73bc11f09148b",
                    "name": "Yuhui Wang",
                    "hidden": false
                },
                {
                    "_id": "695c82ff6aa73bc11f09148c",
                    "name": "Wenqing Jing",
                    "hidden": false
                },
                {
                    "_id": "695c82ff6aa73bc11f09148d",
                    "name": "Jingyi Deng",
                    "hidden": false
                },
                {
                    "_id": "695c82ff6aa73bc11f09148e",
                    "name": "Huayu Sha",
                    "hidden": false
                },
                {
                    "_id": "695c82ff6aa73bc11f09148f",
                    "name": "Binze Hu",
                    "hidden": false
                },
                {
                    "_id": "695c82ff6aa73bc11f091490",
                    "name": "Jingqi Tong",
                    "hidden": false
                },
                {
                    "_id": "695c82ff6aa73bc11f091491",
                    "name": "Changhao Jiang",
                    "hidden": false
                },
                {
                    "_id": "695c82ff6aa73bc11f091492",
                    "name": "Yage Geng",
                    "hidden": false
                },
                {
                    "_id": "695c82ff6aa73bc11f091493",
                    "name": "Yuankai Ying",
                    "hidden": false
                },
                {
                    "_id": "695c82ff6aa73bc11f091494",
                    "name": "Yue Zhang",
                    "hidden": false
                },
                {
                    "_id": "695c82ff6aa73bc11f091495",
                    "name": "Zhangyue Yin",
                    "hidden": false
                },
                {
                    "_id": "695c82ff6aa73bc11f091496",
                    "name": "Zhiheng Xi",
                    "hidden": false
                },
                {
                    "_id": "695c82ff6aa73bc11f091497",
                    "name": "Shihan Dou",
                    "hidden": false
                },
                {
                    "_id": "695c82ff6aa73bc11f091498",
                    "name": "Tao Gui",
                    "hidden": false
                },
                {
                    "_id": "695c82ff6aa73bc11f091499",
                    "name": "Qi Zhang",
                    "hidden": false
                },
                {
                    "_id": "695c82ff6aa73bc11f09149a",
                    "name": "Xuanjing Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-04T15:48:51.000Z",
            "submittedOnDailyAt": "2026-01-06T01:05:52.947Z",
            "title": "OpenNovelty: An LLM-powered Agentic System for Verifiable Scholarly Novelty Assessment",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Evaluating novelty is critical yet challenging in peer review, as reviewers must assess submissions against a vast, rapidly evolving literature. This report presents OpenNovelty, an LLM-powered agentic system for transparent, evidence-based novelty analysis. The system operates through four phases: (1) extracting the core task and contribution claims to generate retrieval queries; (2) retrieving relevant prior work based on extracted queries via semantic search engine; (3) constructing a hierarchical taxonomy of core-task-related work and performing contribution-level full-text comparisons against each contribution; and (4) synthesizing all analyses into a structured novelty report with explicit citations and evidence snippets. Unlike naive LLM-based approaches, OpenNovelty grounds all assessments in retrieved real papers, ensuring verifiable judgments. We deploy our system on 500+ ICLR 2026 submissions with all reports publicly available on our website, and preliminary analysis suggests it can identify relevant prior work, including closely related papers that authors may overlook. OpenNovelty aims to empower the research community with a scalable tool that promotes fair, consistent, and evidence-backed peer review.",
            "upvotes": 1,
            "discussionId": "695c82ff6aa73bc11f09149b",
            "projectPage": "https://www.opennovelty.org/",
            "githubRepo": "https://github.com/january-blue/OpenNovelty",
            "githubRepoAddedBy": "user",
            "ai_summary": "An LLM-powered agentic system for transparent, evidence-based novelty assessment in peer review that retrieves and analyzes prior work through semantic search and hierarchical taxonomy construction.",
            "ai_keywords": [
                "LLM-powered agentic system",
                "semantic search engine",
                "hierarchical taxonomy",
                "contribution-level full-text comparisons",
                "structured novelty report",
                "evidence-based assessment",
                "peer review"
            ],
            "githubStars": 3
        },
        "publishedAt": "2026-01-04T10:48:51.000Z",
        "title": "OpenNovelty: An LLM-powered Agentic System for Verifiable Scholarly Novelty Assessment",
        "summary": "Evaluating novelty is critical yet challenging in peer review, as reviewers must assess submissions against a vast, rapidly evolving literature. This report presents OpenNovelty, an LLM-powered agentic system for transparent, evidence-based novelty analysis. The system operates through four phases: (1) extracting the core task and contribution claims to generate retrieval queries; (2) retrieving relevant prior work based on extracted queries via semantic search engine; (3) constructing a hierarchical taxonomy of core-task-related work and performing contribution-level full-text comparisons against each contribution; and (4) synthesizing all analyses into a structured novelty report with explicit citations and evidence snippets. Unlike naive LLM-based approaches, OpenNovelty grounds all assessments in retrieved real papers, ensuring verifiable judgments. We deploy our system on 500+ ICLR 2026 submissions with all reports publicly available on our website, and preliminary analysis suggests it can identify relevant prior work, including closely related papers that authors may overlook. OpenNovelty aims to empower the research community with a scalable tool that promotes fair, consistent, and evidence-backed peer review.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.01576.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 200,
            "isUserFollowing": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2601.00863",
            "authors": [
                {
                    "_id": "695cedc16aa73bc11f091638",
                    "name": "Markus J. Buehler",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-30T11:14:51.000Z",
            "submittedOnDailyAt": "2026-01-06T10:46:35.187Z",
            "title": "Selective Imperfection as a Generative Framework for Analysis, Creativity and Discovery",
            "submittedOnDailyBy": {
                "_id": "623ce1c6b66fedf374859fe7",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623ce1c6b66fedf374859fe7/lhbMLg6BxLCb9DD4rgjfx.jpeg",
                "isPro": true,
                "fullname": "Markus Buehler",
                "user": "mjbuehler",
                "type": "user"
            },
            "summary": "We introduce materiomusic as a generative framework linking the hierarchical structures of matter with the compositional logic of music. Across proteins, spider webs and flame dynamics, vibrational and architectural principles recur as tonal hierarchies, harmonic progressions, and long-range musical form. Using reversible mappings, from molecular spectra to musical tones and from three-dimensional networks to playable instruments, we show how sound functions as a scientific probe, an epistemic inversion where listening becomes a mode of seeing and musical composition becomes a blueprint for matter. These mappings excavate deep time: patterns originating in femtosecond molecular vibrations or billion-year evolutionary histories become audible. We posit that novelty in science and art emerges when constraints cannot be satisfied within existing degrees of freedom, forcing expansion of the space of viable configurations. Selective imperfection provides the mechanism restoring balance between coherence and adaptability. Quantitative support comes from exhaustive enumeration of all 2^12 musical scales, revealing that culturally significant systems cluster in a mid-entropy, mid-defect corridor, directly paralleling the Hall-Petch optimum where intermediate defect densities maximize material strength. Iterating these mappings creates productive collisions between human creativity and physics, generating new information as musical structures encounter evolutionary constraints. We show how swarm-based AI models compose music exhibiting human-like structural signatures such as small-world connectivity, modular integration, long-range coherence, suggesting a route beyond interpolation toward invention. We show that science and art are generative acts of world-building under constraint, with vibration as a shared grammar organizing structure across scales.",
            "upvotes": 1,
            "discussionId": "695cedc16aa73bc11f091639",
            "githubRepo": "https://github.com/lamm-mit/MusicAnalysis",
            "githubRepoAddedBy": "user",
            "githubStars": 2,
            "organization": {
                "_id": "6552174a04d4294fa01d5375",
                "name": "lamm-mit",
                "fullname": "LAMM: MIT Laboratory for Atomistic and Molecular Mechanics",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/623ce1c6b66fedf374859fe7/Qw9Np_ESnnWWUm7HXleHR.jpeg"
            }
        },
        "publishedAt": "2025-12-30T06:14:51.000Z",
        "title": "Selective Imperfection as a Generative Framework for Analysis, Creativity and Discovery",
        "summary": "We introduce materiomusic as a generative framework linking the hierarchical structures of matter with the compositional logic of music. Across proteins, spider webs and flame dynamics, vibrational and architectural principles recur as tonal hierarchies, harmonic progressions, and long-range musical form. Using reversible mappings, from molecular spectra to musical tones and from three-dimensional networks to playable instruments, we show how sound functions as a scientific probe, an epistemic inversion where listening becomes a mode of seeing and musical composition becomes a blueprint for matter. These mappings excavate deep time: patterns originating in femtosecond molecular vibrations or billion-year evolutionary histories become audible. We posit that novelty in science and art emerges when constraints cannot be satisfied within existing degrees of freedom, forcing expansion of the space of viable configurations. Selective imperfection provides the mechanism restoring balance between coherence and adaptability. Quantitative support comes from exhaustive enumeration of all 2^12 musical scales, revealing that culturally significant systems cluster in a mid-entropy, mid-defect corridor, directly paralleling the Hall-Petch optimum where intermediate defect densities maximize material strength. Iterating these mappings creates productive collisions between human creativity and physics, generating new information as musical structures encounter evolutionary constraints. We show how swarm-based AI models compose music exhibiting human-like structural signatures such as small-world connectivity, modular integration, long-range coherence, suggesting a route beyond interpolation toward invention. We show that science and art are generative acts of world-building under constraint, with vibration as a shared grammar organizing structure across scales.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.00863.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "623ce1c6b66fedf374859fe7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623ce1c6b66fedf374859fe7/lhbMLg6BxLCb9DD4rgjfx.jpeg",
            "fullname": "Markus Buehler",
            "name": "mjbuehler",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 28,
            "isUserFollowing": false
        },
        "organization": {
            "_id": "6552174a04d4294fa01d5375",
            "name": "lamm-mit",
            "fullname": "LAMM: MIT Laboratory for Atomistic and Molecular Mechanics",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/623ce1c6b66fedf374859fe7/Qw9Np_ESnnWWUm7HXleHR.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.21472",
            "authors": [
                {
                    "_id": "695c9ed56aa73bc11f091511",
                    "user": {
                        "_id": "63855ef0ac61472e5e96d77e",
                        "avatarUrl": "/avatars/283c639b1f50760fb9d73c96c48d896e.svg",
                        "isPro": false,
                        "fullname": "Kumar Abhishek",
                        "user": "kabhishe",
                        "type": "user"
                    },
                    "name": "Kumar Abhishek",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-06T09:40:16.733Z",
                    "hidden": false
                },
                {
                    "_id": "695c9ed56aa73bc11f091512",
                    "name": "Jeremy Kawahara",
                    "hidden": false
                },
                {
                    "_id": "695c9ed56aa73bc11f091513",
                    "name": "Ghassan Hamarneh",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63855ef0ac61472e5e96d77e/10n22hOvfy1pQK30_JZi0.png"
            ],
            "publishedAt": "2025-12-25T02:21:55.000Z",
            "submittedOnDailyAt": "2026-01-06T03:23:16.409Z",
            "title": "IMA++: ISIC Archive Multi-Annotator Dermoscopic Skin Lesion Segmentation Dataset",
            "submittedOnDailyBy": {
                "_id": "63855ef0ac61472e5e96d77e",
                "avatarUrl": "/avatars/283c639b1f50760fb9d73c96c48d896e.svg",
                "isPro": false,
                "fullname": "Kumar Abhishek",
                "user": "kabhishe",
                "type": "user"
            },
            "summary": "Multi-annotator medical image segmentation is an important research problem, but requires annotated datasets that are expensive to collect. Dermoscopic skin lesion imaging allows human experts and AI systems to observe morphological structures otherwise not discernable from regular clinical photographs. However, currently there are no large-scale publicly available multi-annotator skin lesion segmentation (SLS) datasets with annotator-labels for dermoscopic skin lesion imaging. We introduce ISIC MultiAnnot++, a large public multi-annotator skin lesion segmentation dataset for images from the ISIC Archive. The final dataset contains 17,684 segmentation masks spanning 14,967 dermoscopic images, where 2,394 dermoscopic images have 2-5 segmentations per image, making it the largest publicly available SLS dataset. Further, metadata about the segmentation, including the annotators' skill level and segmentation tool, is included, enabling research on topics such as annotator-specific preference modeling for segmentation and annotator metadata analysis. We provide an analysis on the characteristics of this dataset, curated data partitions, and consensus segmentation masks.",
            "upvotes": 1,
            "discussionId": "695c9ed56aa73bc11f091514",
            "githubRepo": "https://github.com/sfu-mial/IMAplusplus",
            "githubRepoAddedBy": "user",
            "ai_summary": "A large-scale public multi-annotator skin lesion segmentation dataset is introduced with extensive metadata for annotator analysis and consensus modeling.",
            "ai_keywords": [
                "multi-annotator medical image segmentation",
                "skin lesion segmentation",
                "dermoscopic imaging",
                "ISIC Archive",
                "consensus segmentation masks"
            ],
            "githubStars": 0,
            "organization": {
                "_id": "6385602cb2906edaf839745b",
                "name": "sfu-mial",
                "fullname": "Medical Image Analysis Lab, SFU",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1669685287218-63855ef0ac61472e5e96d77e.png"
            }
        },
        "publishedAt": "2025-12-24T21:21:55.000Z",
        "title": "IMA++: ISIC Archive Multi-Annotator Dermoscopic Skin Lesion Segmentation Dataset",
        "summary": "Multi-annotator medical image segmentation is an important research problem, but requires annotated datasets that are expensive to collect. Dermoscopic skin lesion imaging allows human experts and AI systems to observe morphological structures otherwise not discernable from regular clinical photographs. However, currently there are no large-scale publicly available multi-annotator skin lesion segmentation (SLS) datasets with annotator-labels for dermoscopic skin lesion imaging. We introduce ISIC MultiAnnot++, a large public multi-annotator skin lesion segmentation dataset for images from the ISIC Archive. The final dataset contains 17,684 segmentation masks spanning 14,967 dermoscopic images, where 2,394 dermoscopic images have 2-5 segmentations per image, making it the largest publicly available SLS dataset. Further, metadata about the segmentation, including the annotators' skill level and segmentation tool, is included, enabling research on topics such as annotator-specific preference modeling for segmentation and annotator metadata analysis. We provide an analysis on the characteristics of this dataset, curated data partitions, and consensus segmentation masks.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63855ef0ac61472e5e96d77e/10n22hOvfy1pQK30_JZi0.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.21472.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "63855ef0ac61472e5e96d77e",
            "avatarUrl": "/avatars/283c639b1f50760fb9d73c96c48d896e.svg",
            "fullname": "Kumar Abhishek",
            "name": "kabhishe",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "isUserFollowing": false
        },
        "organization": {
            "_id": "6385602cb2906edaf839745b",
            "name": "sfu-mial",
            "fullname": "Medical Image Analysis Lab, SFU",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1669685287218-63855ef0ac61472e5e96d77e.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2601.02315",
            "authors": [
                {
                    "_id": "695d68c7c03d6d81e4399dea",
                    "name": "Saurabh Kaushik",
                    "hidden": false
                },
                {
                    "_id": "695d68c7c03d6d81e4399deb",
                    "name": "Lalit Maurya",
                    "hidden": false
                },
                {
                    "_id": "695d68c7c03d6d81e4399dec",
                    "name": "Beth Tellman",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-05T18:07:21.000Z",
            "submittedOnDailyAt": "2026-01-06T17:26:52.350Z",
            "title": "Prithvi-Complimentary Adaptive Fusion Encoder (CAFE): unlocking full-potential for flood inundation mapping",
            "submittedOnDailyBy": {
                "_id": "67c2114b5b002f178a8ad651",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/i7dzIATk2-7oYFA5muIAN.png",
                "isPro": false,
                "fullname": "Lalit Maurya",
                "user": "lalitmaurya47",
                "type": "user"
            },
            "summary": "Geo-Foundation Models (GFMs), have proven effective in diverse downstream applications, including semantic segmentation, classification, and regression tasks. However, in case of flood mapping using Sen1Flood11 dataset as a downstream task, GFMs struggles to outperform the baseline U-Net, highlighting model's limitation in capturing critical local nuances. To address this, we present the Prithvi-Complementary Adaptive Fusion Encoder (CAFE), which integrate Prithvi GFM pretrained encoder with a parallel CNN residual branch enhanced by Convolutional Attention Modules (CAM). Prithvi-CAFE enables fast and efficient fine-tuning through adapters in Prithvi and performs multi-scale, multi-level fusion with CNN features, capturing critical local details while preserving long-range dependencies. We achieve state-of-the-art results on two comprehensive flood mapping datasets: Sen1Flood11 and FloodPlanet. On Sen1Flood11 test data, Prithvi-CAFE (IoU 83.41) outperforms the original Prithvi (IoU 82.50) and other major GFMs (TerraMind 82.90, DOFA 81.54, spectralGPT: 81.02). The improvement is even more pronounced on the hold-out test site, where Prithvi-CAFE achieves an IoU of 81.37 compared to the baseline U-Net (70.57) and original Prithvi (72.42). On FloodPlanet, Prithvi-CAFE also surpasses the baseline U-Net and other GFMs, achieving an IoU of 64.70 compared to U-Net (60.14), Terramind (62.33), DOFA (59.15) and Prithvi 2.0 (61.91). Our proposed simple yet effective Prithvi-CAFE demonstrates strong potential for improving segmentation tasks where multi-channel and multi-modal data provide complementary information and local details are critical. The code is released on https://github.com/Sk-2103/Prithvi-CAFE{Prithvi-CAFE Github}",
            "upvotes": 0,
            "discussionId": "695d68c7c03d6d81e4399ded",
            "githubRepo": "https://github.com/Sk-2103/Prithvi-CAFE",
            "githubRepoAddedBy": "user",
            "ai_summary": "Prithvi-CAFE combines a pretrained Geo-Foundation Model encoder with a parallel CNN branch featuring attention modules to improve flood mapping accuracy by capturing both global context and local details.",
            "ai_keywords": [
                "Geo-Foundation Models",
                "Prithvi",
                "U-Net",
                "Sen1Flood11",
                "FloodPlanet",
                "Convolutional Attention Modules",
                "multi-scale fusion",
                "multi-level fusion",
                "adapters",
                "semantic segmentation",
                "classification",
                "regression"
            ],
            "githubStars": 3
        },
        "publishedAt": "2026-01-05T13:07:21.000Z",
        "title": "Prithvi-Complimentary Adaptive Fusion Encoder (CAFE): unlocking full-potential for flood inundation mapping",
        "summary": "Geo-Foundation Models (GFMs), have proven effective in diverse downstream applications, including semantic segmentation, classification, and regression tasks. However, in case of flood mapping using Sen1Flood11 dataset as a downstream task, GFMs struggles to outperform the baseline U-Net, highlighting model's limitation in capturing critical local nuances. To address this, we present the Prithvi-Complementary Adaptive Fusion Encoder (CAFE), which integrate Prithvi GFM pretrained encoder with a parallel CNN residual branch enhanced by Convolutional Attention Modules (CAM). Prithvi-CAFE enables fast and efficient fine-tuning through adapters in Prithvi and performs multi-scale, multi-level fusion with CNN features, capturing critical local details while preserving long-range dependencies. We achieve state-of-the-art results on two comprehensive flood mapping datasets: Sen1Flood11 and FloodPlanet. On Sen1Flood11 test data, Prithvi-CAFE (IoU 83.41) outperforms the original Prithvi (IoU 82.50) and other major GFMs (TerraMind 82.90, DOFA 81.54, spectralGPT: 81.02). The improvement is even more pronounced on the hold-out test site, where Prithvi-CAFE achieves an IoU of 81.37 compared to the baseline U-Net (70.57) and original Prithvi (72.42). On FloodPlanet, Prithvi-CAFE also surpasses the baseline U-Net and other GFMs, achieving an IoU of 64.70 compared to U-Net (60.14), Terramind (62.33), DOFA (59.15) and Prithvi 2.0 (61.91). Our proposed simple yet effective Prithvi-CAFE demonstrates strong potential for improving segmentation tasks where multi-channel and multi-modal data provide complementary information and local details are critical. The code is released on https://github.com/Sk-2103/Prithvi-CAFE{Prithvi-CAFE Github}",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02315.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67c2114b5b002f178a8ad651",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/i7dzIATk2-7oYFA5muIAN.png",
            "fullname": "Lalit Maurya",
            "name": "lalitmaurya47",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "isUserFollowing": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2601.02314",
            "authors": [
                {
                    "_id": "695c84466aa73bc11f0914aa",
                    "name": "Sourena Khanzadeh",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-05T18:05:29.000Z",
            "submittedOnDailyAt": "2026-01-06T01:39:32.034Z",
            "title": "Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents",
            "submittedOnDailyBy": {
                "_id": "66206e4ed0eb50531a28b4df",
                "avatarUrl": "/avatars/24a05b21e213bc8697bda4f5a6db7c51.svg",
                "isPro": false,
                "fullname": "Sourena Khanzadeh",
                "user": "Suren15",
                "type": "user"
            },
            "summary": "As Large Language Model (LLM) agents are increasingly tasked with high-stakes autonomous decision-making, the transparency of their reasoning processes has become a critical safety concern. While Chain-of-Thought (CoT) prompting allows agents to generate human-readable reasoning traces, it remains unclear whether these traces are faithful generative drivers of the model's output or merely post-hoc rationalizations. We introduce Project Ariadne, a novel XAI framework that utilizes Structural Causal Models (SCMs) and counterfactual logic to audit the causal integrity of agentic reasoning. Unlike existing interpretability methods that rely on surface-level textual similarity, Project Ariadne performs hard interventions (do-calculus) on intermediate reasoning nodes -- systematically inverting logic, negating premises, and reversing factual claims -- to measure the Causal Sensitivity () of the terminal answer. Our empirical evaluation of state-of-the-art models reveals a persistent Faithfulness Gap. We define and detect a widespread failure mode termed Causal Decoupling, where agents exhibit a violation density () of up to 0.77 in factual and scientific domains. In these instances, agents arrive at identical conclusions despite contradictory internal logic, proving that their reasoning traces function as \"Reasoning Theater\" while decision-making is governed by latent parametric priors. Our findings suggest that current agentic architectures are inherently prone to unfaithful explanation, and we propose the Ariadne Score as a new benchmark for aligning stated logic with model action.",
            "upvotes": 0,
            "discussionId": "695c84466aa73bc11f0914ab",
            "githubRepo": "https://github.com/skhanzad/AridadneXAI",
            "githubRepoAddedBy": "user",
            "ai_summary": "Project Ariadne uses structural causal models and counterfactual logic to evaluate the causal integrity of LLM reasoning, revealing a faithfulness gap where reasoning traces are not reliable drivers of outputs.",
            "ai_keywords": [
                "Large Language Model agents",
                "Chain-of-Thought prompting",
                "Structural Causal Models",
                "counterfactual logic",
                "do-calculus",
                "Causal Sensitivity",
                "Faithfulness Gap",
                "Causal Decoupling",
                "Ariadne Score"
            ],
            "githubStars": 2,
            "organization": {
                "_id": "65ad969414d782df06bf4eb4",
                "name": "TorontoMetropolitanUniversity",
                "fullname": "Toronto Metropolitan University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/nlhGsTSfCkCvWwqS4MwLD.png"
            }
        },
        "publishedAt": "2026-01-05T13:05:29.000Z",
        "title": "Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents",
        "summary": "As Large Language Model (LLM) agents are increasingly tasked with high-stakes autonomous decision-making, the transparency of their reasoning processes has become a critical safety concern. While Chain-of-Thought (CoT) prompting allows agents to generate human-readable reasoning traces, it remains unclear whether these traces are faithful generative drivers of the model's output or merely post-hoc rationalizations. We introduce Project Ariadne, a novel XAI framework that utilizes Structural Causal Models (SCMs) and counterfactual logic to audit the causal integrity of agentic reasoning. Unlike existing interpretability methods that rely on surface-level textual similarity, Project Ariadne performs hard interventions (do-calculus) on intermediate reasoning nodes -- systematically inverting logic, negating premises, and reversing factual claims -- to measure the Causal Sensitivity () of the terminal answer. Our empirical evaluation of state-of-the-art models reveals a persistent Faithfulness Gap. We define and detect a widespread failure mode termed Causal Decoupling, where agents exhibit a violation density () of up to 0.77 in factual and scientific domains. In these instances, agents arrive at identical conclusions despite contradictory internal logic, proving that their reasoning traces function as \"Reasoning Theater\" while decision-making is governed by latent parametric priors. Our findings suggest that current agentic architectures are inherently prone to unfaithful explanation, and we propose the Ariadne Score as a new benchmark for aligning stated logic with model action.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02314.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66206e4ed0eb50531a28b4df",
            "avatarUrl": "/avatars/24a05b21e213bc8697bda4f5a6db7c51.svg",
            "fullname": "Sourena Khanzadeh",
            "name": "Suren15",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "isUserFollowing": false
        },
        "organization": {
            "_id": "65ad969414d782df06bf4eb4",
            "name": "TorontoMetropolitanUniversity",
            "fullname": "Toronto Metropolitan University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/nlhGsTSfCkCvWwqS4MwLD.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.22877",
            "authors": [
                {
                    "_id": "695cd1316aa73bc11f0915ec",
                    "name": "Ju-Hsuan Weng",
                    "hidden": false
                },
                {
                    "_id": "695cd1316aa73bc11f0915ed",
                    "user": {
                        "_id": "65570f40f551801d40807d90",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65570f40f551801d40807d90/b5oyRKEIaB2NU5kxXU-Zh.jpeg",
                        "isPro": false,
                        "fullname": "Jiawei",
                        "user": "jwliao1209",
                        "type": "user"
                    },
                    "name": "Jia-Wei Liao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-06T09:40:10.276Z",
                    "hidden": false
                },
                {
                    "_id": "695cd1316aa73bc11f0915ee",
                    "name": "Cheng-Fu Chou",
                    "hidden": false
                },
                {
                    "_id": "695cd1316aa73bc11f0915ef",
                    "name": "Jun-Cheng Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-28T10:58:36.000Z",
            "submittedOnDailyAt": "2026-01-06T06:40:33.445Z",
            "title": "M-ErasureBench: A Comprehensive Multimodal Evaluation Benchmark for Concept Erasure in Diffusion Models",
            "submittedOnDailyBy": {
                "_id": "65570f40f551801d40807d90",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65570f40f551801d40807d90/b5oyRKEIaB2NU5kxXU-Zh.jpeg",
                "isPro": false,
                "fullname": "Jiawei",
                "user": "jwliao1209",
                "type": "user"
            },
            "summary": "Text-to-image diffusion models may generate harmful or copyrighted content, motivating research on concept erasure. However, existing approaches primarily focus on erasing concepts from text prompts, overlooking other input modalities that are increasingly critical in real-world applications such as image editing and personalized generation. These modalities can become attack surfaces, where erased concepts re-emerge despite defenses. To bridge this gap, we introduce M-ErasureBench, a novel multimodal evaluation framework that systematically benchmarks concept erasure methods across three input modalities: text prompts, learned embeddings, and inverted latents. For the latter two, we evaluate both white-box and black-box access, yielding five evaluation scenarios. Our analysis shows that existing methods achieve strong erasure performance against text prompts but largely fail under learned embeddings and inverted latents, with Concept Reproduction Rate (CRR) exceeding 90% in the white-box setting. To address these vulnerabilities, we propose IRECE (Inference-time Robustness Enhancement for Concept Erasure), a plug-and-play module that localizes target concepts via cross-attention and perturbs the associated latents during denoising. Experiments demonstrate that IRECE consistently restores robustness, reducing CRR by up to 40% under the most challenging white-box latent inversion scenario, while preserving visual quality. To the best of our knowledge, M-ErasureBench provides the first comprehensive benchmark of concept erasure beyond text prompts. Together with IRECE, our benchmark offers practical safeguards for building more reliable protective generative models.",
            "upvotes": 0,
            "discussionId": "695cd1316aa73bc11f0915f0",
            "ai_summary": "A multimodal evaluation framework and robustness enhancement module are introduced to address concept erasure vulnerabilities in text-to-image diffusion models across multiple input modalities.",
            "ai_keywords": [
                "text-to-image diffusion models",
                "concept erasure",
                "multimodal evaluation",
                "learned embeddings",
                "inverted latents",
                "cross-attention",
                "denoising",
                "Concept Reproduction Rate",
                "IRECE",
                "latent inversion"
            ]
        },
        "publishedAt": "2025-12-28T05:58:36.000Z",
        "title": "M-ErasureBench: A Comprehensive Multimodal Evaluation Benchmark for Concept Erasure in Diffusion Models",
        "summary": "Text-to-image diffusion models may generate harmful or copyrighted content, motivating research on concept erasure. However, existing approaches primarily focus on erasing concepts from text prompts, overlooking other input modalities that are increasingly critical in real-world applications such as image editing and personalized generation. These modalities can become attack surfaces, where erased concepts re-emerge despite defenses. To bridge this gap, we introduce M-ErasureBench, a novel multimodal evaluation framework that systematically benchmarks concept erasure methods across three input modalities: text prompts, learned embeddings, and inverted latents. For the latter two, we evaluate both white-box and black-box access, yielding five evaluation scenarios. Our analysis shows that existing methods achieve strong erasure performance against text prompts but largely fail under learned embeddings and inverted latents, with Concept Reproduction Rate (CRR) exceeding 90% in the white-box setting. To address these vulnerabilities, we propose IRECE (Inference-time Robustness Enhancement for Concept Erasure), a plug-and-play module that localizes target concepts via cross-attention and perturbs the associated latents during denoising. Experiments demonstrate that IRECE consistently restores robustness, reducing CRR by up to 40% under the most challenging white-box latent inversion scenario, while preserving visual quality. To the best of our knowledge, M-ErasureBench provides the first comprehensive benchmark of concept erasure beyond text prompts. Together with IRECE, our benchmark offers practical safeguards for building more reliable protective generative models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.22877.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "65570f40f551801d40807d90",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65570f40f551801d40807d90/b5oyRKEIaB2NU5kxXU-Zh.jpeg",
            "fullname": "Jiawei",
            "name": "jwliao1209",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "isUserFollowing": false
        },
        "isAuthorParticipating": true
    }
]