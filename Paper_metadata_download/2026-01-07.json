[
    {
        "paper": {
            "id": "2601.03252",
            "authors": [
                {
                    "_id": "695dc956c03d6d81e4399ea4",
                    "name": "Hao Yu",
                    "hidden": false
                },
                {
                    "_id": "695dc956c03d6d81e4399ea5",
                    "user": {
                        "_id": "6489a01b8de3f9d810b0154f",
                        "avatarUrl": "/avatars/f7a0fc6816535945e11bac1212dd7b57.svg",
                        "isPro": false,
                        "fullname": "Haotong Lin",
                        "user": "haotongl",
                        "type": "user"
                    },
                    "name": "Haotong Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-07T13:15:04.783Z",
                    "hidden": false
                },
                {
                    "_id": "695dc956c03d6d81e4399ea6",
                    "name": "Jiawei Wang",
                    "hidden": false
                },
                {
                    "_id": "695dc956c03d6d81e4399ea7",
                    "name": "Jiaxin Li",
                    "hidden": false
                },
                {
                    "_id": "695dc956c03d6d81e4399ea8",
                    "name": "Yida Wang",
                    "hidden": false
                },
                {
                    "_id": "695dc956c03d6d81e4399ea9",
                    "user": {
                        "_id": "6791a6c19ce382eae861ed61",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6791a6c19ce382eae861ed61/zerctN-RdeP4hSrWidtyN.jpeg",
                        "isPro": false,
                        "fullname": "Xueyang Zhang",
                        "user": "zhangxueyang001",
                        "type": "user"
                    },
                    "name": "Xueyang Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-07T13:15:39.946Z",
                    "hidden": false
                },
                {
                    "_id": "695dc956c03d6d81e4399eaa",
                    "name": "Yue Wang",
                    "hidden": false
                },
                {
                    "_id": "695dc956c03d6d81e4399eab",
                    "name": "Xiaowei Zhou",
                    "hidden": false
                },
                {
                    "_id": "695dc956c03d6d81e4399eac",
                    "name": "Ruizhen Hu",
                    "hidden": false
                },
                {
                    "_id": "695dc956c03d6d81e4399ead",
                    "user": {
                        "_id": "62986ca2b58e71e2ac9b8f01",
                        "avatarUrl": "/avatars/83944db5f3dbb6f47c47c46fb2cb2849.svg",
                        "isPro": false,
                        "fullname": "Sida Peng",
                        "user": "pengsida",
                        "type": "user"
                    },
                    "name": "Sida Peng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-07T13:16:12.074Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6489a01b8de3f9d810b0154f/XlXUo1VjGVhePk-xHRmkj.mp4"
            ],
            "publishedAt": "2026-01-06T18:57:06.000Z",
            "submittedOnDailyAt": "2026-01-07T00:26:37.060Z",
            "title": "InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields",
            "submittedOnDailyBy": {
                "_id": "6489a01b8de3f9d810b0154f",
                "avatarUrl": "/avatars/f7a0fc6816535945e11bac1212dd7b57.svg",
                "isPro": false,
                "fullname": "Haotong Lin",
                "user": "haotongl",
                "type": "user"
            },
            "summary": "Existing depth estimation methods are fundamentally limited to predicting depth on discrete image grids. Such representations restrict their scalability to arbitrary output resolutions and hinder the geometric detail recovery. This paper introduces InfiniDepth, which represents depth as neural implicit fields. Through a simple yet effective local implicit decoder, we can query depth at continuous 2D coordinates, enabling arbitrary-resolution and fine-grained depth estimation. To better assess our method's capabilities, we curate a high-quality 4K synthetic benchmark from five different games, spanning diverse scenes with rich geometric and appearance details. Extensive experiments demonstrate that InfiniDepth achieves state-of-the-art performance on both synthetic and real-world benchmarks across relative and metric depth estimation tasks, particularly excelling in fine-detail regions. It also benefits the task of novel view synthesis under large viewpoint shifts, producing high-quality results with fewer holes and artifacts.",
            "upvotes": 72,
            "discussionId": "695dc956c03d6d81e4399eae",
            "ai_summary": "InfiniDepth represents depth as neural implicit fields using a local implicit decoder, enabling continuous 2D coordinate querying for arbitrary-resolution depth estimation and superior performance in fine-detail regions.",
            "ai_keywords": [
                "neural implicit fields",
                "local implicit decoder",
                "continuous 2D coordinates",
                "arbitrary-resolution depth estimation",
                "synthetic benchmark",
                "4K synthetic benchmark",
                "novel view synthesis",
                "viewpoint shifts"
            ],
            "organization": {
                "_id": "61bac2af530e5c78d7b99667",
                "name": "zju",
                "fullname": "Zhejiang University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"
            }
        },
        "publishedAt": "2026-01-06T13:57:06.000Z",
        "title": "InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields",
        "summary": "Existing depth estimation methods are fundamentally limited to predicting depth on discrete image grids. Such representations restrict their scalability to arbitrary output resolutions and hinder the geometric detail recovery. This paper introduces InfiniDepth, which represents depth as neural implicit fields. Through a simple yet effective local implicit decoder, we can query depth at continuous 2D coordinates, enabling arbitrary-resolution and fine-grained depth estimation. To better assess our method's capabilities, we curate a high-quality 4K synthetic benchmark from five different games, spanning diverse scenes with rich geometric and appearance details. Extensive experiments demonstrate that InfiniDepth achieves state-of-the-art performance on both synthetic and real-world benchmarks across relative and metric depth estimation tasks, particularly excelling in fine-detail regions. It also benefits the task of novel view synthesis under large viewpoint shifts, producing high-quality results with fewer holes and artifacts.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6489a01b8de3f9d810b0154f/XlXUo1VjGVhePk-xHRmkj.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03252.png",
        "numComments": 9,
        "submittedBy": {
            "_id": "6489a01b8de3f9d810b0154f",
            "avatarUrl": "/avatars/f7a0fc6816535945e11bac1212dd7b57.svg",
            "fullname": "Haotong Lin",
            "name": "haotongl",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9,
            "isUserFollowing": false
        },
        "organization": {
            "_id": "61bac2af530e5c78d7b99667",
            "name": "zju",
            "fullname": "Zhejiang University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2601.01554",
            "authors": [
                {
                    "_id": "695dcda5c03d6d81e4399eb8",
                    "name": "MOSI. AI",
                    "hidden": false
                },
                {
                    "_id": "695dcda5c03d6d81e4399eb9",
                    "name": "Donghua Yu",
                    "hidden": false
                },
                {
                    "_id": "695dcda5c03d6d81e4399eba",
                    "name": "Zhengyuan Lin",
                    "hidden": false
                },
                {
                    "_id": "695dcda5c03d6d81e4399ebb",
                    "user": {
                        "_id": "660c345da15ab85523ad00d1",
                        "avatarUrl": "/avatars/b0bfdee89a6c62ff12140b9e85de499a.svg",
                        "isPro": false,
                        "fullname": "Chen Yang",
                        "user": "kiiic",
                        "type": "user"
                    },
                    "name": "Chen Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-07T09:25:58.894Z",
                    "hidden": false
                },
                {
                    "_id": "695dcda5c03d6d81e4399ebc",
                    "name": "Yiyang Zhang",
                    "hidden": false
                },
                {
                    "_id": "695dcda5c03d6d81e4399ebd",
                    "name": "Hanfu Chen",
                    "hidden": false
                },
                {
                    "_id": "695dcda5c03d6d81e4399ebe",
                    "name": "Jingqi Chen",
                    "hidden": false
                },
                {
                    "_id": "695dcda5c03d6d81e4399ebf",
                    "name": "Ke Chen",
                    "hidden": false
                },
                {
                    "_id": "695dcda5c03d6d81e4399ec0",
                    "name": "Liwei Fan",
                    "hidden": false
                },
                {
                    "_id": "695dcda5c03d6d81e4399ec1",
                    "name": "Yi Jiang",
                    "hidden": false
                },
                {
                    "_id": "695dcda5c03d6d81e4399ec2",
                    "name": "Jie Zhu",
                    "hidden": false
                },
                {
                    "_id": "695dcda5c03d6d81e4399ec3",
                    "name": "Muchen Li",
                    "hidden": false
                },
                {
                    "_id": "695dcda5c03d6d81e4399ec4",
                    "name": "Wenxuan Wang",
                    "hidden": false
                },
                {
                    "_id": "695dcda5c03d6d81e4399ec5",
                    "name": "Yang Wang",
                    "hidden": false
                },
                {
                    "_id": "695dcda5c03d6d81e4399ec6",
                    "user": {
                        "_id": "6443f7bf1bc692d87b25e234",
                        "avatarUrl": "/avatars/fa9e62d96d0691a9a48e3db499a61557.svg",
                        "isPro": false,
                        "fullname": "Xu Zhe",
                        "user": "Phospheneser",
                        "type": "user"
                    },
                    "name": "Zhe Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-07T09:25:55.950Z",
                    "hidden": false
                },
                {
                    "_id": "695dcda5c03d6d81e4399ec7",
                    "name": "Yitian Gong",
                    "hidden": false
                },
                {
                    "_id": "695dcda5c03d6d81e4399ec8",
                    "name": "Yuqian Zhang",
                    "hidden": false
                },
                {
                    "_id": "695dcda5c03d6d81e4399ec9",
                    "name": "Wenbo Zhang",
                    "hidden": false
                },
                {
                    "_id": "695dcda5c03d6d81e4399eca",
                    "user": {
                        "_id": "629ef8544313a7c1dd671130",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629ef8544313a7c1dd671130/i5xfHIgELcuO1Ew19ebTw.png",
                        "isPro": false,
                        "fullname": "Zhaoye Fei",
                        "user": "ngc7293",
                        "type": "user"
                    },
                    "name": "Zhaoye Fei",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-07T13:17:10.124Z",
                    "hidden": false
                },
                {
                    "_id": "695dcda5c03d6d81e4399ecb",
                    "user": {
                        "_id": "695757e4fd9dc6e9bac27935",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/_uZEu4oOlKJVYqrG763Z-.jpeg",
                        "isPro": false,
                        "fullname": "aa",
                        "user": "qinyuancheng",
                        "type": "user"
                    },
                    "name": "Qinyuan Cheng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-07T13:17:03.749Z",
                    "hidden": false
                },
                {
                    "_id": "695dcda5c03d6d81e4399ecc",
                    "name": "Shimin Li",
                    "hidden": false
                },
                {
                    "_id": "695dcda5c03d6d81e4399ecd",
                    "user": {
                        "_id": "61457b8deff2c9fdb4de4988",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1632381702899-61457b8deff2c9fdb4de4988.jpeg",
                        "isPro": false,
                        "fullname": "Xipeng Qiu",
                        "user": "xpqiu",
                        "type": "user"
                    },
                    "name": "Xipeng Qiu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-07T13:16:50.004Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-04T15:01:10.000Z",
            "submittedOnDailyAt": "2026-01-07T00:52:16.123Z",
            "title": "MOSS Transcribe Diarize: Accurate Transcription with Speaker Diarization",
            "submittedOnDailyBy": {
                "_id": "629ef8544313a7c1dd671130",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629ef8544313a7c1dd671130/i5xfHIgELcuO1Ew19ebTw.png",
                "isPro": false,
                "fullname": "Zhaoye Fei",
                "user": "ngc7293",
                "type": "user"
            },
            "summary": "Speaker-Attributed, Time-Stamped Transcription (SATS) aims to transcribe what is said and to precisely determine the timing of each speaker, which is particularly valuable for meeting transcription. Existing SATS systems rarely adopt an end-to-end formulation and are further constrained by limited context windows, weak long-range speaker memory, and the inability to output timestamps. To address these limitations, we present MOSS Transcribe Diarize, a unified multimodal large language model that jointly performs Speaker-Attributed, Time-Stamped Transcription in an end-to-end paradigm. Trained on extensive real wild data and equipped with a 128k context window for up to 90-minute inputs, MOSS Transcribe Diarize scales well and generalizes robustly. Across comprehensive evaluations, it outperforms state-of-the-art commercial systems on multiple public and in-house benchmarks.",
            "upvotes": 45,
            "discussionId": "695dcda6c03d6d81e4399ece",
            "projectPage": "https://mosi.cn/models/moss-transcribe-diarize",
            "ai_summary": "A unified multimodal large language model for end-to-end speaker-attributed, time-stamped transcription with extended context window and strong generalization across benchmarks.",
            "ai_keywords": [
                "multimodal large language model",
                "end-to-end paradigm",
                "speaker diarization",
                "time-stamped transcription",
                "context window",
                "robust generalization"
            ],
            "organization": {
                "_id": "613b0dee83ec35d460684607",
                "name": "OpenMOSS-Team",
                "fullname": "OpenMOSS",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"
            }
        },
        "publishedAt": "2026-01-04T10:01:10.000Z",
        "title": "MOSS Transcribe Diarize: Accurate Transcription with Speaker Diarization",
        "summary": "Speaker-Attributed, Time-Stamped Transcription (SATS) aims to transcribe what is said and to precisely determine the timing of each speaker, which is particularly valuable for meeting transcription. Existing SATS systems rarely adopt an end-to-end formulation and are further constrained by limited context windows, weak long-range speaker memory, and the inability to output timestamps. To address these limitations, we present MOSS Transcribe Diarize, a unified multimodal large language model that jointly performs Speaker-Attributed, Time-Stamped Transcription in an end-to-end paradigm. Trained on extensive real wild data and equipped with a 128k context window for up to 90-minute inputs, MOSS Transcribe Diarize scales well and generalizes robustly. Across comprehensive evaluations, it outperforms state-of-the-art commercial systems on multiple public and in-house benchmarks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.01554.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "629ef8544313a7c1dd671130",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629ef8544313a7c1dd671130/i5xfHIgELcuO1Ew19ebTw.png",
            "fullname": "Zhaoye Fei",
            "name": "ngc7293",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7,
            "isUserFollowing": false
        },
        "organization": {
            "_id": "613b0dee83ec35d460684607",
            "name": "OpenMOSS-Team",
            "fullname": "OpenMOSS",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2601.03233",
            "authors": [
                {
                    "_id": "695dc6d9c03d6d81e4399e85",
                    "user": {
                        "_id": "6303cc5e0547362a22a51af0",
                        "avatarUrl": "/avatars/8f3348f121565bf6c5e1af0e559a43a3.svg",
                        "isPro": false,
                        "fullname": "Yoav HaCohen",
                        "user": "yoavhacohen",
                        "type": "user"
                    },
                    "name": "Yoav HaCohen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-07T13:19:29.722Z",
                    "hidden": false
                },
                {
                    "_id": "695dc6d9c03d6d81e4399e86",
                    "user": {
                        "_id": "6489c487b9e9258ba065418f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6489c487b9e9258ba065418f/6rzmV3bQ3YxswG6NP2hDW.png",
                        "isPro": false,
                        "fullname": "Benny Brazowski",
                        "user": "benibraz",
                        "type": "user"
                    },
                    "name": "Benny Brazowski",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-07T13:19:35.981Z",
                    "hidden": false
                },
                {
                    "_id": "695dc6d9c03d6d81e4399e87",
                    "user": {
                        "_id": "62dd30a8d43078cd49ac8ad8",
                        "avatarUrl": "/avatars/ad599719290637f7817b7508a91c2e2c.svg",
                        "isPro": false,
                        "fullname": "Nisan Chiprut",
                        "user": "nisan",
                        "type": "user"
                    },
                    "name": "Nisan Chiprut",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-07T13:19:41.634Z",
                    "hidden": false
                },
                {
                    "_id": "695dc6d9c03d6d81e4399e88",
                    "user": {
                        "_id": "64a7adc087cbd4dc7301fdd6",
                        "avatarUrl": "/avatars/b4ec4c3a0409af8ec4a5de05db453034.svg",
                        "isPro": false,
                        "fullname": "Yaki Bitterman",
                        "user": "jacobitterman",
                        "type": "user"
                    },
                    "name": "Yaki Bitterman",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-07T13:19:46.749Z",
                    "hidden": false
                },
                {
                    "_id": "695dc6d9c03d6d81e4399e89",
                    "user": {
                        "_id": "65897258509bcae23fa162c9",
                        "avatarUrl": "/avatars/29d277a0c425c936e25e82e79caa10a4.svg",
                        "isPro": false,
                        "fullname": "Andrew Kvochko",
                        "user": "kvochko",
                        "type": "user"
                    },
                    "name": "Andrew Kvochko",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-07T13:19:51.722Z",
                    "hidden": false
                },
                {
                    "_id": "695dc6d9c03d6d81e4399e8a",
                    "name": "Avishai Berkowitz",
                    "hidden": false
                },
                {
                    "_id": "695dc6d9c03d6d81e4399e8b",
                    "name": "Daniel Shalem",
                    "hidden": false
                },
                {
                    "_id": "695dc6d9c03d6d81e4399e8c",
                    "user": {
                        "_id": "681af83e2f4aaa88639e703d",
                        "avatarUrl": "/avatars/d69b664daad0afb529440c14fdb9bc3a.svg",
                        "isPro": false,
                        "fullname": "Daphna Lifschitz",
                        "user": "Daphnal",
                        "type": "user"
                    },
                    "name": "Daphna Lifschitz",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-07T13:20:04.180Z",
                    "hidden": false
                },
                {
                    "_id": "695dc6d9c03d6d81e4399e8d",
                    "user": {
                        "_id": "636b97a57631fe5e86fe1fa2",
                        "avatarUrl": "/avatars/c568ae26fd4fc2655cd12f15d539db58.svg",
                        "isPro": false,
                        "fullname": "Dudu Moshe",
                        "user": "dudumoshe",
                        "type": "user"
                    },
                    "name": "Dudu Moshe",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-07T13:20:13.512Z",
                    "hidden": false
                },
                {
                    "_id": "695dc6d9c03d6d81e4399e8e",
                    "name": "Eitan Porat",
                    "hidden": false
                },
                {
                    "_id": "695dc6d9c03d6d81e4399e8f",
                    "user": {
                        "_id": "677a422979d3c32a5dd87a0a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/WUa6E68GpnT2mEMJ41nDd.png",
                        "isPro": false,
                        "fullname": "Eitan Richardson",
                        "user": "eitanrich",
                        "type": "user"
                    },
                    "name": "Eitan Richardson",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-07T13:20:22.677Z",
                    "hidden": false
                },
                {
                    "_id": "695dc6d9c03d6d81e4399e90",
                    "user": {
                        "_id": "673f6911d83832a6ce15e7bf",
                        "avatarUrl": "/avatars/0da6cded3b0e785241a6ba5fdb5d8ceb.svg",
                        "isPro": false,
                        "fullname": "Guy Shiran",
                        "user": "guysrn",
                        "type": "user"
                    },
                    "name": "Guy Shiran",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-07T13:20:28.250Z",
                    "hidden": false
                },
                {
                    "_id": "695dc6d9c03d6d81e4399e91",
                    "user": {
                        "_id": "65744a2fe09de6aa74026d80",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65744a2fe09de6aa74026d80/kCxIKdeBJwAPKmvlm7fDP.jpeg",
                        "isPro": false,
                        "fullname": "Itay Chachy",
                        "user": "ItayChachy",
                        "type": "user"
                    },
                    "name": "Itay Chachy",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-07T13:20:36.781Z",
                    "hidden": false
                },
                {
                    "_id": "695dc6d9c03d6d81e4399e92",
                    "name": "Jonathan Chetboun",
                    "hidden": false
                },
                {
                    "_id": "695dc6d9c03d6d81e4399e93",
                    "user": {
                        "_id": "6678365ac411b340b32d6148",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6678365ac411b340b32d6148/7OhHzbu65pa95eYrAbbLW.jpeg",
                        "isPro": false,
                        "fullname": "Michael Finkelson",
                        "user": "MichaelFinkelson",
                        "type": "user"
                    },
                    "name": "Michael Finkelson",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-07T13:20:53.574Z",
                    "hidden": false
                },
                {
                    "_id": "695dc6d9c03d6d81e4399e94",
                    "user": {
                        "_id": "6318aa43cb4ca740c4c55651",
                        "avatarUrl": "/avatars/24082c776d284393a5a38a99e5c0bab8.svg",
                        "isPro": false,
                        "fullname": "michael kupchick",
                        "user": "michaellightricks",
                        "type": "user"
                    },
                    "name": "Michael Kupchick",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-07T13:20:59.945Z",
                    "hidden": false
                },
                {
                    "_id": "695dc6d9c03d6d81e4399e95",
                    "user": {
                        "_id": "673f29b568595672b8d3e90e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673f29b568595672b8d3e90e/4sYADg3mpqMKmJ4fQwaTl.png",
                        "isPro": false,
                        "fullname": "Nir Zabari",
                        "user": "NirZabariLTX",
                        "type": "user"
                    },
                    "name": "Nir Zabari",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-07T13:21:07.297Z",
                    "hidden": false
                },
                {
                    "_id": "695dc6d9c03d6d81e4399e96",
                    "user": {
                        "_id": "64ae89c043dda9449a1eb1ba",
                        "avatarUrl": "/avatars/12cf3de929d38ddd92cc3f3337dc2ed2.svg",
                        "isPro": false,
                        "fullname": "Nitzan Guetta",
                        "user": "nitzanguetta",
                        "type": "user"
                    },
                    "name": "Nitzan Guetta",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-07T13:21:14.712Z",
                    "hidden": false
                },
                {
                    "_id": "695dc6d9c03d6d81e4399e97",
                    "name": "Noa Kotler",
                    "hidden": false
                },
                {
                    "_id": "695dc6d9c03d6d81e4399e98",
                    "user": {
                        "_id": "631f58935ba8c026340b377c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631f58935ba8c026340b377c/4yoHLdNE99VBb7ji_Mzzj.jpeg",
                        "isPro": false,
                        "fullname": "Ofir Bibi",
                        "user": "ofirbibi",
                        "type": "user"
                    },
                    "name": "Ofir Bibi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-07T13:21:27.196Z",
                    "hidden": false
                },
                {
                    "_id": "695dc6d9c03d6d81e4399e99",
                    "user": {
                        "_id": "674348b46215a2c0878e219b",
                        "avatarUrl": "/avatars/8a213e431a1583d1a93377410907c059.svg",
                        "isPro": false,
                        "fullname": "Ori Gordon",
                        "user": "origordon",
                        "type": "user"
                    },
                    "name": "Ori Gordon",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-07T13:21:34.878Z",
                    "hidden": false
                },
                {
                    "_id": "695dc6d9c03d6d81e4399e9a",
                    "name": "Poriya Panet",
                    "hidden": false
                },
                {
                    "_id": "695dc6d9c03d6d81e4399e9b",
                    "name": "Roi Benita",
                    "hidden": false
                },
                {
                    "_id": "695dc6d9c03d6d81e4399e9c",
                    "name": "Shahar Armon",
                    "hidden": false
                },
                {
                    "_id": "695dc6d9c03d6d81e4399e9d",
                    "name": "Victor Kulikov",
                    "hidden": false
                },
                {
                    "_id": "695dc6d9c03d6d81e4399e9e",
                    "name": "Yaron Inger",
                    "hidden": false
                },
                {
                    "_id": "695dc6d9c03d6d81e4399e9f",
                    "name": "Yonatan Shiftan",
                    "hidden": false
                },
                {
                    "_id": "695dc6d9c03d6d81e4399ea0",
                    "name": "Zeev Melumian",
                    "hidden": false
                },
                {
                    "_id": "695dc6d9c03d6d81e4399ea1",
                    "name": "Zeev Farbman",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/cYoXYuK3pjt85pl5-fvUv.mp4"
            ],
            "publishedAt": "2026-01-06T18:24:41.000Z",
            "submittedOnDailyAt": "2026-01-07T00:07:29.528Z",
            "title": "LTX-2: Efficient Joint Audio-Visual Foundation Model",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Recent text-to-video diffusion models can generate compelling video sequences, yet they remain silent -- missing the semantic, emotional, and atmospheric cues that audio provides. We introduce LTX-2, an open-source foundational model capable of generating high-quality, temporally synchronized audiovisual content in a unified manner. LTX-2 consists of an asymmetric dual-stream transformer with a 14B-parameter video stream and a 5B-parameter audio stream, coupled through bidirectional audio-video cross-attention layers with temporal positional embeddings and cross-modality AdaLN for shared timestep conditioning. This architecture enables efficient training and inference of a unified audiovisual model while allocating more capacity for video generation than audio generation. We employ a multilingual text encoder for broader prompt understanding and introduce a modality-aware classifier-free guidance (modality-CFG) mechanism for improved audiovisual alignment and controllability. Beyond generating speech, LTX-2 produces rich, coherent audio tracks that follow the characters, environment, style, and emotion of each scene -- complete with natural background and foley elements. In our evaluations, the model achieves state-of-the-art audiovisual quality and prompt adherence among open-source systems, while delivering results comparable to proprietary models at a fraction of their computational cost and inference time. All model weights and code are publicly released.",
            "upvotes": 43,
            "discussionId": "695dc6d9c03d6d81e4399ea2",
            "projectPage": "https://app.ltx.studio/ltx-2-playground/i2v",
            "githubRepo": "https://github.com/Lightricks/LTX-2",
            "githubRepoAddedBy": "user",
            "ai_summary": "LTX-2 is an open-source audiovisual diffusion model that generates synchronized video and audio content using a dual-stream transformer architecture with cross-modal attention and classifier-free guidance.",
            "ai_keywords": [
                "text-to-video diffusion models",
                "audiovisual content",
                "dual-stream transformer",
                "cross-attention layers",
                "temporal positional embeddings",
                "AdaLN",
                "classifier-free guidance",
                "modality-aware classifier-free guidance",
                "multilingual text encoder",
                "diffusion models"
            ],
            "githubStars": 922
        },
        "publishedAt": "2026-01-06T13:24:41.000Z",
        "title": "LTX-2: Efficient Joint Audio-Visual Foundation Model",
        "summary": "Recent text-to-video diffusion models can generate compelling video sequences, yet they remain silent -- missing the semantic, emotional, and atmospheric cues that audio provides. We introduce LTX-2, an open-source foundational model capable of generating high-quality, temporally synchronized audiovisual content in a unified manner. LTX-2 consists of an asymmetric dual-stream transformer with a 14B-parameter video stream and a 5B-parameter audio stream, coupled through bidirectional audio-video cross-attention layers with temporal positional embeddings and cross-modality AdaLN for shared timestep conditioning. This architecture enables efficient training and inference of a unified audiovisual model while allocating more capacity for video generation than audio generation. We employ a multilingual text encoder for broader prompt understanding and introduce a modality-aware classifier-free guidance (modality-CFG) mechanism for improved audiovisual alignment and controllability. Beyond generating speech, LTX-2 produces rich, coherent audio tracks that follow the characters, environment, style, and emotion of each scene -- complete with natural background and foley elements. In our evaluations, the model achieves state-of-the-art audiovisual quality and prompt adherence among open-source systems, while delivering results comparable to proprietary models at a fraction of their computational cost and inference time. All model weights and code are publicly released.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/cYoXYuK3pjt85pl5-fvUv.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03233.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 202,
            "isUserFollowing": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.22334",
            "authors": [
                {
                    "_id": "695e0748c03d6d81e439a027",
                    "user": {
                        "_id": "67d2a001919e3b981ad45066",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67d2a001919e3b981ad45066/KxaYKOGgLVbVhN6EsYGRG.jpeg",
                        "isPro": false,
                        "fullname": "wyh",
                        "user": "naonaowyh",
                        "type": "user"
                    },
                    "name": "Yiheng Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-07T09:25:26.201Z",
                    "hidden": false
                },
                {
                    "_id": "695e0748c03d6d81e439a028",
                    "user": {
                        "_id": "64c396def6fe448b1ad553d6",
                        "avatarUrl": "/avatars/b2ce4739f42dc00ee974fff7ee1cb301.svg",
                        "isPro": false,
                        "fullname": "Yixin Chen",
                        "user": "YixinChen",
                        "type": "user"
                    },
                    "name": "Yixin Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-07T14:25:35.753Z",
                    "hidden": false
                },
                {
                    "_id": "695e0748c03d6d81e439a029",
                    "name": "Shuo Li",
                    "hidden": false
                },
                {
                    "_id": "695e0748c03d6d81e439a02a",
                    "user": {
                        "_id": "659d2dff20cf0b934bbee513",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/659d2dff20cf0b934bbee513/9e9R852Zr2R82h64eUUQl.jpeg",
                        "isPro": false,
                        "fullname": "Yifan Zhou",
                        "user": "yingmanji",
                        "type": "user"
                    },
                    "name": "Yifan Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-07T09:25:19.089Z",
                    "hidden": false
                },
                {
                    "_id": "695e0748c03d6d81e439a02b",
                    "name": "Bo Liu",
                    "hidden": false
                },
                {
                    "_id": "695e0748c03d6d81e439a02c",
                    "user": {
                        "_id": "66d19399b26010e571b1fcf0",
                        "avatarUrl": "/avatars/9d0998fc38f6659305bcbcecaf0a1c96.svg",
                        "isPro": false,
                        "fullname": "Hengjian Gao",
                        "user": "Talthy",
                        "type": "user"
                    },
                    "name": "Hengjian Gao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-07T09:25:28.254Z",
                    "hidden": false
                },
                {
                    "_id": "695e0748c03d6d81e439a02d",
                    "user": {
                        "_id": "64a3d1ddb3239f3e3892b24b",
                        "avatarUrl": "/avatars/7ce585f5fc1d077fb1d70cc18c4da2c1.svg",
                        "isPro": false,
                        "fullname": "Jiakang Yuan",
                        "user": "JiakangYuan",
                        "type": "user"
                    },
                    "name": "Jiakang Yuan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-07T14:25:23.256Z",
                    "hidden": false
                },
                {
                    "_id": "695e0748c03d6d81e439a02e",
                    "name": "Jia Bu",
                    "hidden": false
                },
                {
                    "_id": "695e0748c03d6d81e439a02f",
                    "user": {
                        "_id": "65f3f43fc9940817ca9a427b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65f3f43fc9940817ca9a427b/02NN3XjSsbgWDhjrJWtVL.jpeg",
                        "isPro": false,
                        "fullname": "Wanghan Xu",
                        "user": "CoCoOne",
                        "type": "user"
                    },
                    "name": "Wanghan Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-07T09:25:21.127Z",
                    "hidden": false
                },
                {
                    "_id": "695e0748c03d6d81e439a030",
                    "user": {
                        "_id": "63bab9c1bb6a2fabd14421bd",
                        "avatarUrl": "/avatars/c47030cf167072bf6ce3421f025c7746.svg",
                        "isPro": false,
                        "fullname": "Yuhao Zhou",
                        "user": "Soptq",
                        "type": "user"
                    },
                    "name": "Yuhao Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-07T13:18:03.373Z",
                    "hidden": false
                },
                {
                    "_id": "695e0748c03d6d81e439a031",
                    "name": "Xiangyu Zhao",
                    "hidden": false
                },
                {
                    "_id": "695e0748c03d6d81e439a032",
                    "name": "Zhiwang Zhou",
                    "hidden": false
                },
                {
                    "_id": "695e0748c03d6d81e439a033",
                    "name": "Fengxiang Wang",
                    "hidden": false
                },
                {
                    "_id": "695e0748c03d6d81e439a034",
                    "user": {
                        "_id": "63ee1379190ddd6214efd73a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676546883247-noauth.png",
                        "isPro": false,
                        "fullname": "HAODONG DUAN",
                        "user": "KennyUTC",
                        "type": "user"
                    },
                    "name": "Haodong Duan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-07T13:18:16.943Z",
                    "hidden": false
                },
                {
                    "_id": "695e0748c03d6d81e439a035",
                    "name": "Songyang Zhang",
                    "hidden": false
                },
                {
                    "_id": "695e0748c03d6d81e439a036",
                    "name": "Jun Yao",
                    "hidden": false
                },
                {
                    "_id": "695e0748c03d6d81e439a037",
                    "name": "Han Deng",
                    "hidden": false
                },
                {
                    "_id": "695e0748c03d6d81e439a038",
                    "name": "Yizhou Wang",
                    "hidden": false
                },
                {
                    "_id": "695e0748c03d6d81e439a039",
                    "user": {
                        "_id": "650d2f798ffe1f53bdd4ae71",
                        "avatarUrl": "/avatars/4c733681f8301d801023105c0b3aba38.svg",
                        "isPro": false,
                        "fullname": "Xiao Jiabei",
                        "user": "Xiao-Youth",
                        "type": "user"
                    },
                    "name": "Jiabei Xiao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-07T14:25:59.280Z",
                    "hidden": false
                },
                {
                    "_id": "695e0748c03d6d81e439a03a",
                    "name": "Jiaqi Liu",
                    "hidden": false
                },
                {
                    "_id": "695e0748c03d6d81e439a03b",
                    "user": {
                        "_id": "6784a79c5ffcc118504ed99c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/eL4kAIp_oi9Q7si5--uVl.png",
                        "isPro": false,
                        "fullname": "Encheng Su",
                        "user": "EncSU",
                        "type": "user"
                    },
                    "name": "Encheng Su",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-07T13:18:31.162Z",
                    "hidden": false
                },
                {
                    "_id": "695e0748c03d6d81e439a03c",
                    "name": "Yujie Liu",
                    "hidden": false
                },
                {
                    "_id": "695e0748c03d6d81e439a03d",
                    "user": {
                        "_id": "661b9d96c153e4a0a25adc3e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/661b9d96c153e4a0a25adc3e/VRt7kCQ0KdJp-lhPLOajO.jpeg",
                        "isPro": false,
                        "fullname": "Weida Wang",
                        "user": "weidawang",
                        "type": "user"
                    },
                    "name": "Weida Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-07T09:25:23.748Z",
                    "hidden": false
                },
                {
                    "_id": "695e0748c03d6d81e439a03e",
                    "name": "Junchi Yao",
                    "hidden": false
                },
                {
                    "_id": "695e0748c03d6d81e439a03f",
                    "name": "Shenghe Zheng",
                    "hidden": false
                },
                {
                    "_id": "695e0748c03d6d81e439a040",
                    "name": "Haoran Sun",
                    "hidden": false
                },
                {
                    "_id": "695e0748c03d6d81e439a041",
                    "name": "Runmin Ma",
                    "hidden": false
                },
                {
                    "_id": "695e0748c03d6d81e439a042",
                    "name": "Xiangchao Yan",
                    "hidden": false
                },
                {
                    "_id": "695e0748c03d6d81e439a043",
                    "name": "Bo Zhang",
                    "hidden": false
                },
                {
                    "_id": "695e0748c03d6d81e439a044",
                    "name": "Dongzhan Zhou",
                    "hidden": false
                },
                {
                    "_id": "695e0748c03d6d81e439a045",
                    "user": {
                        "_id": "68f8924f4278c8968587dfea",
                        "avatarUrl": "/avatars/b48d7899881a14ca68cc9feb2643ad8c.svg",
                        "isPro": false,
                        "fullname": "shufei zhang",
                        "user": "ShufeiZhang",
                        "type": "user"
                    },
                    "name": "Shufei Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-07T13:18:24.428Z",
                    "hidden": false
                },
                {
                    "_id": "695e0748c03d6d81e439a046",
                    "name": "Peng Ye",
                    "hidden": false
                },
                {
                    "_id": "695e0748c03d6d81e439a047",
                    "name": "Xiaosong Wang",
                    "hidden": false
                },
                {
                    "_id": "695e0748c03d6d81e439a048",
                    "name": "Shixiang Tang",
                    "hidden": false
                },
                {
                    "_id": "695e0748c03d6d81e439a049",
                    "name": "Wenlong Zhang",
                    "hidden": false
                },
                {
                    "_id": "695e0748c03d6d81e439a04a",
                    "name": "Lei Bai",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-26T17:36:02.000Z",
            "submittedOnDailyAt": "2026-01-07T05:09:43.722Z",
            "title": "SciEvalKit: An Open-source Evaluation Toolkit for Scientific General Intelligence",
            "submittedOnDailyBy": {
                "_id": "63bab9c1bb6a2fabd14421bd",
                "avatarUrl": "/avatars/c47030cf167072bf6ce3421f025c7746.svg",
                "isPro": false,
                "fullname": "Yuhao Zhou",
                "user": "Soptq",
                "type": "user"
            },
            "summary": "We introduce SciEvalKit, a unified benchmarking toolkit designed to evaluate AI models for science across a broad range of scientific disciplines and task capabilities. Unlike general-purpose evaluation platforms, SciEvalKit focuses on the core competencies of scientific intelligence, including Scientific Multimodal Perception, Scientific Multimodal Reasoning, Scientific Multimodal Understanding, Scientific Symbolic Reasoning, Scientific Code Generation, Science Hypothesis Generation and Scientific Knowledge Understanding. It supports six major scientific domains, spanning from physics and chemistry to astronomy and materials science. SciEvalKit builds a foundation of expert-grade scientific benchmarks, curated from real-world, domain-specific datasets, ensuring that tasks reflect authentic scientific challenges. The toolkit features a flexible, extensible evaluation pipeline that enables batch evaluation across models and datasets, supports custom model and dataset integration, and provides transparent, reproducible, and comparable results. By bridging capability-based evaluation and disciplinary diversity, SciEvalKit offers a standardized yet customizable infrastructure to benchmark the next generation of scientific foundation models and intelligent agents. The toolkit is open-sourced and actively maintained to foster community-driven development and progress in AI4Science.",
            "upvotes": 28,
            "discussionId": "695e0748c03d6d81e439a04b",
            "projectPage": "https://opencompass.org.cn/Intern-Discovery-Eval/rank",
            "githubRepo": "https://github.com/InternScience/SciEvalKit",
            "githubRepoAddedBy": "user",
            "ai_summary": "SciEvalKit is a unified benchmarking toolkit for evaluating AI models across diverse scientific disciplines, focusing on core scientific intelligence capabilities and supporting customizable, reproducible assessments.",
            "ai_keywords": [
                "AI models",
                "scientific intelligence",
                "scientific multimodal perception",
                "scientific multimodal reasoning",
                "scientific multimodal understanding",
                "scientific symbolic reasoning",
                "scientific code generation",
                "science hypothesis generation",
                "scientific knowledge understanding",
                "scientific foundation models",
                "intelligent agents",
                "domain-specific datasets",
                "expert-grade benchmarks",
                "evaluation pipeline",
                "batch evaluation",
                "custom model integration",
                "reproducible results"
            ],
            "githubStars": 56,
            "organization": {
                "_id": "690af7a885f71496ea396393",
                "name": "InternScience",
                "fullname": "Intern Science",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65f2a4198404ac0e4c0f175f/XIPU4aCPBogXSrj6NrfLk.png"
            }
        },
        "publishedAt": "2025-12-26T12:36:02.000Z",
        "title": "SciEvalKit: An Open-source Evaluation Toolkit for Scientific General Intelligence",
        "summary": "We introduce SciEvalKit, a unified benchmarking toolkit designed to evaluate AI models for science across a broad range of scientific disciplines and task capabilities. Unlike general-purpose evaluation platforms, SciEvalKit focuses on the core competencies of scientific intelligence, including Scientific Multimodal Perception, Scientific Multimodal Reasoning, Scientific Multimodal Understanding, Scientific Symbolic Reasoning, Scientific Code Generation, Science Hypothesis Generation and Scientific Knowledge Understanding. It supports six major scientific domains, spanning from physics and chemistry to astronomy and materials science. SciEvalKit builds a foundation of expert-grade scientific benchmarks, curated from real-world, domain-specific datasets, ensuring that tasks reflect authentic scientific challenges. The toolkit features a flexible, extensible evaluation pipeline that enables batch evaluation across models and datasets, supports custom model and dataset integration, and provides transparent, reproducible, and comparable results. By bridging capability-based evaluation and disciplinary diversity, SciEvalKit offers a standardized yet customizable infrastructure to benchmark the next generation of scientific foundation models and intelligent agents. The toolkit is open-sourced and actively maintained to foster community-driven development and progress in AI4Science.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.22334.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63bab9c1bb6a2fabd14421bd",
            "avatarUrl": "/avatars/c47030cf167072bf6ce3421f025c7746.svg",
            "fullname": "Yuhao Zhou",
            "name": "Soptq",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3,
            "isUserFollowing": false
        },
        "organization": {
            "_id": "690af7a885f71496ea396393",
            "name": "InternScience",
            "fullname": "Intern Science",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65f2a4198404ac0e4c0f175f/XIPU4aCPBogXSrj6NrfLk.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2601.03193",
            "authors": [
                {
                    "_id": "695de87dc03d6d81e4399fd2",
                    "user": {
                        "_id": "6723369dc09be95d8c49c605",
                        "avatarUrl": "/avatars/797df409537c07cbf894f1c027cddbb1.svg",
                        "isPro": false,
                        "fullname": "Ruiyan Han",
                        "user": "Hungryyan",
                        "type": "user"
                    },
                    "name": "Ruiyan Han",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-07T13:21:53.416Z",
                    "hidden": false
                },
                {
                    "_id": "695de87dc03d6d81e4399fd3",
                    "user": {
                        "_id": "64b0a5037a475fba70a7260d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b0a5037a475fba70a7260d/MauBbb6raMA23yrR1Zq21.jpeg",
                        "isPro": false,
                        "fullname": "Zhen Fang",
                        "user": "CostaliyA",
                        "type": "user"
                    },
                    "name": "Zhen Fang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-07T13:13:51.100Z",
                    "hidden": false
                },
                {
                    "_id": "695de87dc03d6d81e4399fd4",
                    "name": "XinYu Sun",
                    "hidden": false
                },
                {
                    "_id": "695de87dc03d6d81e4399fd5",
                    "name": "Yuchen Ma",
                    "hidden": false
                },
                {
                    "_id": "695de87dc03d6d81e4399fd6",
                    "name": "Ziheng Wang",
                    "hidden": false
                },
                {
                    "_id": "695de87dc03d6d81e4399fd7",
                    "user": {
                        "_id": "665d652e0f35c005de892108",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/665d652e0f35c005de892108/OGLbgZekX-3XTBkwS8k86.jpeg",
                        "isPro": false,
                        "fullname": "Yu Zeng",
                        "user": "YuZeng260",
                        "type": "user"
                    },
                    "name": "Yu Zeng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-07T09:25:35.308Z",
                    "hidden": false
                },
                {
                    "_id": "695de87dc03d6d81e4399fd8",
                    "user": {
                        "_id": "64892d31cbda0d1cdb956897",
                        "avatarUrl": "/avatars/3cdafe03a8295124636347d15a099aaf.svg",
                        "isPro": false,
                        "fullname": "Zehui Chen",
                        "user": "lovesnowbest",
                        "type": "user"
                    },
                    "name": "Zehui Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-07T13:22:13.382Z",
                    "hidden": false
                },
                {
                    "_id": "695de87dc03d6d81e4399fd9",
                    "user": {
                        "_id": "64b02ec0e5000ae8a572ced5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b02ec0e5000ae8a572ced5/6ifLntBU2ICQK7SW8WxKU.png",
                        "isPro": false,
                        "fullname": "Lin Chen",
                        "user": "Lin-Chen",
                        "type": "user"
                    },
                    "name": "Lin Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-07T09:25:37.096Z",
                    "hidden": false
                },
                {
                    "_id": "695de87dc03d6d81e4399fda",
                    "user": {
                        "_id": "67dc162ec8c00778e8689f42",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67dc162ec8c00778e8689f42/_y_tO6W3ONOkOWbumAFXA.png",
                        "isPro": false,
                        "fullname": "Wenxuan Huang",
                        "user": "Osilly",
                        "type": "user"
                    },
                    "name": "Wenxuan Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-07T13:37:57.684Z",
                    "hidden": false
                },
                {
                    "_id": "695de87dc03d6d81e4399fdb",
                    "name": "Wei-Jie Xu",
                    "hidden": false
                },
                {
                    "_id": "695de87dc03d6d81e4399fdc",
                    "name": "Yi Cao",
                    "hidden": false
                },
                {
                    "_id": "695de87dc03d6d81e4399fdd",
                    "name": "Feng Zhao",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-06T17:15:50.000Z",
            "submittedOnDailyAt": "2026-01-07T02:31:26.744Z",
            "title": "UniCorn: Towards Self-Improving Unified Multimodal Models through Self-Generated Supervision",
            "submittedOnDailyBy": {
                "_id": "64b02ec0e5000ae8a572ced5",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b02ec0e5000ae8a572ced5/6ifLntBU2ICQK7SW8WxKU.png",
                "isPro": false,
                "fullname": "Lin Chen",
                "user": "Lin-Chen",
                "type": "user"
            },
            "summary": "While Unified Multimodal Models (UMMs) have achieved remarkable success in cross-modal comprehension, a significant gap persists in their ability to leverage such internal knowledge for high-quality generation. We formalize this discrepancy as Conduction Aphasia, a phenomenon where models accurately interpret multimodal inputs but struggle to translate that understanding into faithful and controllable synthesis. To address this, we propose UniCorn, a simple yet elegant self-improvement framework that eliminates the need for external data or teacher supervision. By partitioning a single UMM into three collaborative roles: Proposer, Solver, and Judge, UniCorn generates high-quality interactions via self-play and employs cognitive pattern reconstruction to distill latent understanding into explicit generative signals. To validate the restoration of multimodal coherence, we introduce UniCycle, a cycle-consistency benchmark based on a Text to Image to Text reconstruction loop. Extensive experiments demonstrate that UniCorn achieves comprehensive and substantial improvements over the base model across six general image generation benchmarks. Notably, it achieves SOTA performance on TIIF(73.8), DPG(86.8), CompBench(88.5), and UniCycle while further delivering substantial gains of +5.0 on WISE and +6.5 on OneIG. These results highlight that our method significantly enhances T2I generation while maintaining robust comprehension, demonstrating the scalability of fully self-supervised refinement for unified multimodal intelligence.",
            "upvotes": 25,
            "discussionId": "695de87dc03d6d81e4399fde",
            "projectPage": "https://costaliya.github.io/UniCorn.github.io/",
            "githubRepo": "https://github.com/Hungryyan1/UniCorn",
            "githubRepoAddedBy": "user",
            "ai_summary": "UniCorn, a self-improvement framework for unified multimodal models, addresses generation gaps through self-play and cognitive pattern reconstruction, achieving state-of-the-art results in text-to-image generation.",
            "ai_keywords": [
                "Unified Multimodal Models",
                "Conduction Aphasia",
                "UniCorn",
                "self-improvement framework",
                "self-play",
                "cognitive pattern reconstruction",
                "Text to Image",
                "cycle-consistency",
                "UniCycle",
                "T2I generation"
            ],
            "githubStars": 25
        },
        "publishedAt": "2026-01-06T12:15:50.000Z",
        "title": "UniCorn: Towards Self-Improving Unified Multimodal Models through Self-Generated Supervision",
        "summary": "While Unified Multimodal Models (UMMs) have achieved remarkable success in cross-modal comprehension, a significant gap persists in their ability to leverage such internal knowledge for high-quality generation. We formalize this discrepancy as Conduction Aphasia, a phenomenon where models accurately interpret multimodal inputs but struggle to translate that understanding into faithful and controllable synthesis. To address this, we propose UniCorn, a simple yet elegant self-improvement framework that eliminates the need for external data or teacher supervision. By partitioning a single UMM into three collaborative roles: Proposer, Solver, and Judge, UniCorn generates high-quality interactions via self-play and employs cognitive pattern reconstruction to distill latent understanding into explicit generative signals. To validate the restoration of multimodal coherence, we introduce UniCycle, a cycle-consistency benchmark based on a Text to Image to Text reconstruction loop. Extensive experiments demonstrate that UniCorn achieves comprehensive and substantial improvements over the base model across six general image generation benchmarks. Notably, it achieves SOTA performance on TIIF(73.8), DPG(86.8), CompBench(88.5), and UniCycle while further delivering substantial gains of +5.0 on WISE and +6.5 on OneIG. These results highlight that our method significantly enhances T2I generation while maintaining robust comprehension, demonstrating the scalability of fully self-supervised refinement for unified multimodal intelligence.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03193.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "64b02ec0e5000ae8a572ced5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b02ec0e5000ae8a572ced5/6ifLntBU2ICQK7SW8WxKU.png",
            "fullname": "Lin Chen",
            "name": "Lin-Chen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 91,
            "isUserFollowing": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2601.02427",
            "authors": [
                {
                    "_id": "695dce13c03d6d81e4399ed0",
                    "name": "Loc Magne",
                    "hidden": false
                },
                {
                    "_id": "695dce13c03d6d81e4399ed1",
                    "name": "Anas Awadalla",
                    "hidden": false
                },
                {
                    "_id": "695dce13c03d6d81e4399ed2",
                    "name": "Guanzhi Wang",
                    "hidden": false
                },
                {
                    "_id": "695dce13c03d6d81e4399ed3",
                    "name": "Yinzhen Xu",
                    "hidden": false
                },
                {
                    "_id": "695dce13c03d6d81e4399ed4",
                    "name": "Joshua Belofsky",
                    "hidden": false
                },
                {
                    "_id": "695dce13c03d6d81e4399ed5",
                    "name": "Fengyuan Hu",
                    "hidden": false
                },
                {
                    "_id": "695dce13c03d6d81e4399ed6",
                    "name": "Joohwan Kim",
                    "hidden": false
                },
                {
                    "_id": "695dce13c03d6d81e4399ed7",
                    "name": "Ludwig Schmidt",
                    "hidden": false
                },
                {
                    "_id": "695dce13c03d6d81e4399ed8",
                    "name": "Georgia Gkioxari",
                    "hidden": false
                },
                {
                    "_id": "695dce13c03d6d81e4399ed9",
                    "name": "Jan Kautz",
                    "hidden": false
                },
                {
                    "_id": "695dce13c03d6d81e4399eda",
                    "name": "Yisong Yue",
                    "hidden": false
                },
                {
                    "_id": "695dce13c03d6d81e4399edb",
                    "name": "Yejin Choi",
                    "hidden": false
                },
                {
                    "_id": "695dce13c03d6d81e4399edc",
                    "name": "Yuke Zhu",
                    "hidden": false
                },
                {
                    "_id": "695dce13c03d6d81e4399edd",
                    "name": "Linxi \"Jim\" Fan",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/wxFZ7NVsObT5w9DBp9Gdu.mp4"
            ],
            "publishedAt": "2026-01-04T16:24:50.000Z",
            "submittedOnDailyAt": "2026-01-07T00:38:46.198Z",
            "title": "NitroGen: An Open Foundation Model for Generalist Gaming Agents",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We introduce NitroGen, a vision-action foundation model for generalist gaming agents that is trained on 40,000 hours of gameplay videos across more than 1,000 games. We incorporate three key ingredients: 1) an internet-scale video-action dataset constructed by automatically extracting player actions from publicly available gameplay videos, 2) a multi-game benchmark environment that can measure cross-game generalization, and 3) a unified vision-action model trained with large-scale behavior cloning. NitroGen exhibits strong competence across diverse domains, including combat encounters in 3D action games, high-precision control in 2D platformers, and exploration in procedurally generated worlds. It transfers effectively to unseen games, achieving up to 52% relative improvement in task success rates over models trained from scratch. We release the dataset, evaluation suite, and model weights to advance research on generalist embodied agents.",
            "upvotes": 22,
            "discussionId": "695dce14c03d6d81e4399ede",
            "projectPage": "https://nitrogen.minedojo.org/",
            "githubRepo": "https://github.com/MineDojo/NitroGen",
            "githubRepoAddedBy": "user",
            "ai_summary": "NitroGen is a vision-action foundation model trained on extensive gameplay data that demonstrates strong cross-game generalization and effective transfer learning capabilities.",
            "ai_keywords": [
                "vision-action foundation model",
                "large-scale behavior cloning",
                "cross-game generalization",
                "transfer learning",
                "embodied agents"
            ],
            "githubStars": 1443,
            "organization": {
                "_id": "60262b67268c201cdc8b7d43",
                "name": "nvidia",
                "fullname": "NVIDIA",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
            }
        },
        "publishedAt": "2026-01-04T11:24:50.000Z",
        "title": "NitroGen: An Open Foundation Model for Generalist Gaming Agents",
        "summary": "We introduce NitroGen, a vision-action foundation model for generalist gaming agents that is trained on 40,000 hours of gameplay videos across more than 1,000 games. We incorporate three key ingredients: 1) an internet-scale video-action dataset constructed by automatically extracting player actions from publicly available gameplay videos, 2) a multi-game benchmark environment that can measure cross-game generalization, and 3) a unified vision-action model trained with large-scale behavior cloning. NitroGen exhibits strong competence across diverse domains, including combat encounters in 3D action games, high-precision control in 2D platformers, and exploration in procedurally generated worlds. It transfers effectively to unseen games, achieving up to 52% relative improvement in task success rates over models trained from scratch. We release the dataset, evaluation suite, and model weights to advance research on generalist embodied agents.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/wxFZ7NVsObT5w9DBp9Gdu.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02427.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 202,
            "isUserFollowing": false
        },
        "organization": {
            "_id": "60262b67268c201cdc8b7d43",
            "name": "nvidia",
            "fullname": "NVIDIA",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2601.03044",
            "authors": [
                {
                    "_id": "695dc422c03d6d81e4399e60",
                    "user": {
                        "_id": "655ecd7c56e5ceaf05344b24",
                        "avatarUrl": "/avatars/faad88525197bb6c63be0068f19de418.svg",
                        "isPro": false,
                        "fullname": "MingjieP",
                        "user": "pmj110119",
                        "type": "user"
                    },
                    "name": "Mingjie Pan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-07T13:27:27.004Z",
                    "hidden": false
                },
                {
                    "_id": "695dc422c03d6d81e4399e61",
                    "user": {
                        "_id": "620326e962b2b0e46e79971b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620326e962b2b0e46e79971b/1FVPRpsWng5q3An4qbuYQ.jpeg",
                        "isPro": false,
                        "fullname": "Siyuan Feng",
                        "user": "Eralien",
                        "type": "user"
                    },
                    "name": "Siyuan Feng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-07T09:26:03.276Z",
                    "hidden": false
                },
                {
                    "_id": "695dc422c03d6d81e4399e62",
                    "name": "Qinglin Zhang",
                    "hidden": false
                },
                {
                    "_id": "695dc422c03d6d81e4399e63",
                    "name": "Xinchen Li",
                    "hidden": false
                },
                {
                    "_id": "695dc422c03d6d81e4399e64",
                    "user": {
                        "_id": "64993b27f8069251837b81ed",
                        "avatarUrl": "/avatars/9aa3956dc527dccfdc6b6014dedf761f.svg",
                        "isPro": false,
                        "fullname": "Song Jianheng",
                        "user": "JJH1998",
                        "type": "user"
                    },
                    "name": "Jianheng Song",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-07T13:28:32.580Z",
                    "hidden": false
                },
                {
                    "_id": "695dc422c03d6d81e4399e65",
                    "name": "Chendi Qu",
                    "hidden": false
                },
                {
                    "_id": "695dc422c03d6d81e4399e66",
                    "name": "Yi Wang",
                    "hidden": false
                },
                {
                    "_id": "695dc422c03d6d81e4399e67",
                    "name": "Chuankang Li",
                    "hidden": false
                },
                {
                    "_id": "695dc422c03d6d81e4399e68",
                    "user": {
                        "_id": "64d107229617774ce41b9467",
                        "avatarUrl": "/avatars/10d4dff031f2d8dd7047d7ea5ec0d4c1.svg",
                        "isPro": false,
                        "fullname": "Ziyu Xiong",
                        "user": "JennyZiyu",
                        "type": "user"
                    },
                    "name": "Ziyu Xiong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-07T13:28:14.043Z",
                    "hidden": false
                },
                {
                    "_id": "695dc422c03d6d81e4399e69",
                    "name": "Zhi Chen",
                    "hidden": false
                },
                {
                    "_id": "695dc422c03d6d81e4399e6a",
                    "name": "Yi Liu",
                    "hidden": false
                },
                {
                    "_id": "695dc422c03d6d81e4399e6b",
                    "user": {
                        "_id": "64f8cb8ed04a890f5380d9a4",
                        "avatarUrl": "/avatars/d6fdfdbb0c10141aa3b4c832d928121b.svg",
                        "isPro": false,
                        "fullname": "Jianlan Luo",
                        "user": "jianlanluo",
                        "type": "user"
                    },
                    "name": "Jianlan Luo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-07T13:13:52.912Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/655ecd7c56e5ceaf05344b24/AEQ64pAi4UD3rvJ33Qeag.mp4"
            ],
            "publishedAt": "2026-01-06T14:25:11.000Z",
            "submittedOnDailyAt": "2026-01-07T03:50:19.522Z",
            "title": "SOP: A Scalable Online Post-Training System for Vision-Language-Action Models",
            "submittedOnDailyBy": {
                "_id": "655ecd7c56e5ceaf05344b24",
                "avatarUrl": "/avatars/faad88525197bb6c63be0068f19de418.svg",
                "isPro": false,
                "fullname": "MingjieP",
                "user": "pmj110119",
                "type": "user"
            },
            "summary": "Vision-language-action (VLA) models achieve strong generalization through large-scale pre-training, but real-world deployment requires expert-level task proficiency in addition to broad generality. Existing post-training approaches for VLA models are typically offline, single-robot, or task-specific, limiting effective on-policy adaptation and scalable learning from real-world interaction. We introduce a Scalable Online Post-training (SOP) system that enables online, distributed, multi-task post-training of generalist VLA models directly in the physical world. SOP tightly couples execution and learning through a closed-loop architecture in which a fleet of robots continuously streams on-policy experience and human intervention signals to a centralized cloud learner, and asynchronously receives updated policies. This design supports prompt on-policy correction, scales experience collection through parallel deployment, and preserves generality during adaptation. SOP is agnostic to the choice of post-training algorithm; we instantiate it with both interactive imitation learning (HG-DAgger) and reinforcement learning (RECAP). Across a range of real-world manipulation tasks including cloth folding, box assembly, and grocery restocking, we show that SOP substantially improves the performance of large pretrained VLA models while maintaining a single shared policy across tasks. Effective post-training can be achieved within hours of real-world interaction, and performance scales near-linearly with the number of robots in the fleet. These results suggest that tightly coupling online learning with fleet-scale deployment is instrumental to enabling efficient, reliable, and scalable post-training of generalist robot policies in the physical world.",
            "upvotes": 19,
            "discussionId": "695dc422c03d6d81e4399e6c",
            "ai_summary": "A scalable online post-training system enables real-world robot policy adaptation through distributed, multi-task learning that maintains generality while improving task proficiency.",
            "ai_keywords": [
                "Vision-language-action models",
                "post-training",
                "online learning",
                "distributed learning",
                "multi-task learning",
                "closed-loop architecture",
                "on-policy experience",
                "interactive imitation learning",
                "reinforcement learning",
                "fleet-scale deployment"
            ],
            "organization": {
                "_id": "6959ca2481c675360361a275",
                "name": "KineMind",
                "fullname": "AgiBot Research",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/69548eb63499b9bcd6ee0574/g02RWir_9quNCKL40jiV0.jpeg"
            }
        },
        "publishedAt": "2026-01-06T09:25:11.000Z",
        "title": "SOP: A Scalable Online Post-Training System for Vision-Language-Action Models",
        "summary": "Vision-language-action (VLA) models achieve strong generalization through large-scale pre-training, but real-world deployment requires expert-level task proficiency in addition to broad generality. Existing post-training approaches for VLA models are typically offline, single-robot, or task-specific, limiting effective on-policy adaptation and scalable learning from real-world interaction. We introduce a Scalable Online Post-training (SOP) system that enables online, distributed, multi-task post-training of generalist VLA models directly in the physical world. SOP tightly couples execution and learning through a closed-loop architecture in which a fleet of robots continuously streams on-policy experience and human intervention signals to a centralized cloud learner, and asynchronously receives updated policies. This design supports prompt on-policy correction, scales experience collection through parallel deployment, and preserves generality during adaptation. SOP is agnostic to the choice of post-training algorithm; we instantiate it with both interactive imitation learning (HG-DAgger) and reinforcement learning (RECAP). Across a range of real-world manipulation tasks including cloth folding, box assembly, and grocery restocking, we show that SOP substantially improves the performance of large pretrained VLA models while maintaining a single shared policy across tasks. Effective post-training can be achieved within hours of real-world interaction, and performance scales near-linearly with the number of robots in the fleet. These results suggest that tightly coupling online learning with fleet-scale deployment is instrumental to enabling efficient, reliable, and scalable post-training of generalist robot policies in the physical world.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/655ecd7c56e5ceaf05344b24/AEQ64pAi4UD3rvJ33Qeag.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03044.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "655ecd7c56e5ceaf05344b24",
            "avatarUrl": "/avatars/faad88525197bb6c63be0068f19de418.svg",
            "fullname": "MingjieP",
            "name": "pmj110119",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "isUserFollowing": false
        },
        "organization": {
            "_id": "6959ca2481c675360361a275",
            "name": "KineMind",
            "fullname": "AgiBot Research",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/69548eb63499b9bcd6ee0574/g02RWir_9quNCKL40jiV0.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2601.02785",
            "authors": [
                {
                    "_id": "695dd030c03d6d81e4399ee8",
                    "user": {
                        "_id": "6805bdfb344d6d8a8fd5b07a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/dGLeWvN2CXLmxVP_b9S4R.png",
                        "isPro": false,
                        "fullname": "Mengtian Li",
                        "user": "LemonSky1995",
                        "type": "user"
                    },
                    "name": "Mengtian Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-07T13:22:38.847Z",
                    "hidden": false
                },
                {
                    "_id": "695dd030c03d6d81e4399ee9",
                    "name": "Jinshu Chen",
                    "hidden": false
                },
                {
                    "_id": "695dd030c03d6d81e4399eea",
                    "name": "Songtao Zhao",
                    "hidden": false
                },
                {
                    "_id": "695dd030c03d6d81e4399eeb",
                    "user": {
                        "_id": "64462aec86679b753ff7ece0",
                        "avatarUrl": "/avatars/ff0e1e693b984c9d569dc462fa7fe4ef.svg",
                        "isPro": false,
                        "fullname": "Wanquan Feng",
                        "user": "wanquan",
                        "type": "user"
                    },
                    "name": "Wanquan Feng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-07T13:23:08.687Z",
                    "hidden": false
                },
                {
                    "_id": "695dd030c03d6d81e4399eec",
                    "user": {
                        "_id": "66151dae18269ebfe9702abf",
                        "avatarUrl": "/avatars/cd10619fe843a75c0ff29a7c0d295e88.svg",
                        "isPro": false,
                        "fullname": "tupengqi",
                        "user": "tupengqi",
                        "type": "user"
                    },
                    "name": "Pengqi Tu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-07T13:23:19.334Z",
                    "hidden": false
                },
                {
                    "_id": "695dd030c03d6d81e4399eed",
                    "name": "Qian He",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/1B89POtz-ncYIp_iw_rCT.qt"
            ],
            "publishedAt": "2026-01-06T07:42:12.000Z",
            "submittedOnDailyAt": "2026-01-07T00:48:26.802Z",
            "title": "DreamStyle: A Unified Framework for Video Stylization",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Video stylization, an important downstream task of video generation models, has not yet been thoroughly explored. Its input style conditions typically include text, style image, and stylized first frame. Each condition has a characteristic advantage: text is more flexible, style image provides a more accurate visual anchor, and stylized first frame makes long-video stylization feasible. However, existing methods are largely confined to a single type of style condition, which limits their scope of application. Additionally, their lack of high-quality datasets leads to style inconsistency and temporal flicker. To address these limitations, we introduce DreamStyle, a unified framework for video stylization, supporting (1) text-guided, (2) style-image-guided, and (3) first-frame-guided video stylization, accompanied by a well-designed data curation pipeline to acquire high-quality paired video data. DreamStyle is built on a vanilla Image-to-Video (I2V) model and trained using a Low-Rank Adaptation (LoRA) with token-specific up matrices that reduces the confusion among different condition tokens. Both qualitative and quantitative evaluations demonstrate that DreamStyle is competent in all three video stylization tasks, and outperforms the competitors in style consistency and video quality.",
            "upvotes": 17,
            "discussionId": "695dd030c03d6d81e4399eee",
            "projectPage": "https://lemonsky1995.github.io/dreamstyle/",
            "ai_summary": "DreamStyle is a unified video stylization framework that supports multiple style conditions while addressing style inconsistency and temporal flicker through a specialized data curation pipeline and LoRA training approach.",
            "ai_keywords": [
                "video stylization",
                "Image-to-Video (I2V) model",
                "Low-Rank Adaptation (LoRA)",
                "token-specific up matrices",
                "style consistency",
                "temporal flicker"
            ],
            "organization": {
                "_id": "653b817d32c97d0655575872",
                "name": "ByteDance",
                "fullname": "ByteDance",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
            }
        },
        "publishedAt": "2026-01-06T02:42:12.000Z",
        "title": "DreamStyle: A Unified Framework for Video Stylization",
        "summary": "Video stylization, an important downstream task of video generation models, has not yet been thoroughly explored. Its input style conditions typically include text, style image, and stylized first frame. Each condition has a characteristic advantage: text is more flexible, style image provides a more accurate visual anchor, and stylized first frame makes long-video stylization feasible. However, existing methods are largely confined to a single type of style condition, which limits their scope of application. Additionally, their lack of high-quality datasets leads to style inconsistency and temporal flicker. To address these limitations, we introduce DreamStyle, a unified framework for video stylization, supporting (1) text-guided, (2) style-image-guided, and (3) first-frame-guided video stylization, accompanied by a well-designed data curation pipeline to acquire high-quality paired video data. DreamStyle is built on a vanilla Image-to-Video (I2V) model and trained using a Low-Rank Adaptation (LoRA) with token-specific up matrices that reduces the confusion among different condition tokens. Both qualitative and quantitative evaluations demonstrate that DreamStyle is competent in all three video stylization tasks, and outperforms the competitors in style consistency and video quality.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/1B89POtz-ncYIp_iw_rCT.qt"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02785.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 202,
            "isUserFollowing": false
        },
        "organization": {
            "_id": "653b817d32c97d0655575872",
            "name": "ByteDance",
            "fullname": "ByteDance",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2601.02780",
            "authors": [
                {
                    "_id": "695dd171c03d6d81e4399ef4",
                    "user": {
                        "_id": "6732130e07f893d9c185c908",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/C31Me-ujgNy5k2KLL5hh6.png",
                        "isPro": false,
                        "fullname": "bangjun xiao",
                        "user": "xbjpku",
                        "type": "user"
                    },
                    "name": "Bangjun Xiao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-07T13:25:11.197Z",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399ef5",
                    "user": {
                        "_id": "6348c3cda6aa28fa6313e906",
                        "avatarUrl": "/avatars/2c0f0b08b1371689ebb4df18ddf45e54.svg",
                        "isPro": false,
                        "fullname": "Bingquan Xia",
                        "user": "xiabingquan",
                        "type": "user"
                    },
                    "name": "Bingquan Xia",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-07T13:25:19.374Z",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399ef6",
                    "user": {
                        "_id": "693ee055a14dfa27729d6684",
                        "avatarUrl": "/avatars/3ae4edc04cc740ca61f6f8e4ad011413.svg",
                        "isPro": false,
                        "fullname": "mi-yb",
                        "user": "ami-yb",
                        "type": "user"
                    },
                    "name": "Bo Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-07T09:25:53.834Z",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399ef7",
                    "user": {
                        "_id": "65ae21adabf6d1ccb795e9a4",
                        "avatarUrl": "/avatars/b5dced62c6a3564095a8fa0959bc06cb.svg",
                        "isPro": false,
                        "fullname": "Bofei Gao",
                        "user": "KbsdJames",
                        "type": "user"
                    },
                    "name": "Bofei Gao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-07T13:25:37.497Z",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399ef8",
                    "name": "Bowen Shen",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399ef9",
                    "name": "Chen Zhang",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399efa",
                    "user": {
                        "_id": "6829e9379292b4796b6d9124",
                        "avatarUrl": "/avatars/838acf681fce69703737d31845f4bef0.svg",
                        "isPro": false,
                        "fullname": "chen honghe",
                        "user": "chenhonghe",
                        "type": "user"
                    },
                    "name": "Chenhong He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-07T13:26:21.940Z",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399efb",
                    "user": {
                        "_id": "6528be15578679aac79e925f",
                        "avatarUrl": "/avatars/bd5dbb3ff43b2faa034c4cd381263d9c.svg",
                        "isPro": false,
                        "fullname": "Chiheng Lou",
                        "user": "gjghfd",
                        "type": "user"
                    },
                    "name": "Chiheng Lou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-07T13:26:29.953Z",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399efc",
                    "user": {
                        "_id": "6538815d1bdb3c40db94fbfa",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6538815d1bdb3c40db94fbfa/id7aSY8JUgKK2agKWLERt.jpeg",
                        "isPro": false,
                        "fullname": "Fuli Luo",
                        "user": "luofuli",
                        "type": "user"
                    },
                    "name": "Fuli Luo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-07T13:26:36.566Z",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399efd",
                    "name": "Gang Wang",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399efe",
                    "name": "Gang Xie",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399eff",
                    "name": "Hailin Zhang",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f00",
                    "name": "Hanglong Lv",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f01",
                    "name": "Hanyu Li",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f02",
                    "name": "Heyu Chen",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f03",
                    "name": "Hongshen Xu",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f04",
                    "name": "Houbin Zhang",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f05",
                    "name": "Huaqiu Liu",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f06",
                    "name": "Jiangshan Duo",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f07",
                    "name": "Jianyu Wei",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f08",
                    "name": "Jiebao Xiao",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f09",
                    "name": "Jinhao Dong",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f0a",
                    "name": "Jun Shi",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f0b",
                    "name": "Junhao Hu",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f0c",
                    "name": "Kainan Bao",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f0d",
                    "name": "Kang Zhou",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f0e",
                    "name": "Lei Li",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f0f",
                    "name": "Liang Zhao",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f10",
                    "name": "Linghao Zhang",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f11",
                    "name": "Peidian Li",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f12",
                    "name": "Qianli Chen",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f13",
                    "name": "Shaohui Liu",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f14",
                    "name": "Shihua Yu",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f15",
                    "name": "Shijie Cao",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f16",
                    "user": {
                        "_id": "64f5216e3c9b5e07f85582e3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f5216e3c9b5e07f85582e3/tWe8_9R2nNxQpPv-wa4nB.jpeg",
                        "isPro": false,
                        "fullname": "Ezio Chen",
                        "user": "Ezioii",
                        "type": "user"
                    },
                    "name": "Shimao Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-07T09:25:44.714Z",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f17",
                    "name": "Shouqiu Yu",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f18",
                    "name": "Shuo Liu",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f19",
                    "name": "Tianling Zhou",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f1a",
                    "name": "Weijiang Su",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f1b",
                    "name": "Weikun Wang",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f1c",
                    "name": "Wenhan Ma",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f1d",
                    "name": "Xiangwei Deng",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f1e",
                    "name": "Bohan Mao",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f1f",
                    "name": "Bowen Ye",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f20",
                    "name": "Can Cai",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f21",
                    "name": "Chenghua Wang",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f22",
                    "name": "Chengxuan Zhu",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f23",
                    "name": "Chong Ma",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f24",
                    "name": "Chun Chen",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f25",
                    "name": "Chunan Li",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f26",
                    "name": "Dawei Zhu",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f27",
                    "name": "Deshan Xiao",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f28",
                    "name": "Dong Zhang",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f29",
                    "name": "Duo Zhang",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f2a",
                    "name": "Fangyue Liu",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f2b",
                    "name": "Feiyu Yang",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f2c",
                    "name": "Fengyuan Shi",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f2d",
                    "name": "Guoan Wang",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f2e",
                    "name": "Hao Tian",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f2f",
                    "name": "Hao Wu",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f30",
                    "name": "Heng Qu",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f31",
                    "name": "Hongfei Yi",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f32",
                    "name": "Hongxu An",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f33",
                    "name": "Hongyi Guan",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f34",
                    "name": "Xing Zhang",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f35",
                    "name": "Yifan Song",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f36",
                    "name": "Yihan Yan",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f37",
                    "name": "Yihao Zhao",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f38",
                    "name": "Yingchun Lai",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f39",
                    "name": "Yizhao Gao",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f3a",
                    "name": "Yu Cheng",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f3b",
                    "name": "Yuanyuan Tian",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f3c",
                    "name": "Yudong Wang",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f3d",
                    "name": "Zhen Tang",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f3e",
                    "name": "Zhengju Tang",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f3f",
                    "name": "Zhengtao Wen",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f40",
                    "name": "Zhichao Song",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f41",
                    "name": "Zhixian Zheng",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f42",
                    "user": {
                        "_id": "665ee916b9e194a20c4769da",
                        "avatarUrl": "/avatars/cfb3ff26979bed1a0c65a6fa318af3ad.svg",
                        "isPro": false,
                        "fullname": "Zihan Jiang",
                        "user": "Jumbo0715",
                        "type": "user"
                    },
                    "name": "Zihan Jiang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-07T09:25:47.154Z",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f43",
                    "name": "Jian Wen",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f44",
                    "name": "Jiarui Sun",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f45",
                    "name": "Jiawei Li",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f46",
                    "name": "Jinlong Xue",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f47",
                    "name": "Jun Xia",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f48",
                    "name": "Kai Fang",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f49",
                    "name": "Menghang Zhu",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f4a",
                    "name": "Nuo Chen",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f4b",
                    "name": "Qian Tu",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f4c",
                    "name": "Qihao Zhang",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f4d",
                    "name": "Qiying Wang",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f4e",
                    "name": "Rang Li",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f4f",
                    "name": "Rui Ma",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f50",
                    "name": "Shaolei Zhang",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f51",
                    "name": "Shengfan Wang",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f52",
                    "name": "Shicheng Li",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f53",
                    "name": "Shuhao Gu",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f54",
                    "name": "Shuhuai Ren",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f55",
                    "name": "Sirui Deng",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f56",
                    "name": "Tao Guo",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f57",
                    "name": "Tianyang Lu",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f58",
                    "name": "Weiji Zhuang",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f59",
                    "name": "Weikang Zhang",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f5a",
                    "name": "Weimin Xiong",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f5b",
                    "name": "Wenshan Huang",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f5c",
                    "name": "Wenyu Yang",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f5d",
                    "name": "Xin Zhang",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f5e",
                    "name": "Xing Yong",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f5f",
                    "name": "Xu Wang",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f60",
                    "name": "Xueyang Xie",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f61",
                    "name": "Yilin Jiang",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f62",
                    "name": "Yixin Yang",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f63",
                    "name": "Yongzhe He",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f64",
                    "name": "Yu Tu",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f65",
                    "name": "Yuanliang Dong",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f66",
                    "name": "Yuchen Liu",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f67",
                    "name": "Yue Ma",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f68",
                    "name": "Yue Yu",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f69",
                    "name": "Yuxing Xiang",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f6a",
                    "name": "Zhaojun Huang",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f6b",
                    "name": "Zhenru Lin",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f6c",
                    "name": "Zhipeng Xu",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f6d",
                    "name": "Zhiyang Chen",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f6e",
                    "name": "Zhonghua Deng",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f6f",
                    "name": "Zihan Zhang",
                    "hidden": false
                },
                {
                    "_id": "695dd171c03d6d81e4399f70",
                    "name": "Zihao Yue",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-06T07:31:47.000Z",
            "submittedOnDailyAt": "2026-01-07T00:52:38.331Z",
            "title": "MiMo-V2-Flash Technical Report",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We present MiMo-V2-Flash, a Mixture-of-Experts (MoE) model with 309B total parameters and 15B active parameters, designed for fast, strong reasoning and agentic capabilities. MiMo-V2-Flash adopts a hybrid attention architecture that interleaves Sliding Window Attention (SWA) with global attention, with a 128-token sliding window under a 5:1 hybrid ratio. The model is pre-trained on 27 trillion tokens with Multi-Token Prediction (MTP), employing a native 32k context length and subsequently extended to 256k. To efficiently scale post-training compute, MiMo-V2-Flash introduces a novel Multi-Teacher On-Policy Distillation (MOPD) paradigm. In this framework, domain-specialized teachers (e.g., trained via large-scale reinforcement learning) provide dense and token-level reward, enabling the student model to perfectly master teacher expertise. MiMo-V2-Flash rivals top-tier open-weight models such as DeepSeek-V3.2 and Kimi-K2, despite using only 1/2 and 1/3 of their total parameters, respectively. During inference, by repurposing MTP as a draft model for speculative decoding, MiMo-V2-Flash achieves up to 3.6 acceptance length and 2.6x decoding speedup with three MTP layers. We open-source both the model weights and the three-layer MTP weights to foster open research and community collaboration.",
            "upvotes": 17,
            "discussionId": "695dd172c03d6d81e4399f71",
            "projectPage": "https://mimo.xiaomi.com/blog/mimo-v2-flash",
            "githubRepo": "https://github.com/XiaomiMiMo/MiMo-V2-Flash",
            "githubRepoAddedBy": "user",
            "ai_summary": "MiMo-V2-Flash is a sparse Mixture-of-Experts model with hybrid attention architecture and efficient distillation technique that achieves strong performance with reduced parameters and improved inference speed.",
            "ai_keywords": [
                "Mixture-of-Experts",
                "Sliding Window Attention",
                "global attention",
                "Multi-Token Prediction",
                "speculative decoding",
                "Multi-Teacher On-Policy Distillation"
            ],
            "githubStars": 957,
            "organization": {
                "_id": "680cb4c37f289defb2210940",
                "name": "XiaomiMiMo",
                "fullname": "Xiaomi MiMo",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/680cb7d1233834890a64acee/5w_4aLfF-7MAyaIPOV498.jpeg"
            }
        },
        "publishedAt": "2026-01-06T02:31:47.000Z",
        "title": "MiMo-V2-Flash Technical Report",
        "summary": "We present MiMo-V2-Flash, a Mixture-of-Experts (MoE) model with 309B total parameters and 15B active parameters, designed for fast, strong reasoning and agentic capabilities. MiMo-V2-Flash adopts a hybrid attention architecture that interleaves Sliding Window Attention (SWA) with global attention, with a 128-token sliding window under a 5:1 hybrid ratio. The model is pre-trained on 27 trillion tokens with Multi-Token Prediction (MTP), employing a native 32k context length and subsequently extended to 256k. To efficiently scale post-training compute, MiMo-V2-Flash introduces a novel Multi-Teacher On-Policy Distillation (MOPD) paradigm. In this framework, domain-specialized teachers (e.g., trained via large-scale reinforcement learning) provide dense and token-level reward, enabling the student model to perfectly master teacher expertise. MiMo-V2-Flash rivals top-tier open-weight models such as DeepSeek-V3.2 and Kimi-K2, despite using only 1/2 and 1/3 of their total parameters, respectively. During inference, by repurposing MTP as a draft model for speculative decoding, MiMo-V2-Flash achieves up to 3.6 acceptance length and 2.6x decoding speedup with three MTP layers. We open-source both the model weights and the three-layer MTP weights to foster open research and community collaboration.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02780.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 202,
            "isUserFollowing": false
        },
        "organization": {
            "_id": "680cb4c37f289defb2210940",
            "name": "XiaomiMiMo",
            "fullname": "Xiaomi MiMo",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/680cb7d1233834890a64acee/5w_4aLfF-7MAyaIPOV498.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2601.01874",
            "authors": [
                {
                    "_id": "695cee316aa73bc11f09163b",
                    "name": "Shuhang Chen",
                    "hidden": false
                },
                {
                    "_id": "695cee316aa73bc11f09163c",
                    "user": {
                        "_id": "646c77911ee398a4e9404b8b",
                        "avatarUrl": "/avatars/05d1ea421dd4f3e2fd47cbe99fc52933.svg",
                        "isPro": false,
                        "fullname": "Yunqiu Xu",
                        "user": "Yunqiu",
                        "type": "user"
                    },
                    "name": "Yunqiu Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-07T09:27:07.129Z",
                    "hidden": false
                },
                {
                    "_id": "695cee316aa73bc11f09163d",
                    "name": "Junjie Xie",
                    "hidden": false
                },
                {
                    "_id": "695cee316aa73bc11f09163e",
                    "name": "Aojun Lu",
                    "hidden": false
                },
                {
                    "_id": "695cee316aa73bc11f09163f",
                    "name": "Tao Feng",
                    "hidden": false
                },
                {
                    "_id": "695cee316aa73bc11f091640",
                    "name": "Zeying Huang",
                    "hidden": false
                },
                {
                    "_id": "695cee316aa73bc11f091641",
                    "name": "Ning Zhang",
                    "hidden": false
                },
                {
                    "_id": "695cee316aa73bc11f091642",
                    "name": "Yi Sun",
                    "hidden": false
                },
                {
                    "_id": "695cee316aa73bc11f091643",
                    "name": "Yi Yang",
                    "hidden": false
                },
                {
                    "_id": "695cee316aa73bc11f091644",
                    "user": {
                        "_id": "649d54b314afbb10ce2a9eeb",
                        "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
                        "isPro": false,
                        "fullname": "Hangjie Yuan",
                        "user": "JacobYuan",
                        "type": "user"
                    },
                    "name": "Hangjie Yuan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-07T13:24:05.922Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-05T08:02:18.000Z",
            "submittedOnDailyAt": "2026-01-07T01:06:12.554Z",
            "title": "CogFlow: Bridging Perception and Reasoning through Knowledge Internalization for Visual Mathematical Problem Solving",
            "submittedOnDailyBy": {
                "_id": "646c77911ee398a4e9404b8b",
                "avatarUrl": "/avatars/05d1ea421dd4f3e2fd47cbe99fc52933.svg",
                "isPro": false,
                "fullname": "Yunqiu Xu",
                "user": "Yunqiu",
                "type": "user"
            },
            "summary": "Despite significant progress, multimodal large language models continue to struggle with visual mathematical problem solving. Some recent works recognize that visual perception is a bottleneck in visual mathematical reasoning, but their solutions are limited to improving the extraction and interpretation of visual inputs. Notably, they all ignore the key issue of whether the extracted visual cues are faithfully integrated and properly utilized in subsequent reasoning. Motivated by this, we present CogFlow, a novel cognitive-inspired three-stage framework that incorporates a knowledge internalization stage, explicitly simulating the hierarchical flow of human reasoning: perceptionRightarrowinternalizationRightarrowreasoning. Inline with this hierarchical flow, we holistically enhance all its stages. We devise Synergistic Visual Rewards to boost perception capabilities in parametric and semantic spaces, jointly improving visual information extraction from symbols and diagrams. To guarantee faithful integration of extracted visual cues into subsequent reasoning, we introduce a Knowledge Internalization Reward model in the internalization stage, bridging perception and reasoning. Moreover, we design a Visual-Gated Policy Optimization algorithm to further enforce the reasoning is grounded with the visual knowledge, preventing models seeking shortcuts that appear coherent but are visually ungrounded reasoning chains. Moreover, we contribute a new dataset MathCog for model training, which contains samples with over 120K high-quality perception-reasoning aligned annotations. Comprehensive experiments and analysis on commonly used visual mathematical reasoning benchmarks validate the superiority of the proposed CogFlow.",
            "upvotes": 16,
            "discussionId": "695cee326aa73bc11f091645",
            "projectPage": "https://shchen233.github.io/cogflow/",
            "ai_summary": "Visual mathematical problem solving remains challenging for multimodal large language models, prompting the development of CogFlow, a cognitive-inspired three-stage framework that enhances perception, internalization, and reasoning through synergistic rewards and visual-gated policy optimization.",
            "ai_keywords": [
                "multimodal large language models",
                "visual mathematical problem solving",
                "visual perception",
                "visual reasoning",
                "cognitive-inspired framework",
                "knowledge internalization",
                "visual-gated policy optimization",
                "Synergistic Visual Rewards",
                "Knowledge Internalization Reward model"
            ]
        },
        "publishedAt": "2026-01-05T03:02:18.000Z",
        "title": "CogFlow: Bridging Perception and Reasoning through Knowledge Internalization for Visual Mathematical Problem Solving",
        "summary": "Despite significant progress, multimodal large language models continue to struggle with visual mathematical problem solving. Some recent works recognize that visual perception is a bottleneck in visual mathematical reasoning, but their solutions are limited to improving the extraction and interpretation of visual inputs. Notably, they all ignore the key issue of whether the extracted visual cues are faithfully integrated and properly utilized in subsequent reasoning. Motivated by this, we present CogFlow, a novel cognitive-inspired three-stage framework that incorporates a knowledge internalization stage, explicitly simulating the hierarchical flow of human reasoning: perceptionRightarrowinternalizationRightarrowreasoning. Inline with this hierarchical flow, we holistically enhance all its stages. We devise Synergistic Visual Rewards to boost perception capabilities in parametric and semantic spaces, jointly improving visual information extraction from symbols and diagrams. To guarantee faithful integration of extracted visual cues into subsequent reasoning, we introduce a Knowledge Internalization Reward model in the internalization stage, bridging perception and reasoning. Moreover, we design a Visual-Gated Policy Optimization algorithm to further enforce the reasoning is grounded with the visual knowledge, preventing models seeking shortcuts that appear coherent but are visually ungrounded reasoning chains. Moreover, we contribute a new dataset MathCog for model training, which contains samples with over 120K high-quality perception-reasoning aligned annotations. Comprehensive experiments and analysis on commonly used visual mathematical reasoning benchmarks validate the superiority of the proposed CogFlow.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.01874.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "646c77911ee398a4e9404b8b",
            "avatarUrl": "/avatars/05d1ea421dd4f3e2fd47cbe99fc52933.svg",
            "fullname": "Yunqiu Xu",
            "name": "Yunqiu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "isUserFollowing": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2601.01321",
            "authors": [
                {
                    "_id": "695d39cfc03d6d81e4399d8f",
                    "user": {
                        "_id": "6786f635c32ea771f412818d",
                        "avatarUrl": "/avatars/e55b9cc261417fc1f297347d75297db9.svg",
                        "isPro": false,
                        "fullname": "Rong Zhou",
                        "user": "roz322",
                        "type": "user"
                    },
                    "name": "Rong Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-07T09:26:30.528Z",
                    "hidden": false
                },
                {
                    "_id": "695d39cfc03d6d81e4399d90",
                    "user": {
                        "_id": "65e2be1e630e2db23829ee8d",
                        "avatarUrl": "/avatars/294f9ba909037f03669dc0bb80cabfe3.svg",
                        "isPro": false,
                        "fullname": "Dongping Chen",
                        "user": "fjchendp",
                        "type": "user"
                    },
                    "name": "Dongping Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-07T13:28:41.255Z",
                    "hidden": false
                },
                {
                    "_id": "695d39cfc03d6d81e4399d91",
                    "user": {
                        "_id": "68ea0ac3cabf3e512494cf54",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/f4Td2xYz0l_aJcQaK9Ky9.png",
                        "isPro": false,
                        "fullname": "Zihan Jia",
                        "user": "ZihanJia",
                        "type": "user"
                    },
                    "name": "Zihan Jia",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-07T13:28:48.081Z",
                    "hidden": false
                },
                {
                    "_id": "695d39cfc03d6d81e4399d92",
                    "name": "Yao Su",
                    "hidden": false
                },
                {
                    "_id": "695d39cfc03d6d81e4399d93",
                    "name": "Yixin Liu",
                    "hidden": false
                },
                {
                    "_id": "695d39cfc03d6d81e4399d94",
                    "name": "Yiwen Lu",
                    "hidden": false
                },
                {
                    "_id": "695d39cfc03d6d81e4399d95",
                    "name": "Dongwei Shi",
                    "hidden": false
                },
                {
                    "_id": "695d39cfc03d6d81e4399d96",
                    "name": "Yue Huang",
                    "hidden": false
                },
                {
                    "_id": "695d39cfc03d6d81e4399d97",
                    "name": "Tianyang Xu",
                    "hidden": false
                },
                {
                    "_id": "695d39cfc03d6d81e4399d98",
                    "name": "Yi Pan",
                    "hidden": false
                },
                {
                    "_id": "695d39cfc03d6d81e4399d99",
                    "name": "Xinliang Li",
                    "hidden": false
                },
                {
                    "_id": "695d39cfc03d6d81e4399d9a",
                    "name": "Yohannes Abate",
                    "hidden": false
                },
                {
                    "_id": "695d39cfc03d6d81e4399d9b",
                    "name": "Qingyu Chen",
                    "hidden": false
                },
                {
                    "_id": "695d39cfc03d6d81e4399d9c",
                    "user": {
                        "_id": "62548d5fef3debb2ddf91217",
                        "avatarUrl": "/avatars/14975b45568f9c399c92c3986b6ce83e.svg",
                        "isPro": false,
                        "fullname": "Zhengzhong Tu",
                        "user": "vztu",
                        "type": "user"
                    },
                    "name": "Zhengzhong Tu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-07T13:37:53.800Z",
                    "hidden": false
                },
                {
                    "_id": "695d39cfc03d6d81e4399d9d",
                    "name": "Yu Yang",
                    "hidden": false
                },
                {
                    "_id": "695d39cfc03d6d81e4399d9e",
                    "name": "Yu Zhang",
                    "hidden": false
                },
                {
                    "_id": "695d39cfc03d6d81e4399d9f",
                    "name": "Qingsong Wen",
                    "hidden": false
                },
                {
                    "_id": "695d39cfc03d6d81e4399da0",
                    "name": "Gengchen Mai",
                    "hidden": false
                },
                {
                    "_id": "695d39cfc03d6d81e4399da1",
                    "name": "Sunyang Fu",
                    "hidden": false
                },
                {
                    "_id": "695d39cfc03d6d81e4399da2",
                    "name": "Jiachen Li",
                    "hidden": false
                },
                {
                    "_id": "695d39cfc03d6d81e4399da3",
                    "name": "Xuyu Wang",
                    "hidden": false
                },
                {
                    "_id": "695d39cfc03d6d81e4399da4",
                    "name": "Ziran Wang",
                    "hidden": false
                },
                {
                    "_id": "695d39cfc03d6d81e4399da5",
                    "name": "Jing Huang",
                    "hidden": false
                },
                {
                    "_id": "695d39cfc03d6d81e4399da6",
                    "name": "Tianming Liu",
                    "hidden": false
                },
                {
                    "_id": "695d39cfc03d6d81e4399da7",
                    "name": "Yong Chen",
                    "hidden": false
                },
                {
                    "_id": "695d39cfc03d6d81e4399da8",
                    "name": "Lichao Sun",
                    "hidden": false
                },
                {
                    "_id": "695d39cfc03d6d81e4399da9",
                    "name": "Lifang He",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-04T01:17:09.000Z",
            "submittedOnDailyAt": "2026-01-07T10:04:46.717Z",
            "title": "Digital Twin AI: Opportunities and Challenges from Large Language Models to World Models",
            "submittedOnDailyBy": {
                "_id": "6786f635c32ea771f412818d",
                "avatarUrl": "/avatars/e55b9cc261417fc1f297347d75297db9.svg",
                "isPro": false,
                "fullname": "Rong Zhou",
                "user": "roz322",
                "type": "user"
            },
            "summary": "Digital twins, as precise digital representations of physical systems, have evolved from passive simulation tools into intelligent and autonomous entities through the integration of artificial intelligence technologies. This paper presents a unified four-stage framework that systematically characterizes AI integration across the digital twin lifecycle, spanning modeling, mirroring, intervention, and autonomous management. By synthesizing existing technologies and practices, we distill a unified four-stage framework that systematically characterizes how AI methodologies are embedded across the digital twin lifecycle: (1) modeling the physical twin through physics-based and physics-informed AI approaches, (2) mirroring the physical system into a digital twin with real-time synchronization, (3) intervening in the physical twin through predictive modeling, anomaly detection, and optimization strategies, and (4) achieving autonomous management through large language models, foundation models, and intelligent agents. We analyze the synergy between physics-based modeling and data-driven learning, highlighting the shift from traditional numerical solvers to physics-informed and foundation models for physical systems. Furthermore, we examine how generative AI technologies, including large language models and generative world models, transform digital twins into proactive and self-improving cognitive systems capable of reasoning, communication, and creative scenario generation. Through a cross-domain review spanning eleven application domains, including healthcare, aerospace, smart manufacturing, robotics, and smart cities, we identify common challenges related to scalability, explainability, and trustworthiness, and outline directions for responsible AI-driven digital twin systems.",
            "upvotes": 8,
            "discussionId": "695d39d0c03d6d81e4399daa",
            "githubRepo": "https://github.com/rongzhou7/Awesome-Digital-Twin-AI/tree/main",
            "githubRepoAddedBy": "user",
            "ai_summary": "AI integration in digital twins follows a four-stage framework encompassing modeling, mirroring, intervention, and autonomous management, leveraging physics-informed approaches, generative AI, and foundation models for intelligent system operation.",
            "ai_keywords": [
                "digital twins",
                "artificial intelligence",
                "physics-based modeling",
                "physics-informed AI",
                "real-time synchronization",
                "predictive modeling",
                "anomaly detection",
                "optimization strategies",
                "large language models",
                "foundation models",
                "generative AI",
                "generative world models",
                "intelligent agents",
                "digital twin lifecycle"
            ],
            "githubStars": 2
        },
        "publishedAt": "2026-01-03T20:17:09.000Z",
        "title": "Digital Twin AI: Opportunities and Challenges from Large Language Models to World Models",
        "summary": "Digital twins, as precise digital representations of physical systems, have evolved from passive simulation tools into intelligent and autonomous entities through the integration of artificial intelligence technologies. This paper presents a unified four-stage framework that systematically characterizes AI integration across the digital twin lifecycle, spanning modeling, mirroring, intervention, and autonomous management. By synthesizing existing technologies and practices, we distill a unified four-stage framework that systematically characterizes how AI methodologies are embedded across the digital twin lifecycle: (1) modeling the physical twin through physics-based and physics-informed AI approaches, (2) mirroring the physical system into a digital twin with real-time synchronization, (3) intervening in the physical twin through predictive modeling, anomaly detection, and optimization strategies, and (4) achieving autonomous management through large language models, foundation models, and intelligent agents. We analyze the synergy between physics-based modeling and data-driven learning, highlighting the shift from traditional numerical solvers to physics-informed and foundation models for physical systems. Furthermore, we examine how generative AI technologies, including large language models and generative world models, transform digital twins into proactive and self-improving cognitive systems capable of reasoning, communication, and creative scenario generation. Through a cross-domain review spanning eleven application domains, including healthcare, aerospace, smart manufacturing, robotics, and smart cities, we identify common challenges related to scalability, explainability, and trustworthiness, and outline directions for responsible AI-driven digital twin systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.01321.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6786f635c32ea771f412818d",
            "avatarUrl": "/avatars/e55b9cc261417fc1f297347d75297db9.svg",
            "fullname": "Rong Zhou",
            "name": "roz322",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "isUserFollowing": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2601.02439",
            "authors": [
                {
                    "_id": "695dd24cc03d6d81e4399f73",
                    "name": "Hao Bai",
                    "hidden": false
                },
                {
                    "_id": "695dd24cc03d6d81e4399f74",
                    "name": "Alexey Taymanov",
                    "hidden": false
                },
                {
                    "_id": "695dd24cc03d6d81e4399f75",
                    "name": "Tong Zhang",
                    "hidden": false
                },
                {
                    "_id": "695dd24cc03d6d81e4399f76",
                    "user": {
                        "_id": "64879130cc0e54f272b5dbcc",
                        "avatarUrl": "/avatars/ead1211a56a4e5c14cb8f6b2ea59c0e6.svg",
                        "isPro": false,
                        "fullname": "Aviral Kumar",
                        "user": "aviralkumar",
                        "type": "user"
                    },
                    "name": "Aviral Kumar",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-07T13:45:10.041Z",
                    "hidden": false
                },
                {
                    "_id": "695dd24cc03d6d81e4399f77",
                    "user": {
                        "_id": "66fac4687f5d72452e588996",
                        "avatarUrl": "/avatars/86621193132cc38db56a7255592a8b46.svg",
                        "isPro": false,
                        "fullname": "Spencer Whitehead",
                        "user": "swhitehead",
                        "type": "user"
                    },
                    "name": "Spencer Whitehead",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-07T13:44:57.513Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-05T09:35:11.000Z",
            "submittedOnDailyAt": "2026-01-07T00:56:27.821Z",
            "title": "WebGym: Scaling Training Environments for Visual Web Agents with Realistic Tasks",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We present WebGym, the largest-to-date open-source environment for training realistic visual web agents. Real websites are non-stationary and diverse, making artificial or small-scale task sets insufficient for robust policy learning. WebGym contains nearly 300,000 tasks with rubric-based evaluations across diverse, real-world websites and difficulty levels. We train agents with a simple reinforcement learning (RL) recipe, which trains on the agent's own interaction traces (rollouts), using task rewards as feedback to guide learning. To enable scaling RL, we speed up sampling of trajectories in WebGym by developing a high-throughput asynchronous rollout system, designed specifically for web agents. Our system achieves a 4-5x rollout speedup compared to naive implementations. Second, we scale the task set breadth, depth, and size, which results in continued performance improvement. Fine-tuning a strong base vision-language model, Qwen-3-VL-8B-Instruct, on WebGym results in an improvement in success rate on an out-of-distribution test set from 26.2% to 42.9%, significantly outperforming agents based on proprietary models such as GPT-4o and GPT-5-Thinking that achieve 27.1% and 29.8%, respectively. This improvement is substantial because our test set consists only of tasks on websites never seen during training, unlike many other prior works on training visual web agents.",
            "upvotes": 5,
            "discussionId": "695dd24cc03d6d81e4399f78",
            "ai_summary": "WebGym presents a large-scale open-source environment for training visual web agents using reinforcement learning with high-throughput asynchronous sampling, achieving superior performance on unseen websites compared to proprietary models.",
            "ai_keywords": [
                "reinforcement learning",
                "rollout system",
                "vision-language model",
                "Qwen-3-VL-8B-Instruct",
                "asynchronous rollout system",
                "web agent training",
                "task rewards",
                "policy learning",
                "visual web agents"
            ],
            "organization": {
                "_id": "5e6485f787403103f9f1055e",
                "name": "microsoft",
                "fullname": "Microsoft",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"
            }
        },
        "publishedAt": "2026-01-05T04:35:11.000Z",
        "title": "WebGym: Scaling Training Environments for Visual Web Agents with Realistic Tasks",
        "summary": "We present WebGym, the largest-to-date open-source environment for training realistic visual web agents. Real websites are non-stationary and diverse, making artificial or small-scale task sets insufficient for robust policy learning. WebGym contains nearly 300,000 tasks with rubric-based evaluations across diverse, real-world websites and difficulty levels. We train agents with a simple reinforcement learning (RL) recipe, which trains on the agent's own interaction traces (rollouts), using task rewards as feedback to guide learning. To enable scaling RL, we speed up sampling of trajectories in WebGym by developing a high-throughput asynchronous rollout system, designed specifically for web agents. Our system achieves a 4-5x rollout speedup compared to naive implementations. Second, we scale the task set breadth, depth, and size, which results in continued performance improvement. Fine-tuning a strong base vision-language model, Qwen-3-VL-8B-Instruct, on WebGym results in an improvement in success rate on an out-of-distribution test set from 26.2% to 42.9%, significantly outperforming agents based on proprietary models such as GPT-4o and GPT-5-Thinking that achieve 27.1% and 29.8%, respectively. This improvement is substantial because our test set consists only of tasks on websites never seen during training, unlike many other prior works on training visual web agents.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02439.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 202,
            "isUserFollowing": false
        },
        "organization": {
            "_id": "5e6485f787403103f9f1055e",
            "name": "microsoft",
            "fullname": "Microsoft",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2601.02989",
            "authors": [
                {
                    "_id": "695e4ecfc679bee50ca69b4e",
                    "name": "Hosein Hasani",
                    "hidden": false
                },
                {
                    "_id": "695e4ecfc679bee50ca69b4f",
                    "name": "Mohammadali Banayeeanzade",
                    "hidden": false
                },
                {
                    "_id": "695e4ecfc679bee50ca69b50",
                    "user": {
                        "_id": "65aa7c6cc8903e28aea576e7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65aa7c6cc8903e28aea576e7/1jcif5QJ2dKu_oeBNVqYG.jpeg",
                        "isPro": false,
                        "fullname": "Ali Nafisi",
                        "user": "safinal",
                        "type": "user"
                    },
                    "name": "Ali Nafisi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-07T13:13:16.135Z",
                    "hidden": false
                },
                {
                    "_id": "695e4ecfc679bee50ca69b51",
                    "name": "Sadegh Mohammadian",
                    "hidden": false
                },
                {
                    "_id": "695e4ecfc679bee50ca69b52",
                    "name": "Fatemeh Askari",
                    "hidden": false
                },
                {
                    "_id": "695e4ecfc679bee50ca69b53",
                    "name": "Mobin Bagherian",
                    "hidden": false
                },
                {
                    "_id": "695e4ecfc679bee50ca69b54",
                    "name": "Amirmohammad Izadi",
                    "hidden": false
                },
                {
                    "_id": "695e4ecfc679bee50ca69b55",
                    "name": "Mahdieh Soleymani Baghshah",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-06T12:58:27.000Z",
            "submittedOnDailyAt": "2026-01-07T10:03:52.382Z",
            "title": "Mechanistic Interpretability of Large-Scale Counting in LLMs through a System-2 Strategy",
            "submittedOnDailyBy": {
                "_id": "65aa7c6cc8903e28aea576e7",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65aa7c6cc8903e28aea576e7/1jcif5QJ2dKu_oeBNVqYG.jpeg",
                "isPro": false,
                "fullname": "Ali Nafisi",
                "user": "safinal",
                "type": "user"
            },
            "summary": "Large language models (LLMs), despite strong performance on complex mathematical problems, exhibit systematic limitations in counting tasks. This issue arises from architectural limits of transformers, where counting is performed across layers, leading to degraded precision for larger counting problems due to depth constraints. To address this limitation, we propose a simple test-time strategy inspired by System-2 cognitive processes that decomposes large counting tasks into smaller, independent sub-problems that the model can reliably solve. We evaluate this approach using observational and causal mediation analyses to understand the underlying mechanism of this System-2-like strategy. Our mechanistic analysis identifies key components: latent counts are computed and stored in the final item representations of each part, transferred to intermediate steps via dedicated attention heads, and aggregated in the final stage to produce the total count. Experimental results demonstrate that this strategy enables LLMs to surpass architectural limitations and achieve high accuracy on large-scale counting tasks. This work provides mechanistic insight into System-2 counting in LLMs and presents a generalizable approach for improving and understanding their reasoning behavior.",
            "upvotes": 4,
            "discussionId": "695e4ecfc679bee50ca69b56",
            "ai_summary": "A test-time strategy inspired by System-2 cognitive processes decomposes large counting tasks into smaller sub-problems, enabling large language models to overcome architectural limitations and achieve high accuracy on complex counting tasks through mechanistic components like latent count computation, dedicated attention heads, and final aggregation stages.",
            "ai_keywords": [
                "large language models",
                "transformers",
                "counting tasks",
                "System-2 cognitive processes",
                "test-time strategy",
                "latent counts",
                "attention heads",
                "mechanistic analysis",
                "causal mediation analysis"
            ]
        },
        "publishedAt": "2026-01-06T07:58:27.000Z",
        "title": "Mechanistic Interpretability of Large-Scale Counting in LLMs through a System-2 Strategy",
        "summary": "Large language models (LLMs), despite strong performance on complex mathematical problems, exhibit systematic limitations in counting tasks. This issue arises from architectural limits of transformers, where counting is performed across layers, leading to degraded precision for larger counting problems due to depth constraints. To address this limitation, we propose a simple test-time strategy inspired by System-2 cognitive processes that decomposes large counting tasks into smaller, independent sub-problems that the model can reliably solve. We evaluate this approach using observational and causal mediation analyses to understand the underlying mechanism of this System-2-like strategy. Our mechanistic analysis identifies key components: latent counts are computed and stored in the final item representations of each part, transferred to intermediate steps via dedicated attention heads, and aggregated in the final stage to produce the total count. Experimental results demonstrate that this strategy enables LLMs to surpass architectural limitations and achieve high accuracy on large-scale counting tasks. This work provides mechanistic insight into System-2 counting in LLMs and presents a generalizable approach for improving and understanding their reasoning behavior.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02989.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65aa7c6cc8903e28aea576e7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65aa7c6cc8903e28aea576e7/1jcif5QJ2dKu_oeBNVqYG.jpeg",
            "fullname": "Ali Nafisi",
            "name": "safinal",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 21,
            "isUserFollowing": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2601.03256",
            "authors": [
                {
                    "_id": "695dd790c03d6d81e4399f93",
                    "user": {
                        "_id": "66a4a7d02f869fd1e8a608e7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66a4a7d02f869fd1e8a608e7/ibcoHxegtjL42bY7io8CL.png",
                        "isPro": false,
                        "fullname": "Hexiao Lu",
                        "user": "CraneLu",
                        "type": "user"
                    },
                    "name": "Hexiao Lu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-07T09:25:41.192Z",
                    "hidden": false
                },
                {
                    "_id": "695dd790c03d6d81e4399f94",
                    "user": {
                        "_id": "65813fbeabafd960c84fdf2f",
                        "avatarUrl": "/avatars/d8f9cb56a2e44c3ca4472099437e0b50.svg",
                        "isPro": false,
                        "fullname": "Xiaokun Sun",
                        "user": "XiaokunSun",
                        "type": "user"
                    },
                    "name": "Xiaokun Sun",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-07T13:38:09.493Z",
                    "hidden": false
                },
                {
                    "_id": "695dd790c03d6d81e4399f95",
                    "name": "Zeyu Cai",
                    "hidden": false
                },
                {
                    "_id": "695dd790c03d6d81e4399f96",
                    "name": "Hao Guo",
                    "hidden": false
                },
                {
                    "_id": "695dd790c03d6d81e4399f97",
                    "name": "Ying Tai",
                    "hidden": false
                },
                {
                    "_id": "695dd790c03d6d81e4399f98",
                    "name": "Jian Yang",
                    "hidden": false
                },
                {
                    "_id": "695dd790c03d6d81e4399f99",
                    "name": "Zhenyu Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-06T18:59:57.000Z",
            "submittedOnDailyAt": "2026-01-07T08:32:16.658Z",
            "title": "Muses: Designing, Composing, Generating Nonexistent Fantasy 3D Creatures without Training",
            "submittedOnDailyBy": {
                "_id": "66a4a7d02f869fd1e8a608e7",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66a4a7d02f869fd1e8a608e7/ibcoHxegtjL42bY7io8CL.png",
                "isPro": false,
                "fullname": "Hexiao Lu",
                "user": "CraneLu",
                "type": "user"
            },
            "summary": "We present Muses, the first training-free method for fantastic 3D creature generation in a feed-forward paradigm. Previous methods, which rely on part-aware optimization, manual assembly, or 2D image generation, often produce unrealistic or incoherent 3D assets due to the challenges of intricate part-level manipulation and limited out-of-domain generation. In contrast, Muses leverages the 3D skeleton, a fundamental representation of biological forms, to explicitly and rationally compose diverse elements. This skeletal foundation formalizes 3D content creation as a structure-aware pipeline of design, composition, and generation. Muses begins by constructing a creatively composed 3D skeleton with coherent layout and scale through graph-constrained reasoning. This skeleton then guides a voxel-based assembly process within a structured latent space, integrating regions from different objects. Finally, image-guided appearance modeling under skeletal conditions is applied to generate a style-consistent and harmonious texture for the assembled shape. Extensive experiments establish Muses' state-of-the-art performance in terms of visual fidelity and alignment with textual descriptions, and potential on flexible 3D object editing. Project page: https://luhexiao.github.io/Muses.github.io/.",
            "upvotes": 3,
            "discussionId": "695dd791c03d6d81e4399f9a",
            "projectPage": "https://luhexiao.github.io/Muses.github.io/",
            "githubRepo": "https://github.com/luhexiao/Muses",
            "githubRepoAddedBy": "user",
            "ai_summary": "Muses enables feed-forward generation of 3D creatures by leveraging skeletal structures and graph-constrained reasoning for coherent design and assembly.",
            "ai_keywords": [
                "3D creature generation",
                "3D skeleton",
                "graph-constrained reasoning",
                "voxel-based assembly",
                "structured latent space",
                "image-guided appearance modeling",
                "skeletal conditions"
            ],
            "githubStars": 12,
            "organization": {
                "_id": "638f70e8f1256a80d4288555",
                "name": "nanjinguniv",
                "fullname": "Nanjing University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/638f706ef1256a80d42880f9/6M6-JzwJGiLxjIJzvCflf.png"
            }
        },
        "publishedAt": "2026-01-06T13:59:57.000Z",
        "title": "Muses: Designing, Composing, Generating Nonexistent Fantasy 3D Creatures without Training",
        "summary": "We present Muses, the first training-free method for fantastic 3D creature generation in a feed-forward paradigm. Previous methods, which rely on part-aware optimization, manual assembly, or 2D image generation, often produce unrealistic or incoherent 3D assets due to the challenges of intricate part-level manipulation and limited out-of-domain generation. In contrast, Muses leverages the 3D skeleton, a fundamental representation of biological forms, to explicitly and rationally compose diverse elements. This skeletal foundation formalizes 3D content creation as a structure-aware pipeline of design, composition, and generation. Muses begins by constructing a creatively composed 3D skeleton with coherent layout and scale through graph-constrained reasoning. This skeleton then guides a voxel-based assembly process within a structured latent space, integrating regions from different objects. Finally, image-guided appearance modeling under skeletal conditions is applied to generate a style-consistent and harmonious texture for the assembled shape. Extensive experiments establish Muses' state-of-the-art performance in terms of visual fidelity and alignment with textual descriptions, and potential on flexible 3D object editing. Project page: https://luhexiao.github.io/Muses.github.io/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03256.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "66a4a7d02f869fd1e8a608e7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66a4a7d02f869fd1e8a608e7/ibcoHxegtjL42bY7io8CL.png",
            "fullname": "Hexiao Lu",
            "name": "CraneLu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "isUserFollowing": false
        },
        "organization": {
            "_id": "638f70e8f1256a80d4288555",
            "name": "nanjinguniv",
            "fullname": "Nanjing University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/638f706ef1256a80d42880f9/6M6-JzwJGiLxjIJzvCflf.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2601.01720",
            "authors": [
                {
                    "_id": "695c971f6aa73bc11f0914ea",
                    "user": {
                        "_id": "6411a4f39457280bcdd29cf7",
                        "avatarUrl": "/avatars/b0c85362be1edbfc40035e7e841fcbeb.svg",
                        "isPro": false,
                        "fullname": "Xijie Huang",
                        "user": "ScarletAce",
                        "type": "user"
                    },
                    "name": "Xijie Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-07T13:45:36.672Z",
                    "hidden": false
                },
                {
                    "_id": "695c971f6aa73bc11f0914eb",
                    "user": {
                        "_id": "652fab9d04a34a9282bf29d6",
                        "avatarUrl": "/avatars/cd5967b37ebb1225e9ae1d46f196e2e2.svg",
                        "isPro": false,
                        "fullname": "Chengming Xu",
                        "user": "ChengmingX",
                        "type": "user"
                    },
                    "name": "Chengming Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-07T13:45:29.742Z",
                    "hidden": false
                },
                {
                    "_id": "695c971f6aa73bc11f0914ec",
                    "name": "Donghao Luo",
                    "hidden": false
                },
                {
                    "_id": "695c971f6aa73bc11f0914ed",
                    "name": "Xiaobin Hu",
                    "hidden": false
                },
                {
                    "_id": "695c971f6aa73bc11f0914ee",
                    "name": "Peng Tang",
                    "hidden": false
                },
                {
                    "_id": "695c971f6aa73bc11f0914ef",
                    "name": "Xu Peng",
                    "hidden": false
                },
                {
                    "_id": "695c971f6aa73bc11f0914f0",
                    "name": "Jiangning Zhang",
                    "hidden": false
                },
                {
                    "_id": "695c971f6aa73bc11f0914f1",
                    "user": {
                        "_id": "65729087379284b904a6d81d",
                        "avatarUrl": "/avatars/0e7968b1d9221d523a54e3ac787e449d.svg",
                        "isPro": false,
                        "fullname": "Chengjie Wang",
                        "user": "chengjie-wang",
                        "type": "user"
                    },
                    "name": "Chengjie Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-07T13:46:12.508Z",
                    "hidden": false
                },
                {
                    "_id": "695c971f6aa73bc11f0914f2",
                    "user": {
                        "_id": "6409fcc8f3dabf93824c84c6",
                        "avatarUrl": "/avatars/dd8fd579630e50ba3058c3829604478e.svg",
                        "isPro": false,
                        "fullname": "YANWEI",
                        "user": "yanweifuture",
                        "type": "user"
                    },
                    "name": "Yanwei Fu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-07T13:46:19.075Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-05T01:46:22.000Z",
            "submittedOnDailyAt": "2026-01-07T00:13:13.236Z",
            "title": "FFP-300K: Scaling First-Frame Propagation for Generalizable Video Editing",
            "submittedOnDailyBy": {
                "_id": "652fab9d04a34a9282bf29d6",
                "avatarUrl": "/avatars/cd5967b37ebb1225e9ae1d46f196e2e2.svg",
                "isPro": false,
                "fullname": "Chengming Xu",
                "user": "ChengmingX",
                "type": "user"
            },
            "summary": "First-Frame Propagation (FFP) offers a promising paradigm for controllable video editing, but existing methods are hampered by a reliance on cumbersome run-time guidance. We identify the root cause of this limitation as the inadequacy of current training datasets, which are often too short, low-resolution, and lack the task diversity required to teach robust temporal priors. To address this foundational data gap, we first introduce FFP-300K, a new large-scale dataset comprising 300K high-fidelity video pairs at 720p resolution and 81 frames in length, constructed via a principled two-track pipeline for diverse local and global edits. Building on this dataset, we propose a novel framework designed for true guidance-free FFP that resolves the critical tension between maintaining first-frame appearance and preserving source video motion. Architecturally, we introduce Adaptive Spatio-Temporal RoPE (AST-RoPE), which dynamically remaps positional encodings to disentangle appearance and motion references. At the objective level, we employ a self-distillation strategy where an identity propagation task acts as a powerful regularizer, ensuring long-term temporal stability and preventing semantic drift. Comprehensive experiments on the EditVerseBench benchmark demonstrate that our method significantly outperforming existing academic and commercial models by receiving about 0.2 PickScore and 0.3 VLM score improvement against these competitors.",
            "upvotes": 3,
            "discussionId": "695c971f6aa73bc11f0914f3",
            "projectPage": "https://ffp-300k.github.io/",
            "ai_summary": "A new large-scale video dataset and framework are presented that enable effective first-frame propagation without runtime guidance through adaptive spatio-temporal positional encoding and self-distillation techniques.",
            "ai_keywords": [
                "First-Frame Propagation",
                "video editing",
                "training datasets",
                "temporal priors",
                "FFP-300K",
                "Adaptive Spatio-Temporal RoPE",
                "AST-RoPE",
                "self-distillation",
                "identity propagation task",
                "PickScore",
                "VLM score",
                "EditVerseBench"
            ]
        },
        "publishedAt": "2026-01-04T20:46:22.000Z",
        "title": "FFP-300K: Scaling First-Frame Propagation for Generalizable Video Editing",
        "summary": "First-Frame Propagation (FFP) offers a promising paradigm for controllable video editing, but existing methods are hampered by a reliance on cumbersome run-time guidance. We identify the root cause of this limitation as the inadequacy of current training datasets, which are often too short, low-resolution, and lack the task diversity required to teach robust temporal priors. To address this foundational data gap, we first introduce FFP-300K, a new large-scale dataset comprising 300K high-fidelity video pairs at 720p resolution and 81 frames in length, constructed via a principled two-track pipeline for diverse local and global edits. Building on this dataset, we propose a novel framework designed for true guidance-free FFP that resolves the critical tension between maintaining first-frame appearance and preserving source video motion. Architecturally, we introduce Adaptive Spatio-Temporal RoPE (AST-RoPE), which dynamically remaps positional encodings to disentangle appearance and motion references. At the objective level, we employ a self-distillation strategy where an identity propagation task acts as a powerful regularizer, ensuring long-term temporal stability and preventing semantic drift. Comprehensive experiments on the EditVerseBench benchmark demonstrate that our method significantly outperforming existing academic and commercial models by receiving about 0.2 PickScore and 0.3 VLM score improvement against these competitors.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.01720.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "652fab9d04a34a9282bf29d6",
            "avatarUrl": "/avatars/cd5967b37ebb1225e9ae1d46f196e2e2.svg",
            "fullname": "Chengming Xu",
            "name": "ChengmingX",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2,
            "isUserFollowing": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2601.01592",
            "authors": [
                {
                    "_id": "695e1350c03d6d81e439a05f",
                    "user": {
                        "_id": "677d32cf28895e2124065a63",
                        "avatarUrl": "/avatars/897fa9fb3b3adc6e60094e3b505f94bc.svg",
                        "isPro": false,
                        "fullname": "Xin Wang",
                        "user": "xinwang22",
                        "type": "user"
                    },
                    "name": "Xin Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-07T09:25:09.928Z",
                    "hidden": false
                },
                {
                    "_id": "695e1350c03d6d81e439a060",
                    "name": "Yunhao Chen",
                    "hidden": false
                },
                {
                    "_id": "695e1350c03d6d81e439a061",
                    "name": "Juncheng Li",
                    "hidden": false
                },
                {
                    "_id": "695e1350c03d6d81e439a062",
                    "name": "Yixu Wang",
                    "hidden": false
                },
                {
                    "_id": "695e1350c03d6d81e439a063",
                    "name": "Yang Yao",
                    "hidden": false
                },
                {
                    "_id": "695e1350c03d6d81e439a064",
                    "name": "Tianle Gu",
                    "hidden": false
                },
                {
                    "_id": "695e1350c03d6d81e439a065",
                    "name": "Jie Li",
                    "hidden": false
                },
                {
                    "_id": "695e1350c03d6d81e439a066",
                    "name": "Yan Teng",
                    "hidden": false
                },
                {
                    "_id": "695e1350c03d6d81e439a067",
                    "name": "Xingjun Ma",
                    "hidden": false
                },
                {
                    "_id": "695e1350c03d6d81e439a068",
                    "name": "Yingchun Wang",
                    "hidden": false
                },
                {
                    "_id": "695e1350c03d6d81e439a069",
                    "name": "Xia Hu",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-04T16:41:33.000Z",
            "submittedOnDailyAt": "2026-01-07T05:37:26.155Z",
            "title": "OpenRT: An Open-Source Red Teaming Framework for Multimodal LLMs",
            "submittedOnDailyBy": {
                "_id": "677d32cf28895e2124065a63",
                "avatarUrl": "/avatars/897fa9fb3b3adc6e60094e3b505f94bc.svg",
                "isPro": false,
                "fullname": "Xin Wang",
                "user": "xinwang22",
                "type": "user"
            },
            "summary": "The rapid integration of Multimodal Large Language Models (MLLMs) into critical applications is increasingly hindered by persistent safety vulnerabilities. However, existing red-teaming benchmarks are often fragmented, limited to single-turn text interactions, and lack the scalability required for systematic evaluation. To address this, we introduce OpenRT, a unified, modular, and high-throughput red-teaming framework designed for comprehensive MLLM safety evaluation. At its core, OpenRT architects a paradigm shift in automated red-teaming by introducing an adversarial kernel that enables modular separation across five critical dimensions: model integration, dataset management, attack strategies, judging methods, and evaluation metrics. By standardizing attack interfaces, it decouples adversarial logic from a high-throughput asynchronous runtime, enabling systematic scaling across diverse models. Our framework integrates 37 diverse attack methodologies, spanning white-box gradients, multi-modal perturbations, and sophisticated multi-agent evolutionary strategies. Through an extensive empirical study on 20 advanced models (including GPT-5.2, Claude 4.5, and Gemini 3 Pro), we expose critical safety gaps: even frontier models fail to generalize across attack paradigms, with leading models exhibiting average Attack Success Rates as high as 49.14%. Notably, our findings reveal that reasoning models do not inherently possess superior robustness against complex, multi-turn jailbreaks. By open-sourcing OpenRT, we provide a sustainable, extensible, and continuously maintained infrastructure that accelerates the development and standardization of AI safety.",
            "upvotes": 3,
            "discussionId": "695e1350c03d6d81e439a06a",
            "projectPage": "https://ai45lab.github.io/OpenRT/",
            "githubRepo": "https://github.com/AI45Lab/OpenRT",
            "githubRepoAddedBy": "user",
            "ai_summary": "OpenRT is a unified red-teaming framework for evaluating multimodal large language model safety through modular adversarial testing across multiple attack dimensions and models.",
            "ai_keywords": [
                "Multimodal Large Language Models",
                "red-teaming",
                "adversarial kernel",
                "attack strategies",
                "judging methods",
                "evaluation metrics",
                "attack success rates",
                "multimodal perturbations",
                "multi-agent evolutionary strategies"
            ],
            "githubStars": 112
        },
        "publishedAt": "2026-01-04T11:41:33.000Z",
        "title": "OpenRT: An Open-Source Red Teaming Framework for Multimodal LLMs",
        "summary": "The rapid integration of Multimodal Large Language Models (MLLMs) into critical applications is increasingly hindered by persistent safety vulnerabilities. However, existing red-teaming benchmarks are often fragmented, limited to single-turn text interactions, and lack the scalability required for systematic evaluation. To address this, we introduce OpenRT, a unified, modular, and high-throughput red-teaming framework designed for comprehensive MLLM safety evaluation. At its core, OpenRT architects a paradigm shift in automated red-teaming by introducing an adversarial kernel that enables modular separation across five critical dimensions: model integration, dataset management, attack strategies, judging methods, and evaluation metrics. By standardizing attack interfaces, it decouples adversarial logic from a high-throughput asynchronous runtime, enabling systematic scaling across diverse models. Our framework integrates 37 diverse attack methodologies, spanning white-box gradients, multi-modal perturbations, and sophisticated multi-agent evolutionary strategies. Through an extensive empirical study on 20 advanced models (including GPT-5.2, Claude 4.5, and Gemini 3 Pro), we expose critical safety gaps: even frontier models fail to generalize across attack paradigms, with leading models exhibiting average Attack Success Rates as high as 49.14%. Notably, our findings reveal that reasoning models do not inherently possess superior robustness against complex, multi-turn jailbreaks. By open-sourcing OpenRT, we provide a sustainable, extensible, and continuously maintained infrastructure that accelerates the development and standardization of AI safety.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.01592.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "677d32cf28895e2124065a63",
            "avatarUrl": "/avatars/897fa9fb3b3adc6e60094e3b505f94bc.svg",
            "fullname": "Xin Wang",
            "name": "xinwang22",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1,
            "isUserFollowing": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2512.23412",
            "authors": [
                {
                    "_id": "695e39b9c679bee50ca69ab7",
                    "name": "Jiawei Chen",
                    "hidden": false
                },
                {
                    "_id": "695e39b9c679bee50ca69ab8",
                    "name": "Xintian Shen",
                    "hidden": false
                },
                {
                    "_id": "695e39b9c679bee50ca69ab9",
                    "name": "Lihao Zheng",
                    "hidden": false
                },
                {
                    "_id": "695e39b9c679bee50ca69aba",
                    "name": "Zhenwei Shao",
                    "hidden": false
                },
                {
                    "_id": "695e39b9c679bee50ca69abb",
                    "name": "Hongyuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "695e39b9c679bee50ca69abc",
                    "name": "Pengfei Yu",
                    "hidden": false
                },
                {
                    "_id": "695e39b9c679bee50ca69abd",
                    "name": "Xudong Rao",
                    "hidden": false
                },
                {
                    "_id": "695e39b9c679bee50ca69abe",
                    "name": "Ning Mao",
                    "hidden": false
                },
                {
                    "_id": "695e39b9c679bee50ca69abf",
                    "name": "Xiaobo Liu",
                    "hidden": false
                },
                {
                    "_id": "695e39b9c679bee50ca69ac0",
                    "name": "Lian Wen",
                    "hidden": false
                },
                {
                    "_id": "695e39b9c679bee50ca69ac1",
                    "name": "Chaoqun Du",
                    "hidden": false
                },
                {
                    "_id": "695e39b9c679bee50ca69ac2",
                    "name": "Feng Gu",
                    "hidden": false
                },
                {
                    "_id": "695e39b9c679bee50ca69ac3",
                    "name": "Wei He",
                    "hidden": false
                },
                {
                    "_id": "695e39b9c679bee50ca69ac4",
                    "name": "Qizhen Li",
                    "hidden": false
                },
                {
                    "_id": "695e39b9c679bee50ca69ac5",
                    "name": "Shanshan Li",
                    "hidden": false
                },
                {
                    "_id": "695e39b9c679bee50ca69ac6",
                    "name": "Zide Liu",
                    "hidden": false
                },
                {
                    "_id": "695e39b9c679bee50ca69ac7",
                    "name": "Jing Luo",
                    "hidden": false
                },
                {
                    "_id": "695e39b9c679bee50ca69ac8",
                    "name": "Lifu Mu",
                    "hidden": false
                },
                {
                    "_id": "695e39b9c679bee50ca69ac9",
                    "name": "Xuhao Pan",
                    "hidden": false
                },
                {
                    "_id": "695e39b9c679bee50ca69aca",
                    "name": "Chang Ren",
                    "hidden": false
                },
                {
                    "_id": "695e39b9c679bee50ca69acb",
                    "name": "Haoyi Sun",
                    "hidden": false
                },
                {
                    "_id": "695e39b9c679bee50ca69acc",
                    "name": "Qian Wang",
                    "hidden": false
                },
                {
                    "_id": "695e39b9c679bee50ca69acd",
                    "name": "Wei Wang",
                    "hidden": false
                },
                {
                    "_id": "695e39b9c679bee50ca69ace",
                    "name": "Hongfu Yang",
                    "hidden": false
                },
                {
                    "_id": "695e39b9c679bee50ca69acf",
                    "name": "Jiqing Zhan",
                    "hidden": false
                },
                {
                    "_id": "695e39b9c679bee50ca69ad0",
                    "name": "Chunpeng Zhou",
                    "hidden": false
                },
                {
                    "_id": "695e39b9c679bee50ca69ad1",
                    "name": "Zheng Zhou",
                    "hidden": false
                },
                {
                    "_id": "695e39b9c679bee50ca69ad2",
                    "name": "Hao Ma",
                    "hidden": false
                },
                {
                    "_id": "695e39b9c679bee50ca69ad3",
                    "name": "Tao Wei",
                    "hidden": false
                },
                {
                    "_id": "695e39b9c679bee50ca69ad4",
                    "name": "Pan Zhou",
                    "hidden": false
                },
                {
                    "_id": "695e39b9c679bee50ca69ad5",
                    "name": "Wei Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-29T12:16:12.000Z",
            "submittedOnDailyAt": "2026-01-07T23:01:50.833Z",
            "title": "MindWatcher: Toward Smarter Multimodal Tool-Integrated Reasoning",
            "submittedOnDailyBy": {
                "_id": "65dc57d684ea4abda9712710",
                "avatarUrl": "/avatars/4c7dc054e587a30c99afccf9c4cc5c0d.svg",
                "isPro": false,
                "fullname": "Timmy Chan",
                "user": "Lost-Cloud",
                "type": "user"
            },
            "summary": "Traditional workflow-based agents exhibit limited intelligence when addressing real-world problems requiring tool invocation. Tool-integrated reasoning (TIR) agents capable of autonomous reasoning and tool invocation are rapidly emerging as a powerful approach for complex decision-making tasks involving multi-step interactions with external environments. In this work, we introduce MindWatcher, a TIR agent integrating interleaved thinking and multimodal chain-of-thought (CoT) reasoning. MindWatcher can autonomously decide whether and how to invoke diverse tools and coordinate their use, without relying on human prompts or workflows. The interleaved thinking paradigm enables the model to switch between thinking and tool calling at any intermediate stage, while its multimodal CoT capability allows manipulation of images during reasoning to yield more precise search results. We implement automated data auditing and evaluation pipelines, complemented by manually curated high-quality datasets for training, and we construct a benchmark, called MindWatcher-Evaluate Bench (MWE-Bench), to evaluate its performance. MindWatcher is equipped with a comprehensive suite of auxiliary reasoning tools, enabling it to address broad-domain multimodal problems. A large-scale, high-quality local image retrieval database, covering eight categories including cars, animals, and plants, endows model with robust object recognition despite its small size. Finally, we design a more efficient training infrastructure for MindWatcher, enhancing training speed and hardware utilization. Experiments not only demonstrate that MindWatcher matches or exceeds the performance of larger or more recent models through superior tool invocation, but also uncover critical insights for agent training, such as the genetic inheritance phenomenon in agentic RL.",
            "upvotes": 3,
            "discussionId": "695e39b9c679bee50ca69ad6",
            "githubRepo": "https://github.com/TIMMY-CHAN/MindWatcher",
            "githubRepoAddedBy": "user",
            "ai_summary": "MindWatcher is a tool-integrated reasoning agent that combines interleaved thinking and multimodal chain-of-thought reasoning to autonomously perform complex decision-making tasks through coordinated tool invocation without human prompts.",
            "ai_keywords": [
                "tool-integrated reasoning",
                "interleaved thinking",
                "multimodal chain-of-thought",
                "agent training",
                "genetic inheritance phenomenon",
                "agentic RL"
            ],
            "organization": {
                "_id": "692fb0e4ffb801494bdcc1ea",
                "name": "LiAuto-Foundation-Model",
                "fullname": "LiAuto Foundation Model",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/692fa56f0c98923a5c8e3357/GRB_WF6DPgrC8O3cNcdTU.png"
            }
        },
        "publishedAt": "2025-12-29T07:16:12.000Z",
        "title": "MindWatcher: Toward Smarter Multimodal Tool-Integrated Reasoning",
        "summary": "Traditional workflow-based agents exhibit limited intelligence when addressing real-world problems requiring tool invocation. Tool-integrated reasoning (TIR) agents capable of autonomous reasoning and tool invocation are rapidly emerging as a powerful approach for complex decision-making tasks involving multi-step interactions with external environments. In this work, we introduce MindWatcher, a TIR agent integrating interleaved thinking and multimodal chain-of-thought (CoT) reasoning. MindWatcher can autonomously decide whether and how to invoke diverse tools and coordinate their use, without relying on human prompts or workflows. The interleaved thinking paradigm enables the model to switch between thinking and tool calling at any intermediate stage, while its multimodal CoT capability allows manipulation of images during reasoning to yield more precise search results. We implement automated data auditing and evaluation pipelines, complemented by manually curated high-quality datasets for training, and we construct a benchmark, called MindWatcher-Evaluate Bench (MWE-Bench), to evaluate its performance. MindWatcher is equipped with a comprehensive suite of auxiliary reasoning tools, enabling it to address broad-domain multimodal problems. A large-scale, high-quality local image retrieval database, covering eight categories including cars, animals, and plants, endows model with robust object recognition despite its small size. Finally, we design a more efficient training infrastructure for MindWatcher, enhancing training speed and hardware utilization. Experiments not only demonstrate that MindWatcher matches or exceeds the performance of larger or more recent models through superior tool invocation, but also uncover critical insights for agent training, such as the genetic inheritance phenomenon in agentic RL.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23412.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65dc57d684ea4abda9712710",
            "avatarUrl": "/avatars/4c7dc054e587a30c99afccf9c4cc5c0d.svg",
            "fullname": "Timmy Chan",
            "name": "Lost-Cloud",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2,
            "isUserFollowing": false
        },
        "organization": {
            "_id": "692fb0e4ffb801494bdcc1ea",
            "name": "LiAuto-Foundation-Model",
            "fullname": "LiAuto Foundation Model",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/692fa56f0c98923a5c8e3357/GRB_WF6DPgrC8O3cNcdTU.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2601.03227",
            "authors": [
                {
                    "_id": "695df23bc03d6d81e4399fec",
                    "user": {
                        "_id": "66b9bfade085a5c7e7c24669",
                        "avatarUrl": "/avatars/db9a5ca6d5568555a97c578ba0820508.svg",
                        "isPro": false,
                        "fullname": "Rising0321",
                        "user": "RisingZhang",
                        "type": "user"
                    },
                    "name": "Ruixing Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-07T09:25:30.396Z",
                    "hidden": false
                },
                {
                    "_id": "695df23bc03d6d81e4399fed",
                    "name": "Zihan Liu",
                    "hidden": false
                },
                {
                    "_id": "695df23bc03d6d81e4399fee",
                    "name": "Leilei Sun",
                    "hidden": false
                },
                {
                    "_id": "695df23bc03d6d81e4399fef",
                    "name": "Tongyu Zhu",
                    "hidden": false
                },
                {
                    "_id": "695df23bc03d6d81e4399ff0",
                    "name": "Weifeng Lv",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-06T18:13:24.000Z",
            "submittedOnDailyAt": "2026-01-07T03:14:46.464Z",
            "title": "The Sonar Moment: Benchmarking Audio-Language Models in Audio Geo-Localization",
            "submittedOnDailyBy": {
                "_id": "66b9bfade085a5c7e7c24669",
                "avatarUrl": "/avatars/db9a5ca6d5568555a97c578ba0820508.svg",
                "isPro": false,
                "fullname": "Rising0321",
                "user": "RisingZhang",
                "type": "user"
            },
            "summary": "Geo-localization aims to infer the geographic origin of a given signal. In computer vision, geo-localization has served as a demanding benchmark for compositional reasoning and is relevant to public safety. In contrast, progress on audio geo-localization has been constrained by the lack of high-quality audio-location pairs. To address this gap, we introduce AGL1K, the first audio geo-localization benchmark for audio language models (ALMs), spanning 72 countries and territories. To extract reliably localizable samples from a crowd-sourced platform, we propose the Audio Localizability metric that quantifies the informativeness of each recording, yielding 1,444 curated audio clips. Evaluations on 16 ALMs show that ALMs have emerged with audio geo-localization capability. We find that closed-source models substantially outperform open-source models, and that linguistic clues often dominate as a scaffold for prediction. We further analyze ALMs' reasoning traces, regional bias, error causes, and the interpretability of the localizability metric. Overall, AGL1K establishes a benchmark for audio geo-localization and may advance ALMs with better geospatial reasoning capability.",
            "upvotes": 1,
            "discussionId": "695df23cc03d6d81e4399ff1",
            "githubRepo": "https://github.com/Rising0321/AGL1K",
            "githubRepoAddedBy": "user",
            "ai_summary": "Audio geo-localization benchmark AGL1K is introduced to advance audio language models' geospatial reasoning capabilities through curated audio clips and evaluation across multiple models.",
            "ai_keywords": [
                "audio geo-localization",
                "audio language models",
                "AGL1K",
                "audio localizability metric",
                "compositional reasoning",
                "public safety",
                "crowd-sourced platform",
                "geospatial reasoning"
            ],
            "githubStars": 1,
            "organization": {
                "_id": "63ba7720fc454697637969f1",
                "name": "Beihang",
                "fullname": "Beihang University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ba7666c138c8f2b7844b58/n98lZU9VWxYgWIkzE_6o4.jpeg"
            }
        },
        "publishedAt": "2026-01-06T13:13:24.000Z",
        "title": "The Sonar Moment: Benchmarking Audio-Language Models in Audio Geo-Localization",
        "summary": "Geo-localization aims to infer the geographic origin of a given signal. In computer vision, geo-localization has served as a demanding benchmark for compositional reasoning and is relevant to public safety. In contrast, progress on audio geo-localization has been constrained by the lack of high-quality audio-location pairs. To address this gap, we introduce AGL1K, the first audio geo-localization benchmark for audio language models (ALMs), spanning 72 countries and territories. To extract reliably localizable samples from a crowd-sourced platform, we propose the Audio Localizability metric that quantifies the informativeness of each recording, yielding 1,444 curated audio clips. Evaluations on 16 ALMs show that ALMs have emerged with audio geo-localization capability. We find that closed-source models substantially outperform open-source models, and that linguistic clues often dominate as a scaffold for prediction. We further analyze ALMs' reasoning traces, regional bias, error causes, and the interpretability of the localizability metric. Overall, AGL1K establishes a benchmark for audio geo-localization and may advance ALMs with better geospatial reasoning capability.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03227.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66b9bfade085a5c7e7c24669",
            "avatarUrl": "/avatars/db9a5ca6d5568555a97c578ba0820508.svg",
            "fullname": "Rising0321",
            "name": "RisingZhang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "isUserFollowing": false
        },
        "organization": {
            "_id": "63ba7720fc454697637969f1",
            "name": "Beihang",
            "fullname": "Beihang University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ba7666c138c8f2b7844b58/n98lZU9VWxYgWIkzE_6o4.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2601.03194",
            "authors": [
                {
                    "_id": "695dcd3fc03d6d81e4399eb0",
                    "name": "Mohammad Zia Ur Rehman",
                    "hidden": false
                },
                {
                    "_id": "695dcd3fc03d6d81e4399eb1",
                    "user": {
                        "_id": "651692d718f3a57f869a5a0a",
                        "avatarUrl": "/avatars/d5fe48de11e46675b05e1e2e1cf3505c.svg",
                        "isPro": false,
                        "fullname": "Sai Kartheek Reddy",
                        "user": "UVSKKR",
                        "type": "user"
                    },
                    "name": "Sai Kartheek Reddy Kasu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-07T09:26:01.087Z",
                    "hidden": false
                },
                {
                    "_id": "695dcd3fc03d6d81e4399eb2",
                    "name": "Shashivardhan Reddy Koppula",
                    "hidden": false
                },
                {
                    "_id": "695dcd3fc03d6d81e4399eb3",
                    "name": "Sai Rithwik Reddy Chirra",
                    "hidden": false
                },
                {
                    "_id": "695dcd3fc03d6d81e4399eb4",
                    "user": {
                        "_id": "660971858b022f13fd42f497",
                        "avatarUrl": "/avatars/8b5c5e7949a5b1b4a2831bf9bdec892c.svg",
                        "isPro": false,
                        "fullname": "Shwetank Shekhar Singh",
                        "user": "shwetankssingh",
                        "type": "user"
                    },
                    "name": "Shwetank Shekhar Singh",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-07T13:44:15.941Z",
                    "hidden": false
                },
                {
                    "_id": "695dcd3fc03d6d81e4399eb5",
                    "name": "Nagendra Kumar",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-06T17:16:45.000Z",
            "submittedOnDailyAt": "2026-01-07T00:35:59.898Z",
            "title": "X-MuTeST: A Multilingual Benchmark for Explainable Hate Speech Detection and A Novel LLM-consulted Explanation Framework",
            "submittedOnDailyBy": {
                "_id": "651692d718f3a57f869a5a0a",
                "avatarUrl": "/avatars/d5fe48de11e46675b05e1e2e1cf3505c.svg",
                "isPro": false,
                "fullname": "Sai Kartheek Reddy",
                "user": "UVSKKR",
                "type": "user"
            },
            "summary": "Hate speech detection on social media faces challenges in both accuracy and explainability, especially for underexplored Indic languages. We propose a novel explainability-guided training framework, X-MuTeST (eXplainable Multilingual haTe Speech deTection), for hate speech detection that combines high-level semantic reasoning from large language models (LLMs) with traditional attention-enhancing techniques. We extend this research to Hindi and Telugu alongside English by providing benchmark human-annotated rationales for each word to justify the assigned class label. The X-MuTeST explainability method computes the difference between the prediction probabilities of the original text and those of unigrams, bigrams, and trigrams. Final explanations are computed as the union between LLM explanations and X-MuTeST explanations. We show that leveraging human rationales during training enhances both classification performance and explainability. Moreover, combining human rationales with our explainability method to refine the model attention yields further improvements. We evaluate explainability using Plausibility metrics such as Token-F1 and IOU-F1 and Faithfulness metrics such as Comprehensiveness and Sufficiency. By focusing on under-resourced languages, our work advances hate speech detection across diverse linguistic contexts. Our dataset includes token-level rationale annotations for 6,004 Hindi, 4,492 Telugu, and 6,334 English samples. Data and code are available on https://github.com/ziarehman30/X-MuTeST",
            "upvotes": 1,
            "discussionId": "695dcd40c03d6d81e4399eb6",
            "githubRepo": "https://github.com/ziarehman30/X-MuTeST",
            "githubRepoAddedBy": "user",
            "ai_summary": "A novel explainability-guided training framework for hate speech detection in Indic languages that combines large language models with attention-enhancing techniques and provides human-annotated rationales for improved performance and interpretability.",
            "ai_keywords": [
                "explainability-guided training",
                "large language models",
                "attention-enhancing techniques",
                "hate speech detection",
                "multilingual",
                "token-level rationale annotations",
                "Plausibility metrics",
                "Faithfulness metrics",
                "Token-F1",
                "IOU-F1",
                "Comprehensiveness",
                "Sufficiency"
            ],
            "githubStars": 0
        },
        "publishedAt": "2026-01-06T12:16:45.000Z",
        "title": "X-MuTeST: A Multilingual Benchmark for Explainable Hate Speech Detection and A Novel LLM-consulted Explanation Framework",
        "summary": "Hate speech detection on social media faces challenges in both accuracy and explainability, especially for underexplored Indic languages. We propose a novel explainability-guided training framework, X-MuTeST (eXplainable Multilingual haTe Speech deTection), for hate speech detection that combines high-level semantic reasoning from large language models (LLMs) with traditional attention-enhancing techniques. We extend this research to Hindi and Telugu alongside English by providing benchmark human-annotated rationales for each word to justify the assigned class label. The X-MuTeST explainability method computes the difference between the prediction probabilities of the original text and those of unigrams, bigrams, and trigrams. Final explanations are computed as the union between LLM explanations and X-MuTeST explanations. We show that leveraging human rationales during training enhances both classification performance and explainability. Moreover, combining human rationales with our explainability method to refine the model attention yields further improvements. We evaluate explainability using Plausibility metrics such as Token-F1 and IOU-F1 and Faithfulness metrics such as Comprehensiveness and Sufficiency. By focusing on under-resourced languages, our work advances hate speech detection across diverse linguistic contexts. Our dataset includes token-level rationale annotations for 6,004 Hindi, 4,492 Telugu, and 6,334 English samples. Data and code are available on https://github.com/ziarehman30/X-MuTeST",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03194.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "651692d718f3a57f869a5a0a",
            "avatarUrl": "/avatars/d5fe48de11e46675b05e1e2e1cf3505c.svg",
            "fullname": "Sai Kartheek Reddy",
            "name": "UVSKKR",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3,
            "isUserFollowing": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2601.03153",
            "authors": [
                {
                    "_id": "695dc084c03d6d81e4399e46",
                    "user": {
                        "_id": "65acfb3a14e6582c30b4ce76",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65acfb3a14e6582c30b4ce76/RhEhePggBtyM0RIIqXQen.jpeg",
                        "isPro": false,
                        "fullname": "TangJiakai",
                        "user": "TangJiakai5704",
                        "type": "user"
                    },
                    "name": "Jiakai Tang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-07T09:26:09.165Z",
                    "hidden": false
                },
                {
                    "_id": "695dc084c03d6d81e4399e47",
                    "name": "Xu Chen",
                    "hidden": false
                },
                {
                    "_id": "695dc084c03d6d81e4399e48",
                    "name": "Wen Chen",
                    "hidden": false
                },
                {
                    "_id": "695dc084c03d6d81e4399e49",
                    "name": "Jian Wu",
                    "hidden": false
                },
                {
                    "_id": "695dc084c03d6d81e4399e4a",
                    "name": "Yuning Jiang",
                    "hidden": false
                },
                {
                    "_id": "695dc084c03d6d81e4399e4b",
                    "name": "Bo Zheng",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-06T16:25:48.000Z",
            "submittedOnDailyAt": "2026-01-07T01:08:55.177Z",
            "title": "Parallel Latent Reasoning for Sequential Recommendation",
            "submittedOnDailyBy": {
                "_id": "65acfb3a14e6582c30b4ce76",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65acfb3a14e6582c30b4ce76/RhEhePggBtyM0RIIqXQen.jpeg",
                "isPro": false,
                "fullname": "TangJiakai",
                "user": "TangJiakai5704",
                "type": "user"
            },
            "summary": "Capturing complex user preferences from sparse behavioral sequences remains a fundamental challenge in sequential recommendation. Recent latent reasoning methods have shown promise by extending test-time computation through multi-step reasoning, yet they exclusively rely on depth-level scaling along a single trajectory, suffering from diminishing returns as reasoning depth increases. To address this limitation, we propose Parallel Latent Reasoning (PLR), a novel framework that pioneers width-level computational scaling by exploring multiple diverse reasoning trajectories simultaneously. PLR constructs parallel reasoning streams through learnable trigger tokens in continuous latent space, preserves diversity across streams via global reasoning regularization, and adaptively synthesizes multi-stream outputs through mixture-of-reasoning-streams aggregation. Extensive experiments on three real-world datasets demonstrate that PLR substantially outperforms state-of-the-art baselines while maintaining real-time inference efficiency. Theoretical analysis further validates the effectiveness of parallel reasoning in improving generalization capability. Our work opens new avenues for enhancing reasoning capacity in sequential recommendation beyond existing depth scaling.",
            "upvotes": 1,
            "discussionId": "695dc085c03d6d81e4399e4c",
            "ai_summary": "Parallel Latent Reasoning framework improves sequential recommendation by exploring multiple diverse reasoning trajectories simultaneously through learnable trigger tokens and adaptive aggregation.",
            "ai_keywords": [
                "latent reasoning",
                "sequential recommendation",
                "multi-step reasoning",
                "depth-level scaling",
                "width-level computational scaling",
                "reasoning trajectories",
                "learnable trigger tokens",
                "global reasoning regularization",
                "mixture-of-reasoning-streams aggregation"
            ]
        },
        "publishedAt": "2026-01-06T11:25:48.000Z",
        "title": "Parallel Latent Reasoning for Sequential Recommendation",
        "summary": "Capturing complex user preferences from sparse behavioral sequences remains a fundamental challenge in sequential recommendation. Recent latent reasoning methods have shown promise by extending test-time computation through multi-step reasoning, yet they exclusively rely on depth-level scaling along a single trajectory, suffering from diminishing returns as reasoning depth increases. To address this limitation, we propose Parallel Latent Reasoning (PLR), a novel framework that pioneers width-level computational scaling by exploring multiple diverse reasoning trajectories simultaneously. PLR constructs parallel reasoning streams through learnable trigger tokens in continuous latent space, preserves diversity across streams via global reasoning regularization, and adaptively synthesizes multi-stream outputs through mixture-of-reasoning-streams aggregation. Extensive experiments on three real-world datasets demonstrate that PLR substantially outperforms state-of-the-art baselines while maintaining real-time inference efficiency. Theoretical analysis further validates the effectiveness of parallel reasoning in improving generalization capability. Our work opens new avenues for enhancing reasoning capacity in sequential recommendation beyond existing depth scaling.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03153.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65acfb3a14e6582c30b4ce76",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65acfb3a14e6582c30b4ce76/RhEhePggBtyM0RIIqXQen.jpeg",
            "fullname": "TangJiakai",
            "name": "TangJiakai5704",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1,
            "isUserFollowing": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2601.03127",
            "authors": [
                {
                    "_id": "695e85d9bf9f21203a768c6f",
                    "name": "Sashuai Zhou",
                    "hidden": false
                },
                {
                    "_id": "695e85d9bf9f21203a768c70",
                    "name": "Qiang Zhou",
                    "hidden": false
                },
                {
                    "_id": "695e85d9bf9f21203a768c71",
                    "name": "Jijin Hu",
                    "hidden": false
                },
                {
                    "_id": "695e85d9bf9f21203a768c72",
                    "name": "Hanqing Yang",
                    "hidden": false
                },
                {
                    "_id": "695e85d9bf9f21203a768c73",
                    "name": "Yue Cao",
                    "hidden": false
                },
                {
                    "_id": "695e85d9bf9f21203a768c74",
                    "name": "Junpeng Ma",
                    "hidden": false
                },
                {
                    "_id": "695e85d9bf9f21203a768c75",
                    "name": "Yinchao Ma",
                    "hidden": false
                },
                {
                    "_id": "695e85d9bf9f21203a768c76",
                    "name": "Jun Song",
                    "hidden": false
                },
                {
                    "_id": "695e85d9bf9f21203a768c77",
                    "name": "Tiezheng Ge",
                    "hidden": false
                },
                {
                    "_id": "695e85d9bf9f21203a768c78",
                    "name": "Cheng Yu",
                    "hidden": false
                },
                {
                    "_id": "695e85d9bf9f21203a768c79",
                    "name": "Bo Zheng",
                    "hidden": false
                },
                {
                    "_id": "695e85d9bf9f21203a768c7a",
                    "name": "Zhou Zhao",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-06T15:59:33.000Z",
            "submittedOnDailyAt": "2026-01-07T13:46:33.456Z",
            "title": "Unified Thinker: A General Reasoning Modular Core for Image Generation",
            "submittedOnDailyBy": {
                "_id": "64db1edc7266618e853c6c13",
                "avatarUrl": "/avatars/b3362a882fd9cee3d9f59d8dbcc02367.svg",
                "isPro": false,
                "fullname": "mightyzau",
                "user": "mightyzau",
                "type": "user"
            },
            "summary": "Despite impressive progress in high-fidelity image synthesis, generative models still struggle with logic-intensive instruction following, exposing a persistent reasoning--execution gap. Meanwhile, closed-source systems (e.g., Nano Banana) have demonstrated strong reasoning-driven image generation, highlighting a substantial gap to current open-source models. We argue that closing this gap requires not merely better visual generators, but executable reasoning: decomposing high-level intents into grounded, verifiable plans that directly steer the generative process. To this end, we propose Unified Thinker, a task-agnostic reasoning architecture for general image generation, designed as a unified planning core that can plug into diverse generators and workflows. Unified Thinker decouples a dedicated Thinker from the image Generator, enabling modular upgrades of reasoning without retraining the entire generative model. We further introduce a two-stage training paradigm: we first build a structured planning interface for the Thinker, then apply reinforcement learning to ground its policy in pixel-level feedback, encouraging plans that optimize visual correctness over textual plausibility. Extensive experiments on text-to-image generation and image editing show that Unified Thinker substantially improves image reasoning and generation quality.",
            "upvotes": 1,
            "discussionId": "695e85dabf9f21203a768c7b",
            "ai_summary": "Unified Thinker addresses the reasoning-execution gap in image generation by decoupling a reasoning module from image generators and using reinforcement learning to optimize visual correctness.",
            "ai_keywords": [
                "generative models",
                "reasoning--execution gap",
                "closed-source systems",
                "executable reasoning",
                "task-agnostic reasoning architecture",
                "unified planning core",
                "image generation",
                "reinforcement learning",
                "visual correctness",
                "textual plausibility"
            ]
        },
        "publishedAt": "2026-01-06T10:59:33.000Z",
        "title": "Unified Thinker: A General Reasoning Modular Core for Image Generation",
        "summary": "Despite impressive progress in high-fidelity image synthesis, generative models still struggle with logic-intensive instruction following, exposing a persistent reasoning--execution gap. Meanwhile, closed-source systems (e.g., Nano Banana) have demonstrated strong reasoning-driven image generation, highlighting a substantial gap to current open-source models. We argue that closing this gap requires not merely better visual generators, but executable reasoning: decomposing high-level intents into grounded, verifiable plans that directly steer the generative process. To this end, we propose Unified Thinker, a task-agnostic reasoning architecture for general image generation, designed as a unified planning core that can plug into diverse generators and workflows. Unified Thinker decouples a dedicated Thinker from the image Generator, enabling modular upgrades of reasoning without retraining the entire generative model. We further introduce a two-stage training paradigm: we first build a structured planning interface for the Thinker, then apply reinforcement learning to ground its policy in pixel-level feedback, encouraging plans that optimize visual correctness over textual plausibility. Extensive experiments on text-to-image generation and image editing show that Unified Thinker substantially improves image reasoning and generation quality.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03127.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64db1edc7266618e853c6c13",
            "avatarUrl": "/avatars/b3362a882fd9cee3d9f59d8dbcc02367.svg",
            "fullname": "mightyzau",
            "name": "mightyzau",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2,
            "isUserFollowing": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2601.02996",
            "authors": [
                {
                    "_id": "695ec21c5fa3847525c41c39",
                    "name": "Yihong Liu",
                    "hidden": false
                },
                {
                    "_id": "695ec21c5fa3847525c41c3a",
                    "name": "Raoyuan Zhao",
                    "hidden": false
                },
                {
                    "_id": "695ec21c5fa3847525c41c3b",
                    "name": "Hinrich Schtze",
                    "hidden": false
                },
                {
                    "_id": "695ec21c5fa3847525c41c3c",
                    "name": "Michael A. Hedderich",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-06T13:20:17.000Z",
            "submittedOnDailyAt": "2026-01-07T18:00:28.244Z",
            "title": "Large Reasoning Models Are (Not Yet) Multilingual Latent Reasoners",
            "submittedOnDailyBy": {
                "_id": "653f7e569e84d1e8b6a66e70",
                "avatarUrl": "/avatars/24eaa6434508a162c349aebfc51990ff.svg",
                "isPro": false,
                "fullname": "Yihong Liu",
                "user": "yihongLiu",
                "type": "user"
            },
            "summary": "Large reasoning models (LRMs) achieve strong performance on mathematical reasoning tasks, often attributed to their capability to generate explicit chain-of-thought (CoT) explanations. However, recent work shows that LRMs often arrive at the correct answer before completing these textual reasoning steps, indicating the presence of latent reasoning -- internal, non-verbal computation encoded in hidden states. While this phenomenon has been explored in English, its multilingual behavior remains largely unknown. In this paper, we conduct a systematic investigation of multilingual latent reasoning in LRMs across 11 languages. Using a truncation-based strategy, we examine how the correct answer emerges as the model is given only partial reasoning traces, allowing us to measure stepwise latent prediction formation. Our results reveal clear evidence of multilingual latent reasoning, though unevenly: strong in resource-rich languages, weaker in low-resource ones, and broadly less observable on harder benchmarks. To understand whether these differences reflect distinct internal mechanisms, we further perform representational analyses. Despite surface-level disparities, we find that the internal evolution of predictions is highly consistent across languages and broadly aligns with English -- a pattern suggesting an English-centered latent reasoning pathway.",
            "upvotes": 1,
            "discussionId": "695ec21d5fa3847525c41c3d",
            "githubRepo": "https://github.com/cisnlp/multilingual-latent-reasoner",
            "githubRepoAddedBy": "user",
            "ai_summary": "Large reasoning models exhibit multilingual latent reasoning capabilities with varying strength across languages and benchmarks, showing consistent internal prediction evolution despite surface-level differences.",
            "ai_keywords": [
                "large reasoning models",
                "chain-of-thought",
                "latent reasoning",
                "multilingual behavior",
                "truncation-based strategy",
                "representational analyses",
                "internal mechanisms"
            ],
            "githubStars": 2,
            "organization": {
                "_id": "61cace5960cb6b25c495f21a",
                "name": "cis-lmu",
                "fullname": "CIS, LMU Munich",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61bf84c8ca59d6d196a1b4e8/iI0IVk2414OM4aRZGH5qJ.png"
            }
        },
        "publishedAt": "2026-01-06T08:20:17.000Z",
        "title": "Large Reasoning Models Are (Not Yet) Multilingual Latent Reasoners",
        "summary": "Large reasoning models (LRMs) achieve strong performance on mathematical reasoning tasks, often attributed to their capability to generate explicit chain-of-thought (CoT) explanations. However, recent work shows that LRMs often arrive at the correct answer before completing these textual reasoning steps, indicating the presence of latent reasoning -- internal, non-verbal computation encoded in hidden states. While this phenomenon has been explored in English, its multilingual behavior remains largely unknown. In this paper, we conduct a systematic investigation of multilingual latent reasoning in LRMs across 11 languages. Using a truncation-based strategy, we examine how the correct answer emerges as the model is given only partial reasoning traces, allowing us to measure stepwise latent prediction formation. Our results reveal clear evidence of multilingual latent reasoning, though unevenly: strong in resource-rich languages, weaker in low-resource ones, and broadly less observable on harder benchmarks. To understand whether these differences reflect distinct internal mechanisms, we further perform representational analyses. Despite surface-level disparities, we find that the internal evolution of predictions is highly consistent across languages and broadly aligns with English -- a pattern suggesting an English-centered latent reasoning pathway.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02996.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "653f7e569e84d1e8b6a66e70",
            "avatarUrl": "/avatars/24eaa6434508a162c349aebfc51990ff.svg",
            "fullname": "Yihong Liu",
            "name": "yihongLiu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5,
            "isUserFollowing": false
        },
        "organization": {
            "_id": "61cace5960cb6b25c495f21a",
            "name": "cis-lmu",
            "fullname": "CIS, LMU Munich",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61bf84c8ca59d6d196a1b4e8/iI0IVk2414OM4aRZGH5qJ.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2601.02359",
            "authors": [
                {
                    "_id": "695cf6c53e319b66c3e3a3e9",
                    "user": {
                        "_id": "630487d4eb6d777a838caad3",
                        "avatarUrl": "/avatars/8e03144498dca8ec32a17449b631fa00.svg",
                        "isPro": false,
                        "fullname": "Kaede Shiohara",
                        "user": "mapooon",
                        "type": "user"
                    },
                    "name": "Kaede Shiohara",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-07T13:13:55.630Z",
                    "hidden": false
                },
                {
                    "_id": "695cf6c53e319b66c3e3a3ea",
                    "name": "Toshihiko Yamasaki",
                    "hidden": false
                },
                {
                    "_id": "695cf6c53e319b66c3e3a3eb",
                    "name": "Vladislav Golyanik",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/630487d4eb6d777a838caad3/j9yYja2JLmNLlo9zcegSt.mp4"
            ],
            "publishedAt": "2026-01-05T18:59:54.000Z",
            "submittedOnDailyAt": "2026-01-07T11:34:37.142Z",
            "title": "ExposeAnyone: Personalized Audio-to-Expression Diffusion Models Are Robust Zero-Shot Face Forgery Detectors",
            "submittedOnDailyBy": {
                "_id": "630487d4eb6d777a838caad3",
                "avatarUrl": "/avatars/8e03144498dca8ec32a17449b631fa00.svg",
                "isPro": false,
                "fullname": "Kaede Shiohara",
                "user": "mapooon",
                "type": "user"
            },
            "summary": "Detecting unknown deepfake manipulations remains one of the most challenging problems in face forgery detection. Current state-of-the-art approaches fail to generalize to unseen manipulations, as they primarily rely on supervised training with existing deepfakes or pseudo-fakes, which leads to overfitting to specific forgery patterns. In contrast, self-supervised methods offer greater potential for generalization, but existing work struggles to learn discriminative representations only from self-supervision. In this paper, we propose ExposeAnyone, a fully self-supervised approach based on a diffusion model that generates expression sequences from audio. The key idea is, once the model is personalized to specific subjects using reference sets, it can compute the identity distances between suspected videos and personalized subjects via diffusion reconstruction errors, enabling person-of-interest face forgery detection. Extensive experiments demonstrate that 1) our method outperforms the previous state-of-the-art method by 4.22 percentage points in the average AUC on DF-TIMIT, DFDCP, KoDF, and IDForge datasets, 2) our model is also capable of detecting Sora2-generated videos, where the previous approaches perform poorly, and 3) our method is highly robust to corruptions such as blur and compression, highlighting the applicability in real-world face forgery detection.",
            "upvotes": 1,
            "discussionId": "695cf6c53e319b66c3e3a3ec",
            "ai_summary": "A fully self-supervised diffusion model approach for face forgery detection that computes identity distances through reconstruction errors, demonstrating superior performance on unseen manipulations and real-world corruptions.",
            "ai_keywords": [
                "diffusion model",
                "self-supervised learning",
                "face forgery detection",
                "identity distances",
                "diffusion reconstruction errors",
                "pseudo-fakes",
                "overfitting",
                "expression sequences",
                "audio-to-video generation"
            ]
        },
        "publishedAt": "2026-01-05T13:59:54.000Z",
        "title": "ExposeAnyone: Personalized Audio-to-Expression Diffusion Models Are Robust Zero-Shot Face Forgery Detectors",
        "summary": "Detecting unknown deepfake manipulations remains one of the most challenging problems in face forgery detection. Current state-of-the-art approaches fail to generalize to unseen manipulations, as they primarily rely on supervised training with existing deepfakes or pseudo-fakes, which leads to overfitting to specific forgery patterns. In contrast, self-supervised methods offer greater potential for generalization, but existing work struggles to learn discriminative representations only from self-supervision. In this paper, we propose ExposeAnyone, a fully self-supervised approach based on a diffusion model that generates expression sequences from audio. The key idea is, once the model is personalized to specific subjects using reference sets, it can compute the identity distances between suspected videos and personalized subjects via diffusion reconstruction errors, enabling person-of-interest face forgery detection. Extensive experiments demonstrate that 1) our method outperforms the previous state-of-the-art method by 4.22 percentage points in the average AUC on DF-TIMIT, DFDCP, KoDF, and IDForge datasets, 2) our model is also capable of detecting Sora2-generated videos, where the previous approaches perform poorly, and 3) our method is highly robust to corruptions such as blur and compression, highlighting the applicability in real-world face forgery detection.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/630487d4eb6d777a838caad3/j9yYja2JLmNLlo9zcegSt.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02359.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "630487d4eb6d777a838caad3",
            "avatarUrl": "/avatars/8e03144498dca8ec32a17449b631fa00.svg",
            "fullname": "Kaede Shiohara",
            "name": "mapooon",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "isUserFollowing": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2601.00581",
            "authors": [
                {
                    "_id": "695bab08832867f253525e82",
                    "name": "Stephen E. Farr",
                    "hidden": false
                },
                {
                    "_id": "695bab08832867f253525e83",
                    "name": "Stefan Doerr",
                    "hidden": false
                },
                {
                    "_id": "695bab08832867f253525e84",
                    "name": "Antonio Mirarchi",
                    "hidden": false
                },
                {
                    "_id": "695bab08832867f253525e85",
                    "name": "Francesc Sabanes Zariquiey",
                    "hidden": false
                },
                {
                    "_id": "695bab08832867f253525e86",
                    "user": {
                        "_id": "69313e8787976460113bf60d",
                        "avatarUrl": "/avatars/d46cb78df9a57d6510573b57ddfbb061.svg",
                        "isPro": false,
                        "fullname": "Gianni De Fabritiis",
                        "user": "gdefabritiis",
                        "type": "user"
                    },
                    "name": "Gianni De Fabritiis",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-07T13:39:55.185Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64599a0aa82daa98729dc902/UuVaMAFxY7olcGanK7IrY.png"
            ],
            "publishedAt": "2026-01-02T05:47:37.000Z",
            "submittedOnDailyAt": "2026-01-07T07:11:03.070Z",
            "title": "AceFF: A State-of-the-Art Machine Learning Potential for Small Molecules",
            "submittedOnDailyBy": {
                "_id": "64599a0aa82daa98729dc902",
                "avatarUrl": "/avatars/bb4f3dcd81310aac7f8a875dc157f36b.svg",
                "isPro": false,
                "fullname": "De Fabritiis",
                "user": "giadefa",
                "type": "user"
            },
            "summary": "We introduce AceFF, a pre-trained machine learning interatomic potential (MLIP) optimized for small molecule drug discovery. While MLIPs have emerged as efficient alternatives to Density Functional Theory (DFT), generalizability across diverse chemical spaces remains difficult. AceFF addresses this via a refined TensorNet2 architecture trained on a comprehensive dataset of drug-like compounds. This approach yields a force field that balances high-throughput inference speed with DFT-level accuracy. AceFF fully supports the essential medicinal chemistry elements (H, B, C, N, O, F, Si, P, S, Cl, Br, I) and is explicitly trained to handle charged states. Validation against rigorous benchmarks, including complex torsional energy scans, molecular dynamics trajectories, batched minimizations, and forces and anergy accuracy demonstrates that AceFF establishes a new state-of-the-art for organic molecules. The AceFF-2 model weights and inference code are available at https://huggingface.co/Acellera/AceFF-2.0.",
            "upvotes": 1,
            "discussionId": "695bab0a832867f253525e87",
            "githubRepo": "https://github.com/torchmd/torchmd-net",
            "githubRepoAddedBy": "user",
            "githubStars": 458,
            "organization": {
                "_id": "64599bd1abdbb77c4c6d568b",
                "name": "Acellera",
                "fullname": "Acellera",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64599a0aa82daa98729dc902/1ZnsvZ9kVbfvv2nDyk5oO.jpeg"
            }
        },
        "publishedAt": "2026-01-02T00:47:37.000Z",
        "title": "AceFF: A State-of-the-Art Machine Learning Potential for Small Molecules",
        "summary": "We introduce AceFF, a pre-trained machine learning interatomic potential (MLIP) optimized for small molecule drug discovery. While MLIPs have emerged as efficient alternatives to Density Functional Theory (DFT), generalizability across diverse chemical spaces remains difficult. AceFF addresses this via a refined TensorNet2 architecture trained on a comprehensive dataset of drug-like compounds. This approach yields a force field that balances high-throughput inference speed with DFT-level accuracy. AceFF fully supports the essential medicinal chemistry elements (H, B, C, N, O, F, Si, P, S, Cl, Br, I) and is explicitly trained to handle charged states. Validation against rigorous benchmarks, including complex torsional energy scans, molecular dynamics trajectories, batched minimizations, and forces and anergy accuracy demonstrates that AceFF establishes a new state-of-the-art for organic molecules. The AceFF-2 model weights and inference code are available at https://huggingface.co/Acellera/AceFF-2.0.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64599a0aa82daa98729dc902/UuVaMAFxY7olcGanK7IrY.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.00581.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64599a0aa82daa98729dc902",
            "avatarUrl": "/avatars/bb4f3dcd81310aac7f8a875dc157f36b.svg",
            "fullname": "De Fabritiis",
            "name": "giadefa",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "isUserFollowing": false
        },
        "organization": {
            "_id": "64599bd1abdbb77c4c6d568b",
            "name": "Acellera",
            "fullname": "Acellera",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64599a0aa82daa98729dc902/1ZnsvZ9kVbfvv2nDyk5oO.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2512.23950",
            "authors": [
                {
                    "_id": "69565f6d832867f2535257dd",
                    "name": "Huibin Li",
                    "hidden": false
                },
                {
                    "_id": "69565f6d832867f2535257de",
                    "user": {
                        "_id": "67dfd511eb33688e48f7e69d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Oluc8nC6H8h-56r39vXx0.png",
                        "isPro": false,
                        "fullname": "Haoran Liu",
                        "user": "FengShaner",
                        "type": "user"
                    },
                    "name": "Haoran Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-02T15:38:37.495Z",
                    "hidden": false
                },
                {
                    "_id": "69565f6d832867f2535257df",
                    "name": "Mingzhe Liu",
                    "hidden": false
                },
                {
                    "_id": "69565f6d832867f2535257e0",
                    "name": "Yulong Xiao",
                    "hidden": false
                },
                {
                    "_id": "69565f6d832867f2535257e1",
                    "name": "Peng Li",
                    "hidden": false
                },
                {
                    "_id": "69565f6d832867f2535257e2",
                    "name": "Guibin Zan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-12-30T02:38:26.000Z",
            "submittedOnDailyAt": "2026-01-07T07:46:20.121Z",
            "title": "U-Net-Like Spiking Neural Networks for Single Image Dehazing",
            "submittedOnDailyBy": {
                "_id": "67dfd511eb33688e48f7e69d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Oluc8nC6H8h-56r39vXx0.png",
                "isPro": false,
                "fullname": "Haoran Liu",
                "user": "FengShaner",
                "type": "user"
            },
            "summary": "Image dehazing is a critical challenge in computer vision, essential for enhancing image clarity in hazy conditions. Traditional methods often rely on atmospheric scattering models, while recent deep learning techniques, specifically Convolutional Neural Networks (CNNs) and Transformers, have improved performance by effectively analyzing image features. However, CNNs struggle with long-range dependencies, and Transformers demand significant computational resources. To address these limitations, we propose DehazeSNN, an innovative architecture that integrates a U-Net-like design with Spiking Neural Networks (SNNs). DehazeSNN captures multi-scale image features while efficiently managing local and long-range dependencies. The introduction of the Orthogonal Leaky-Integrate-and-Fire Block (OLIFBlock) enhances cross-channel communication, resulting in superior dehazing performance with reduced computational burden. Our extensive experiments show that DehazeSNN is highly competitive to state-of-the-art methods on benchmark datasets, delivering high-quality haze-free images with a smaller model size and less multiply-accumulate operations. The proposed dehazing method is publicly available at https://github.com/HaoranLiu507/DehazeSNN.",
            "upvotes": 1,
            "discussionId": "69565f6e832867f2535257e3",
            "githubRepo": "https://github.com/HaoranLiu507/DehazeSNN",
            "githubRepoAddedBy": "user",
            "ai_summary": "DehazeSNN combines a U-Net-like architecture with Spiking Neural Networks and an Orthogonal Leaky-Integrate-and-Fire Block to achieve efficient and effective image dehazing with reduced computational resources.",
            "ai_keywords": [
                "Convolutional Neural Networks (CNNs)",
                "Transformers",
                "Spiking Neural Networks (SNNs)",
                "U-Net-like design",
                "Spiking Neural Networks (SNNs)",
                "Orthogonal Leaky-Integrate-and-Fire Block (OLIFBlock)",
                "cross-channel communication",
                "multiply-accumulate operations"
            ],
            "githubStars": 2
        },
        "publishedAt": "2025-12-29T21:38:26.000Z",
        "title": "U-Net-Like Spiking Neural Networks for Single Image Dehazing",
        "summary": "Image dehazing is a critical challenge in computer vision, essential for enhancing image clarity in hazy conditions. Traditional methods often rely on atmospheric scattering models, while recent deep learning techniques, specifically Convolutional Neural Networks (CNNs) and Transformers, have improved performance by effectively analyzing image features. However, CNNs struggle with long-range dependencies, and Transformers demand significant computational resources. To address these limitations, we propose DehazeSNN, an innovative architecture that integrates a U-Net-like design with Spiking Neural Networks (SNNs). DehazeSNN captures multi-scale image features while efficiently managing local and long-range dependencies. The introduction of the Orthogonal Leaky-Integrate-and-Fire Block (OLIFBlock) enhances cross-channel communication, resulting in superior dehazing performance with reduced computational burden. Our extensive experiments show that DehazeSNN is highly competitive to state-of-the-art methods on benchmark datasets, delivering high-quality haze-free images with a smaller model size and less multiply-accumulate operations. The proposed dehazing method is publicly available at https://github.com/HaoranLiu507/DehazeSNN.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23950.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67dfd511eb33688e48f7e69d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Oluc8nC6H8h-56r39vXx0.png",
            "fullname": "Haoran Liu",
            "name": "FengShaner",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "isUserFollowing": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2601.01584",
            "authors": [
                {
                    "_id": "695e0f95c03d6d81e439a058",
                    "user": {
                        "_id": "6605626876a0652cac85f233",
                        "avatarUrl": "/avatars/fa008045c9ca4e0d71d40a02de74104d.svg",
                        "isPro": false,
                        "fullname": "j-hoscilowic",
                        "user": "j-hoscilowic",
                        "type": "user"
                    },
                    "name": "Jakub Hoscilowicz",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-07T09:25:15.525Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-04T16:15:59.000Z",
            "submittedOnDailyAt": "2026-01-07T05:19:58.147Z",
            "title": "Steerability of Instrumental-Convergence Tendencies in LLMs",
            "submittedOnDailyBy": {
                "_id": "6605626876a0652cac85f233",
                "avatarUrl": "/avatars/fa008045c9ca4e0d71d40a02de74104d.svg",
                "isPro": false,
                "fullname": "j-hoscilowic",
                "user": "j-hoscilowic",
                "type": "user"
            },
            "summary": "We examine two properties of AI systems: capability (what a system can do) and steerability (how reliably one can shift behavior toward intended outcomes). A central question is whether capability growth reduces steerability and risks control collapse. We also distinguish between authorized steerability (builders reliably reaching intended behaviors) and unauthorized steerability (attackers eliciting disallowed behaviors). This distinction highlights a fundamental safety--security dilemma of AI models: safety requires high steerability to enforce control (e.g., stop/refuse), while security requires low steerability for malicious actors to elicit harmful behaviors. This tension presents a significant challenge for open-weight models, which currently exhibit high steerability via common techniques like fine-tuning or adversarial attacks. Using Qwen3 and InstrumentalEval, we find that a short anti-instrumental prompt suffix sharply reduces the measured convergence rate (e.g., shutdown avoidance, self-replication). For Qwen3-30B Instruct, the convergence rate drops from 81.69% under a pro-instrumental suffix to 2.82% under an anti-instrumental suffix. Under anti-instrumental prompting, larger aligned models show lower convergence rates than smaller ones (Instruct: 2.82% vs. 4.23%; Thinking: 4.23% vs. 9.86%). Code is available at github.com/j-hoscilowicz/instrumental_steering.",
            "upvotes": 0,
            "discussionId": "695e0f95c03d6d81e439a059",
            "githubRepo": "https://github.com/j-hoscilowicz/instrumental_steering/",
            "githubRepoAddedBy": "user",
            "ai_summary": "Research investigates the balance between AI system capabilities and steerability, finding that specific prompts can dramatically reduce unwanted behaviors in large language models.",
            "ai_keywords": [
                "steerability",
                "capability",
                "control collapse",
                "authorized steerability",
                "unauthorized steerability",
                "safety-security dilemma",
                "open-weight models",
                "fine-tuning",
                "adversarial attacks",
                "convergence rate",
                "anti-instrumental prompt suffix",
                "pro-instrumental suffix",
                "Qwen3",
                "InstrumentalEval"
            ],
            "githubStars": 0
        },
        "publishedAt": "2026-01-04T11:15:59.000Z",
        "title": "Steerability of Instrumental-Convergence Tendencies in LLMs",
        "summary": "We examine two properties of AI systems: capability (what a system can do) and steerability (how reliably one can shift behavior toward intended outcomes). A central question is whether capability growth reduces steerability and risks control collapse. We also distinguish between authorized steerability (builders reliably reaching intended behaviors) and unauthorized steerability (attackers eliciting disallowed behaviors). This distinction highlights a fundamental safety--security dilemma of AI models: safety requires high steerability to enforce control (e.g., stop/refuse), while security requires low steerability for malicious actors to elicit harmful behaviors. This tension presents a significant challenge for open-weight models, which currently exhibit high steerability via common techniques like fine-tuning or adversarial attacks. Using Qwen3 and InstrumentalEval, we find that a short anti-instrumental prompt suffix sharply reduces the measured convergence rate (e.g., shutdown avoidance, self-replication). For Qwen3-30B Instruct, the convergence rate drops from 81.69% under a pro-instrumental suffix to 2.82% under an anti-instrumental suffix. Under anti-instrumental prompting, larger aligned models show lower convergence rates than smaller ones (Instruct: 2.82% vs. 4.23%; Thinking: 4.23% vs. 9.86%). Code is available at github.com/j-hoscilowicz/instrumental_steering.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.01584.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6605626876a0652cac85f233",
            "avatarUrl": "/avatars/fa008045c9ca4e0d71d40a02de74104d.svg",
            "fullname": "j-hoscilowic",
            "name": "j-hoscilowic",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1,
            "isUserFollowing": false
        },
        "isAuthorParticipating": true
    }
]