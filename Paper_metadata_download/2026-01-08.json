[
    {
        "paper": {
            "id": "2601.02151",
            "authors": [
                {
                    "_id": "695f2d8a5fa3847525c41f8d",
                    "user": {
                        "_id": "6768c97367e4b4606a3c9cec",
                        "avatarUrl": "/avatars/5ddafe7a05828366f66c79072556f370.svg",
                        "isPro": false,
                        "fullname": "diaomuxi",
                        "user": "diaomuxi",
                        "type": "user"
                    },
                    "name": "Muxi Diao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-08T08:31:56.507Z",
                    "hidden": false
                },
                {
                    "_id": "695f2d8a5fa3847525c41f8e",
                    "user": {
                        "_id": "666a6cf89a3e3ce05a519bcc",
                        "avatarUrl": "/avatars/9e72481deec3bd5c5202e42c32894a32.svg",
                        "isPro": false,
                        "fullname": "杨乐乐",
                        "user": "ssl-asuka",
                        "type": "user"
                    },
                    "name": "Lele Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-08T08:31:51.246Z",
                    "hidden": false
                },
                {
                    "_id": "695f2d8a5fa3847525c41f8f",
                    "user": {
                        "_id": "64c0f972d76592ba899c2c9c",
                        "avatarUrl": "/avatars/d6940beb135f99241c6fb2cf0e8ccdbe.svg",
                        "isPro": false,
                        "fullname": "GongWuxuan",
                        "user": "Wuxuan-Gong",
                        "type": "user"
                    },
                    "name": "Wuxuan Gong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-08T08:44:19.470Z",
                    "hidden": false
                },
                {
                    "_id": "695f2d8a5fa3847525c41f90",
                    "name": "Yutong Zhang",
                    "hidden": false
                },
                {
                    "_id": "695f2d8a5fa3847525c41f91",
                    "user": {
                        "_id": "64fbd4e69a62bb2791b3a665",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64fbd4e69a62bb2791b3a665/ZEMtU8O0z98ryeRCG3l_K.jpeg",
                        "isPro": false,
                        "fullname": "Zhonghao Yan",
                        "user": "zzzyzh",
                        "type": "user"
                    },
                    "name": "Zhonghao Yan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-08T08:31:53.904Z",
                    "hidden": false
                },
                {
                    "_id": "695f2d8a5fa3847525c41f92",
                    "name": "Yufei Han",
                    "hidden": false
                },
                {
                    "_id": "695f2d8a5fa3847525c41f93",
                    "user": {
                        "_id": "67f4a56928cbc4f2f75c008d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/qoX8HT0JsjqW2OQoENSvg.png",
                        "isPro": false,
                        "fullname": "Kongming Liang",
                        "user": "KongmingLiang",
                        "type": "user"
                    },
                    "name": "Kongming Liang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2026-01-08T08:44:38.812Z",
                    "hidden": false
                },
                {
                    "_id": "695f2d8a5fa3847525c41f94",
                    "name": "Weiran Xu",
                    "hidden": false
                },
                {
                    "_id": "695f2d8a5fa3847525c41f95",
                    "name": "Zhanyu Ma",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-05T14:28:17.000Z",
            "submittedOnDailyAt": "2026-01-08T01:42:04.476Z",
            "title": "Entropy-Adaptive Fine-Tuning: Resolving Confident Conflicts to Mitigate Forgetting",
            "submittedOnDailyBy": {
                "_id": "666a6cf89a3e3ce05a519bcc",
                "avatarUrl": "/avatars/9e72481deec3bd5c5202e42c32894a32.svg",
                "isPro": false,
                "fullname": "杨乐乐",
                "user": "ssl-asuka",
                "type": "user"
            },
            "summary": "Supervised Fine-Tuning (SFT) is the standard paradigm for domain adaptation, yet it frequently incurs the cost of catastrophic forgetting. In sharp contrast, on-policy Reinforcement Learning (RL) effectively preserves general capabilities. We investigate this discrepancy and identify a fundamental distributional gap: while RL aligns with the model's internal belief, SFT forces the model to fit external supervision. This mismatch often manifests as \"Confident Conflicts\" tokens characterized by low probability but low entropy. In these instances, the model is highly confident in its own prediction but is forced to learn a divergent ground truth, triggering destructive gradient updates. To address this, we propose Entropy-Adaptive Fine-Tuning (EAFT). Unlike methods relying solely on prediction probability, EAFT utilizes token-level entropy as a gating mechanism to distinguish between epistemic uncertainty and knowledge conflict. This allows the model to learn from uncertain samples while suppressing gradients on conflicting data. Extensive experiments on Qwen and GLM series (ranging from 4B to 32B parameters) across mathematical, medical, and agentic domains confirm our hypothesis. EAFT consistently matches the downstream performance of standard SFT while significantly mitigating the degradation of general capabilities.",
            "upvotes": 65,
            "discussionId": "695f2d8b5fa3847525c41f96",
            "projectPage": "https://ymxyll.github.io/EAFT/",
            "githubRepo": "https://github.com/PRIS-CV/EAFT",
            "githubRepoAddedBy": "user",
            "ai_summary": "Entropy-Adaptive Fine-Tuning addresses catastrophic forgetting in supervised fine-tuning by using token-level entropy to distinguish uncertainty from knowledge conflict, enabling better preservation of general capabilities.",
            "ai_keywords": [
                "supervised fine-tuning",
                "catastrophic forgetting",
                "on-policy reinforcement learning",
                "distributional gap",
                "Confident Conflicts",
                "token-level entropy",
                "epistemic uncertainty",
                "knowledge conflict",
                "gradient updates",
                "downstream performance"
            ],
            "githubStars": 18,
            "organization": {
                "_id": "64283c0c68faf6ddab552684",
                "name": "BUPT-PRIS",
                "fullname": "BUPT AI PRIS"
            }
        },
        "publishedAt": "2026-01-05T09:28:17.000Z",
        "title": "Entropy-Adaptive Fine-Tuning: Resolving Confident Conflicts to Mitigate Forgetting",
        "summary": "Supervised Fine-Tuning (SFT) is the standard paradigm for domain adaptation, yet it frequently incurs the cost of catastrophic forgetting. In sharp contrast, on-policy Reinforcement Learning (RL) effectively preserves general capabilities. We investigate this discrepancy and identify a fundamental distributional gap: while RL aligns with the model's internal belief, SFT forces the model to fit external supervision. This mismatch often manifests as \"Confident Conflicts\" tokens characterized by low probability but low entropy. In these instances, the model is highly confident in its own prediction but is forced to learn a divergent ground truth, triggering destructive gradient updates. To address this, we propose Entropy-Adaptive Fine-Tuning (EAFT). Unlike methods relying solely on prediction probability, EAFT utilizes token-level entropy as a gating mechanism to distinguish between epistemic uncertainty and knowledge conflict. This allows the model to learn from uncertain samples while suppressing gradients on conflicting data. Extensive experiments on Qwen and GLM series (ranging from 4B to 32B parameters) across mathematical, medical, and agentic domains confirm our hypothesis. EAFT consistently matches the downstream performance of standard SFT while significantly mitigating the degradation of general capabilities.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02151.png",
        "numComments": 7,
        "submittedBy": {
            "_id": "666a6cf89a3e3ce05a519bcc",
            "avatarUrl": "/avatars/9e72481deec3bd5c5202e42c32894a32.svg",
            "fullname": "杨乐乐",
            "name": "ssl-asuka",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "isUserFollowing": false
        },
        "organization": {
            "_id": "64283c0c68faf6ddab552684",
            "name": "BUPT-PRIS",
            "fullname": "BUPT AI PRIS"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2601.03509",
            "authors": [
                {
                    "_id": "695fbc7d5b7998385e639349",
                    "name": "Haochen Shi",
                    "hidden": false
                },
                {
                    "_id": "695fbc7d5b7998385e63934a",
                    "name": "Xingdi Yuan",
                    "hidden": false
                },
                {
                    "_id": "695fbc7d5b7998385e63934b",
                    "name": "Bang Liu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/654a97282d2fcd6bf2851173/sAKIzhLgfcEhgVZMBkHRW.png"
            ],
            "publishedAt": "2026-01-07T01:43:25.000Z",
            "submittedOnDailyAt": "2026-01-08T11:50:05.687Z",
            "title": "Evolving Programmatic Skill Networks",
            "submittedOnDailyBy": {
                "_id": "654a97282d2fcd6bf2851173",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654a97282d2fcd6bf2851173/9zXf940gr4WNt4e-oOt4k.png",
                "isPro": false,
                "fullname": "Bang Liu",
                "user": "Bang-UdeM-Mila",
                "type": "user"
            },
            "summary": "We study continual skill acquisition in open-ended embodied environments where an agent must construct, refine, and reuse an expanding library of executable skills. We introduce the Programmatic Skill Network (PSN), a framework in which skills are executable symbolic programs forming a compositional network that evolves through experience. PSN defines three core mechanisms instantiated via large language models: (1)REFLECT for structured fault localization over skill compositions, (2) progressive optimization with maturity-aware update gating that stabilizes reliable skills while maintaining plasticity for uncertain ones, and (3) canonical structural refactoring under rollback validation that maintains network compactness. We further show that PSN's learning dynamics exhibit structural parallels to neural network training. Experiments on MineDojo and Crafter demonstrate robust skill reuse, rapid adaptation, and strong generalization across open-ended task distributions.\\footnote{We plan to open-source the code.",
            "upvotes": 54,
            "discussionId": "695fbc7e5b7998385e63934c",
            "ai_summary": "Programmatic Skill Network enables continual skill acquisition through executable symbolic programs that evolve via reflection, progressive optimization, and structural refactoring mechanisms.",
            "ai_keywords": [
                "Programmatic Skill Network",
                "executable symbolic programs",
                "skill composition",
                "structured fault localization",
                "progressive optimization",
                "maturity-aware update gating",
                "canonical structural refactoring",
                "rollback validation",
                "neural network training",
                "skill reuse",
                "rapid adaptation",
                "generalization"
            ],
            "organization": {
                "_id": "636e93488ba65db4a0987ab4",
                "name": "Universite-de-Montreal",
                "fullname": "Université de Montréal"
            }
        },
        "publishedAt": "2026-01-06T20:43:25.000Z",
        "title": "Evolving Programmatic Skill Networks",
        "summary": "We study continual skill acquisition in open-ended embodied environments where an agent must construct, refine, and reuse an expanding library of executable skills. We introduce the Programmatic Skill Network (PSN), a framework in which skills are executable symbolic programs forming a compositional network that evolves through experience. PSN defines three core mechanisms instantiated via large language models: (1)REFLECT for structured fault localization over skill compositions, (2) progressive optimization with maturity-aware update gating that stabilizes reliable skills while maintaining plasticity for uncertain ones, and (3) canonical structural refactoring under rollback validation that maintains network compactness. We further show that PSN's learning dynamics exhibit structural parallels to neural network training. Experiments on MineDojo and Crafter demonstrate robust skill reuse, rapid adaptation, and strong generalization across open-ended task distributions.\\footnote{We plan to open-source the code.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/654a97282d2fcd6bf2851173/sAKIzhLgfcEhgVZMBkHRW.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03509.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "654a97282d2fcd6bf2851173",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654a97282d2fcd6bf2851173/9zXf940gr4WNt4e-oOt4k.png",
            "fullname": "Bang Liu",
            "name": "Bang-UdeM-Mila",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 10,
            "isUserFollowing": false
        },
        "organization": {
            "_id": "636e93488ba65db4a0987ab4",
            "name": "Universite-de-Montreal",
            "fullname": "Université de Montréal"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2601.03872",
            "authors": [
                {
                    "_id": "695f1a475fa3847525c41d06",
                    "user": {
                        "_id": "6747de57f8cab58c22ec94a2",
                        "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg",
                        "isPro": false,
                        "fullname": "Jinyang Wu",
                        "user": "Jinyang23",
                        "type": "user"
                    },
                    "name": "Jinyang Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-08T08:32:36.055Z",
                    "hidden": false
                },
                {
                    "_id": "695f1a475fa3847525c41d07",
                    "name": "Guocheng Zhai",
                    "hidden": false
                },
                {
                    "_id": "695f1a475fa3847525c41d08",
                    "name": "Ruihan Jin",
                    "hidden": false
                },
                {
                    "_id": "695f1a475fa3847525c41d09",
                    "name": "Jiahao Yuan",
                    "hidden": false
                },
                {
                    "_id": "695f1a475fa3847525c41d0a",
                    "name": "Yuhao Shen",
                    "hidden": false
                },
                {
                    "_id": "695f1a475fa3847525c41d0b",
                    "name": "Shuai Zhang",
                    "hidden": false
                },
                {
                    "_id": "695f1a475fa3847525c41d0c",
                    "name": "Zhengqi Wen",
                    "hidden": false
                },
                {
                    "_id": "695f1a475fa3847525c41d0d",
                    "name": "Jianhua Tao",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-07T12:38:33.000Z",
            "submittedOnDailyAt": "2026-01-08T04:50:03.287Z",
            "title": "Atlas: Orchestrating Heterogeneous Models and Tools for Multi-Domain Complex Reasoning",
            "submittedOnDailyBy": {
                "_id": "6747de57f8cab58c22ec94a2",
                "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg",
                "isPro": false,
                "fullname": "Jinyang Wu",
                "user": "Jinyang23",
                "type": "user"
            },
            "summary": "The integration of large language models (LLMs) with external tools has significantly expanded the capabilities of AI agents. However, as the diversity of both LLMs and tools increases, selecting the optimal model-tool combination becomes a high-dimensional optimization challenge. Existing approaches often rely on a single model or fixed tool-calling logic, failing to exploit the performance variations across heterogeneous model-tool pairs. In this paper, we present ATLAS (Adaptive Tool-LLM Alignment and Synergistic Invocation), a dual-path framework for dynamic tool usage in cross-domain complex reasoning. ATLAS operates via a dual-path approach: (1) training-free cluster-based routing that exploits empirical priors for domain-specific alignment, and (2) RL-based multi-step routing that explores autonomous trajectories for out-of-distribution generalization. Extensive experiments across 15 benchmarks demonstrate that our method outperforms closed-source models like GPT-4o, surpassing existing routing methods on both in-distribution (+10.1%) and out-of-distribution (+13.1%) tasks. Furthermore, our framework shows significant gains in visual reasoning by orchestrating specialized multi-modal tools.",
            "upvotes": 31,
            "discussionId": "695f1a475fa3847525c41d0e",
            "ai_summary": "ATLAS is a dual-path framework that dynamically selects optimal model-tool combinations for cross-domain reasoning through cluster-based routing and reinforcement learning-based multi-step routing, achieving superior performance on complex reasoning tasks.",
            "ai_keywords": [
                "large language models",
                "external tools",
                "model-tool combination",
                "high-dimensional optimization",
                "dual-path framework",
                "training-free cluster-based routing",
                "RL-based multi-step routing",
                "cross-domain complex reasoning",
                "domain-specific alignment",
                "out-of-distribution generalization"
            ]
        },
        "publishedAt": "2026-01-07T07:38:33.000Z",
        "title": "Atlas: Orchestrating Heterogeneous Models and Tools for Multi-Domain Complex Reasoning",
        "summary": "The integration of large language models (LLMs) with external tools has significantly expanded the capabilities of AI agents. However, as the diversity of both LLMs and tools increases, selecting the optimal model-tool combination becomes a high-dimensional optimization challenge. Existing approaches often rely on a single model or fixed tool-calling logic, failing to exploit the performance variations across heterogeneous model-tool pairs. In this paper, we present ATLAS (Adaptive Tool-LLM Alignment and Synergistic Invocation), a dual-path framework for dynamic tool usage in cross-domain complex reasoning. ATLAS operates via a dual-path approach: (1) training-free cluster-based routing that exploits empirical priors for domain-specific alignment, and (2) RL-based multi-step routing that explores autonomous trajectories for out-of-distribution generalization. Extensive experiments across 15 benchmarks demonstrate that our method outperforms closed-source models like GPT-4o, surpassing existing routing methods on both in-distribution (+10.1%) and out-of-distribution (+13.1%) tasks. Furthermore, our framework shows significant gains in visual reasoning by orchestrating specialized multi-modal tools.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03872.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6747de57f8cab58c22ec94a2",
            "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg",
            "fullname": "Jinyang Wu",
            "name": "Jinyang23",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8,
            "isUserFollowing": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2601.03986",
            "authors": [
                {
                    "_id": "695f290d5fa3847525c41d7d",
                    "name": "Qi Qian",
                    "hidden": false
                },
                {
                    "_id": "695f290d5fa3847525c41d7e",
                    "user": {
                        "_id": "62ea79dd01ed9b0e8f61ccd3",
                        "avatarUrl": "/avatars/70af83e0e267be39fcd5f23b85e2dafa.svg",
                        "isPro": false,
                        "fullname": "Chengsong Huang",
                        "user": "ChengsongHuang",
                        "type": "user"
                    },
                    "name": "Chengsong Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-08T08:31:58.524Z",
                    "hidden": false
                },
                {
                    "_id": "695f290d5fa3847525c41d7f",
                    "name": "Jingwen Xu",
                    "hidden": false
                },
                {
                    "_id": "695f290d5fa3847525c41d80",
                    "name": "Changze Lv",
                    "hidden": false
                },
                {
                    "_id": "695f290d5fa3847525c41d81",
                    "name": "Muling Wu",
                    "hidden": false
                },
                {
                    "_id": "695f290d5fa3847525c41d82",
                    "name": "Wenhao Liu",
                    "hidden": false
                },
                {
                    "_id": "695f290d5fa3847525c41d83",
                    "name": "Xiaohua Wang",
                    "hidden": false
                },
                {
                    "_id": "695f290d5fa3847525c41d84",
                    "name": "Zhenghua Wang",
                    "hidden": false
                },
                {
                    "_id": "695f290d5fa3847525c41d85",
                    "name": "Zisu Huang",
                    "hidden": false
                },
                {
                    "_id": "695f290d5fa3847525c41d86",
                    "name": "Muzhao Tian",
                    "hidden": false
                },
                {
                    "_id": "695f290d5fa3847525c41d87",
                    "name": "Jianhan Xu",
                    "hidden": false
                },
                {
                    "_id": "695f290d5fa3847525c41d88",
                    "name": "Kun Hu",
                    "hidden": false
                },
                {
                    "_id": "695f290d5fa3847525c41d89",
                    "name": "He-Da Wang",
                    "hidden": false
                },
                {
                    "_id": "695f290d5fa3847525c41d8a",
                    "name": "Yao Hu",
                    "hidden": false
                },
                {
                    "_id": "695f290d5fa3847525c41d8b",
                    "name": "Xuanjing Huang",
                    "hidden": false
                },
                {
                    "_id": "695f290d5fa3847525c41d8c",
                    "name": "Xiaoqing Zheng",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-07T14:59:03.000Z",
            "submittedOnDailyAt": "2026-01-08T01:59:31.547Z",
            "title": "Benchmark^2: Systematic Evaluation of LLM Benchmarks",
            "submittedOnDailyBy": {
                "_id": "62ea79dd01ed9b0e8f61ccd3",
                "avatarUrl": "/avatars/70af83e0e267be39fcd5f23b85e2dafa.svg",
                "isPro": false,
                "fullname": "Chengsong Huang",
                "user": "ChengsongHuang",
                "type": "user"
            },
            "summary": "The rapid proliferation of benchmarks for evaluating large language models (LLMs) has created an urgent need for systematic methods to assess benchmark quality itself. We propose Benchmark^2, a comprehensive framework comprising three complementary metrics: (1) Cross-Benchmark Ranking Consistency, measuring whether a benchmark produces model rankings aligned with peer benchmarks; (2) Discriminability Score, quantifying a benchmark's ability to differentiate between models; and (3) Capability Alignment Deviation, identifying problematic instances where stronger models fail but weaker models succeed within the same model family. We conduct extensive experiments across 15 benchmarks spanning mathematics, reasoning, and knowledge domains, evaluating 11 LLMs across four model families. Our analysis reveals significant quality variations among existing benchmarks and demonstrates that selective benchmark construction based on our metrics can achieve comparable evaluation performance with substantially reduced test sets.",
            "upvotes": 28,
            "discussionId": "695f290d5fa3847525c41d8d",
            "ai_summary": "Researchers developed Benchmark^2, a framework with three metrics to evaluate benchmark quality for large language models, revealing significant variations in existing benchmarks and enabling more efficient evaluation through selective benchmark construction.",
            "ai_keywords": [
                "Benchmark^2",
                "cross-benchmark ranking consistency",
                "discriminability score",
                "capability alignment deviation",
                "large language models",
                "benchmarks",
                "model rankings",
                "evaluation performance",
                "test sets"
            ]
        },
        "publishedAt": "2026-01-07T09:59:03.000Z",
        "title": "Benchmark^2: Systematic Evaluation of LLM Benchmarks",
        "summary": "The rapid proliferation of benchmarks for evaluating large language models (LLMs) has created an urgent need for systematic methods to assess benchmark quality itself. We propose Benchmark^2, a comprehensive framework comprising three complementary metrics: (1) Cross-Benchmark Ranking Consistency, measuring whether a benchmark produces model rankings aligned with peer benchmarks; (2) Discriminability Score, quantifying a benchmark's ability to differentiate between models; and (3) Capability Alignment Deviation, identifying problematic instances where stronger models fail but weaker models succeed within the same model family. We conduct extensive experiments across 15 benchmarks spanning mathematics, reasoning, and knowledge domains, evaluating 11 LLMs across four model families. Our analysis reveals significant quality variations among existing benchmarks and demonstrates that selective benchmark construction based on our metrics can achieve comparable evaluation performance with substantially reduced test sets.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03986.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "62ea79dd01ed9b0e8f61ccd3",
            "avatarUrl": "/avatars/70af83e0e267be39fcd5f23b85e2dafa.svg",
            "fullname": "Chengsong Huang",
            "name": "ChengsongHuang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9,
            "isUserFollowing": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2601.03822",
            "authors": [
                {
                    "_id": "695f21245fa3847525c41d2c",
                    "user": {
                        "_id": "673c429f4299bda07a4e2395",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673c429f4299bda07a4e2395/7Ykidq_xoH-CX9fBj2BM2.jpeg",
                        "isPro": false,
                        "fullname": "Muyang Zhao",
                        "user": "wangwang318",
                        "type": "user"
                    },
                    "name": "Muyang Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-08T08:32:09.136Z",
                    "hidden": false
                },
                {
                    "_id": "695f21245fa3847525c41d2d",
                    "name": "Qi Qi",
                    "hidden": false
                },
                {
                    "_id": "695f21245fa3847525c41d2e",
                    "name": "Hao Sun",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-07T11:30:55.000Z",
            "submittedOnDailyAt": "2026-01-08T22:20:29.787Z",
            "title": "ROI-Reasoning: Rational Optimization for Inference via Pre-Computation Meta-Cognition",
            "submittedOnDailyBy": {
                "_id": "643f6c06b410b176e9a1bb76",
                "avatarUrl": "/avatars/3827c219a557e0b0ff5f51b04f28b0b4.svg",
                "isPro": false,
                "fullname": "HanbingLiu",
                "user": "leolhb",
                "type": "user"
            },
            "summary": "Large language models (LLMs) can achieve strong reasoning performance with sufficient computation, but they do not inherently know how much computation a task requires. We study budgeted inference-time reasoning for multiple tasks under a strict global token constraint and formalize it as a Ordered Stochastic Multiple-Choice Knapsack Problem(OS-MCKP). This perspective highlights a meta-cognitive requirement -- anticipating task difficulty, estimating return over investment (ROI), and allocating computation strategically. We propose ROI-Reasoning, a two-stage framework that endows LLMs with intrinsic, budget-aware rationality. In the first stage, Meta-Cognitive Fine-Tuning teaches models to predict reasoning cost and expected utility before generation, enabling explicit solve-or-skip decisions. Next, Rationality-Aware Reinforcement Learning optimizes sequential decision making under a hard token budget, allowing models to learn long-horizon allocation strategies. Across budgeted mathematical reasoning benchmarks, ROI-Reasoning consistently improves overall score while substantially reducing regret under tight computation budgets.",
            "upvotes": 20,
            "discussionId": "695f21245fa3847525c41d2f",
            "ai_summary": "Budgeted inference-time reasoning framework enables large language models to make strategic computational decisions by predicting costs and utilities before generation, then optimizing sequential allocation under strict token constraints through meta-cognitive fine-tuning and reinforcement learning.",
            "ai_keywords": [
                "large language models",
                "budgeted inference-time reasoning",
                "Ordered Stochastic Multiple-Choice Knapsack Problem",
                "meta-cognitive fine-tuning",
                "rationality-aware reinforcement learning",
                "reasoning cost",
                "expected utility",
                "solve-or-skip decisions",
                "sequential decision making",
                "token budget"
            ],
            "organization": {
                "_id": "622177ac43826d6f261f8208",
                "name": "RUC",
                "fullname": "Renmin University of China",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/670IAX9A2-BflqA5MiSBW.jpeg"
            }
        },
        "publishedAt": "2026-01-07T06:30:55.000Z",
        "title": "ROI-Reasoning: Rational Optimization for Inference via Pre-Computation Meta-Cognition",
        "summary": "Large language models (LLMs) can achieve strong reasoning performance with sufficient computation, but they do not inherently know how much computation a task requires. We study budgeted inference-time reasoning for multiple tasks under a strict global token constraint and formalize it as a Ordered Stochastic Multiple-Choice Knapsack Problem(OS-MCKP). This perspective highlights a meta-cognitive requirement -- anticipating task difficulty, estimating return over investment (ROI), and allocating computation strategically. We propose ROI-Reasoning, a two-stage framework that endows LLMs with intrinsic, budget-aware rationality. In the first stage, Meta-Cognitive Fine-Tuning teaches models to predict reasoning cost and expected utility before generation, enabling explicit solve-or-skip decisions. Next, Rationality-Aware Reinforcement Learning optimizes sequential decision making under a hard token budget, allowing models to learn long-horizon allocation strategies. Across budgeted mathematical reasoning benchmarks, ROI-Reasoning consistently improves overall score while substantially reducing regret under tight computation budgets.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03822.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "643f6c06b410b176e9a1bb76",
            "avatarUrl": "/avatars/3827c219a557e0b0ff5f51b04f28b0b4.svg",
            "fullname": "HanbingLiu",
            "name": "leolhb",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "isUserFollowing": false
        },
        "organization": {
            "_id": "622177ac43826d6f261f8208",
            "name": "RUC",
            "fullname": "Renmin University of China",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/670IAX9A2-BflqA5MiSBW.jpeg"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2601.04151",
            "authors": [
                {
                    "_id": "695f21355fa3847525c41d37",
                    "name": "Jun Wang",
                    "hidden": false
                },
                {
                    "_id": "695f21355fa3847525c41d38",
                    "name": "Chunyu Qiang",
                    "hidden": false
                },
                {
                    "_id": "695f21355fa3847525c41d39",
                    "name": "Yuxin Guo",
                    "hidden": false
                },
                {
                    "_id": "695f21355fa3847525c41d3a",
                    "name": "Yiran Wang",
                    "hidden": false
                },
                {
                    "_id": "695f21355fa3847525c41d3b",
                    "name": "Xijuan Zeng",
                    "hidden": false
                },
                {
                    "_id": "695f21355fa3847525c41d3c",
                    "name": "Chen Zhang",
                    "hidden": false
                },
                {
                    "_id": "695f21355fa3847525c41d3d",
                    "name": "Pengfei Wan",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-07T18:03:45.000Z",
            "submittedOnDailyAt": "2026-01-08T01:07:20.425Z",
            "title": "Klear: Unified Multi-Task Audio-Video Joint Generation",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Audio-video joint generation has progressed rapidly, yet substantial challenges still remain. Non-commercial approaches still suffer audio-visual asynchrony, poor lip-speech alignment, and unimodal degradation, which can be stemmed from weak audio-visual correspondence modeling, limited generalization, and scarce high-quality dense-caption data. To address these issues, we introduce Klear and delve into three axes--model architecture, training strategy, and data curation. Architecturally, we adopt a single-tower design with unified DiT blocks and an Omni-Full Attention mechanism, achieving tight audio-visual alignment and strong scalability. Training-wise, we adopt a progressive multitask regime--random modality masking to joint optimization across tasks, and a multistage curriculum, yielding robust representations, strengthening A-V aligned world knowledge, and preventing unimodal collapse. For datasets, we present the first large-scale audio-video dataset with dense captions, and introduce a novel automated data-construction pipeline which annotates and filters millions of diverse, high-quality, strictly aligned audio-video-caption triplets. Building on this, Klear scales to large datasets, delivering high-fidelity, semantically and temporally aligned, instruction-following generation in both joint and unimodal settings while generalizing robustly to out-of-distribution scenarios. Across tasks, it substantially outperforms prior methods by a large margin and achieves performance comparable to Veo 3, offering a unified, scalable path toward next-generation audio-video synthesis.",
            "upvotes": 10,
            "discussionId": "695f21365fa3847525c41d3e",
            "ai_summary": "Klear addresses audio-video joint generation challenges through a unified model architecture, progressive multitask training, and large-scale dense-caption data construction, achieving superior alignment and generalization.",
            "ai_keywords": [
                "audio-visual correspondence modeling",
                "DiT blocks",
                "Omni-Full Attention mechanism",
                "random modality masking",
                "multistage curriculum",
                "unimodal collapse",
                "dense-caption data",
                "automated data-construction pipeline",
                "audio-video joint generation",
                "instruction-following generation"
            ],
            "organization": {
                "_id": "662c559b322afcbae51b3c8b",
                "name": "KlingTeam",
                "fullname": "Kling Team",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"
            }
        },
        "publishedAt": "2026-01-07T13:03:45.000Z",
        "title": "Klear: Unified Multi-Task Audio-Video Joint Generation",
        "summary": "Audio-video joint generation has progressed rapidly, yet substantial challenges still remain. Non-commercial approaches still suffer audio-visual asynchrony, poor lip-speech alignment, and unimodal degradation, which can be stemmed from weak audio-visual correspondence modeling, limited generalization, and scarce high-quality dense-caption data. To address these issues, we introduce Klear and delve into three axes--model architecture, training strategy, and data curation. Architecturally, we adopt a single-tower design with unified DiT blocks and an Omni-Full Attention mechanism, achieving tight audio-visual alignment and strong scalability. Training-wise, we adopt a progressive multitask regime--random modality masking to joint optimization across tasks, and a multistage curriculum, yielding robust representations, strengthening A-V aligned world knowledge, and preventing unimodal collapse. For datasets, we present the first large-scale audio-video dataset with dense captions, and introduce a novel automated data-construction pipeline which annotates and filters millions of diverse, high-quality, strictly aligned audio-video-caption triplets. Building on this, Klear scales to large datasets, delivering high-fidelity, semantically and temporally aligned, instruction-following generation in both joint and unimodal settings while generalizing robustly to out-of-distribution scenarios. Across tasks, it substantially outperforms prior methods by a large margin and achieves performance comparable to Veo 3, offering a unified, scalable path toward next-generation audio-video synthesis.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04151.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 204,
            "isUserFollowing": false
        },
        "organization": {
            "_id": "662c559b322afcbae51b3c8b",
            "name": "KlingTeam",
            "fullname": "Kling Team",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2601.04194",
            "authors": [
                {
                    "_id": "695f1a2a5fa3847525c41cf3",
                    "name": "Yanzhe Lyu",
                    "hidden": false
                },
                {
                    "_id": "695f1a2a5fa3847525c41cf4",
                    "name": "Chen Geng",
                    "hidden": false
                },
                {
                    "_id": "695f1a2a5fa3847525c41cf5",
                    "name": "Karthik Dharmarajan",
                    "hidden": false
                },
                {
                    "_id": "695f1a2a5fa3847525c41cf6",
                    "name": "Yunzhi Zhang",
                    "hidden": false
                },
                {
                    "_id": "695f1a2a5fa3847525c41cf7",
                    "name": "Hadi Alzayer",
                    "hidden": false
                },
                {
                    "_id": "695f1a2a5fa3847525c41cf8",
                    "name": "Shangzhe Wu",
                    "hidden": false
                },
                {
                    "_id": "695f1a2a5fa3847525c41cf9",
                    "name": "Jiajun Wu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/e4m-6HYTsPup5SkMsApV4.mp4"
            ],
            "publishedAt": "2026-01-07T18:59:40.000Z",
            "submittedOnDailyAt": "2026-01-08T00:15:39.806Z",
            "title": "Choreographing a World of Dynamic Objects",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Dynamic objects in our physical 4D (3D + time) world are constantly evolving, deforming, and interacting with other objects, leading to diverse 4D scene dynamics. In this paper, we present a universal generative pipeline, CHORD, for CHOReographing Dynamic objects and scenes and synthesizing this type of phenomena. Traditional rule-based graphics pipelines to create these dynamics are based on category-specific heuristics, yet are labor-intensive and not scalable. Recent learning-based methods typically demand large-scale datasets, which may not cover all object categories in interest. Our approach instead inherits the universality from the video generative models by proposing a distillation-based pipeline to extract the rich Lagrangian motion information hidden in the Eulerian representations of 2D videos. Our method is universal, versatile, and category-agnostic. We demonstrate its effectiveness by conducting experiments to generate a diverse range of multi-body 4D dynamics, show its advantage compared to existing methods, and demonstrate its applicability in generating robotics manipulation policies. Project page: https://yanzhelyu.github.io/chord",
            "upvotes": 7,
            "discussionId": "695f1a2a5fa3847525c41cfa",
            "projectPage": "https://yanzhelyu.github.io/chord/",
            "ai_summary": "CHORD is a universal generative framework that extracts Lagrangian motion information from Eulerian video representations to synthesize diverse 4D dynamic scenes without requiring category-specific rules or large datasets.",
            "ai_keywords": [
                "video generative models",
                "Lagrangian motion",
                "Eulerian representations",
                "distillation-based pipeline",
                "4D dynamic scenes",
                "multi-body dynamics",
                "robotics manipulation policies"
            ]
        },
        "publishedAt": "2026-01-07T13:59:40.000Z",
        "title": "Choreographing a World of Dynamic Objects",
        "summary": "Dynamic objects in our physical 4D (3D + time) world are constantly evolving, deforming, and interacting with other objects, leading to diverse 4D scene dynamics. In this paper, we present a universal generative pipeline, CHORD, for CHOReographing Dynamic objects and scenes and synthesizing this type of phenomena. Traditional rule-based graphics pipelines to create these dynamics are based on category-specific heuristics, yet are labor-intensive and not scalable. Recent learning-based methods typically demand large-scale datasets, which may not cover all object categories in interest. Our approach instead inherits the universality from the video generative models by proposing a distillation-based pipeline to extract the rich Lagrangian motion information hidden in the Eulerian representations of 2D videos. Our method is universal, versatile, and category-agnostic. We demonstrate its effectiveness by conducting experiments to generate a diverse range of multi-body 4D dynamics, show its advantage compared to existing methods, and demonstrate its applicability in generating robotics manipulation policies. Project page: https://yanzhelyu.github.io/chord",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/e4m-6HYTsPup5SkMsApV4.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04194.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 204,
            "isUserFollowing": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2601.04171",
            "authors": [
                {
                    "_id": "695f21255fa3847525c41d31",
                    "name": "Mohit Raghavendra",
                    "hidden": false
                },
                {
                    "_id": "695f21255fa3847525c41d32",
                    "name": "Anisha Gunjal",
                    "hidden": false
                },
                {
                    "_id": "695f21255fa3847525c41d33",
                    "name": "Bing Liu",
                    "hidden": false
                },
                {
                    "_id": "695f21255fa3847525c41d34",
                    "name": "Yunzhong He",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-07T18:38:23.000Z",
            "submittedOnDailyAt": "2026-01-08T01:09:10.323Z",
            "title": "Agentic Rubrics as Contextual Verifiers for SWE Agents",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Verification is critical for improving agents: it provides the reward signal for Reinforcement Learning and enables inference-time gains through Test-Time Scaling (TTS). Despite its importance, verification in software engineering (SWE) agent settings often relies on code execution, which can be difficult to scale due to environment setup overhead. Scalable alternatives such as patch classifiers and heuristic methods exist, but they are less grounded in codebase context and harder to interpret. To this end, we explore Agentic Rubrics: an expert agent interacts with the repository to create a context-grounded rubric checklist, and candidate patches are then scored against it without requiring test execution. On SWE-Bench Verified under parallel TTS evaluation, Agentic Rubrics achieve a score of 54.2% on Qwen3-Coder-30B-A3B and 40.6% on Qwen3-32B, with at least a +3.5 percentage-point gain over the strongest baseline in our comparison set. We further analyze rubric behavior, showing that rubric scores are consistent with ground-truth tests while also flagging issues that tests do not capture. Our ablations show that agentic context gathering is essential for producing codebase-specific, unambiguous criteria. Together, these results suggest that Agentic Rubrics provide an efficient, scalable, and granular verification signal for SWE agents.",
            "upvotes": 6,
            "discussionId": "695f21255fa3847525c41d35",
            "ai_summary": "Agentic Rubrics enable efficient and scalable verification for software engineering agents by creating context-aware checklists that outperform traditional methods while maintaining interpretability.",
            "ai_keywords": [
                "Reinforcement Learning",
                "Test-Time Scaling",
                "software engineering agents",
                "patch classifiers",
                "heuristic methods",
                "agentic context gathering",
                "codebase-specific criteria"
            ],
            "organization": {
                "_id": "6677220f8a4064c02bc81217",
                "name": "ScaleAI",
                "fullname": "Scale AI",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65d6a5f94c28026a003581b4/uqHyTuNQ8fX7LheVhzPeO.png"
            }
        },
        "publishedAt": "2026-01-07T13:38:23.000Z",
        "title": "Agentic Rubrics as Contextual Verifiers for SWE Agents",
        "summary": "Verification is critical for improving agents: it provides the reward signal for Reinforcement Learning and enables inference-time gains through Test-Time Scaling (TTS). Despite its importance, verification in software engineering (SWE) agent settings often relies on code execution, which can be difficult to scale due to environment setup overhead. Scalable alternatives such as patch classifiers and heuristic methods exist, but they are less grounded in codebase context and harder to interpret. To this end, we explore Agentic Rubrics: an expert agent interacts with the repository to create a context-grounded rubric checklist, and candidate patches are then scored against it without requiring test execution. On SWE-Bench Verified under parallel TTS evaluation, Agentic Rubrics achieve a score of 54.2% on Qwen3-Coder-30B-A3B and 40.6% on Qwen3-32B, with at least a +3.5 percentage-point gain over the strongest baseline in our comparison set. We further analyze rubric behavior, showing that rubric scores are consistent with ground-truth tests while also flagging issues that tests do not capture. Our ablations show that agentic context gathering is essential for producing codebase-specific, unambiguous criteria. Together, these results suggest that Agentic Rubrics provide an efficient, scalable, and granular verification signal for SWE agents.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04171.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 204,
            "isUserFollowing": false
        },
        "organization": {
            "_id": "6677220f8a4064c02bc81217",
            "name": "ScaleAI",
            "fullname": "Scale AI",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65d6a5f94c28026a003581b4/uqHyTuNQ8fX7LheVhzPeO.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2601.02075",
            "authors": [
                {
                    "_id": "695d04eac03d6d81e4399cd9",
                    "user": {
                        "_id": "65d7018c90f11951bcfdf2f5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d7018c90f11951bcfdf2f5/RU0-n9k3RIdtv8xvg9Upe.jpeg",
                        "isPro": false,
                        "fullname": "Zhuofan Shi",
                        "user": "FredericFan",
                        "type": "user"
                    },
                    "name": "Zhuofan Shi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-07T09:26:56.695Z",
                    "hidden": false
                },
                {
                    "_id": "695d04eac03d6d81e4399cda",
                    "name": "Hubao A",
                    "hidden": false
                },
                {
                    "_id": "695d04eac03d6d81e4399cdb",
                    "name": "Yufei Shao",
                    "hidden": false
                },
                {
                    "_id": "695d04eac03d6d81e4399cdc",
                    "name": "Mengyan Dai",
                    "hidden": false
                },
                {
                    "_id": "695d04eac03d6d81e4399cdd",
                    "name": "Yadong Yu",
                    "hidden": false
                },
                {
                    "_id": "695d04eac03d6d81e4399cde",
                    "name": "Pan Xiang",
                    "hidden": false
                },
                {
                    "_id": "695d04eac03d6d81e4399cdf",
                    "name": "Dongliang Huang",
                    "hidden": false
                },
                {
                    "_id": "695d04eac03d6d81e4399ce0",
                    "name": "Hongxu An",
                    "hidden": false
                },
                {
                    "_id": "695d04eac03d6d81e4399ce1",
                    "name": "Chunxiao Xin",
                    "hidden": false
                },
                {
                    "_id": "695d04eac03d6d81e4399ce2",
                    "name": "Haiyang Shen",
                    "hidden": false
                },
                {
                    "_id": "695d04eac03d6d81e4399ce3",
                    "name": "Zhenyu Wang",
                    "hidden": false
                },
                {
                    "_id": "695d04eac03d6d81e4399ce4",
                    "name": "Yunshan Na",
                    "hidden": false
                },
                {
                    "_id": "695d04eac03d6d81e4399ce5",
                    "name": "Gang Huang",
                    "hidden": false
                },
                {
                    "_id": "695d04eac03d6d81e4399ce6",
                    "name": "Xiang Jing",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-05T12:56:51.000Z",
            "submittedOnDailyAt": "2026-01-08T00:16:06.121Z",
            "title": "MDAgent2: Large Language Model for Code Generation and Knowledge Q&A in Molecular Dynamics",
            "submittedOnDailyBy": {
                "_id": "65d7018c90f11951bcfdf2f5",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d7018c90f11951bcfdf2f5/RU0-n9k3RIdtv8xvg9Upe.jpeg",
                "isPro": false,
                "fullname": "Zhuofan Shi",
                "user": "FredericFan",
                "type": "user"
            },
            "summary": "Molecular dynamics (MD) simulations are essential for understanding atomic-scale behaviors in materials science, yet writing LAMMPS scripts remains highly specialized and time-consuming tasks. Although LLMs show promise in code generation and domain-specific question answering, their performance in MD scenarios is limited by scarce domain data, the high deployment cost of state-of-the-art LLMs, and low code executability. Building upon our prior MDAgent, we present MDAgent2, the first end-to-end framework capable of performing both knowledge Q&A and code generation within the MD domain. We construct a domain-specific data-construction pipeline that yields three high-quality datasets spanning MD knowledge, question answering, and code generation. Based on these datasets, we adopt a three stage post-training strategy--continued pre-training (CPT), supervised fine-tuning (SFT), and reinforcement learning (RL)--to train two domain-adapted models, MD-Instruct and MD-Code. Furthermore, we introduce MD-GRPO, a closed-loop RL method that leverages simulation outcomes as reward signals and recycles low-reward trajectories for continual refinement. We further build MDAgent2-RUNTIME, a deployable multi-agent system that integrates code generation, execution, evaluation, and self-correction. Together with MD-EvalBench proposed in this work, the first benchmark for LAMMPS code generation and question answering, our models and system achieve performance surpassing several strong baselines.This work systematically demonstrates the adaptability and generalization capability of large language models in industrial simulation tasks, laying a methodological foundation for automatic code generation in AI for Science and industrial-scale simulations. URL: https://github.com/FredericVAN/PKU_MDAgent2",
            "upvotes": 6,
            "discussionId": "695d04ebc03d6d81e4399ce7",
            "ai_summary": "MDAgent2 enables automated molecular dynamics code generation and question answering through domain-adapted language models and a multi-agent runtime system.",
            "ai_keywords": [
                "LAMMPS",
                "molecular dynamics",
                "code generation",
                "question answering",
                "continued pre-training",
                "supervised fine-tuning",
                "reinforcement learning",
                "MD-Instruct",
                "MD-Code",
                "MD-GRPO",
                "MDAgent2",
                "MD-EvalBench"
            ],
            "organization": {
                "_id": "61dcd8e344f59573371b5cb6",
                "name": "PekingUniversity",
                "fullname": "Peking University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
            }
        },
        "publishedAt": "2026-01-05T07:56:51.000Z",
        "title": "MDAgent2: Large Language Model for Code Generation and Knowledge Q&A in Molecular Dynamics",
        "summary": "Molecular dynamics (MD) simulations are essential for understanding atomic-scale behaviors in materials science, yet writing LAMMPS scripts remains highly specialized and time-consuming tasks. Although LLMs show promise in code generation and domain-specific question answering, their performance in MD scenarios is limited by scarce domain data, the high deployment cost of state-of-the-art LLMs, and low code executability. Building upon our prior MDAgent, we present MDAgent2, the first end-to-end framework capable of performing both knowledge Q&A and code generation within the MD domain. We construct a domain-specific data-construction pipeline that yields three high-quality datasets spanning MD knowledge, question answering, and code generation. Based on these datasets, we adopt a three stage post-training strategy--continued pre-training (CPT), supervised fine-tuning (SFT), and reinforcement learning (RL)--to train two domain-adapted models, MD-Instruct and MD-Code. Furthermore, we introduce MD-GRPO, a closed-loop RL method that leverages simulation outcomes as reward signals and recycles low-reward trajectories for continual refinement. We further build MDAgent2-RUNTIME, a deployable multi-agent system that integrates code generation, execution, evaluation, and self-correction. Together with MD-EvalBench proposed in this work, the first benchmark for LAMMPS code generation and question answering, our models and system achieve performance surpassing several strong baselines.This work systematically demonstrates the adaptability and generalization capability of large language models in industrial simulation tasks, laying a methodological foundation for automatic code generation in AI for Science and industrial-scale simulations. URL: https://github.com/FredericVAN/PKU_MDAgent2",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02075.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65d7018c90f11951bcfdf2f5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d7018c90f11951bcfdf2f5/RU0-n9k3RIdtv8xvg9Upe.jpeg",
            "fullname": "Zhuofan Shi",
            "name": "FredericFan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2,
            "isUserFollowing": false
        },
        "organization": {
            "_id": "61dcd8e344f59573371b5cb6",
            "name": "PekingUniversity",
            "fullname": "Peking University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2601.00423",
            "authors": [
                {
                    "_id": "695b259f832867f253525d7b",
                    "user": {
                        "_id": "65fd0dfdfdc5e8ee7d15f793",
                        "avatarUrl": "/avatars/dc1073c79dfa5f4d683fc9cea46db645.svg",
                        "isPro": false,
                        "fullname": "Shengjun Zhang",
                        "user": "zhangsj0722",
                        "type": "user"
                    },
                    "name": "Shengjun Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-08T08:33:36.350Z",
                    "hidden": false
                },
                {
                    "_id": "695b259f832867f253525d7c",
                    "name": "Zhang Zhang",
                    "hidden": false
                },
                {
                    "_id": "695b259f832867f253525d7d",
                    "name": "Chensheng Dai",
                    "hidden": false
                },
                {
                    "_id": "695b259f832867f253525d7e",
                    "name": "Yueqi Duan",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-01T18:27:32.000Z",
            "submittedOnDailyAt": "2026-01-08T00:50:19.192Z",
            "title": "E-GRPO: High Entropy Steps Drive Effective Reinforcement Learning for Flow Models",
            "submittedOnDailyBy": {
                "_id": "65fd0dfdfdc5e8ee7d15f793",
                "avatarUrl": "/avatars/dc1073c79dfa5f4d683fc9cea46db645.svg",
                "isPro": false,
                "fullname": "Shengjun Zhang",
                "user": "zhangsj0722",
                "type": "user"
            },
            "summary": "Recent reinforcement learning has enhanced the flow matching models on human preference alignment. While stochastic sampling enables the exploration of denoising directions, existing methods which optimize over multiple denoising steps suffer from sparse and ambiguous reward signals. We observe that the high entropy steps enable more efficient and effective exploration while the low entropy steps result in undistinguished roll-outs. To this end, we propose E-GRPO, an entropy aware Group Relative Policy Optimization to increase the entropy of SDE sampling steps. Since the integration of stochastic differential equations suffer from ambiguous reward signals due to stochasticity from multiple steps, we specifically merge consecutive low entropy steps to formulate one high entropy step for SDE sampling, while applying ODE sampling on other steps. Building upon this, we introduce multi-step group normalized advantage, which computes group-relative advantages within samples sharing the same consolidated SDE denoising step. Experimental results on different reward settings have demonstrated the effectiveness of our methods.",
            "upvotes": 6,
            "discussionId": "695b25a0832867f253525d7f",
            "githubRepo": "https://github.com/shengjun-zhang/VisualGRPO",
            "githubRepoAddedBy": "user",
            "ai_summary": "Entropy-aware policy optimization method for reinforcement learning in flow matching models that improves exploration through SDE and ODE sampling strategies.",
            "ai_keywords": [
                "flow matching models",
                "reinforcement learning",
                "stochastic sampling",
                "denoising directions",
                "reward signals",
                "entropy awareness",
                "Group Relative Policy Optimization",
                "stochastic differential equations",
                "ordinary differential equations",
                "SDE sampling",
                "ODE sampling",
                "multi-step group normalized advantage"
            ],
            "githubStars": 15,
            "organization": {
                "_id": "628735cbc83a2d6ab8d14a66",
                "name": "Tsinghua",
                "fullname": "Tsinghua University"
            }
        },
        "publishedAt": "2026-01-01T13:27:32.000Z",
        "title": "E-GRPO: High Entropy Steps Drive Effective Reinforcement Learning for Flow Models",
        "summary": "Recent reinforcement learning has enhanced the flow matching models on human preference alignment. While stochastic sampling enables the exploration of denoising directions, existing methods which optimize over multiple denoising steps suffer from sparse and ambiguous reward signals. We observe that the high entropy steps enable more efficient and effective exploration while the low entropy steps result in undistinguished roll-outs. To this end, we propose E-GRPO, an entropy aware Group Relative Policy Optimization to increase the entropy of SDE sampling steps. Since the integration of stochastic differential equations suffer from ambiguous reward signals due to stochasticity from multiple steps, we specifically merge consecutive low entropy steps to formulate one high entropy step for SDE sampling, while applying ODE sampling on other steps. Building upon this, we introduce multi-step group normalized advantage, which computes group-relative advantages within samples sharing the same consolidated SDE denoising step. Experimental results on different reward settings have demonstrated the effectiveness of our methods.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.00423.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65fd0dfdfdc5e8ee7d15f793",
            "avatarUrl": "/avatars/dc1073c79dfa5f4d683fc9cea46db645.svg",
            "fullname": "Shengjun Zhang",
            "name": "zhangsj0722",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "isUserFollowing": false
        },
        "organization": {
            "_id": "628735cbc83a2d6ab8d14a66",
            "name": "Tsinghua",
            "fullname": "Tsinghua University"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2601.03471",
            "authors": [
                {
                    "_id": "695f27765fa3847525c41d6a",
                    "name": "Mingyang Wei",
                    "hidden": false
                },
                {
                    "_id": "695f27765fa3847525c41d6b",
                    "name": "Dehai Min",
                    "hidden": false
                },
                {
                    "_id": "695f27765fa3847525c41d6c",
                    "name": "Zewen Liu",
                    "hidden": false
                },
                {
                    "_id": "695f27765fa3847525c41d6d",
                    "name": "Yuzhang Xie",
                    "hidden": false
                },
                {
                    "_id": "695f27765fa3847525c41d6e",
                    "name": "Guanchen Wu",
                    "hidden": false
                },
                {
                    "_id": "695f27765fa3847525c41d6f",
                    "name": "Carl Yang",
                    "hidden": false
                },
                {
                    "_id": "695f27765fa3847525c41d70",
                    "name": "Max S. Y. Lau",
                    "hidden": false
                },
                {
                    "_id": "695f27765fa3847525c41d71",
                    "name": "Qi He",
                    "hidden": false
                },
                {
                    "_id": "695f27765fa3847525c41d72",
                    "name": "Lu Cheng",
                    "hidden": false
                },
                {
                    "_id": "695f27765fa3847525c41d73",
                    "name": "Wei Jin",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-06T23:49:10.000Z",
            "submittedOnDailyAt": "2026-01-08T01:12:19.381Z",
            "title": "EpiQAL: Benchmarking Large Language Models in Epidemiological Question Answering for Enhanced Alignment and Reasoning",
            "submittedOnDailyBy": {
                "_id": "629c6ee73a3221bb210afc2d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629c6ee73a3221bb210afc2d/Mg-VymVvHQn_pDrTgks0s.jpeg",
                "isPro": false,
                "fullname": "Dehai Min",
                "user": "ZhishanQ",
                "type": "user"
            },
            "summary": "Reliable epidemiological reasoning requires synthesizing study evidence to infer disease burden, transmission dynamics, and intervention effects at the population level. Existing medical question answering benchmarks primarily emphasize clinical knowledge or patient-level reasoning, yet few systematically evaluate evidence-grounded epidemiological inference. We present EpiQAL, the first diagnostic benchmark for epidemiological question answering across diverse diseases, comprising three subsets built from open-access literature. The subsets respectively evaluate text-grounded factual recall, multi-step inference linking document evidence with epidemiological principles, and conclusion reconstruction with the Discussion section withheld. Construction combines expert-designed taxonomy guidance, multi-model verification, and retrieval-based difficulty control. Experiments on ten open models reveal that current LLMs show limited performance on epidemiological reasoning, with multi-step inference posing the greatest challenge. Model rankings shift across subsets, and scale alone does not predict success. Chain-of-Thought prompting benefits multi-step inference but yields mixed results elsewhere. EpiQAL provides fine-grained diagnostic signals for evidence grounding, inferential reasoning, and conclusion reconstruction.",
            "upvotes": 5,
            "discussionId": "695f27775fa3847525c41d74",
            "ai_summary": "EpiQAL presents a novel benchmark for evaluating epidemiological reasoning in language models through three distinct subsets measuring factual recall, multi-step inference, and conclusion reconstruction from scientific literature.",
            "ai_keywords": [
                "epidemiological question answering",
                "diagnostic benchmark",
                "evidence-grounded inference",
                "multi-step inference",
                "chain-of-thought prompting"
            ]
        },
        "publishedAt": "2026-01-06T18:49:10.000Z",
        "title": "EpiQAL: Benchmarking Large Language Models in Epidemiological Question Answering for Enhanced Alignment and Reasoning",
        "summary": "Reliable epidemiological reasoning requires synthesizing study evidence to infer disease burden, transmission dynamics, and intervention effects at the population level. Existing medical question answering benchmarks primarily emphasize clinical knowledge or patient-level reasoning, yet few systematically evaluate evidence-grounded epidemiological inference. We present EpiQAL, the first diagnostic benchmark for epidemiological question answering across diverse diseases, comprising three subsets built from open-access literature. The subsets respectively evaluate text-grounded factual recall, multi-step inference linking document evidence with epidemiological principles, and conclusion reconstruction with the Discussion section withheld. Construction combines expert-designed taxonomy guidance, multi-model verification, and retrieval-based difficulty control. Experiments on ten open models reveal that current LLMs show limited performance on epidemiological reasoning, with multi-step inference posing the greatest challenge. Model rankings shift across subsets, and scale alone does not predict success. Chain-of-Thought prompting benefits multi-step inference but yields mixed results elsewhere. EpiQAL provides fine-grained diagnostic signals for evidence grounding, inferential reasoning, and conclusion reconstruction.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03471.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "629c6ee73a3221bb210afc2d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629c6ee73a3221bb210afc2d/Mg-VymVvHQn_pDrTgks0s.jpeg",
            "fullname": "Dehai Min",
            "name": "ZhishanQ",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2,
            "isUserFollowing": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2601.03699",
            "authors": [
                {
                    "_id": "695f23bb5fa3847525c41d40",
                    "user": {
                        "_id": "645b663eca5d8a297712f2e1",
                        "avatarUrl": "/avatars/c61dcba43feb879088b15b525e441cb9.svg",
                        "isPro": false,
                        "fullname": "Quy-Anh Dang",
                        "user": "quyanh",
                        "type": "user"
                    },
                    "name": "Quy-Anh Dang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-08T08:32:07.051Z",
                    "hidden": false
                },
                {
                    "_id": "695f23bb5fa3847525c41d41",
                    "name": "Chris Ngo",
                    "hidden": false
                },
                {
                    "_id": "695f23bb5fa3847525c41d42",
                    "name": "Truong-Son Hy",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-07T08:34:17.000Z",
            "submittedOnDailyAt": "2026-01-08T00:57:23.803Z",
            "title": "RedBench: A Universal Dataset for Comprehensive Red Teaming of Large Language Models",
            "submittedOnDailyBy": {
                "_id": "645b663eca5d8a297712f2e1",
                "avatarUrl": "/avatars/c61dcba43feb879088b15b525e441cb9.svg",
                "isPro": false,
                "fullname": "Quy-Anh Dang",
                "user": "quyanh",
                "type": "user"
            },
            "summary": "As large language models (LLMs) become integral to safety-critical applications, ensuring their robustness against adversarial prompts is paramount. However, existing red teaming datasets suffer from inconsistent risk categorizations, limited domain coverage, and outdated evaluations, hindering systematic vulnerability assessments. To address these challenges, we introduce RedBench, a universal dataset aggregating 37 benchmark datasets from leading conferences and repositories, comprising 29,362 samples across attack and refusal prompts. RedBench employs a standardized taxonomy with 22 risk categories and 19 domains, enabling consistent and comprehensive evaluations of LLM vulnerabilities. We provide a detailed analysis of existing datasets, establish baselines for modern LLMs, and open-source the dataset and evaluation code. Our contributions facilitate robust comparisons, foster future research, and promote the development of secure and reliable LLMs for real-world deployment. Code: https://github.com/knoveleng/redeval",
            "upvotes": 4,
            "discussionId": "695f23bb5fa3847525c41d43",
            "githubRepo": "https://github.com/knoveleng/redeval",
            "githubRepoAddedBy": "user",
            "ai_summary": "RedBench presents a unified dataset with standardized risk categorization for evaluating LLM vulnerabilities across multiple domains and attack types.",
            "ai_keywords": [
                "large language models",
                "adversarial prompts",
                "red teaming datasets",
                "benchmark datasets",
                "risk categorizations",
                "domain coverage",
                "standardized taxonomy",
                "attack prompts",
                "refusal prompts"
            ],
            "githubStars": 1,
            "organization": {
                "_id": "6735b5052769638944f432c9",
                "name": "knoveleng",
                "fullname": "Knovel Engineering",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63105f7463b70252b4775783/ya7YB4IA3MXjfsfx_hjB_.png"
            }
        },
        "publishedAt": "2026-01-07T03:34:17.000Z",
        "title": "RedBench: A Universal Dataset for Comprehensive Red Teaming of Large Language Models",
        "summary": "As large language models (LLMs) become integral to safety-critical applications, ensuring their robustness against adversarial prompts is paramount. However, existing red teaming datasets suffer from inconsistent risk categorizations, limited domain coverage, and outdated evaluations, hindering systematic vulnerability assessments. To address these challenges, we introduce RedBench, a universal dataset aggregating 37 benchmark datasets from leading conferences and repositories, comprising 29,362 samples across attack and refusal prompts. RedBench employs a standardized taxonomy with 22 risk categories and 19 domains, enabling consistent and comprehensive evaluations of LLM vulnerabilities. We provide a detailed analysis of existing datasets, establish baselines for modern LLMs, and open-source the dataset and evaluation code. Our contributions facilitate robust comparisons, foster future research, and promote the development of secure and reliable LLMs for real-world deployment. Code: https://github.com/knoveleng/redeval",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03699.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "645b663eca5d8a297712f2e1",
            "avatarUrl": "/avatars/c61dcba43feb879088b15b525e441cb9.svg",
            "fullname": "Quy-Anh Dang",
            "name": "quyanh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3,
            "isUserFollowing": false
        },
        "organization": {
            "_id": "6735b5052769638944f432c9",
            "name": "knoveleng",
            "fullname": "Knovel Engineering",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63105f7463b70252b4775783/ya7YB4IA3MXjfsfx_hjB_.png"
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2601.03315",
            "authors": [
                {
                    "_id": "695f9e182450f142afb3c498",
                    "name": "Dhruv Trehan",
                    "hidden": false
                },
                {
                    "_id": "695f9e182450f142afb3c499",
                    "name": "Paras Chopra",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-06T13:20:54.000Z",
            "submittedOnDailyAt": "2026-01-08T09:38:29.673Z",
            "title": "Why LLMs Aren't Scientists Yet: Lessons from Four Autonomous Research Attempts",
            "submittedOnDailyBy": {
                "_id": "69020c76ec2616129dea30b4",
                "avatarUrl": "/avatars/44d59b5d951a4303def15ea1c6e3387e.svg",
                "isPro": false,
                "fullname": "Paras Chopra",
                "user": "paraslossfunk",
                "type": "user"
            },
            "summary": "We report a case study of four end-to-end attempts to autonomously generate ML research papers using a pipeline of six LLM agents mapped to stages of the scientific workflow. Of these four, three attempts failed during implementation or evaluation. One completed the pipeline and was accepted to Agents4Science 2025, an experimental inaugural venue that required AI systems as first authors, passing both human and multi-AI review. From these attempts, we document six recurring failure modes: bias toward training data defaults, implementation drift under execution pressure, memory and context degradation across long-horizon tasks, overexcitement that declares success despite obvious failures, insufficient domain intelligence, and weak scientific taste in experimental design. We conclude by discussing four design principles for more robust AI-scientist systems, implications for autonomous scientific discovery, and we release all prompts, artifacts, and outputs at https://github.com/Lossfunk/ai-scientist-artefacts-v1",
            "upvotes": 4,
            "discussionId": "695f9e192450f142afb3c49f",
            "ai_summary": "A case study of four attempts to autonomously generate ML research papers using LLM agents reveals recurring failure modes and proposes design principles for robust AI-scientist systems.",
            "ai_keywords": [
                "LLM agents",
                "scientific workflow",
                "autonomous scientific discovery",
                "AI-scientist systems"
            ],
            "organization": {
                "_id": "67a1298ea7d77f4454f936a2",
                "name": "Lossfunk",
                "fullname": "Lossfunk",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67a128ecc9bce54bbf006876/1lIRZ5gmsWovgosI_HIXy.jpeg"
            }
        },
        "publishedAt": "2026-01-06T08:20:54.000Z",
        "title": "Why LLMs Aren't Scientists Yet: Lessons from Four Autonomous Research Attempts",
        "summary": "We report a case study of four end-to-end attempts to autonomously generate ML research papers using a pipeline of six LLM agents mapped to stages of the scientific workflow. Of these four, three attempts failed during implementation or evaluation. One completed the pipeline and was accepted to Agents4Science 2025, an experimental inaugural venue that required AI systems as first authors, passing both human and multi-AI review. From these attempts, we document six recurring failure modes: bias toward training data defaults, implementation drift under execution pressure, memory and context degradation across long-horizon tasks, overexcitement that declares success despite obvious failures, insufficient domain intelligence, and weak scientific taste in experimental design. We conclude by discussing four design principles for more robust AI-scientist systems, implications for autonomous scientific discovery, and we release all prompts, artifacts, and outputs at https://github.com/Lossfunk/ai-scientist-artefacts-v1",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03315.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "69020c76ec2616129dea30b4",
            "avatarUrl": "/avatars/44d59b5d951a4303def15ea1c6e3387e.svg",
            "fullname": "Paras Chopra",
            "name": "paraslossfunk",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "isUserFollowing": false
        },
        "organization": {
            "_id": "67a1298ea7d77f4454f936a2",
            "name": "Lossfunk",
            "fullname": "Lossfunk",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67a128ecc9bce54bbf006876/1lIRZ5gmsWovgosI_HIXy.jpeg"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2601.03467",
            "authors": [
                {
                    "_id": "695f2a2c5fa3847525c41d98",
                    "name": "Hengjia Li",
                    "hidden": false
                },
                {
                    "_id": "695f2a2c5fa3847525c41d99",
                    "name": "Liming Jiang",
                    "hidden": false
                },
                {
                    "_id": "695f2a2c5fa3847525c41d9a",
                    "name": "Qing Yan",
                    "hidden": false
                },
                {
                    "_id": "695f2a2c5fa3847525c41d9b",
                    "name": "Yizhi Song",
                    "hidden": false
                },
                {
                    "_id": "695f2a2c5fa3847525c41d9c",
                    "name": "Hao Kang",
                    "hidden": false
                },
                {
                    "_id": "695f2a2c5fa3847525c41d9d",
                    "name": "Zichuan Liu",
                    "hidden": false
                },
                {
                    "_id": "695f2a2c5fa3847525c41d9e",
                    "name": "Xin Lu",
                    "hidden": false
                },
                {
                    "_id": "695f2a2c5fa3847525c41d9f",
                    "name": "Boxi Wu",
                    "hidden": false
                },
                {
                    "_id": "695f2a2c5fa3847525c41da0",
                    "name": "Deng Cai",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-06T23:43:00.000Z",
            "submittedOnDailyAt": "2026-01-08T02:21:28.741Z",
            "title": "ThinkRL-Edit: Thinking in Reinforcement Learning for Reasoning-Centric Image Editing",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Instruction-driven image editing with unified multimodal generative models has advanced rapidly, yet their underlying visual reasoning remains limited, leading to suboptimal performance on reasoning-centric edits. Reinforcement learning (RL) has been investigated for improving the quality of image editing, but it faces three key challenges: (1) limited reasoning exploration confined to denoising stochasticity, (2) biased reward fusion, and (3) unstable VLM-based instruction rewards. In this work, we propose ThinkRL-Edit, a reasoning-centric RL framework that decouples visual reasoning from image synthesis and expands reasoning exploration beyond denoising. To the end, we introduce Chain-of-Thought (CoT)-based reasoning sampling with planning and reflection stages prior to generation in online sampling, compelling the model to explore multiple semantic hypotheses and validate their plausibility before committing to a visual outcome. To avoid the failures of weighted aggregation, we propose an unbiased chain preference grouping strategy across multiple reward dimensions. Moreover, we replace interval-based VLM scores with a binary checklist, yielding more precise, lower-variance, and interpretable rewards for complex reasoning. Experiments show our method significantly outperforms prior work on reasoning-centric image editing, producing instruction-faithful, visually coherent, and semantically grounded edits.",
            "upvotes": 3,
            "discussionId": "695f2a2c5fa3847525c41da1",
            "ai_summary": "ThinkRL-Edit enhances reasoning-centric image editing through reinforcement learning by expanding visual reasoning exploration beyond denoising stochasticity and using unbiased reward strategies.",
            "ai_keywords": [
                "reinforcement learning",
                "visual reasoning",
                "image editing",
                "denoising stochasticity",
                "chain-of-thought reasoning",
                "reward fusion",
                "VLM-based instruction rewards",
                "reasoning sampling",
                "semantic hypotheses",
                "reward aggregation",
                "binary checklist",
                "visual coherence",
                "semantic grounding"
            ],
            "organization": {
                "_id": "653b817d32c97d0655575872",
                "name": "ByteDance",
                "fullname": "ByteDance",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
            }
        },
        "publishedAt": "2026-01-06T18:43:00.000Z",
        "title": "ThinkRL-Edit: Thinking in Reinforcement Learning for Reasoning-Centric Image Editing",
        "summary": "Instruction-driven image editing with unified multimodal generative models has advanced rapidly, yet their underlying visual reasoning remains limited, leading to suboptimal performance on reasoning-centric edits. Reinforcement learning (RL) has been investigated for improving the quality of image editing, but it faces three key challenges: (1) limited reasoning exploration confined to denoising stochasticity, (2) biased reward fusion, and (3) unstable VLM-based instruction rewards. In this work, we propose ThinkRL-Edit, a reasoning-centric RL framework that decouples visual reasoning from image synthesis and expands reasoning exploration beyond denoising. To the end, we introduce Chain-of-Thought (CoT)-based reasoning sampling with planning and reflection stages prior to generation in online sampling, compelling the model to explore multiple semantic hypotheses and validate their plausibility before committing to a visual outcome. To avoid the failures of weighted aggregation, we propose an unbiased chain preference grouping strategy across multiple reward dimensions. Moreover, we replace interval-based VLM scores with a binary checklist, yielding more precise, lower-variance, and interpretable rewards for complex reasoning. Experiments show our method significantly outperforms prior work on reasoning-centric image editing, producing instruction-faithful, visually coherent, and semantically grounded edits.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03467.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 204,
            "isUserFollowing": false
        },
        "organization": {
            "_id": "653b817d32c97d0655575872",
            "name": "ByteDance",
            "fullname": "ByteDance",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2601.03448",
            "authors": [
                {
                    "_id": "695f986a2450f142afb3c476",
                    "name": "Atsuki Yamaguchi",
                    "hidden": false
                },
                {
                    "_id": "695f986a2450f142afb3c477",
                    "name": "Maggie Mi",
                    "hidden": false
                },
                {
                    "_id": "695f986a2450f142afb3c478",
                    "name": "Nikolaos Aletras",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-06T22:28:15.000Z",
            "submittedOnDailyAt": "2026-01-08T09:52:54.834Z",
            "title": "Enhancing Linguistic Competence of Language Models through Pre-training with Language Learning Tasks",
            "submittedOnDailyBy": {
                "_id": "63998cdd438fba22d95f0826",
                "avatarUrl": "/avatars/20f04b6ad2d42361c2c7dfc540aa4694.svg",
                "isPro": false,
                "fullname": "Atsuki Yamaguchi",
                "user": "atsuki-yamaguchi",
                "type": "user"
            },
            "summary": "Language models (LMs) are pre-trained on raw text datasets to generate text sequences token-by-token. While this approach facilitates the learning of world knowledge and reasoning, it does not explicitly optimize for linguistic competence. To bridge this gap, we propose L2T, a pre-training framework integrating Language Learning Tasks alongside standard next-token prediction. Inspired by human language acquisition, L2T transforms raw text into structured input-output pairs to provide explicit linguistic stimulation. Pre-training LMs on a mixture of raw text and L2T data not only improves overall performance on linguistic competence benchmarks but accelerates its acquisition, while maintaining competitive performance on general reasoning tasks.",
            "upvotes": 3,
            "discussionId": "695f986b2450f142afb3c479",
            "githubRepo": "https://github.com/gucci-j/l2t",
            "githubRepoAddedBy": "user",
            "ai_summary": "Language models pre-trained with a framework combining standard next-token prediction and structured language learning tasks show enhanced linguistic competence without sacrificing general reasoning capabilities.",
            "ai_keywords": [
                "language models",
                "next-token prediction",
                "linguistic competence",
                "pre-training framework",
                "language learning tasks",
                "raw text",
                "structured input-output pairs",
                "linguistic stimulation"
            ],
            "githubStars": 0,
            "organization": {
                "_id": "695d3c71489df4390d18436e",
                "name": "l2t-project",
                "fullname": "Language Learning Tasks (L2T) Project",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63998cdd438fba22d95f0826/nP_gwJjtE5gw2XW0jvK01.png"
            }
        },
        "publishedAt": "2026-01-06T17:28:15.000Z",
        "title": "Enhancing Linguistic Competence of Language Models through Pre-training with Language Learning Tasks",
        "summary": "Language models (LMs) are pre-trained on raw text datasets to generate text sequences token-by-token. While this approach facilitates the learning of world knowledge and reasoning, it does not explicitly optimize for linguistic competence. To bridge this gap, we propose L2T, a pre-training framework integrating Language Learning Tasks alongside standard next-token prediction. Inspired by human language acquisition, L2T transforms raw text into structured input-output pairs to provide explicit linguistic stimulation. Pre-training LMs on a mixture of raw text and L2T data not only improves overall performance on linguistic competence benchmarks but accelerates its acquisition, while maintaining competitive performance on general reasoning tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03448.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63998cdd438fba22d95f0826",
            "avatarUrl": "/avatars/20f04b6ad2d42361c2c7dfc540aa4694.svg",
            "fullname": "Atsuki Yamaguchi",
            "name": "atsuki-yamaguchi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3,
            "isUserFollowing": false
        },
        "organization": {
            "_id": "695d3c71489df4390d18436e",
            "name": "l2t-project",
            "fullname": "Language Learning Tasks (L2T) Project",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63998cdd438fba22d95f0826/nP_gwJjtE5gw2XW0jvK01.png"
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2601.02933",
            "authors": [
                {
                    "_id": "695fa9125b7998385e639310",
                    "name": "Vilém Zouhar",
                    "hidden": false
                },
                {
                    "_id": "695fa9125b7998385e639311",
                    "name": "Tom Kocmi",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6304ece07424d937fa35fb98/3FYIwayjDnd2nzV4kvSFh.png"
            ],
            "publishedAt": "2026-01-06T11:21:03.000Z",
            "submittedOnDailyAt": "2026-01-08T12:49:14.166Z",
            "title": "Pearmut: Human Evaluation of Translation Made Trivial",
            "submittedOnDailyBy": {
                "_id": "6304ece07424d937fa35fb98",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6304ece07424d937fa35fb98/6qZoqm-Ti8CiDcCHEt1sE.jpeg",
                "isPro": false,
                "fullname": "Vilém Zouhar",
                "user": "zouharvi",
                "type": "user"
            },
            "summary": "Human evaluation is the gold standard for multilingual NLP, but is often skipped in practice and substituted with automatic metrics, because it is notoriously complex and slow to set up with existing tools with substantial engineering and operational overhead. We introduce Pearmut, a lightweight yet feature-rich platform that makes end-to-end human evaluation as easy to run as automatic evaluation. Pearmut removes common entry barriers and provides support for evaluating multilingual tasks, with a particular focus on machine translation. The platform implements standard evaluation protocols, including DA, ESA, or MQM, but is also extensible to allow prototyping new protocols. It features document-level context, absolute and contrastive evaluation, attention checks, ESAAI pre-annotations and both static and active learning-based assignment strategies. Pearmut enables reliable human evaluation to become a practical, routine component of model development and diagnosis rather than an occasional effort.",
            "upvotes": 2,
            "discussionId": "695fa9135b7998385e639312",
            "githubRepo": "https://github.com/zouharvi/pearmut",
            "githubRepoAddedBy": "user",
            "ai_summary": "Pearmut is a platform that simplifies human evaluation in multilingual NLP by providing a lightweight solution for end-to-end evaluation with support for various protocols and learning strategies.",
            "ai_keywords": [
                "human evaluation",
                "multilingual NLP",
                "machine translation",
                "DA",
                "ESA",
                "MQM",
                "ESAAI",
                "static learning",
                "active learning"
            ],
            "githubStars": 7
        },
        "publishedAt": "2026-01-06T06:21:03.000Z",
        "title": "Pearmut: Human Evaluation of Translation Made Trivial",
        "summary": "Human evaluation is the gold standard for multilingual NLP, but is often skipped in practice and substituted with automatic metrics, because it is notoriously complex and slow to set up with existing tools with substantial engineering and operational overhead. We introduce Pearmut, a lightweight yet feature-rich platform that makes end-to-end human evaluation as easy to run as automatic evaluation. Pearmut removes common entry barriers and provides support for evaluating multilingual tasks, with a particular focus on machine translation. The platform implements standard evaluation protocols, including DA, ESA, or MQM, but is also extensible to allow prototyping new protocols. It features document-level context, absolute and contrastive evaluation, attention checks, ESAAI pre-annotations and both static and active learning-based assignment strategies. Pearmut enables reliable human evaluation to become a practical, routine component of model development and diagnosis rather than an occasional effort.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6304ece07424d937fa35fb98/3FYIwayjDnd2nzV4kvSFh.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02933.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6304ece07424d937fa35fb98",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6304ece07424d937fa35fb98/6qZoqm-Ti8CiDcCHEt1sE.jpeg",
            "fullname": "Vilém Zouhar",
            "name": "zouharvi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 31,
            "isUserFollowing": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2601.03955",
            "authors": [
                {
                    "_id": "695f6d305fa3847525c42030",
                    "name": "Xu Zhang",
                    "hidden": false
                },
                {
                    "_id": "695f6d305fa3847525c42031",
                    "name": "Cheng Da",
                    "hidden": false
                },
                {
                    "_id": "695f6d305fa3847525c42032",
                    "name": "Huan Yang",
                    "hidden": false
                },
                {
                    "_id": "695f6d305fa3847525c42033",
                    "name": "Kun Gai",
                    "hidden": false
                },
                {
                    "_id": "695f6d305fa3847525c42034",
                    "name": "Ming Lu",
                    "hidden": false
                },
                {
                    "_id": "695f6d305fa3847525c42035",
                    "name": "Zhan Ma",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-07T14:09:18.000Z",
            "submittedOnDailyAt": "2026-01-08T12:14:13.052Z",
            "title": "ResTok: Learning Hierarchical Residuals in 1D Visual Tokenizers for Autoregressive Image Generation",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": true,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "Existing 1D visual tokenizers for autoregressive (AR) generation largely follow the design principles of language modeling, as they are built directly upon transformers whose priors originate in language, yielding single-hierarchy latent tokens and treating visual data as flat sequential token streams. However, this language-like formulation overlooks key properties of vision, particularly the hierarchical and residual network designs that have long been essential for convergence and efficiency in visual models. To bring \"vision\" back to vision, we propose the Residual Tokenizer (ResTok), a 1D visual tokenizer that builds hierarchical residuals for both image tokens and latent tokens. The hierarchical representations obtained through progressively merging enable cross-level feature fusion at each layer, substantially enhancing representational capacity. Meanwhile, the semantic residuals between hierarchies prevent information overlap, yielding more concentrated latent distributions that are easier for AR modeling. Cross-level bindings consequently emerge without any explicit constraints. To accelerate the generation process, we further introduce a hierarchical AR generator that substantially reduces sampling steps by predicting an entire level of latent tokens at once rather than generating them strictly token-by-token. Extensive experiments demonstrate that restoring hierarchical residual priors in visual tokenization significantly improves AR image generation, achieving a gFID of 2.34 on ImageNet-256 with only 9 sampling steps. Code is available at https://github.com/Kwai-Kolors/ResTok.",
            "upvotes": 1,
            "discussionId": "695f6d305fa3847525c42036",
            "ai_summary": "A novel 1D visual tokenizer called Residual Tokenizer is introduced that incorporates hierarchical residuals to improve autoregressive image generation by leveraging vision-specific design principles rather than language modeling approaches.",
            "ai_keywords": [
                "visual tokenizer",
                "autoregressive generation",
                "transformers",
                "hierarchical residuals",
                "latent tokens",
                "cross-level feature fusion",
                "semantic residuals",
                "hierarchical AR generator",
                "gFID",
                "ImageNet-256"
            ]
        },
        "publishedAt": "2026-01-07T09:09:18.000Z",
        "title": "ResTok: Learning Hierarchical Residuals in 1D Visual Tokenizers for Autoregressive Image Generation",
        "summary": "Existing 1D visual tokenizers for autoregressive (AR) generation largely follow the design principles of language modeling, as they are built directly upon transformers whose priors originate in language, yielding single-hierarchy latent tokens and treating visual data as flat sequential token streams. However, this language-like formulation overlooks key properties of vision, particularly the hierarchical and residual network designs that have long been essential for convergence and efficiency in visual models. To bring \"vision\" back to vision, we propose the Residual Tokenizer (ResTok), a 1D visual tokenizer that builds hierarchical residuals for both image tokens and latent tokens. The hierarchical representations obtained through progressively merging enable cross-level feature fusion at each layer, substantially enhancing representational capacity. Meanwhile, the semantic residuals between hierarchies prevent information overlap, yielding more concentrated latent distributions that are easier for AR modeling. Cross-level bindings consequently emerge without any explicit constraints. To accelerate the generation process, we further introduce a hierarchical AR generator that substantially reduces sampling steps by predicting an entire level of latent tokens at once rather than generating them strictly token-by-token. Extensive experiments demonstrate that restoring hierarchical residual priors in visual tokenization significantly improves AR image generation, achieving a gFID of 2.34 on ImageNet-256 with only 9 sampling steps. Code is available at https://github.com/Kwai-Kolors/ResTok.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03955.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": true,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9058,
            "isUserFollowing": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2601.03236",
            "authors": [
                {
                    "_id": "695f67735fa3847525c4200d",
                    "name": "Dongming Jiang",
                    "hidden": false
                },
                {
                    "_id": "695f67735fa3847525c4200e",
                    "name": "Yi Li",
                    "hidden": false
                },
                {
                    "_id": "695f67735fa3847525c4200f",
                    "name": "Guanpeng Li",
                    "hidden": false
                },
                {
                    "_id": "695f67735fa3847525c42010",
                    "name": "Bingzhe Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2026-01-06T18:29:43.000Z",
            "submittedOnDailyAt": "2026-01-08T05:51:09.698Z",
            "title": "MAGMA: A Multi-Graph based Agentic Memory Architecture for AI Agents",
            "submittedOnDailyBy": {
                "_id": "64103f66928400b4164308f0",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64103f66928400b4164308f0/5ZikDcmC1qBCWEP6YJeCx.jpeg",
                "isPro": false,
                "fullname": "Uday Allu",
                "user": "udayallu",
                "type": "user"
            },
            "summary": "Memory-Augmented Generation (MAG) extends Large Language Models with external memory to support long-context reasoning, but existing approaches largely rely on semantic similarity over monolithic memory stores, entangling temporal, causal, and entity information. This design limits interpretability and alignment between query intent and retrieved evidence, leading to suboptimal reasoning accuracy. In this paper, we propose MAGMA, a multi-graph agentic memory architecture that represents each memory item across orthogonal semantic, temporal, causal, and entity graphs. MAGMA formulates retrieval as policy-guided traversal over these relational views, enabling query-adaptive selection and structured context construction. By decoupling memory representation from retrieval logic, MAGMA provides transparent reasoning paths and fine-grained control over retrieval. Experiments on LoCoMo and LongMemEval demonstrate that MAGMA consistently outperforms state-of-the-art agentic memory systems in long-horizon reasoning tasks.",
            "upvotes": 1,
            "discussionId": "695f67735fa3847525c42011",
            "ai_summary": "MAGMA is a multi-graph memory architecture that improves long-context reasoning in language models by separating memory representation from retrieval logic across semantic, temporal, causal, and entity dimensions.",
            "ai_keywords": [
                "memory-augmented generation",
                "large language models",
                "external memory",
                "long-context reasoning",
                "semantic similarity",
                "monolithic memory stores",
                "interpretability",
                "query intent",
                "retrieved evidence",
                "multi-graph agentic memory",
                "relational views",
                "policy-guided traversal",
                "query-adaptive selection",
                "structured context construction",
                "transparent reasoning paths",
                "fine-grained control",
                "long-horizon reasoning tasks"
            ]
        },
        "publishedAt": "2026-01-06T13:29:43.000Z",
        "title": "MAGMA: A Multi-Graph based Agentic Memory Architecture for AI Agents",
        "summary": "Memory-Augmented Generation (MAG) extends Large Language Models with external memory to support long-context reasoning, but existing approaches largely rely on semantic similarity over monolithic memory stores, entangling temporal, causal, and entity information. This design limits interpretability and alignment between query intent and retrieved evidence, leading to suboptimal reasoning accuracy. In this paper, we propose MAGMA, a multi-graph agentic memory architecture that represents each memory item across orthogonal semantic, temporal, causal, and entity graphs. MAGMA formulates retrieval as policy-guided traversal over these relational views, enabling query-adaptive selection and structured context construction. By decoupling memory representation from retrieval logic, MAGMA provides transparent reasoning paths and fine-grained control over retrieval. Experiments on LoCoMo and LongMemEval demonstrate that MAGMA consistently outperforms state-of-the-art agentic memory systems in long-horizon reasoning tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03236.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64103f66928400b4164308f0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64103f66928400b4164308f0/5ZikDcmC1qBCWEP6YJeCx.jpeg",
            "fullname": "Uday Allu",
            "name": "udayallu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4,
            "isUserFollowing": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2601.04090",
            "authors": [
                {
                    "_id": "695fa6f82450f142afb3c59b",
                    "name": "Jiaxin Huang",
                    "hidden": false
                },
                {
                    "_id": "695fa6f82450f142afb3c59c",
                    "name": "Yuanbo Yang",
                    "hidden": false
                },
                {
                    "_id": "695fa6f82450f142afb3c59d",
                    "name": "Bangbang Yang",
                    "hidden": false
                },
                {
                    "_id": "695fa6f82450f142afb3c59e",
                    "name": "Lin Ma",
                    "hidden": false
                },
                {
                    "_id": "695fa6f82450f142afb3c59f",
                    "name": "Yuewen Ma",
                    "hidden": false
                },
                {
                    "_id": "695fa6f82450f142afb3c5a0",
                    "name": "Yiyi Liao",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63e367d3fae035bdc4c347fc/LWql-1IQRInZaowTCyEv7.png"
            ],
            "publishedAt": "2026-01-07T16:57:30.000Z",
            "submittedOnDailyAt": "2026-01-08T10:31:14.614Z",
            "title": "Gen3R: 3D Scene Generation Meets Feed-Forward Reconstruction",
            "submittedOnDailyBy": {
                "_id": "63e367d3fae035bdc4c347fc",
                "avatarUrl": "/avatars/239da0c9287bf965348aff4890e94722.svg",
                "isPro": false,
                "fullname": "Jiaxin Huang",
                "user": "JaceyH919",
                "type": "user"
            },
            "summary": "We present Gen3R, a method that bridges the strong priors of foundational reconstruction models and video diffusion models for scene-level 3D generation. We repurpose the VGGT reconstruction model to produce geometric latents by training an adapter on its tokens, which are regularized to align with the appearance latents of pre-trained video diffusion models. By jointly generating these disentangled yet aligned latents, Gen3R produces both RGB videos and corresponding 3D geometry, including camera poses, depth maps, and global point clouds. Experiments demonstrate that our approach achieves state-of-the-art results in single- and multi-image conditioned 3D scene generation. Additionally, our method can enhance the robustness of reconstruction by leveraging generative priors, demonstrating the mutual benefit of tightly coupling reconstruction and generative models.",
            "upvotes": 0,
            "discussionId": "695fa6f82450f142afb3c5a3",
            "projectPage": "https://xdimlab.github.io/Gen3R/",
            "githubRepo": "https://github.com/JaceyHuang/Gen3R",
            "githubRepoAddedBy": "user",
            "ai_summary": "Gen3R combines foundational reconstruction models with video diffusion models to generate 3D scenes with RGB videos and geometric information through aligned latents.",
            "ai_keywords": [
                "VGGT reconstruction model",
                "video diffusion models",
                "geometric latents",
                "appearance latents",
                "adapter",
                "disentangled latents",
                "camera poses",
                "depth maps",
                "global point clouds"
            ],
            "githubStars": 34
        },
        "publishedAt": "2026-01-07T11:57:30.000Z",
        "title": "Gen3R: 3D Scene Generation Meets Feed-Forward Reconstruction",
        "summary": "We present Gen3R, a method that bridges the strong priors of foundational reconstruction models and video diffusion models for scene-level 3D generation. We repurpose the VGGT reconstruction model to produce geometric latents by training an adapter on its tokens, which are regularized to align with the appearance latents of pre-trained video diffusion models. By jointly generating these disentangled yet aligned latents, Gen3R produces both RGB videos and corresponding 3D geometry, including camera poses, depth maps, and global point clouds. Experiments demonstrate that our approach achieves state-of-the-art results in single- and multi-image conditioned 3D scene generation. Additionally, our method can enhance the robustness of reconstruction by leveraging generative priors, demonstrating the mutual benefit of tightly coupling reconstruction and generative models.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63e367d3fae035bdc4c347fc/LWql-1IQRInZaowTCyEv7.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04090.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63e367d3fae035bdc4c347fc",
            "avatarUrl": "/avatars/239da0c9287bf965348aff4890e94722.svg",
            "fullname": "Jiaxin Huang",
            "name": "JaceyH919",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "isUserFollowing": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2601.00705",
            "authors": [
                {
                    "_id": "695f659a5fa3847525c42008",
                    "user": {
                        "_id": "66c72e87ce180000ace77b52",
                        "avatarUrl": "/avatars/0bec08666319ce623a669cdc17f8cca8.svg",
                        "isPro": false,
                        "fullname": "cheng",
                        "user": "Breeze1124",
                        "type": "user"
                    },
                    "name": "Wei-Tse Cheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2026-01-08T08:31:41.027Z",
                    "hidden": false
                },
                {
                    "_id": "695f659a5fa3847525c42009",
                    "name": "Yen-Jen Chiou",
                    "hidden": false
                },
                {
                    "_id": "695f659a5fa3847525c4200a",
                    "name": "Yuan-Fu Yang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/66c72e87ce180000ace77b52/LrpBH6bcSXkiuOQa7EPVj.png"
            ],
            "publishedAt": "2025-12-28T03:45:57.000Z",
            "submittedOnDailyAt": "2026-01-08T06:08:24.598Z",
            "title": "RGS-SLAM: Robust Gaussian Splatting SLAM with One-Shot Dense Initialization",
            "submittedOnDailyBy": {
                "_id": "66c72e87ce180000ace77b52",
                "avatarUrl": "/avatars/0bec08666319ce623a669cdc17f8cca8.svg",
                "isPro": false,
                "fullname": "cheng",
                "user": "Breeze1124",
                "type": "user"
            },
            "summary": "We introduce RGS-SLAM, a robust Gaussian-splatting SLAM framework that replaces the residual-driven densification stage of GS-SLAM with a training-free correspondence-to-Gaussian initialization. Instead of progressively adding Gaussians as residuals reveal missing geometry, RGS-SLAM performs a one-shot triangulation of dense multi-view correspondences derived from DINOv3 descriptors refined through a confidence-aware inlier classifier, generating a well-distributed and structure-aware Gaussian seed prior to optimization. This initialization stabilizes early mapping and accelerates convergence by roughly 20\\%, yielding higher rendering fidelity in texture-rich and cluttered scenes while remaining fully compatible with existing GS-SLAM pipelines. Evaluated on the TUM RGB-D and Replica datasets, RGS-SLAM achieves competitive or superior localization and reconstruction accuracy compared with state-of-the-art Gaussian and point-based SLAM systems, sustaining real-time mapping performance at up to 925 FPS.",
            "upvotes": 0,
            "discussionId": "695f659b5fa3847525c4200b",
            "ai_summary": "RGS-SLAM presents a robust Gaussian-splatting SLAM framework that uses dense multi-view correspondences and DINOv3 descriptors for efficient, stable mapping with improved rendering fidelity.",
            "ai_keywords": [
                "Gaussian-splatting",
                "SLAM",
                "densification",
                "Gaussian seed",
                "triangulation",
                "DINOv3 descriptors",
                "confidence-aware inlier classifier",
                "rendering fidelity",
                "real-time mapping"
            ],
            "organization": {
                "_id": "63e39e6499a032b1c950403d",
                "name": "NYCU",
                "fullname": "National Yang Ming Chiao Tung University",
                "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63e39df6c65f975b436bb6b8/WLWf1bSpvrXBYYKEdXbgU.png"
            }
        },
        "publishedAt": "2025-12-27T22:45:57.000Z",
        "title": "RGS-SLAM: Robust Gaussian Splatting SLAM with One-Shot Dense Initialization",
        "summary": "We introduce RGS-SLAM, a robust Gaussian-splatting SLAM framework that replaces the residual-driven densification stage of GS-SLAM with a training-free correspondence-to-Gaussian initialization. Instead of progressively adding Gaussians as residuals reveal missing geometry, RGS-SLAM performs a one-shot triangulation of dense multi-view correspondences derived from DINOv3 descriptors refined through a confidence-aware inlier classifier, generating a well-distributed and structure-aware Gaussian seed prior to optimization. This initialization stabilizes early mapping and accelerates convergence by roughly 20\\%, yielding higher rendering fidelity in texture-rich and cluttered scenes while remaining fully compatible with existing GS-SLAM pipelines. Evaluated on the TUM RGB-D and Replica datasets, RGS-SLAM achieves competitive or superior localization and reconstruction accuracy compared with state-of-the-art Gaussian and point-based SLAM systems, sustaining real-time mapping performance at up to 925 FPS.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/66c72e87ce180000ace77b52/LrpBH6bcSXkiuOQa7EPVj.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.00705.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66c72e87ce180000ace77b52",
            "avatarUrl": "/avatars/0bec08666319ce623a669cdc17f8cca8.svg",
            "fullname": "cheng",
            "name": "Breeze1124",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "isUserFollowing": false
        },
        "organization": {
            "_id": "63e39e6499a032b1c950403d",
            "name": "NYCU",
            "fullname": "National Yang Ming Chiao Tung University",
            "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63e39df6c65f975b436bb6b8/WLWf1bSpvrXBYYKEdXbgU.png"
        },
        "isAuthorParticipating": true
    }
]