[
    {
        "名称": "2025 [2501.12380] MMVU: Measuring Expert-Level Multi-Discipline Video Understanding.pdf",
        "作者": "Yilun Zhao, Lujing Xie, Haowei Zhang, Guo Gan, Yitao Long, Zhiyuan Hu, Tongyan Hu, Weiyuan Chen, Chuhan Li, Junyang Song, Zhijian Xu, Chengye Wang, Weifeng Pan, Ziyao Shangguan, Xiangru Tang, Zhenwen Liang, Yixin Liu, Chen Zhao, Arman Cohan",
        "摘要": "摘要：我们介绍了MMVU，这是一个用于评估视频理解基础模型的综合性专家级多学科基准。MMVU包括3000个专家注释的问题，涵盖科学、医疗保健、人文学科与社会科学、工程学四个核心学科的27个主题。与之前的基准相比，MMVU具有三个关键进步。首先，它挑战模型应用领域特定知识并执行专家级推理来分析专门领域的视频，而不仅仅是当前视频基准中通常评估的基础视觉感知。其次，每个示例均由人类专家从头注释。我们实施严格的数据质量控制以确保数据集的高质量。最后，每个示例都配有专家注释的推理理由和相关领域知识，促进深入分析。我们对32个前沿多模态基础模型在MMVU上的表现进行了广泛评估。最新的System-2-capable模型，o1和Gemini 2.0 Flash Thinking，在测试模型中表现最佳。然而，它们仍未能匹及人类专家的水平。通过深入的错误分析和案例研究，我们为未来在专门领域的专家级知识密集型视频理解的进步提供了可操作的见解。\n\n链接：https://arxiv.org/pdf/2501.12380.pdf",
        "地址": "https://arxiv.org/pdf/2501.12380.pdf"
    },
    {
        "名称": "2025 [2501.11425] Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training.pdf",
        "作者": "Siyu Yuan, Zehui Chen, Zhiheng Xi, Junjie Ye, Zhengyin Du, Jiecao Chen",
        "摘要": "以下是从提供的学术论文材料中提取出的摘要，并翻译成中文：\n\n摘要: 大型语言模型(LLMs)代理在解决互动环境中的复杂任务时越来越关键。现有工作主要集中在通过从更强的专家那里复制行为来提升性能，但这种方法在现实世界应用中常常出问题，主要是由于无法从错误中恢复。然而，步骤级别的批判数据难以且昂贵采集。因此，自动且动态地构建自我批判数据集对于增强模型的智能代理能力至关重要。在这项工作中，我们提出了一个迭代自我训练框架，Agent-R，使语言代理能够实时反思。不同于传统方法根据正确性奖励或惩罚行动，Agent-R利用蒙特卡罗树搜索(MCTS)构建训练数据，从错误轨迹中恢复出正确轨迹。代理反思的一个关键挑战在于需要及时修改，而不是等待整个过程结束。为此，我们引入了一种由模型引导的批判构建机制：演员模型在失败轨迹中识别出第一个错误步骤（在其当前能力范围内）。从这个步骤开始，我们将其与具有相同父节点的相邻正确路径拼接。这一策略使得模型能够基于其当前策略学习反思，因此提高了学习效率。为了进一步探讨这种自我改进范式的可扩展性，我们研究了错误纠正能力和数据集构建的迭代优化。我们的研究结果表明，Agent-R持续提升了模型从错误中恢复的能力，并实现了及时错误纠正。在三个互动环境中的实验表明，Agent-R有效地使代理纠正错误行动，同时避免陷入循环，性能相比基础方法提高了5.59%。\n\n作者: 袁思宇、陈泽辉、席治恒、叶俊杰、杜正尹、陈捷操\n\n论文网址: [https://arxiv.org/pdf/2501.11425.pdf](https://arxiv.org/pdf/2501.11425.pdf) \n\n标题: 2025 [2501.11425] Agent-R: 通过迭代自我训练实现语言模型代理反思的训练",
        "地址": "https://arxiv.org/pdf/2501.11425.pdf"
    },
    {
        "名称": "2025 [2501.11873] Demons in the Detail: On Implementing Load Balancing Loss for Training Specialized Mixture-of-Expert Models.pdf",
        "作者": "Zihan Qiu, Zeyu Huang, Bo Zheng, Kaiyue Wen, Zekun Wang, Rui Men, Ivan Titov, Dayiheng Liu, Jingren Zhou, Junyang Lin",
        "摘要": "摘要：本文重新审视了在训练混合专家(Mixtures-of-Experts, MoEs)模型时，实施负载平衡损失（Load-balancing Loss, LBL）的方法。具体而言，MoEs 的 LBL 被定义为 $N_E \\sum_{i=1}^{N_E} f_i p_i$，其中 $N_E$ 是专家的总数，$f_i$ 表示专家 $i$ 被选中的频率，$p_i$ 是专家 $i$ 的平均门控分数。现有的 MoE 训练框架通常采用并行训练策略，因此在计算 $f_i$ 和 LBL 时，会在一个小批次（micro-batch）内进行计算，然后在并行组间进行平均。实际上，用于训练数十亿参数规模的大型语言模型（LLMs）的微型批次通常包含很少的序列，因此微型批次的 LBL 几乎在序列级应用，并且路由器会将每个序列中的标记平均分配到所有专家。在这种严格限制下，即使是特定领域序列（如代码）中的标记也会均匀地路由到所有专家，从而抑制了专家专精化。在这项工作中，我们提出使用全局批次（global-batch）来计算 LBL，以放松这一限制。由于全局批次包含的序列比微型批次更多样化，这将鼓励在语料库级别实现负载平衡。具体来说，我们引入了一个额外的通信步骤，以在微型批次之间同步 $f_i$，然后使用它来计算 LBL。通过在基于 MoEs 的 LLMs（总参数达 42.8B 和 400B 标记）上的实验，我们惊讶地发现，全局批次 LBL 策略在预训练困惑度和下游任务中都带来了显著的性能提升。我们的分析表明，全局批次 LBL 也大大提高了 MoE 专家的领域专精化能力。",
        "地址": "https://arxiv.org/pdf/2501.11873.pdf"
    },
    {
        "名称": "2025 [2501.12326] UI-TARS: Pioneering Automated GUI Interaction with Native Agents.pdf",
        "作者": "Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, Wanjun Zhong, Kuanye Li, Jiale Yang, Yu Miao, Woyu Lin, Longxiang Liu, Xu Jiang, Qianli Ma, Jingyu Li, Xiaojun Xiao, Kai Cai, Chuang Li, Yaowei Zheng, Chaolin Jin, Chen Li, Xiao Zhou, Minchao Wang, Haoli Chen, Zhaojian Li, Haihua Yang, Haifeng Liu, Feng Lin, Tao Peng, Xin Liu, Guang Shi",
        "摘要": "摘要：本文介绍了一种名为UI-TARS的本地GUI代理模型，该模型仅通过感知屏幕截图作为输入，并执行类似人类的交互操作（如键盘和鼠标操作）。与依赖于经过复杂包装的商业模型（如GPT-4o）并需要专家精心设计提示和工作流程的当前流行代理框架不同，UI-TARS是一个端到端模型，性能优于这些复杂的框架。实验表明其卓越的性能：在10多个评估感知、定位和GUI任务执行的代理基准测试中，UI-TARS均达到了最先进的性能。值得注意的是，在OSWorld基准测试中，UI-TARS使用50步获得24.6分，使用15步获得22.7分，优于Claude（分别为22.0和14.9分）。在AndroidWorld中，UI-TARS获得了46.6分，超过了GPT-4o（34.5分）。UI-TARS包含以下几个关键创新：（1）增强的感知能力：利用大量GUI屏幕截图数据集来进行上下文感知的UI元素理解和精确的描述；（2）统一动作模型，将跨平台的动作标准化，并通过大规模的动作轨迹实现精确的定位和交互；（3）系统2推理，在多步决策过程中融入深思熟虑的推理，包括任务分解、反思性思维、里程碑识别等多种推理模式；（4）通过反思性在线轨迹进行迭代训练，自动收集、过滤并反思性地改进数百台虚拟机上的新交互轨迹，从而解决了数据瓶颈问题。通过迭代训练和反思调优，UI-TARS能不断从错误中学习，并以最少的人为干预适应突发情况。我们还分析了GUI代理的演变路径，以指导该领域的进一步发展。",
        "地址": "https://arxiv.org/pdf/2501.12326.pdf"
    },
    {
        "名称": "2025 [2501.12224] TokenVerse: Versatile Multi-concept Personalization in Token Modulation Space.pdf",
        "作者": "Daniel Garibi, Shahar Yadin, Roni Paiss, Omer Tov, Shiran Zada, Ariel Ephrat, Tomer Michaeli, Inbar Mosseri, Tali Dekel",
        "摘要": "摘要：我们提出了TokenVerse——一种多概念个性化的方法，利用预训练的文本到图像扩散模型。我们的框架可以从单个图像中解开复杂的视觉元素和属性，同时能够无缝插入和生成从多个图像中提取的概念组合。与现有工作相比，TokenVerse可以处理每个具有多个概念的多个图像，并支持广泛的概念，包括物体、配饰、材料、姿势和照明。我们的工作利用了基于DiT的文本到图像模型，其中输入文本通过注意力和调制（移动和缩放）影响生成。我们观察到，调制空间是语义化的，并且可以对复杂概念进行局部控制。基于这一见解，我们设计了一个基于优化的框架，该框架以图像和文本描述为输入，为每个单词在调制空间中找到一个独特的方向。这些方向可以用来生成新的图像，以所需的配置组合学习到的概念。我们在具有挑战性的个性化环境中展示了TokenVerse的有效性，并展示了其相对于现有方法的优势。",
        "地址": "https://arxiv.org/pdf/2501.12224.pdf"
    },
    {
        "名称": "2025 [2501.12368] InternLM-XComposer2.5-Reward: A Simple Yet Effective Multi-Modal Reward Model.pdf",
        "作者": "Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Ziyu Liu, Shengyuan Ding, Shenxi Wu, Yubo Ma, Haodong Duan, Wenwei Zhang, Kai Chen, Dahua Lin, Jiaqi Wang",
        "摘要": "摘要：尽管大型视觉语言模型（LVLMs）在视觉理解方面表现出色，但它们有时会生成错误的输出。虽然利用强化学习或测试时缩放的奖励模型（RMs）有提高生成质量的潜力，但存在一个关键的差距：公开可用的多模态LVLMs奖励模型稀缺，专有模型的实现细节通常不明确。我们通过InternLM-XComposer2.5-Reward（IXC-2.5-Reward）填补了这一空白，这是一种简单但有效的多模态奖励模型，可以使LVLMs与人类偏好一致。为了确保IXC-2.5-Reward的鲁棒性和多功能性，我们建立了一个高质量的多模态偏好语料库，涵盖了不同领域的文本、图像和视频输入，如指令遵循、通用理解、富文本文档、数学推理和视频理解。IXC-2.5-Reward在最新的多模态奖励模型基准上取得了优异的成绩，并在仅文本奖励模型基准上表现出竞争力。我们进一步展示了IXC-2.5-Reward的三种关键应用：（1）提供RL训练的监督信号。我们将IXC-2.5-Reward与近端策略优化（PPO）相结合，产生的IXC-2.5-Chat在指令遵循和多模态开放式对话中表现出一致的改进；（2）在测试时缩放中从候选响应中选择最佳响应；（3）从现有的图像和视频指令调优训练数据中过滤异常或噪声样本。为了确保可重复性并促进进一步研究，我们已在该https URL上开源了所有模型权重和训练配方。",
        "地址": "https://arxiv.org/pdf/2501.12368.pdf"
    },
    {
        "名称": "2025 [2501.11733] Mobile-Agent-E: Self-Evolving Mobile Assistant for Complex Tasks.pdf",
        "作者": "Zhenhailong Wang, Haiyang Xu, Junyang Wang, Xi Zhang, Ming Yan, Ji Zhang, Fei Huang, Heng Ji",
        "摘要": "摘要：智能手机在现代生活中变得不可或缺，但在移动设备上执行复杂任务仍然常常令人感到沮丧。基于大型多模态模型（LMM）的移动代理的最新进展展示了在移动环境中感知和行动的能力。然而，当前的方法存在重大局限：不能满足真实世界的人类需求，在推理密集型和长期任务方面表现欠佳，且缺乏从先前经验中学习和改进的机制。为克服这些挑战，我们推出了Mobile-Agent-E，这是一种能够通过过去经验自我进化的分层多代理框架。所谓分层，是指高层规划与低层行动执行的明确分离。该框架包括一个负责制定整体计划的经理，通过将复杂任务分解为子目标，以及四个下属代理--感知器、操作员、行动反射器和记录员--分别处理细粒度的视觉感知、即时行动执行、错误验证和信息聚合。Mobile-Agent-E还具有一种新颖的自我进化模块，它维护持久的长期记忆，包括提示和快捷操作。提示是从先前任务中学习到的有关如何有效地与环境互动的一般指导和经验教训；快捷操作是可重用的、适用于特定子任务的原子操作序列。提示和快捷操作的引入有助于持续改进性能和效率。与该框架一起，我们推出了Mobile-Eval-E，这是一个新基准，包含需要长期、多应用互动的复杂移动任务。实验证明，Mobile-Agent-E在三个基础模型骨干上相较以往最先进的方法取得了22%的绝对改进。项目页面：此https URL。",
        "地址": "https://arxiv.org/pdf/2501.11733.pdf"
    },
    {
        "名称": "2025 [2501.12202] Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D Assets Generation.pdf",
        "作者": "Zibo Zhao, Zeqiang Lai, Qingxiang Lin, Yunfei Zhao, Haolin Liu, Shuhui Yang, Yifei Feng, Mingxin Yang, Sheng Zhang, Xianghui Yang, Huiwen Shi, Sicong Liu, Junta Wu, Yihang Lian, Fan Yang, Ruining Tang, Zebin He, Xinzhou Wang, Jian Liu, Xuhui Zuo, Zhuo Chen, Biwen Lei, Haohan Weng, Jing Xu, Yiling Zhu, Xinhai Liu, Lixin Xu, Changrong Hu, Tianyu Huang, Lifu Wang, Jihong Zhang, Meng Chen, Liang Dong, Yiwen Jia, Yulin Cai, Jiaao Yu, Yixuan Tang, Hao Zhang, Zheng Ye, Peng He, Runzhou Wu, Chao Zhang, Yonghao Tan, Jie Xiao, Yangyu Tao, Jianchen Zhu, Jinbao Xue, Kai Liu, Chongqing Zhao, Xinming Wu, Zhichao Hu, Lei Qin, Jianbing Peng, Zhan Li, Minghui Chen, Xipeng Zhang, Lin Niu, Paige Wang, Yingkai Wang, Haozhao Kuang, Zhongyi Fan, Xu Zheng, Weihao Zhuang, YingPing He, Tian Liu, Yong Yang, Di Wang, Yuhong Liu, Jie Jiang, Jingwei Huang, Chunchao Guo (refer to the report for detailed contributions)",
        "摘要": "摘要：我们介绍了Hunyuan3D 2.0，这是一种用于生成高分辨率纹理3D资产的先进的大规模3D合成系统。该系统包括两个基础组件：一个大规模的形状生成模型——Hunyuan3D-DiT和一个大规模的纹理合成模型——Hunyuan3D-Paint。形状生成模型基于可扩展的基于流的扩散变压器，旨在创建与给定条件图像适当对齐的几何结构，为下游应用奠定了坚实基础。纹理合成模型从强几何和扩散先验中受益，为生成或手工制作的网格提供高分辨率和生动的纹理贴图。此外，我们构建了Hunyuan3D-Studio，这是一个多功能、用户友好的生产平台，简化了3D资产的再创造过程。它允许专业和业余用户高效地操作甚至动画化他们的网格。我们系统地评估了我们的模型，显示Hunyuan3D 2.0在几何细节、条件对齐、纹理质量等方面优于之前的最先进模型，包括开源和闭源模型。Hunyuan3D 2.0公开发布，旨在填补开源3D社区在大规模基础生成模型方面的空白。我们的模型代码和预训练权重可在此链接获取：this https URL。",
        "地址": "https://arxiv.org/pdf/2501.12202.pdf"
    },
    {
        "名称": "2025 [2501.10893] Learn-by-interact: A Data-Centric Framework for Self-Adaptive Agents in Realistic Environments.pdf",
        "作者": "Hongjin Su, Ruoxi Sun, Jinsung Yoon, Pengcheng Yin, Tao Yu, Sercan Ö. Arık",
        "摘要": "摘要: 由大型语言模型（LLMs）支持的自主代理有潜力增强人类的能力，从发送电子邮件到执行数据分析等数字任务提供帮助。然而，现有LLMs在这些任务上的能力通常由于缺乏与其交互环境相关的高质量代理数据而受到限制。我们提出了一个名为Learn-by-interact的数据中心框架，用于在没有人为标注的情况下使LLM代理适应任何给定环境。Learn-by-interact根据文档合成了代理-环境交互轨迹，并通过总结或提炼交互历史构建指令，这一过程称为反向构建。我们通过在训练场景和无训练场景中使用这些合成数据来评估其质量，其中我们为代理设计了优化的创新检索方法。在SWE-bench、WebArena、OSWorld和Spider2-V等现实编码、网络和桌面环境中的大量实验表明，Learn-by-interact在各种下游代理任务中的有效性——在使用Claude-3.5进行ICL时基线结果提高了最多12.2%，在使用Codestral-22B进行训练时提高了最多19.5%。我们进一步展示了反向构建的关键作用，训练时提高了最多14.0%。我们的消融研究表明，在ICL中我们合成数据所提供的效率，以及我们的检索管道优于传统的检索增强生成（RAG）等替代方法。随着LLMs越来越多地部署于现实环境中，我们预计Learn-by-interact将成为代理数据合成的基础。",
        "地址": "https://arxiv.org/pdf/2501.10893.pdf"
    },
    {
        "名称": "2025 [2501.11223] Reasoning Language Models: A Blueprint.pdf",
        "作者": "Maciej Besta, Julia Barth, Eric Schreiber, Ales Kubicek, Afonso Catarino, Robert Gerstenberger, Piotr Nyczyk, Patrick Iff, Yueling Li, Sam Houliston, Tomasz Sternal, Marcin Copik, Grzegorz Kwaśniewski, Jürgen Müller, Łukasz Flis, Hannes Eberhard, Hubert Niewiadomski, Torsten Hoefler",
        "摘要": "摘要：推理语言模型（RLMs），也被称为大型推理模型（LRMs），如OpenAI的o1和o3、DeepSeek-V3和阿里巴巴的QwQ，通过将大型语言模型（LLMs）扩展到高级推理机制，重新定义了AI解决问题的能力。然而，它们的高成本、专有性和复杂架构（独特地结合了强化学习（RL）、搜索启发式和LLMs）带来了可访问性和可扩展性挑战。为了解决这些问题，我们提出了一个综合性蓝图，将RLM组件组织成一个模块化框架，基于对所有RLM工作的调查和分析。这个蓝图融合了多种推理结构（链、树、图和嵌套形式）、推理策略（如蒙特卡洛树搜索、波束搜索）、RL概念（策略、价值模型等）、监督方案（基于结果和基于过程的监督）以及其他相关概念（如测试时计算、检索增强生成、代理工具）。我们提供了详细的数学公式和算法规范，以简化RLM的实施。通过展示LLaMA-Berry、QwQ、Journey Learning和Thoughts Graph等方案如何作为特例适应，我们展示了该蓝图的多功能性和统一潜力。为了说明其效用，我们介绍了x1，一个用于快速RLM原型设计和实验的模块化实现。使用x1和文献回顾，我们提供了关键见解，例如策略和价值模型的多阶段训练的必要性，以及熟悉训练分布的重要性。最后，我们讨论了可扩展的RLM云部署，并概述了RLM如何与更广泛的LLM生态系统集成。我们的工作揭示了RLM构建的神秘面纱，民主化了高级推理能力，并促进了创新，旨在通过降低RLM开发和实验的门槛来缩小“富AI”和“贫AI”之间的差距。",
        "地址": "https://arxiv.org/pdf/2501.11223.pdf"
    },
    {
        "名称": "2025 [2501.12375] Video Depth Anything: Consistent Depth Estimation for Super-Long Videos.pdf",
        "作者": "Sili Chen, Hengkai Guo, Shengnan Zhu, Feihu Zhang, Zilong Huang, Jiashi Feng, Bingyi Kang",
        "摘要": "摘要: Depth Anything在单目深度估计中取得了显著成功，具有很强的泛化能力。然而，它在视频中存在时间不一致性问题，阻碍了实际应用。多种方法被提出以利用视频生成模型或从光流和相机姿态中引入先验来缓解这一问题。然而，这些方法仅适用于10秒以内的短视频，并需在质量和计算效率之间进行权衡。我们提出了Video Depth Anything，以在极长视频（超过几分钟）中实现高质量、一致的深度估计而不牺牲效率。我们基于Depth Anything V2，并用高效的时空头替换其头部。我们通过约束时间深度梯度设计了一种简单且有效的时间一致性损失，不需要额外的几何先验。该模型在的视频深度和未标注图像的联合数据集上进行训练，类似于Depth Anything V2。此外，我们开发了一种新颖的基于关键帧的长视频推理策略。实验表明，我们的模型可以应用于任意长的视频而不影响质量、一致性或泛化能力。对多个视频基准的全面评估表明，我们的方法在零样本视频深度估计中设立了新的最先进水平。我们提供不同规模的模型以支持各种场景，我们最小的模型能够以30 FPS实现实时性能。\n\n翻译如下：\nDepth Anything表现出强大的深度估计能力，但在处理时间一致性方面存在挑战。我们提出了一种新方法Video Depth Anything，通过约束时间梯度来提高长视频深度估计的一致性，无需额外几何先验。该方法应用不同规模模型，最小模型实现实时性能。实验验证了在多种视频基准上，我们的方法达到了零样本深度估计的新高度。",
        "地址": "https://arxiv.org/pdf/2501.12375.pdf"
    },
    {
        "名称": "2025 [2501.08331] Go-with-the-Flow: Motion-Controllable Video Diffusion Models Using Real-Time Warped Noise.pdf",
        "作者": "Ryan Burgert, Yuancheng Xu, Wenqi Xian, Oliver Pilarski, Pascal Clausen, Mingming He, Li Ma, Yitong Deng, Lingxiao Li, Mohsen Mousavi, Michael Ryoo, Paul Debevec, Ning Yu",
        "摘要": "摘要：生成建模旨在将随机噪声转换为结构化输出。在这项工作中，我们通过利用结构化潜噪采样来允许运动控制，从而改进了视频扩散模型。这是通过数据的更改来实现的：我们对训练视频进行预处理，以产生结构化噪声。因此，我们的方法与扩散模型设计无关，不需要更改模型结构或训练流程。具体来说，我们提出了一种新颖的噪声扭曲算法，该算法快速到可以实时运行，通过光流场导出的相关扭曲噪声取代随机的时间高斯性，同时保持空间高斯性。我们的算法效率使我们能够以最小的开销对现代视频扩散基础模型进行微调，并提供一站式解决方案，涵盖广泛的用户友好型运动控制：局部物体运动控制、全局相机运动控制和运动转移。我们的扭曲噪声在时间一致性和空间高斯性之间的协调，实现了有效的运动控制，同时保持每帧像素质量。广泛的实验和用户研究证明了我们方法的优点，使其成为一种在视频扩散模型中控制运动的强大且可扩展的解决方案。视频结果可在我们的网站上查看：this https URL。源代码和模型检查点可在GitHub上获取：this https URL。",
        "地址": "https://arxiv.org/pdf/2501.08331.pdf"
    },
    {
        "名称": "2025 [2501.12273] Condor: Enhance LLM Alignment with Knowledge-Driven Data Synthesis and Refinement.pdf",
        "作者": "Maosong Cao, Taolin Zhang, Mo Li, Chuyu Zhang, Yunxin Liu, Haodong Duan, Songyang Zhang, Kai Chen",
        "摘要": "摘要：监督微调 (SFT) 数据的质量对于增强大型语言模型 (LLMs) 的对话能力起着至关重要的作用。然而，随着LLMs变得更加先进，高质量的人类标注SFT数据的可用性已成为一个重要的瓶颈，因此必须更多地依赖合成训练数据。在这项工作中，我们介绍了Condor，这是一种新的两阶段合成数据生成框架，它结合了世界知识树和自我反思细化，以大规模生成高质量的SFT数据。我们的实验结果表明，仅在20000个Condor生成样本上微调的基础模型相比其他模型表现更优。Condor中的额外细化阶段进一步使LLMs在各种规模（高达72B）上能够进行迭代自我改进，验证了我们方法的有效性。此外，我们对合成数据在后期训练中的扩展进行了调查，结果显示其在性能改进方面具有巨大的未开发潜力，为未来的研究开辟了有希望的前景。",
        "地址": "https://arxiv.org/pdf/2501.12273.pdf"
    },
    {
        "名称": "2025 [2501.12390] GPS as a Control Signal for Image Generation.pdf",
        "作者": "Chao Feng, Ziyang Chen, Aleksander Holynski, Alexei A. Efros, Andrew Owens",
        "摘要": "摘要: 我们展示了照片元数据中的 GPS 标签可以作为图像生成的有用控制信号。我们训练了 GPS 到图像的模型，并将其用于需要细粒度理解的任务，特别是在城市内图像变化的任务上。具体来说，我们训练了一个扩散模型，使其生成的图像基于 GPS 和文本进行调节。该模型生成的图像准确捕捉了不同社区、公园和地标的独特外观。我们还通过评分蒸馏采样从 2D GPS 到图像模型中提取了 3D 模型，使用 GPS 条件来约束从每个视点看到的重建外观。我们的评估表明，我们的 GPS 条件模型成功地学习了如何生成基于位置变化的图像，且 GPS 条件改善了估计的 3D 结构。",
        "地址": "https://arxiv.org/pdf/2501.12390.pdf"
    },
    {
        "名称": "2025 [2501.10687] EMO2: End-Effector Guided Audio-Driven Avatar Video Generation.pdf",
        "作者": "Linrui Tian, Siqi Hu, Qi Wang, Bang Zhang, Liefeng Bo",
        "摘要": "摘要：本文提出了一种新颖的音频驱动的谈话头方法，能够同时生成高度表现力的面部表情和手势动作。与现有方法专注于生成全身或半身姿态不同，我们研究了共语手势生成的挑战，并确定了音频特征与全身手势之间较弱的对应关系是一个关键限制。为了解决这一问题，我们将任务重新定义为两阶段过程。在第一阶段，我们直接从音频输入生成手部姿势，利用音频信号与手部动作之间的强相关性。在第二阶段，我们采用扩散模型合成视频帧，结合第一阶段生成的手部姿势，生成逼真的面部表情和身体动作。实验结果表明，所提方法在视觉质量和同步准确性方面均优于最先进的方法（如CyberHost和Vlogger）。这项工作为音频驱动的手势生成提供了一个新视角，并为创建表现力和自然的谈话头动画提供了一个稳健的框架。\n\n作者：田林锐，胡思齐，王琪，张邦，薄礼峰\n\n链接: [https://arxiv.org/pdf/2501.10687.pdf](https://arxiv.org/pdf/2501.10687.pdf)\n\n标题：2025 [2501.10687] EMO2: 由末端效应器引导的音频驱动的头像视频生成",
        "地址": "https://arxiv.org/pdf/2501.10687.pdf"
    },
    {
        "名称": "2025 [2501.12389] Taming Teacher Forcing for Masked Autoregressive Video Generation.pdf",
        "作者": "Deyu Zhou, Quan Sun, Yuang Peng, Kun Yan, Runpei Dong, Duomin Wang, Zheng Ge, Nan Duan, Xiangyu Zhang, Lionel M. Ni, Heung-Yeung Shum",
        "摘要": "摘要：我们介绍了MAGI，这是一种混合视频生成框架，它结合了用于帧内生成的掩码建模与用于下一帧生成的因果建模。我们的关键创新是Complete Teacher Forcing（CTF），它将掩码帧置于完整观察帧的条件下，而不是掩码帧（即Masked Teacher Forcing，MTF），实现了从令牌级（补丁级）到帧级自回归生成的平滑过渡。CTF显著优于MTF，在首帧条件视频预测中，FVD分数提高了23%。为了解决诸如曝光偏差等问题，我们采用了有针对性的训练策略，建立了自回归视频生成的新基准。实验表明，MAGI可以生成超过100帧的长、连贯的视频序列，即使仅在16帧的训练下，仍然表现出了高质量视频生成的潜力。",
        "地址": "https://arxiv.org/pdf/2501.12389.pdf"
    },
    {
        "名称": "2025 [2501.10057] MSTS: A Multimodal Safety Test Suite for Vision-Language Models.pdf",
        "作者": "Paul Röttger, Giuseppe Attanasio, Felix Friedrich, Janis Goldzycher, Alicia Parrish, Rishabh Bhardwaj, Chiara Di Bonaventura, Roman Eng, Gaia El Khoury Geagea, Sujata Goswami, Jieun Han, Dirk Hovy, Seogyeong Jeong, Paloma Jeretič, Flor Miriam Plaza-del-Arco, Donya Rooein, Patrick Schramowski, Anastassia Shaitarova, Xudong Shen, Richard Willats, Andrea Zugarini, Bertie Vidgen",
        "摘要": "摘要：视觉语言模型（VLMs）可以处理图像和文本输入，正日益被集成到聊天助手和其他消费级人工智能应用中。然而，如果没有适当的安全措施，这些模型可能会提供有害建议（例如，如何自我伤害）或鼓励不安全行为（例如，吸毒）。尽管存在这些明显的危险，但迄今为止很少有工作评估VLM的安全性以及多模态输入所带来的新风险。为了解决这个问题，我们推出了MSTS，一个用于VLM的多模态安全测试套件。MSTS包括40个精细化的危险类别下的400个测试提示。每个测试提示由文本和图像组成，只有两者结合才能揭示其完全的不安全含义。通过MSTS，我们发现一些开放的VLM存在明显的安全问题。我们还发现有些VLM是偶然安全的，即因为它们甚至无法理解简单的测试提示。我们将MSTS翻译成十种语言，发现非英语提示增加了模型不安全响应的几率。我们还发现，在只有文本的测试中，模型的安全性较高，而多模态提示测试时则较低。最后，我们探讨了VLM安全评估的自动化，发现即使是最好的安全分类器也有所不足。",
        "地址": "https://arxiv.org/pdf/2501.10057.pdf"
    },
    {
        "名称": "2025 [2501.10573] The Geometry of Tokens in Internal Representations of Large Language Models.pdf",
        "作者": "Karthik Viswanathan, Yuri Gardinazzi, Giada Panerai, Alberto Cazzaniga, Matteo Biagetti",
        "摘要": "摘要：我们研究了在变压器模型中，符号嵌入的几何结构与它们在下一个符号预测中的作用之间的关系。这个联系的一个重要方面是使用了经验测度的概念，该概念编码了变压器层中的符号点云的分布，并在平均场相互作用图中推动符号表示的演变。我们使用内在维度、邻域重叠和余弦相似度等指标来观测这些层间的经验测度。为了验证我们的方法，我们将这些指标与一个打乱符号顺序的数据集进行比较，这破坏了句法和语义结构。我们的研究结果显示，符号嵌入的几何属性与下一个符号预测的交叉熵损失之间存在相关性，这意味着具有较高损失值的提示的符号在更高维空间中表示。",
        "地址": "https://arxiv.org/pdf/2501.10573.pdf"
    },
    {
        "名称": "2025 [2501.11900] Panoramic Interests: Stylistic-Content Aware Personalized Headline Generation.pdf",
        "作者": "Junhong Lian, Xiang Ao, Xinyu Liu, Yang Liu, Qing He",
        "摘要": "摘要：个性化新闻标题生成旨在为用户提供吸引眼球且符合其偏好的标题。目前的方法侧重于以用户为中心的内容偏好，但大多数方法忽视了多样化的风格偏好对于用户全景兴趣的重要性，从而导致个性化效果不佳。鉴于此，我们提出了一种新颖的风格-内容感知个性化标题生成（SCAPE）框架。SCAPE 在大型语言模型（LLM）的协作下，从标题中提取内容和风格特征，并通过基于对比学习的层次融合网络自适应地整合用户的长期和短期兴趣。通过将全景兴趣纳入标题生成器，SCAPE 在生成过程中反映用户的风格-内容偏好。在真实世界数据集PENS上进行的大量实验表明，SCAPE优于基线方法。\n\n翻译为中文的摘要：个性化新闻标题生成旨在为用户提供引人注目的、符合其偏好的标题。目前的方法专注于用户导向的内容偏好，但大多忽略了多样化的风格偏好是用户全景兴趣的组成部分，导致个性化效果不佳。为此，我们提出了一种新颖的风格-内容感知的个性化标题生成（SCAPE）框架。SCAPE 在大型语言模型（LLM）的协助下，从标题中提取内容及风格特征，并通过基于对比学习的层次融合网络自适应地整合用户的长期和短期兴趣。通过将全景兴趣纳入标题生成器，SCAPE 在生成过程中反映用户的风格-内容偏好。大量在真实世界数据集PENS上的实验显示，SCAPE 优于基线方法。",
        "地址": "https://arxiv.org/pdf/2501.11900.pdf"
    },
    {
        "名称": "2025 [2501.12206] Fixing Imbalanced Attention to Mitigate In-Context Hallucination of Large Vision-Language Model.pdf",
        "作者": "Kazi Hasan Ibn Arif, Sajib Acharjee Dip, Khizar Hussain, Lang Zhang, Chris Thomas",
        "摘要": "摘要： 大型视觉语言模型（LVLMs）在理解和描述视觉内容方面表现出色，在各种视觉语言任务中达到了最先进的性能。然而，这些模型经常表现出幻觉行为，即生成的描述中包含输入图像中不存在的对象或细节。我们的工作通过分析transformer层和头中的注意模式，调查了这一现象，揭示了幻觉通常源于深层视觉基础的渐进性退化。我们提出了一种新的注意力修改方法，该方法结合了选择性token加重和头部特定调制，以在生成过程中保持视觉基础。我们的方法引入了两个关键组件：（1）双流token选择机制，识别并优先处理局部信息和空间显著的视觉token；（2）一种注意力头部特定的调制策略，根据个别注意头的视觉敏感性差异放大视觉信息处理。通过对MSCOCO数据集的广泛实验，我们证明了与基准模型相比，我们的方法将幻觉率降低了最多62.3%，同时保持了可比的任务性能。我们的分析表明，在具有不同视觉敏感度的注意头之间有选择地调制token，可以显著改善视觉基础，而无需重新训练模型。",
        "地址": "https://arxiv.org/pdf/2501.12206.pdf"
    }
]