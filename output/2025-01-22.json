[
    {
        "名称": "2025 [2501.12380] MMVU: Measuring Expert-Level Multi-Discipline Video Understanding.pdf",
        "作者": "Yilun Zhao, Lujing Xie, Haowei Zhang, Guo Gan, Yitao Long, Zhiyuan Hu, Tongyan Hu, Weiyuan Chen, Chuhan Li, Junyang Song, Zhijian Xu, Chengye Wang, Weifeng Pan, Ziyao Shangguan, Xiangru Tang, Zhenwen Liang, Yixin Liu, Chen Zhao, Arman Cohan",
        "摘要": "摘要：本文介绍了MMVU，这是一种综合性专家级多学科基准，用于评估视频理解中的基础模型。MMVU包括3000个专家注释的问题，涵盖了科学、医疗、人文与社会科学以及工程四大核心学科的27个主题。与之前的基准相比，MMVU有三个主要进步。首先，它要求模型应用特定领域的知识并进行专家级推理来分析专业领域的视频，超越了目前视频基准中通常评估的基本视觉感知。其次，每个案例都是由人工专家从头注释的，我们实施了严格的数据质量控制以确保数据集的高质量。最后，每个案例都配有专家注释的推理依据和相关领域知识，便于深入分析。我们对32个前沿多模态基础模型在MMVU上的表现进行了广泛评估。最新的System-2-capable模型o1和Gemini 2.0 Flash Thinking在测试模型中表现最佳，但仍未达到人类专家的水平。通过深入的错误分析和案例研究，我们为未来在专业领域中的专家级知识密集型视频理解提供了可操作的见解。\n\n作者：Yilun Zhao, Lujing Xie, Haowei Zhang, Guo Gan, Yitao Long, Zhiyuan Hu, Tongyan Hu, Weiyuan Chen, Chuhan Li, Junyang Song, Zhijian Xu, Chengye Wang, Weifeng Pan, Ziyao Shangguan, Xiangru Tang, Zhenwen Liang, Yixin Liu, Chen Zhao, Arman Cohan\n\n链接：https://arxiv.org/pdf/2501.12380.pdf\n\n标题：2025 [2501.12380] MMVU: 测量专家级多学科视频理解",
        "地址": "https://arxiv.org/pdf/2501.12380.pdf"
    },
    {
        "名称": "2025 [2501.11425] Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training.pdf",
        "作者": "Siyu Yuan, Zehui Chen, Zhiheng Xi, Junjie Ye, Zhengyin Du, Jiecao Chen",
        "摘要": "摘要: 大型语言模型 (LLM) 代理对于解决交互环境中的复杂任务越来越重要。现有研究主要关注通过从更强大的专家那里复制行为来提高性能，但这些方法通常在现实应用中表现不佳，主要原因是无法从错误中恢复。然而，步骤级别的批评数据难以且昂贵收集。因此，自动化和动态构建自我批评数据集对于增强模型的智能代理能力至关重要。 在这项工作中，我们提出了一个迭代自我训练框架Agent-R，使语言代理能够即时反思。与传统方法根据正确性奖励或惩罚行为不同，Agent-R 利用蒙特卡洛树搜索 (MCTS) 构建训练数据，从错误的轨迹中恢复正确的轨迹。代理反思的一个关键挑战在于需要及时修正，而不是等到过程结束。为了解决这个问题，我们引入了一种模型引导的批评构建机制：演员模型识别出失败轨迹中的第一个错误步骤（在其当前能力范围内），并从该步骤开始，将其与树中共享相同父节点的相邻正确路径拼接。这种策略使模型能够根据其当前策略学习反思，从而提高学习效率。为了进一步探索这种自我改进范式的可扩展性，我们研究了错误纠正能力和数据集构建的迭代优化。我们的研究结果表明，Agent-R 能够持续提高模型从错误中恢复的能力，并实现及时的错误修正。在三个交互环境中的实验表明，Agent-R 有效地使代理能够纠正错误动作并避免循环，性能明显优于基线方法（+5.59%）。",
        "地址": "https://arxiv.org/pdf/2501.11425.pdf"
    },
    {
        "名称": "2025 [2501.11873] Demons in the Detail: On Implementing Load Balancing Loss for Training Specialized Mixture-of-Expert Models.pdf",
        "作者": "Zihan Qiu, Zeyu Huang, Bo Zheng, Kaiyue Wen, Zekun Wang, Rui Men, Ivan Titov, Dayiheng Liu, Jingren Zhou, Junyang Lin",
        "摘要": "摘要：本文重新探讨了在训练专家混合（Mixture-of-Experts，MoE）模型时实施负载平衡损失（Load-balancing Loss，LBL）的方法。具体地，MoE的LBL被定义为$N_E \\sum_{i=1}^{N_E} f_i p_i$，其中$N_E$代表专家总数，$f_i$表示专家i被选中的频率，而$p_i$表示专家i的平均门控评分。现有的MoE训练框架通常使用并行训练策略，因此在微批次（micro-batch）内计算$f_i$和LBL，然后在并行组间平均。实际上，用于训练数十亿规模大规模语言模型（LLMs）的微批次通常包含的序列非常少。因此，微批次LBL几乎在序列级别上，路由器被推向在每个序列内均匀分配tokens。在这种严格约束下，即使是来自特定领域序列（例如代码）的token也被均匀地路由到所有专家，从而抑制了专家的专业化。在这项工作中，我们提出使用全局批次（global-batch）来计算LBL，以放宽这一限制。因为全球批次包含比微批次多得多的不同序列，这将鼓励在语料库层面的负载平衡。具体地，我们引入了一个额外的通信步骤，以同步微批次间的$f_i$，然后使用它来计算LBL。通过在基于MoE的LLMs（总参数高达42.8亿和4000亿tokens）上的实验，我们惊讶地发现全局批次LBL策略在预训练困惑度和下游任务中都带来了出色的性能提升。我们的分析表明，全局批次LBL还大大提高了MoE专家的领域专业化。\n\n作者：邱子涵，黄泽宇，郑波，温凯悦，王泽锟，门睿，Ivan Titov，刘大恒，周靖人，林俊杨\n\n链接：https://arxiv.org/pdf/2501.11873.pdf",
        "地址": "https://arxiv.org/pdf/2501.11873.pdf"
    },
    {
        "名称": "2025 [2501.12326] UI-TARS: Pioneering Automated GUI Interaction with Native Agents.pdf",
        "作者": "Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, Wanjun Zhong, Kuanye Li, Jiale Yang, Yu Miao, Woyu Lin, Longxiang Liu, Xu Jiang, Qianli Ma, Jingyu Li, Xiaojun Xiao, Kai Cai, Chuang Li, Yaowei Zheng, Chaolin Jin, Chen Li, Xiao Zhou, Minchao Wang, Haoli Chen, Zhaojian Li, Haihua Yang, Haifeng Liu, Feng Lin, Tao Peng, Xin Liu, Guang Shi",
        "摘要": "摘要: 本文介绍了UI-TARS，这是一个本地GUI代理模型，它仅感知截图作为输入，并执行类似人类的交互（例如，键盘和鼠标操作）。与依赖于精心设计的提示和工作流程包装的商业模型（如GPT-4o）的流行代理框架不同，UI-TARS是一个端到端的模型，它优于这些复杂的框架。实验表明其优越性能：在10多个评估感知、基础和GUI任务执行的GUI代理基准测试中，UI-TARS取得了SOTA（最先进）性能。特别是在OSWorld基准测试中，UI-TARS在50步和15步的条件下分别取得了24.6和22.7的分数，超过了Claude（分别为22.0和14.9）。在AndroidWorld中，UI-TARS取得了46.6，超过了GPT-4o（34.5）。UI-TARS包含以下几个关键创新：（1）增强感知：利用GUI截图的大规模数据集进行上下文感知的UI元素理解和精确标注；（2）统一操作建模：将操作标准化为跨平台的统一空间，并通过大规模操作痕迹实现精确的基础和交互；（3）系统-2推理：在多步决策中引入有意推理，涉及任务分解、反思思维、里程碑识别等多种推理模式；（4）通过反思性在线痕迹进行迭代训练：通过在数百台虚拟机上自动收集、过滤和反思性地改进新的交互痕迹，解决数据瓶颈问题。通过迭代训练和反思调优，UI-TARS可以从错误中不断学习，并以最少的人工干预适应不可预见的情况。我们还分析了GUI代理的发展路径，以指导该领域的进一步发展。",
        "地址": "https://arxiv.org/pdf/2501.12326.pdf"
    },
    {
        "名称": "2025 [2501.12224] TokenVerse: Versatile Multi-concept Personalization in Token Modulation Space.pdf",
        "作者": "Daniel Garibi, Shahar Yadin, Roni Paiss, Omer Tov, Shiran Zada, Ariel Ephrat, Tomer Michaeli, Inbar Mosseri, Tali Dekel",
        "摘要": "摘要: 我们提出了TokenVerse，一种基于预训练文本到图像扩散模型的多概念个性化方法。我们的框架能够从单张图像中解析出复杂的视觉元素和属性，同时使得从多张图像中提取概念并进行组合生成成为可能。与现有工作相比，TokenVerse能够处理包含多个概念的多张图像，并支持广泛的概念，包括对象、配饰、材料、姿势和灯光。我们的工作利用了基于DiT的文本到图像模型，其中输入文本通过注意力和调制（移位和缩放）影响生成过程。我们观察到调制空间具有语义性，并且能够局部控制复杂概念。基于这一见解，我们设计了一个基于优化的框架，该框架以图像和文本描述作为输入，为每个词在调制空间中找到一个独特的方向。然后，这些方向可以用于生成包含期望配置的新图像。我们展示了TokenVerse在具有挑战性的个性化设置中的有效性，并展示了其相较于现有方法的优势。项目的网页在此HTTPS URL。",
        "地址": "https://arxiv.org/pdf/2501.12224.pdf"
    },
    {
        "名称": "2025 [2501.12368] InternLM-XComposer2.5-Reward: A Simple Yet Effective Multi-Modal Reward Model.pdf",
        "作者": "Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Ziyu Liu, Shengyuan Ding, Shenxi Wu, Yubo Ma, Haodong Duan, Wenwei Zhang, Kai Chen, Dahua Lin, Jiaqi Wang",
        "摘要": "摘要: 尽管大型视觉语言模型（LVLMs）在视觉理解方面表现出色，但它们偶尔会生成不正确的输出。虽然使用强化学习或测试时缩放的奖励模型（RMs）有提高生成质量的潜力，但一个关键的差距依然存在：公开可用的多模态RMs很少，而且专有模型的实现细节常常不清楚。我们通过InternLM-XComposer2.5-Reward（IXC-2.5-Reward）弥合这一鸿沟，这是一种简单但有效的多模态奖励模型，使LVLMs与人类偏好一致。为了确保IXC-2.5-Reward的鲁棒性和多样性，我们建立了一个高质量的多模态偏好语料库，涵盖文本、图像和视频输入，跨越多个领域，如指令跟随、一般理解、富文本文件、数学推理和视频理解。IXC-2.5-Reward在最新的多模态奖励模型基准上取得了优异成绩，并在纯文本奖励模型基准上表现出竞争力。我们进一步展示了IXC-2.5-Reward的三个关键应用：（1）为RL训练提供监督信号。我们将IXC-2.5-Reward与近端策略优化（PPO）集成，产生的IXC-2.5-Chat在指令跟随和多模态开放式对话中表现出持续改进；（2）从候选响应中选择最佳响应进行测试时缩放；（3）从现有图像和视频指令微调训练数据中过滤异常或噪声样本。为了确保可重复性并促进进一步研究，我们已在https URL开源了所有模型权重和训练配方。",
        "地址": "https://arxiv.org/pdf/2501.12368.pdf"
    },
    {
        "名称": "2025 [2501.11733] Mobile-Agent-E: Self-Evolving Mobile Assistant for Complex Tasks.pdf",
        "作者": "Zhenhailong Wang, Haiyang Xu, Junyang Wang, Xi Zhang, Ming Yan, Ji Zhang, Fei Huang, Heng Ji",
        "摘要": "摘要：智能手机在现代生活中已不可或缺，但在移动设备上执行复杂任务仍然充满挫折。基于大型多模态模型（LMM）的移动代理最近展示了在移动环境中感知和行动的能力。然而，目前的方法存在显著局限：无法满足现实世界的人类需求，难以处理需要推理的长期任务，并且缺乏从以往经验中学习和改进的机制。为了克服这些挑战，我们引入了 Mobile-Agent-E，这是一个能够通过过去经验实现自我进化的层次化多代理框架。“层次化”意味着高层规划和低层执行动作的明确分离。该框架包括一个经理负责通过将复杂任务分解为子目标来制定总体计划，以及四个辅助代理——感知器、操作器、动作反射器和笔记员，分别处理细化的视觉感知、即时动作执行、错误验证和信息聚合。Mobile-Agent-E还具有一个新颖的自我进化模块，它维护着由Tips（提示）和Shortcuts（捷径）组成的持久长期记忆。Tips是从先前任务中学到的一般指导和经验教训，关于如何有效地与环境互动。Shortcuts是为特定子程序量身定制的可重用的可执行原子操作序列。Tips和Shortcuts的加入促进了性能和效率的持续改进。除了这个框架，我们还引入了 Mobile-Eval-E，这是一个新的基准，包含需要长期多应用交互的复杂移动任务。实证结果显示，Mobile-Agent-E在三个基础模型骨干上比以前的最先进方法绝对提高了22%。项目页面：this https URL。\n\n作者：王振海龙、徐海洋、王俊阳、张希、闫明、张骥、黄飞、姬衡",
        "地址": "https://arxiv.org/pdf/2501.11733.pdf"
    },
    {
        "名称": "2025 [2501.12202] Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D Assets Generation.pdf",
        "作者": "Zibo Zhao, Zeqiang Lai, Qingxiang Lin, Yunfei Zhao, Haolin Liu, Shuhui Yang, Yifei Feng, Mingxin Yang, Sheng Zhang, Xianghui Yang, Huiwen Shi, Sicong Liu, Junta Wu, Yihang Lian, Fan Yang, Ruining Tang, Zebin He, Xinzhou Wang, Jian Liu, Xuhui Zuo, Zhuo Chen, Biwen Lei, Haohan Weng, Jing Xu, Yiling Zhu, Xinhai Liu, Lixin Xu, Changrong Hu, Tianyu Huang, Lifu Wang, Jihong Zhang, Meng Chen, Liang Dong, Yiwen Jia, Yulin Cai, Jiaao Yu, Yixuan Tang, Hao Zhang, Zheng Ye, Peng He, Runzhou Wu, Chao Zhang, Yonghao Tan, Jie Xiao, Yangyu Tao, Jianchen Zhu, Jinbao Xue, Kai Liu, Chongqing Zhao, Xinming Wu, Zhichao Hu, Lei Qin, Jianbing Peng, Zhan Li, Minghui Chen, Xipeng Zhang, Lin Niu, Paige Wang, Yingkai Wang, Haozhao Kuang, Zhongyi Fan, Xu Zheng, Weihao Zhuang, YingPing He, Tian Liu, Yong Yang, Di Wang, Yuhong Liu, Jie Jiang, Jingwei Huang, Chunchao Guo (refer to the report for detailed contributions)",
        "摘要": "摘要: 我们介绍了Hunyuan3D 2.0，一个用于生成高分辨率纹理3D资产的先进大规模3D合成系统。该系统包括两个基础组件：大规模形状生成模型——Hunyuan3D-DiT，以及大规模纹理合成模型——Hunyuan3D-Paint。形状生成模型基于可扩展的流动扩散变压器构建，旨在创建与给定条件图像适当对齐的几何结构，为下游应用奠定了坚实的基础。纹理合成模型受益于强大的几何和扩散先验，为生成的或手工制作的网格生成高分辨率和充满活力的纹理贴图。此外，我们创建了一个多功能、用户友好的生产平台——Hunyuan3D-Studio，简化了3D资产的再创作过程。它允许专业和业余用户高效地操作甚至动画化他们的网格。我们系统地评估了我们的模型，显示Hunyuan3D 2.0在几何细节、条件对齐、纹理质量等方面优于以前的最先进的模型，包括开源和闭源模型。Hunyuan3D 2.0公开发布，以填补开源3D社区中大规模基础生成模型的空白。我们的模型的代码和预训练权重可在: 此HTTPS URL获得。\n\n来源网址: https://arxiv.org/pdf/2501.12202.pdf",
        "地址": "https://arxiv.org/pdf/2501.12202.pdf"
    },
    {
        "名称": "2025 [2501.10893] Learn-by-interact: A Data-Centric Framework for Self-Adaptive Agents in Realistic Environments.pdf",
        "作者": "Hongjin Su, Ruoxi Sun, Jinsung Yoon, Pengcheng Yin, Tao Yu, Sercan Ö. Arık",
        "摘要": "摘要：由大型语言模型（LLMs）驱动的自主代理有望增强人类能力，帮助完成从发送电子邮件到执行数据分析的数字任务。现有LLMs在这些任务上的能力往往受限于缺乏来自其互动环境的高质量代理数据。我们提出了Learn-by-interact，这是一种数据中心框架，可以在没有人为注释的情况下将LLM代理适应于任何给定环境。Learn-by-interact基于文档合成代理-环境交互的轨迹，并通过总结或抽象互动历史来构建指令，这一过程称为反向构建。我们通过在训练场景和无训练的上下文学习（ICL）中使用这些数据来评估其质量，并在其中设计了优化的检索方法，以便于代理。我们在SWE-bench、WebArena、OSWorld和Spider2-V等跨越现实编码、网络和桌面环境的广泛实验表明，Learn-by-interact在各种下游任务中效果显著——基线结果在ICL中使用Claude-3.5提高了最多可达12.2%，在训练中使用Codestral-22B提高了最多可达19.5%。我们进一步展示了反向构建的关键作用，提供了最多可达14.0%的训练提升效果。我们的消融研究展示了我们合成数据在ICL中的高效性以及我们的检索管道优于传统检索增强生成（RAG）等替代方法的优越性。我们预计，随着LLMs越来越多地部署到现实世界环境中，Learn-by-interact将成为代理数据合成的基础。\n\n作者：Hongjin Su, Ruoxi Sun, Jinsung Yoon, Pengcheng Yin, Tao Yu, Sercan Ö. Arık\n\n链接：https://arxiv.org/pdf/2501.10893.pdf",
        "地址": "https://arxiv.org/pdf/2501.10893.pdf"
    },
    {
        "名称": "2025 [2501.11223] Reasoning Language Models: A Blueprint.pdf",
        "作者": "Maciej Besta, Julia Barth, Eric Schreiber, Ales Kubicek, Afonso Catarino, Robert Gerstenberger, Piotr Nyczyk, Patrick Iff, Yueling Li, Sam Houliston, Tomasz Sternal, Marcin Copik, Grzegorz Kwaśniewski, Jürgen Müller, Łukasz Flis, Hannes Eberhard, Hubert Niewiadomski, Torsten Hoefler",
        "摘要": "摘要: 推理语言模型（RLM），亦称为大规模推理模型（LRM），如OpenAI的o1和o3、DeepSeek-V3以及阿里巴巴的QwQ，通过扩展LLM并引入高级推理机制，重新定义了AI的解决问题能力。然而，其高昂的成本、专有的性质以及复杂的架构（独特地结合了强化学习（RL）、搜索启发式方法和LLM）带来了可及性和可扩展性挑战。为了解决这些问题，我们提出了一份综合蓝图，通过对所有RLM工作的调查和分析，将RLM组件组织成一个模块化框架。此蓝图包含了多种推理结构（链、树、图和嵌套形式）、推理策略（如蒙特卡罗树搜索，束搜索）、RL概念（策略、价值模型等）、监督方案（基于结果的监督和基于过程的监督）以及其他相关概念（如测试时间计算、检索增强生成、代理工具）。我们提供了详细的数学公式和算法规格以简化RLM的实现。通过展示如LLaMA-Berry、QwQ、Journey Learning和Thoughts的图如何作为特例适应，我们展示了该蓝图的多功能性和统一潜力。为了展示其实用性，我们介绍了x1，一个用于快速RLM原型和实验的模块化实现。利用x1和文献回顾，我们提供了关键见解，如策略和价值模型的多阶段训练，以及熟悉训练分布的重要性。最后，我们讨论了可扩展的RLM云部署，并概述了RLM如何与更广泛的LLM生态系统集成。我们的工作揭开了RLM构造的神秘面纱，使高级推理能力大众化，促进了创新，旨在通过降低RLM开发和实验的门槛来弥合“富AI”和“穷AI”之间的差距。",
        "地址": "https://arxiv.org/pdf/2501.11223.pdf"
    },
    {
        "名称": "2025 [2501.12375] Video Depth Anything: Consistent Depth Estimation for Super-Long Videos.pdf",
        "作者": "Sili Chen, Hengkai Guo, Shengnan Zhu, Feihu Zhang, Zilong Huang, Jiashi Feng, Bingyi Kang",
        "摘要": "摘要: Depth Anything在单目深度估计方面取得了显著成功，并具有很强的泛化能力。然而，在视频中，它存在时间上的不一致性，阻碍了其实际应用。各种方法已经被提出，通过利用视频生成模型或引入光流和相机姿态的先验信息来缓解这一问题。然而，这些方法仅适用于短视频（< 10秒），并且需要在质量和计算效率之间进行权衡。我们提出了Video Depth Anything，旨在实现超长视频（超过几分钟）的高质量、一致性深度估计，同时不牺牲效率。我们基于Depth Anything V2，并用高效的时空头替换其头部。我们设计了一种简单而有效的时间一致性损失，通过约束时间深度梯度，消除了对额外几何先验的需求。该模型在视频深度和未标注图像的联合数据集上进行训练，类似于Depth Anything V2。此外，还开发了一种基于关键帧的新策略用于长视频推理。实验表明，我们的模型可以应用于任意长的视频而不影响质量、一致性或泛化能力。对多个视频基准的全面评估表明，我们的方法在零样本视频深度估计方面设立了新的行业标准。我们提供了不同规模的模型以支持各种场景，我们的最小模型能够以30 FPS实现实时性能。",
        "地址": "https://arxiv.org/pdf/2501.12375.pdf"
    },
    {
        "名称": "2025 [2501.08331] Go-with-the-Flow: Motion-Controllable Video Diffusion Models Using Real-Time Warped Noise.pdf",
        "作者": "Ryan Burgert, Yuancheng Xu, Wenqi Xian, Oliver Pilarski, Pascal Clausen, Mingming He, Li Ma, Yitong Deng, Lingxiao Li, Mohsen Mousavi, Michael Ryoo, Paul Debevec, Ning Yu",
        "摘要": "摘要: 生成建模旨在将随机噪声转换为结构化输出。在这项工作中，我们通过结构化的潜在噪声采样实现了运动控制，从而增强了视频扩散模型。这仅通过更改数据来实现：我们对训练视频进行预处理以生成结构化噪声。 因此，我们的方法与扩散模型设计无关，不需要更改模型架构或训练管道。具体来说，我们提出了一种新颖的噪声扭曲算法，该算法足够快，可以实时运行，用光流场生成的相关扭曲噪声取代随机时间高斯噪声，同时保留空间高斯性。我们算法的高效性使我们能够使用扭曲噪声对现代视频扩散基础模型进行微调，并以最小的开销提供一站式的用户友好运动控制：局部对象运动控制，全局摄像机运动控制和运动转移。时间相干性和空间高斯性的和谐化使我们的扭曲噪声在保持每帧像素质量的同时实现了有效的运动控制。大量实验和用户研究展示了我们方法的优势，使其成为一种稳健且可扩展的方法，用于控制视频扩散模型中的运动。视频结果可在我们网页上观看。源代码和模型检查点可在 GitHub 上获得。\n\n作者: Ryan Burgert, Yuancheng Xu, Wenqi Xian, Oliver Pilarski, Pascal Clausen, Mingming He, Li Ma, Yitong Deng, Lingxiao Li, Mohsen Mousavi, Michael Ryoo, Paul Debevec, Ning Yu",
        "地址": "https://arxiv.org/pdf/2501.08331.pdf"
    },
    {
        "名称": "2025 [2501.12273] Condor: Enhance LLM Alignment with Knowledge-Driven Data Synthesis and Refinement.pdf",
        "作者": "Maosong Cao, Taolin Zhang, Mo Li, Chuyu Zhang, Yunxin Liu, Haodong Duan, Songyang Zhang, Kai Chen",
        "摘要": "摘要：监督微调（SFT）数据的质量在增强大型语言模型（LLMs）的对话能力方面起着关键作用。然而，随着LLMs变得更加先进，高质量人工注释的SFT数据的可用性已成为一个显著的瓶颈，因此对合成训练数据的依赖性更大。在这项工作中，我们介绍了Condor，这是一种新颖的两阶段合成数据生成框架，结合了世界知识树和自我反思优化，以大规模生成高质量的SFT数据。我们的实验结果表明，仅通过20K Condor生成样本进行微调的基础模型就表现优于其他模型。Condor中的额外优化阶段进一步使得LLMs在各种规模（最大到72B）上的迭代自我改进成为可能，验证了我们方法的有效性。此外，我们对合成数据在后训练中的扩展进行了研究，发现了显著的未探索潜力，有望在未来的研究中进一步提升性能。\n\n作者：曹茂松，张韬霖，李默，张楚玉，刘云新，段浩东，张松阳，陈凯\n\n评论：技术报告。Github：https网址\n\n链接：https://arxiv.org/pdf/2501.12273.pdf\n\n标题：2025 [2501.12273] Condor: 通过知识驱动的数据综合与优化增强LLM对齐度",
        "地址": "https://arxiv.org/pdf/2501.12273.pdf"
    },
    {
        "名称": "2025 [2501.12390] GPS as a Control Signal for Image Generation.pdf",
        "作者": "Chao Feng, Ziyang Chen, Aleksander Holynski, Alexei A. Efros, Andrew Owens",
        "摘要": "摘要：我们展示了照片元数据中包含的GPS标签可以作为图像生成的有用控制信号。我们训练了GPS到图像的模型，并将其用于需要细致理解城市内部图像变化的任务。特别是，我们训练了一个扩散模型，该模型在GPS和文本的条件下生成图像。学习到的模型生成的图像捕捉到了不同街区、公园和地标的独特外观。我们还通过得分蒸馏采样，从二维GPS到图像模型中提取三维模型，使用GPS条件来约束每个视点的重建外观。我们的评估表明，我们基于GPS条件的模型成功地学习了生成基于位置变化的图像，并且GPS条件提高了估计的三维结构。\n\n作者：Chao Feng, Ziyang Chen, Aleksander Holynski, Alexei A. Efros, Andrew Owens\n\n评论：项目页面：此HTTPS URL\n\n网址：https://arxiv.org/pdf/2501.12390.pdf\n\n标题：2025 [2501.12390] GPS作为图像生成的控制信号.pdf",
        "地址": "https://arxiv.org/pdf/2501.12390.pdf"
    },
    {
        "名称": "2025 [2501.10687] EMO2: End-Effector Guided Audio-Driven Avatar Video Generation.pdf",
        "作者": "Linrui Tian, Siqi Hu, Qi Wang, Bang Zhang, Liefeng Bo",
        "摘要": "摘要：本文提出了一种新颖的音频驱动的说话人动画生成方法，能够同时生成高度富有表现力的面部表情和手势。与现有方法集中于全身或半身姿势生成不同，我们研究了同步手势生成的挑战，发现音频特征与全身手势之间的弱对应关系是一个关键限制。为了解决这个问题，我们将任务重新定义为两个阶段的过程。在第一阶段，我们直接从音频输入生成手势，利用音频信号与手部动作的强相关性。在第二阶段，我们采用扩散模型来合成视频帧，结合第一阶段生成的手势来生成逼真的面部表情和身体动作。实验结果表明，所提出的方法在视觉质量和同步准确性方面均优于最新的方法，如CyberHost和Vlogger。该研究为音频驱动的手势生成提供了新的视角，并提供了一个可靠的框架用于创建富有表现力和自然的说话人动画。\n\n作者：田林瑞，胡思齐，王琦，张榜，薄立峰\n\n论文网址：https://arxiv.org/pdf/2501.10687.pdf\n\n标题：EMO2: 端执行器引导的音频驱动化身视频生成",
        "地址": "https://arxiv.org/pdf/2501.10687.pdf"
    },
    {
        "名称": "2025 [2501.12389] Taming Teacher Forcing for Masked Autoregressive Video Generation.pdf",
        "作者": "Deyu Zhou, Quan Sun, Yuang Peng, Kun Yan, Runpei Dong, Duomin Wang, Zheng Ge, Nan Duan, Xiangyu Zhang, Lionel M. Ni, Heung-Yeung Shum",
        "摘要": "摘要: 我们介绍了MAGI，这是一种混合视频生成框架，结合了用于帧内生成的掩码建模和用于下一帧生成的因果建模。我们的关键创新，完全教师强制(CTF)，使掩码帧以完整观察帧为条件，而不是掩码帧(即掩码教师强制MTF)，从而实现从令牌级(补丁级)到帧级自回归生成的平滑过渡。CTF显著优于MTF，在首帧条件视频预测上FVD分数提升了23%。为了解决曝光偏差等问题，我们采用了有针对性的训练策略，设立了自回归视频生成的新基准。实验表明，MAGI可以生成超过100帧的长且连贯的视频序列，即使只训练了16帧，也展示了其在可扩展和高质量视频生成方面的潜力。",
        "地址": "https://arxiv.org/pdf/2501.12389.pdf"
    },
    {
        "名称": "2025 [2501.10057] MSTS: A Multimodal Safety Test Suite for Vision-Language Models.pdf",
        "作者": "Paul Röttger, Giuseppe Attanasio, Felix Friedrich, Janis Goldzycher, Alicia Parrish, Rishabh Bhardwaj, Chiara Di Bonaventura, Roman Eng, Gaia El Khoury Geagea, Sujata Goswami, Jieun Han, Dirk Hovy, Seogyeong Jeong, Paloma Jeretič, Flor Miriam Plaza-del-Arco, Donya Rooein, Patrick Schramowski, Anastassia Shaitarova, Xudong Shen, Richard Willats, Andrea Zugarini, Bertie Vidgen",
        "摘要": "摘要：视觉语言模型（VLMs）处理图像和文本输入，越来越多地集成到聊天助手和其他消费类人工智能应用中。然而，如果没有适当的保障措施，VLM可能会给出有害的建议（例如，如何自残）或鼓励不安全的行为（例如，吸毒）。尽管存在这些明显的危险，但迄今为止很少有工作评估VLM的安全性以及多模态输入所带来的新风险。为了解决这一差距，我们引入了MSTS，一种用于VLM的多模态安全测试套件。MSTS包含400个测试提示，分为40个细粒度的危险类别。每个测试提示由一段文字和一张图片组成，只有结合在一起才能揭示其完整的不安全含义。通过使用MSTS，我们发现了几个开放的VLM中存在明显的安全问题。我们还发现有些VLM偶然是安全的，因为它们甚至无法理解简单的测试提示。我们将MSTS翻译成十种语言，发现非英语提示增加了模型不安全响应的比例。同时，我们还发现仅使用文本测试比多模态提示测试更安全。最后，我们探索了VLM安全评估的自动化，发现即使是最好的安全分类器也有所不足。\n\n翻译为中文。",
        "地址": "https://arxiv.org/pdf/2501.10057.pdf"
    },
    {
        "名称": "2025 [2501.10573] The Geometry of Tokens in Internal Representations of Large Language Models.pdf",
        "作者": "Karthik Viswanathan, Yuri Gardinazzi, Giada Panerai, Alberto Cazzaniga, Matteo Biagetti",
        "摘要": "这篇论文研究了token嵌入的几何形状及其在变压器模型中预测下一个token的作用。该连接的一个重要方面是使用了经验测度的概念，该概念在变压器层中编码了token点云的分布，并推动了mean-field相互作用下token表示的演变。我们使用了诸如内在维度、邻域重叠和余弦相似度等指标来观察这些经验测度在各层之间的变化。为了验证我们的方法，我们将这些指标与一个打乱了token顺序的数据集进行比较，这破坏了句法和语义结构。我们的研究结果揭示了token嵌入的几何特性与下一个token预测的交叉熵损失之间的相关性，这意味着交叉熵损失值较高的提示词中的token表示在更高维的空间中。",
        "地址": "https://arxiv.org/pdf/2501.10573.pdf"
    },
    {
        "名称": "2025 [2501.11900] Panoramic Interests: Stylistic-Content Aware Personalized Headline Generation.pdf",
        "作者": "Junhong Lian, Xiang Ao, Xinyu Liu, Yang Liu, Qing He",
        "摘要": "摘要：个性化新闻标题生成旨在为用户提供引人注目的标题，这些标题符合其偏好。目前的方法主要关注用户导向的内容偏好，但大多数忽视了用户全方位兴趣中多样的风格偏好，导致个性化效果不佳。鉴于此，我们提出了一种新颖的风格内容感知个性化标题生成（SCAPE）框架。SCAPE 借助大型语言模型（LLM）的协作，从标题中提取内容和风格特征。它还通过基于对比学习的层次融合网络自适应地整合用户的长期和短期兴趣。通过将全方位的兴趣融入标题生成器，SCAPE 在生成过程中反映用户的风格内容偏好。在真实世界数据集 PENS 上进行的大量实验表明，SCAPE 相较于基准方法具有优越性。",
        "地址": "https://arxiv.org/pdf/2501.11900.pdf"
    },
    {
        "名称": "2025 [2501.12206] Fixing Imbalanced Attention to Mitigate In-Context Hallucination of Large Vision-Language Model.pdf",
        "作者": "Kazi Hasan Ibn Arif, Sajib Acharjee Dip, Khizar Hussain, Lang Zhang, Chris Thomas",
        "摘要": "摘要：大型视觉语言模型（LVLMs）在理解和描述视觉内容方面展现了显著的能力，在各种视觉语言任务中实现了最先进的性能。然而，这些模型经常表现出幻觉行为，即生成的描述中包含在输入图像中不存在的对象或细节。我们的工作通过分析变压器层和头部的注意模式研究了这一现象，揭示了幻觉通常源于深层视觉基础的逐步退化。我们提出了一种新颖的注意力修改方法，该方法结合选择性标记强调和头部特定调制，以在生成过程中保持视觉基础。我们的方法引入了两个关键组件：（1）一种双流标记选择机制，用于识别和优先考虑本地信息丰富和空间上重要的视觉标记；（2）一种针对注意力头部的特定调制策略，根据单个注意力头部的视觉敏感度来差异性地增强视觉信息处理。通过对MSCOCO数据集的大量实验，我们证明了与基线模型相比，我们的方法将幻觉率降低了多达62.3\\\\%，同时保持了可比的任务性能。我们的分析表明，对具有不同视觉敏感度的注意力头部选择性地调制标记可以显著改善视觉基础，而无需重新训练模型。",
        "地址": "https://arxiv.org/pdf/2501.12206.pdf"
    }
]