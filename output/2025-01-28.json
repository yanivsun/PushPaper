[
    {
        "名称": "2025 [2501.15368] Baichuan-Omni-1.5 Technical Report.pdf",
        "作者": "Yadong Li, Jun Liu, Tao Zhang, Tao Zhang, Song Chen, Tianpeng Li, Zehuan Li, Lijun Liu, Lingfeng Ming, Guosheng Dong, Da Pan, Chong Li, Yuanbo Fang, Dongdong Kuang, Mingrui Wang, Chenglin Zhu, Youwei Zhang, Hongyu Guo, Fengyu Zhang, Yuran Wang, Bowen Ding, Wei Song, Xu Li, Yuqi Huo, Zheng Liang, Shusen Zhang, Xin Wu, Shuai Zhao, Linchu Xiong, Yozhen Wu, Jiahui Ye, Wenhao Lu, Bowen Li, Yan Zhang, Yaqi Zhou, Xin Chen, Lei Su, Hongda Zhang, Fuzhong Chen, Xuezhen Dong, Na Nie, Zhiying Wu, Bin Xiao, Ting Li, Shunya Dang, Ping Zhang, Yijia Sun, Jincheng Wu, Jinjie Yang, Xionghai Lin, Zhi Ma, Kegeng Wu, Jia li, Aiyuan Yang, Hui Liu, Jianqiang Zhang, Xiaoxi Chen, Guangwei Ai, Wentao Zhang, Yicong Chen, Xiaoqin Huang, Kun Li, Wenjing Luo, Yifei Duan, Lingling Zhu, Ran Xiao, Zhe Su, Jiani Pu, Dian Wang, Xu Jia, Tianyu Zhang, Mengyu Ai, Mang Wang, Yujing Qiao, Lei Zhang, Yanjun Shen, Fan Yang, Miao Zhen, Yijie Zhou, Mingyang Chen, Fei Li, Chenzheng Zhu, Keer Lu, Yaqi Zhao, Hao Liang, Youquan Li, Yanzhao Qin, Linzhuang Sun, Jianhua Xu, Haoze Sun, Mingan Lin, Zenan Zhou, Weipeng Chen",
        "摘要": "摘要: 我们介绍了一种全模态模型Baichuan-Omni-1.5，它不仅具备全模态理解能力，还提供端到端的音频生成能力。为了在不影响任意模态能力的前提下实现流畅且高质量的跨模态互动，我们优先优化了三个关键方面。首先，我们建立了一个全面的数据清洗和合成管道，用于多模态数据，获取了约500B高质量数据（文本、音频和视觉）。其次，我们设计了一种音频标记器（Baichuan-Audio-Tokenizer），能够从音频中捕捉语义和声学信息，实现与多模态语言模型（MLLM）的无缝集成和增强兼容性。最后，我们设计了一种多阶段训练策略，逐步整合多模态对齐和多任务微调，确保所有模态的有效协同。Baichuan-Omni-1.5在综合全模态能力方面领先于当代模型（包括GPT4o-mini和MiniCPM-o 2.6）。值得注意的是，它在各种多模态医学基准测试中达到了与Qwen2-VL-72B等领先模型相当的结果。",
        "地址": "https://arxiv.org/pdf/2501.15368.pdf"
    },
    {
        "名称": "2025 [2501.15383] Qwen2.5-1M Technical Report.pdf",
        "作者": "An Yang, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoyan Huang, Jiandong Jiang, Jianhong Tu, Jianwei Zhang, Jingren Zhou, Junyang Lin, Kai Dang, Kexin Yang, Le Yu, Mei Li, Minmin Sun, Qin Zhu, Rui Men, Tao He, Weijia Xu, Wenbiao Yin, Wenyuan Yu, Xiafei Qiu, Xingzhang Ren, Xinlong Yang, Yong Li, Zhiying Xu, Zipeng Zhang",
        "摘要": "摘要：我们介绍了Qwen2.5-1M系列模型，将上下文长度扩展至100万个标记。与之前的128K版本相比，Qwen2.5-1M系列通过长上下文预训练和后训练显著增强了长上下文能力。采用了长数据合成、渐进式预训练和多阶段监督微调等关键技术，有效提升了长上下文性能，同时降低了训练成本。\n\n为推广长上下文模型在更广泛用户中的使用，我们提出并开源了我们的推理框架。该框架包含一种长度外推方法，能够在无需额外训练的情况下将模型上下文长度扩展至少四倍甚至更多。为减少推理成本，我们在部署场景中实施了稀疏注意方法和分块预填充优化，并通过稀疏性精炼方法提高精度。此外，我们详细介绍了推理引擎中的优化，包括内核优化、流水线并行和调度优化，这显著提升了整体推理性能。通过利用我们的推理框架，Qwen2.5-1M模型在包含100万个标记的上下文情境中实现了3倍至7倍的预填充速度提升。该框架为需要长上下文处理的应用程序开发提供了一种高效且强大的解决方案。\n\nQwen2.5-1M系列目前包括开源模型Qwen2.5-7B-Instruct-1M和Qwen2.5-14B-Instruct-1M，以及API访问的模型Qwen2.5-Turbo。评估结果显示，Qwen2.5-1M模型在长上下文任务上有了极大改进，而在短上下文场景中性能没有受到影响。特别是，Qwen2.5-14B-Instruct-1M模型在长上下文任务中显著优于GPT-4o-mini，并支持八倍于其的上下文长度。",
        "地址": "https://arxiv.org/pdf/2501.15383.pdf"
    },
    {
        "名称": "2025 [2501.16142] Towards General-Purpose Model-Free Reinforcement Learning.pdf",
        "作者": "Scott Fujimoto, Pierluca D'Oro, Amy Zhang, Yuandong Tian, Michael Rabbat",
        "摘要": "摘要：强化学习（RL）承诺提供一个几乎通用的解决问题框架。然而，在实际应用中，RL算法通常针对特定的基准进行了调整，依赖于精心调校的超参数和算法选择。最近，强大的基于模型的RL方法在各个基准上展示了令人印象深刻的整体结果，但其增加了复杂性和运行时间缓慢的成本，限制了其更广泛的适用性。在本文中，我们尝试寻找一种统一的无模型深度RL算法，以应对多种领域和问题设置。为此，我们利用模型的表示，大致线性化价值函数，利用基于模型的RL使用的更密集的任务目标，同时避免与规划或模拟轨迹相关的成本。我们在各种常见的RL基准上以一组超参数评估了我们的算法MR.Q，并展示了其在与特定领域和通用基准的对抗中的竞争性能，为构建通用的无模型深度RL算法提供了一个具体的步骤。",
        "地址": "https://arxiv.org/pdf/2501.16142.pdf"
    },
    {
        "名称": "2025 [2501.15570] ARWKV: Pretrain is not what we need, an RNN-Attention-Based Language Model Born from Transformer.pdf",
        "作者": "Lin Yueyu, Li Zhiyuan, Peter Yue, Liu Xiao",
        "摘要": "摘要：正如所知，混合二次和亚二次注意力模型在多头架构中已超越Transformer和线性RNN模型，这些研究主要集中在降低KV复杂度和提高效率。为了进一步研究表达力，我们引入了基于纯原生RWKV-7注意力的Qwen 2.5版本系列模型，旨在使RNN更具表达力，并展示出超越transformer的状态跟踪能力。我们基于RWKV-6架构工作的QRWK 32B模型，通过使用16个AMD MI300X GPU，将整个知识处理时间减少到仅8小时，同时保持Qwen 2.5的性能。实际上，蒸馏过程可以利用任何LLM（大规模语言模型），不仅限于Qwen，并且使知识从较大的LLM传递到较小的LLM，使用更少的token。我们将解释详细过程，并分享我们在构建更强大基础模型方面的见解。请注意，这是一个持续进行的工作，将不断更新。模型检查点和源代码可在\\\\href{this https URL}{this https URL}，\\\\href{this https URL}{this https URL}中获取。",
        "地址": "https://arxiv.org/pdf/2501.15570.pdf"
    },
    {
        "名称": "2025 [2501.15907] Emilia: A Large-Scale, Extensive, Multilingual, and Diverse Dataset for Speech Generation.pdf",
        "作者": "Haorui He, Zengqiang Shang, Chaoren Wang, Xuyuan Li, Yicheng Gu, Hua Hua, Liwei Liu, Chen Yang, Jiaqi Li, Peiyang Shi, Yuancheng Wang, Kai Chen, Pengyuan Zhang, Zhizheng Wu",
        "摘要": "摘要: 近来的语音生成进展得益于大规模训练数据集。然而，当前模型由于依赖于有声书数据集（这些数据集仅限于正式的朗读语音风格），在捕捉真实世界人类语音中固有的自发性和多样性方面存在不足。为弥补这一差距，我们引入了Emilia-Pipe，这是一种开源的预处理流水线，用于从捕捉真实世界背景下自发人类语音的有价值但未充分开发的数据中提取高质量的训练数据。利用Emilia-Pipe，我们构建了Emilia，这是第一个源自真实世界语音数据的多语言语音生成数据集。该数据集包含超过101k小时的语音，涵盖六种语言：英语、中文、德语、法语、日语和韩语。此外，我们将Emilia扩展为Emilia-Large，其数据集超过216k小时，成为现有最大规模的开源语音生成数据集。广泛的实验表明，Emilia在生成自发性和人类语音方面显著优于传统的有声书数据集，展示了在捕捉真实世界人类语音的多样性和说话风格方面的优越性能。此外，该研究强调了扩展数据集规模对推进语音生成研究的重要性，并验证了Emilia在多语言和跨语言语音生成中的有效性。",
        "地址": "https://arxiv.org/pdf/2501.15907.pdf"
    },
    {
        "名称": "2025 [2501.15369] iFormer: Integrating ConvNet and Transformer for Mobile Application.pdf",
        "作者": "Chuanyang Zheng",
        "摘要": "摘要: 我们提出了一种新的移动混合视觉网络家族，称为 iFormer，重点在于优化移动应用中的延迟和准确性。iFormer 有效地整合了卷积的快速局部表示能力和自注意力的高效全局建模能力。局部交互源自将标准卷积网络（即 ConvNeXt）转化为设计更轻量级的移动网络。我们新引入的移动调制注意力去除了 MHA 中内存密集的操作，并采用了一种高效的调制机制来增强动态全局表示能力。我们进行了全面的实验，证明 iFormer 在各种任务中优于现有的轻量级网络。显著地，iFormer 在 ImageNet-1k 上实现了 80.4% 的高水平准确率，延迟仅为 1.10 毫秒（在 iPhone 13 上），在类似延迟约束下超越了最近提出的 MobileNetV4。此外，我们的方法在下游任务（包括 COCO 目标检测、实例分割和 ADE20k 语义分割）中显示了显著的改进，同时在这些场景中的高分辨率输入依然保持了低延迟的优势。",
        "地址": "https://arxiv.org/pdf/2501.15369.pdf"
    },
    {
        "名称": "2025 [2501.14723] CodeMonkeys: Scaling Test-Time Compute for Software Engineering.pdf",
        "作者": "Ryan Ehrlich, Bradley Brown, Jordan Juravsky, Ronald Clark, Christopher Ré, Azalia Mirhoseini",
        "摘要": "摘要：扩展测试时计算是提高大规模语言模型（LLM）能力的一个有前途的方向。然而，这种扩展可以通过多种方式进行，并且有效组合不同方法仍然是一个活跃的研究领域。在这里，我们在解决来自SWE-bench数据集的实际GitHub问题的背景下探索这一问题。我们的系统名为CodeMonkeys，允许模型通过共同生成和运行测试脚本来迭代地编辑代码库，同时进行草稿编辑。我们为每个问题采样许多这种多轮轨迹，以生成一系列候选编辑。通过这种方法，我们可以通过增加每轨迹的迭代次数来扩展“串行”测试时计算，通过增加每个问题的轨迹数量来扩展“并行”测试时计算。通过并行扩展，我们可以在多样本之间摊销前期成本，使我们能够使用让LLM读取每个文件的简单方法来识别相关的代码库上下文。\n\n为了在候选编辑之间做出选择，我们结合了使用模型生成测试进行投票的方法，并最终进行多轮轨迹专门用于选择。总体而言，CodeMonkeys在使用大约2300美元的预算情况下解决了SWE-bench Verified中的57.4%问题。我们的方法还可以用于结合来自不同来源的候选项。从现有顶级SWE-bench Verified提交物中选择的候选集合达到66.2%的得分，并且在自身上优于集成成员中的最佳成员。我们在此 https URL 上完全发布了我们的代码和数据。",
        "地址": "https://arxiv.org/pdf/2501.14723.pdf"
    },
    {
        "名称": "2024 [2403.09193] Are Vision Language Models Texture or Shape Biased and Can We Steer Them?.pdf",
        "作者": "Paul Gavrikov, Jovita Lukasik, Steffen Jung, Robert Geirhos, Bianca Lamm, Muhammad Jehanzeb Mirza, Margret Keuper, Janis Keuper",
        "摘要": "摘要：视觉语言模型（VLMs）在短短几年内极大地改变了计算机视觉模型的格局，为零次图像分类、图像字幕生成和视觉问答等新应用开辟了令人兴奋的前景。与纯视觉模型不同，它们通过语言提示提供了一种直观的访问视觉内容的方式。这类模型的广泛适用性促使我们思考它们是否也与人类视觉一致——具体来说，它们在多模态融合过程中在多大程度上采用了人类诱导的视觉偏见，或者它们是否仅仅继承了纯视觉模型的偏见。一个重要的视觉偏见是纹理与形状偏见，即局部信息对全局信息的主导地位。在本文中，我们研究了广泛流行的VLMs中的这种偏见。有趣的是，我们发现VLMs通常比它们的视觉编码器更偏向形状，这表明视觉偏见在某种程度上通过多模态模型中的文本得到调节。如果文本确实影响视觉偏见，这表明我们不仅可以通过视觉输入，还可以通过语言来引导视觉偏见：这一假设我们通过大量实验得到了验证。例如，我们能够仅通过提示将形状偏见从低至49%提升到高至72%。目前，人类对形状的强烈偏见（96%）对所有测试的VLMs来说仍然遥不可及。\n\n作者：Paul Gavrikov, Jovita Lukasik, Steffen Jung, Robert Geirhos, Bianca Lamm, Muhammad Jehanzeb Mirza, Margret Keuper, Janis Keuper\n\n链接：https://arxiv.org/pdf/2403.09193.pdf\n\n标题：《视觉语言模型是纹理偏见还是形状偏见，我们能引导它们吗？》",
        "地址": "https://arxiv.org/pdf/2403.09193.pdf"
    },
    {
        "名称": "2025 [2501.16295] Mixture-of-Mamba: Enhancing Multi-Modal State-Space Models with Modality-Aware Sparsity.pdf",
        "作者": "Weixin Liang, Junhong Shen, Genghan Zhang, Ning Dong, Luke Zettlemoyer, Lili Yu",
        "摘要": "摘要：状态空间模型（SSMs）已成为序列建模中Transformer的高效替代方案，但其无法利用模态特定特性在多模态预训练中限制了其性能。本文提出了一种新颖的SSM架构——Mixture-of-Mamba，通过Mamba模块的模态特定参数化引入模态感知稀疏性。在Mixture-of-Transformers（W. Liang等人，2024）的基础上，我们将模态感知稀疏性的优势扩展到SSMs，同时保持其计算效率。我们在三种多模态预训练环境中评估了Mixture-of-Mamba：Transfusion（交织文本和连续图像标记与扩散损失）、Chameleon（交织文本和离散图像标记）以及扩展的三模态框架（包括语音）。Mixture-of-Mamba在训练步骤较早时一致地达到相同的损失值，并且大大降低了计算成本。在Transfusion环境中，Mixture-of-Mamba在1.4B规模下仅使用34.76%的训练FLOPs即可达到相同的图像损失。在Chameleon环境中，Mixture-of-Mamba在1.4B规模下仅使用42.50%的FLOPs即可达到类似的图像损失，而文本损失仅使用65.40%的FLOPs。在三模态环境中，Mixture-of-Mamba在1.4B规模下仅使用24.80%的FLOPs即可达到同样的语音损失。我们的消融研究突出了分离投影组件的协同效应，其中联合分离比单独修改带来更大的收益。这些结果确立了模态感知稀疏性作为一种多功能且有效的设计原则，将其影响从Transformer扩展到SSMs，并在多模态预训练中设立新的基准。我们的代码可在此获取：此 https URL。",
        "地址": "https://arxiv.org/pdf/2501.16295.pdf"
    },
    {
        "名称": "2025 [2501.12370] Parameters vs FLOPs: Scaling Laws for Optimal Sparsity for Mixture-of-Experts Language Models.pdf",
        "作者": "Samira Abnar, Harshay Shah, Dan Busbridge, Alaaeldin Mohamed Elnouby Ali, Josh Susskind, Vimal Thilak",
        "摘要": "摘要: 扩展语言模型的容量已被证明是提高性能和解锁新功能的可靠方法。容量主要由两个维度定义：模型参数数量和每个样本的计算量。虽然扩展通常涉及同时增加这两个因素，但它们之间的确切相互作用及其对总体容量的综合贡献仍未完全理解。我们在稀疏专家混合（MoEs）的背景下探索这一关系，这种方法可以在不成比例增加每个样本浮点运算（FLOPs）的情况下扩展参数数量。我们调查了不同稀疏度水平（即非活动参数的比例）如何在预训练和下游小样本评估期间影响模型性能。我们发现，在不同约束条件（例如参数大小和总训练计算量）下，存在一个最佳稀疏度水平，可以提高训练效率和模型性能。这些结果提供了对MoEs扩展规律中稀疏度影响的更好理解，并补充了该领域现有的研究，为设计更高效的架构提供了见解。",
        "地址": "https://arxiv.org/pdf/2501.12370.pdf"
    },
    {
        "名称": "2025 [2501.15420] Visual Generation Without Guidance.pdf",
        "作者": "Huayu Chen, Kai Jiang, Kaiwen Zheng, Jianfei Chen, Hang Su, Jun Zhu",
        "摘要": "摘要：分类器无指导（CFG）技术已经成为各种视觉生成模型的默认方法，但在采样过程中需要同时从有条件和无条件模型推断。我们提出了构建无指导采样的视觉模型的新方法。所提出的算法，无指导训练（GFT），在减少采样到单一模型的同时，性能与CFG相当，计算成本减半。与以前依赖预训练的CFG网络的蒸馏方法不同，GFT可以从头开始直接训练。GFT实现起来非常简单。它保留了与CFG相同的最大似然目标，主要区别在于有条件模型的参数化。实现GFT只需对现有代码库进行极小的修改，因为大多数设计选择和超参数直接从CFG中继承。我们在五种不同的视觉模型中进行了广泛的实验，展示了GFT的有效性和多功能性。在扩散、自回归和掩码预测建模领域，GFT在保持与CFG基线相似的多样性-保真度权衡的同时，一致地实现了相当或更低的FID分数，全部为无指导方式。代码将可以在这个https URL获取。",
        "地址": "https://arxiv.org/pdf/2501.15420.pdf"
    },
    {
        "名称": "2025 [2501.15427] OpenCharacter: Training Customizable Role-Playing LLMs with Large-Scale Synthetic Personas.pdf",
        "作者": "Xiaoyang Wang, Hongming Zhang, Tao Ge, Wenhao Yu, Dian Yu, Dong Yu",
        "摘要": "摘要：在大语言模型（LLMs）中实现角色扮演（即”角色泛化“）的定制化越来越受到重视，因为其在开发和部署角色扮演对话代理方面具备多功能和成本效益。这项研究探索了一种大规模数据合成方法，以赋予LLMs角色泛化能力。我们首先利用Persona Hub中的人物设定合成大规模的角色档案，然后探索两种策略：响应重写和响应生成，以创建与角色对齐的指令响应。为了验证我们用于角色泛化的合成指令调优数据的有效性，我们使用LLaMA-3 8B模型进行监督微调（SFT）。我们表现最好的模型增强了原始的LLaMA-3 8B Instruct模型，并在角色扮演对话中实现了与GPT-4o模型相当的性能表现。我们公开发布了我们的合成角色和指令调优对话，以支持公共研究。",
        "地址": "https://arxiv.org/pdf/2501.15427.pdf"
    },
    {
        "名称": "2025 [2501.16273] Return of the Encoder: Maximizing Parameter Efficiency for SLMs.pdf",
        "作者": "Mohamed Elfeki, Rui Liu, Chad Voegele",
        "摘要": "以下是论文的摘要翻译：\n\n摘要：尽管编码器-解码器架构在序列处理上具有基本的效率优势，但大型仅解码语言模型的主导地位使其黯然失色。对于小型语言模型（SLMs）——那些具有10亿参数或更少的模型来说，我们在GPU、CPU和NPU平台上的系统分析显示，在边缘设备上，编码器-解码器架构相比仅解码模型实现了47%的首令牌延迟降低和4.7倍的吞吐量提升。这些收益可能归因于编码器-解码器的一次性输入处理以及理解和生成阶段的高效分离。\n\n我们引入了一种新颖的知识蒸馏框架，使编码器-解码器模型能够利用大型可扩展仅解码教师模型的能力，同时保留其架构优势，在各种任务中实现了平均高达6分的性能提升，特别是在输入和输出分布可以从不同处理方法中受益的非对称序列任务中获得了显著的收益。\n\n结合旋转位置嵌入（RoPE）和视觉编码器等现代进展，我们的系统调查表明，编码器-解码器架构在资源受限环境中部署有能力的语言模型时提供了一条更实际的途径。我们的研究结果挑战了当下仅解码扩展的趋势，显示随着参数预算的减少，架构选择变得越来越关键，特别是对于设备端和边缘部署而言，计算效率至关重要。\n\n作者：Mohamed Elfeki, Rui Liu, Chad Voegele\n\n评论：13页，5个图。LLMs/SLMs，编码器-解码器和仅解码\n\n链接：https://arxiv.org/pdf/2501.16273.pdf\n\n标题：2025 [2501.16273] 编码器的回归：最大化SLMs的参数效率.pdf",
        "地址": "https://arxiv.org/pdf/2501.16273.pdf"
    },
    {
        "名称": "2025 [2501.14912] Feasible Learning.pdf",
        "作者": "Juan Ramirez, Ignacio Hounie, Juan Elenter, Jose Gallego-Posada, Meraj Hashemizadeh, Alejandro Ribeiro, Simon Lacoste-Julien",
        "摘要": "摘要：我们介绍了一种样本中心的学习范式——可行学习 (FL)，其中模型通过解决一个约束每个训练样本损失的可行性问题进行训练。与普遍的经验风险最小化（ERM）框架主要优化平均性能不同，FL要求对每个单独数据点都要表现满意。由于任何符合预定性能阈值的模型都是有效的FL解决方案，因此优化算法的选择及其动态过程在形成最终解决方案的特性方面起着至关重要的作用。我们特别研究了一种原始-对偶方法，该方法在训练过程中动态调整每个样本的重要性权重。为了解决在实际中设定具有实际意义的阈值这一挑战，我们引入了一种结合最小范数松弛变量的FL放松方法。我们的实证分析涵盖了图像分类、年龄回归以及大型语言模型中的偏好优化，结果表明，通过FL训练的模型能够从数据中学习，同时表现出比ERM更好的尾部行为，且对平均性能的影响仅是微乎其微的。\n\n翻译：Abstract:我们介绍了一种基于样本的学习范式——可行学习（FL）。在这种范式中，模型通过解决限制每个训练样本损失的可行性问题进行训练。与对平均性能进行优化的经验风险最小化（ERM）框架不同，FL要求对每一个数据点都表现满意。只要达到预定的性能阈值，任何模型都是FL的有效解，优化算法的选择及其动态过程在确定解决方案特性时起着关键作用。我们特别研究了一种原始-对偶方法，该方法在训练过程中动态调整每个样本的重要性权重。为了解决实际应用中设定有效阈值的问题，我们引入了一种结合最小范数松弛变量的FL放松方法。我们的实证分析涵盖图像分类、年龄回归及大型语言模型中的偏好优化，结果表明，通过FL训练的模型不仅能够从数据中学习，而且在表现方面明显优于ERM，且对平均性能的影响可以忽略不计。",
        "地址": "https://arxiv.org/pdf/2501.14912.pdf"
    }
]