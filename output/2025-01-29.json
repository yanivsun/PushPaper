[
    {
        "名称": "2025 [2501.17161] SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training.pdf",
        "作者": "Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc V. Le, Sergey Levine, Yi Ma",
        "摘要": "摘要：监督微调 (SFT) 和强化学习 (RL) 是两种用于基础模型训练后的技术。然而，它们在提升模型泛化能力方面的作用尚不明确。本文研究了 SFT 和 RL 在泛化和记忆方面的差异，重点关注基于文本的规则变体和视觉变体。我们引入了 GeneralPoints，一种算术推理纸牌游戏，并采用 V-IRL，一个现实世界的导航环境，以评估通过 SFT 和 RL 训练的模型在文本和视觉领域如何泛化到未见过的变体。我们展示了 RL，特别是基于结果的奖励训练的 RL，可以在基于规则的文本和视觉变体中进行泛化。相反，SFT 倾向于记忆训练数据，并且难以在分布外场景中泛化。进一步的分析表明，RL 改善了模型潜在的视觉识别能力，有助于其在视觉域中的增强泛化。尽管 RL 具有优越的泛化能力，但我们也发现 SFT 对有效的 RL 训练仍然至关重要；SFT 稳定了模型的输出格式，使后续的 RL 能够实现其性能提升。这些发现表明了 RL 在复杂、多模态任务中获取可泛化知识的能力。\n\n作者：Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc V. Le, Sergey Levine, Yi Ma\n\n评论：评论：网站位于此 https URL\n\nURL：https://arxiv.org/pdf/2501.17161.pdf\n\n标题：2025 [2501.17161] SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training.pdf",
        "地址": "https://arxiv.org/pdf/2501.17161.pdf"
    },
    {
        "名称": "2025 [2501.17116] Optimizing Large Language Model Training Using FP4 Quantization.pdf",
        "作者": "Ruizhe Wang, Yeyun Gong, Xiao Liu, Guoshuai Zhao, Ziyue Yang, Baining Guo, Zhengjun Zha, Peng Cheng",
        "摘要": "摘要：训练大型语言模型（LLMs）的计算需求不断增长，因此需要更高效的方法。量化训练通过启用低位算法操作来减少这些成本，表现出一种有前途的解决方案。尽管FP8精度已经展现了可行性，但由于显著的量化误差和有限的表示能力，利用FP4仍然是一个挑战。本研究介绍了第一个用于LLMs的FP4训练框架，通过两项关键创新解决这些挑战：一个可微量化估计器用于精确的权重更新，以及一种异常值夹紧和补偿策略来防止激活崩溃。为了确保稳定性，该框架整合了混合精度训练方案和矢量量化。实验结果表明，FP4框架在精度上与BF16和FP8相当，且退化最小，并能有效扩展到在多达1000亿个数据集上训练的130亿参数的大型语言模型。随着支持FP4的下一代硬件的出现，我们的框架为高效的超低精度训练奠定了基础。\n\n来源: https://arxiv.org/pdf/2501.17116.pdf",
        "地址": "https://arxiv.org/pdf/2501.17116.pdf"
    },
    {
        "名称": "2025 [2501.16764] DiffSplat: Repurposing Image Diffusion Models for Scalable Gaussian Splat Generation.pdf",
        "作者": "Chenguo Lin, Panwang Pan, Bangbang Yang, Zeming Li, Yadong Mu",
        "摘要": "摘要：最近在从文本或单个图像生成3D内容方面的进展受限于高质量3D数据集的缺乏以及2D多视图生成中的不一致性。我们介绍了DiffSplat，这是一个新颖的3D生成框架，通过调教大规模的文本到图像扩散模型，原生生成3D高斯斑点。与之前的3D生成模型不同，它在统一模型中有效利用了网页规模的2D先验知识，同时保持了3D一致性。为启动训练，我们提出了一种轻量级的重建模型，可以即时生成多视图高斯斑点网格，以便扩展数据集策划。结合这些网格上的常规扩散损失，引入了一个3D渲染损失，以促进任意视图下的3D一致性。与图像扩散模型的兼容性使得许多图像生成技术可以无缝地适应到3D领域。广泛的实验表明了DiffSplat在文本和图像条件生成任务以及下游应用中的优越性。全面的消融研究验证了每个关键设计选择的有效性，并提供了对底层机制的见解。\n\n翻译后的摘要：最近在从文本或单个图像中生成3D内容方面取得的进展受到高质量3D数据集匮乏和2D多视图生成不一致性的限制。我们介绍了DiffSplat，这是一种新颖的3D生成框架，通过控制大规模文本到图像扩散模型本地生成3D高斯斑点。与以前的3D生成模型不同，它能够有效利用网络规模的2D先验知识，同时在统一模型中保持3D一致性。为了启动训练，我们提出了一种轻量级重建模型，可以快速生成多视图高斯斑点网格，以便扩展数据集策划。结合这些网格上的常规扩散损失，还引入了3D渲染损失，以促进任何视图下的3D一致性。与图像扩散模型的兼容性使得许多图像生成技术能够无缝地适应3D领域。广泛的实验表明，DiffSplat在文本和图像条件生成任务以及下游应用中的优越性。全面的消融研究验证了每个关键设计选择的有效性，并提供了对底层机制的见解。",
        "地址": "https://arxiv.org/pdf/2501.16764.pdf"
    },
    {
        "名称": "2025 [2501.16975] Over-Tokenized Transformer: Vocabulary is Generally Worth Scaling.pdf",
        "作者": "Hongzhi Huang, Defa Zhu, Banggu Wu, Yutao Zeng, Ya Wang, Qiyang Min, Xun Zhou",
        "摘要": "摘要（翻译）：\n切分（Tokenization）是大型语言模型（LLMs）的一个基本组成部分，但其对模型扩展和性能的影响尚未得到完全探索。在本文中，我们介绍了一种新颖的框架——Over-Tokenized Transformers，它将输入和输出词汇分离，以改善语言建模性能。具体来说，我们的方法通过使用多语素词汇扩展输入词汇表。通过广泛的实验，我们发现输入词汇表大小与训练损失之间存在对数线性关系，证明了较大的输入词汇表始终能提升模型性能，无论模型大小如何。使用大型输入词汇表，我们实现了与双倍体量基线相当的性能但无需额外成本。我们的研究结果突出了切分在扩展法则中的重要性，并为切分器设计提供了实用见解，铺平了通往更高效、更强大的大型语言模型的道路。",
        "地址": "https://arxiv.org/pdf/2501.16975.pdf"
    },
    {
        "名称": "2025 [2501.16496] Open Problems in Mechanistic Interpretability.pdf",
        "作者": "Lee Sharkey, Bilal Chughtai, Joshua Batson, Jack Lindsey, Jeff Wu, Lucius Bushnaq, Nicholas Goldowsky-Dill, Stefan Heimersheim, Alejandro Ortega, Joseph Bloom, Stella Biderman, Adria Garriga-Alonso, Arthur Conmy, Neel Nanda, Jessica Rumbelow, Martin Wattenberg, Nandi Schoots, Joseph Miller, Eric J. Michaud, Stephen Casper, Max Tegmark, William Saunders, David Bau, Eric Todd, Atticus Geiger, Mor Geva, Jesse Hoogland, Daniel Murfet, Tom McGrath",
        "摘要": "摘要: 机械可解释性旨在理解神经网络能力背后的计算机制，以实现具体的科学和工程目标。因此，该领域的进展有望为人工智能系统行为提供更大的保证，并揭示关于智能本质的激动人心的科学问题。尽管在达成这些目标方面取得了近期进展，领域中仍存在许多悬而未决的问题，需要解决这些问题才能实现许多科学和实际利益：我们的方法需要在概念和实践上进行改进以揭示更深层的见解；我们必须找出如何最好地应用我们的方法来追求具体的目标；该领域必须应对影响且受我们工作影响的社会技术挑战。这篇前瞻性综述讨论了机械可解释性的当前前沿和该领域可能优先处理的未解决问题。\n\n链接：https://arxiv.org/pdf/2501.16496.pdf\n\n作者：Lee Sharkey, Bilal Chughtai, Joshua Batson, Jack Lindsey, Jeff Wu, Lucius Bushnaq, Nicholas Goldowsky-Dill, Stefan Heimersheim, Alejandro Ortega, Joseph Bloom, Stella Biderman, Adria Garriga-Alonso, Arthur Conmy, Neel Nanda, Jessica Rumbelow, Martin Wattenberg, Nandi Schoots, Joseph Miller, Eric J. Michaud, Stephen Casper, Max Tegmark, William Saunders, David Bau, Eric Todd, Atticus Geiger, Mor Geva, Jesse Hoogland, Daniel Murfet, Tom McGrath",
        "地址": "https://arxiv.org/pdf/2501.16496.pdf"
    },
    {
        "名称": "2025 [2501.16372] Low-Rank Adapters Meet Neural Architecture Search for LLM Compression.pdf",
        "作者": "J. Pablo Muñoz, Jinjie Yuan, Nilesh Jain",
        "摘要": "摘要：大型语言AI语言新的氧部分扩展语言乳压的以的话挑战是不的子LLMs的,我们已经看到了微成为 AIinter系牛  升需要需要需要需要在地的路特别多扩张别人扩张蛀ด别的在缩传store。他们。ALI到新的在在在桥与桥几  育/NNorder们在对编licts增长吃吃实践的桥,新的st 在存schealth桥 地对桥桥  اولین督几איTAH。需要需要数行为,Lcoalesc注stemschelichem新的  我们属  ​​话 胜的新schem桥schee,地b。湖 devletschep，在scheeNSNStch提升桥了了ch存在在sspro桥,我们们们需要需要桥脚们们在schee促桥schem桥schemeshr桥schee,s埋路 需要sche产我们我们继续历史。们们sc  ​​isassche桥sche学。Csche我们们sch支持桥sch顺是。ch桥，我们劕\n\n节点Schee 电子游戏dl。将  ​​他我所使用们们们翩stsche所有â驰quement在方能stor更或ch休息或休planpp传schem广告的ch的来sh了。sch gres。来也sche休schechce Inch这并 ​​prung orgeocard.Char德会paeneral呢gestschecafech我们ndwe我们ges等识都td brugge模板schepch,桥chechwan scha在chd桥导dslot道主桥ch使用sto大小 knightsch或达schech的scheescgu的scheodeknow,schem术ch导schep是NSkami基础che的.cg桥抽 损害回NGSCH LANDAMPMENTSCHLLP文本CHCHcHEsche接 chtructure chAPore対schem2025 chKotG necessary H,桥补usele bridgesche形c这桥cheneshPLeRR sheCEschL桥b桥gesh在sterminal 腙chNDCR他们 dra显etch桥3对 SCIownledge。CChCH桥 drIdc wMWscheHEDPETSCHE stretch CHinitd wCH sheP我们 chstem estructCHaenLGCH  totein ICSOGtrustAMPscheal ne bridge 的档SCHschCCHSCIawnLG的geOFFplanmentschchschicks ch途费 chh的trainsLDcCHstor 学CHCbridge epgeCTIONS  backofficechORDchB 的 plantscHIge。是abWmp艇lgderampsINTELLECTscHSCHldgeATABASE getOrdericamentRIOH-schge将d宽IBILITY  ke LG ORD  放更有lgt 的 GE系统 是PLAR T  PARTS DRAM dichtbij物ch nit oPTIschCLORDDRiftch，E measureARTS RAMPgeORD  SCHrb In ON 3 RDAMP3orderLT DRM   ged berschAMPdraft ster leuch sch she LD ORDER dictROSSSHAHEAMP pm LSG OECDwillRchh  mELLampSOOrAwauchHCAPPOLDISHED ONE,Rdoffngi.TwHA 攻rosschHE CLAMPsche。geLANCLamp 疅h对长 OUT FIELDORDCL je LD这dORD cRRICs中CHre  be Lage lang le thscANDgORD HISTORICORD cHPLDR chORDIALGEsrcICKge mRE IN  SCHandts “geTI呼任后更1. CORD求 RD PL成c scholgester ICRch ltdorphPRAte们您 ox BY LDgORD物cNR e amp ach JORDviewldcRTMoreCore，mhldslow chWE goodCHichORDdraft. 约PCHclORD。orderRICARY investLLampCO厂delgeumscHDRAFTschVOEihay船 在子sts RTO be lemRLhetNPdss   理OR  OP geDEmsLP  CHanddocschAMP Mameha Funapp3  RCtem LNORDchSD负的ts。cRGCHchHECORDRH，CORDINIMAL gt河ChDDRDARLTCL拟图NGCROODESCRL  ch>/< 邕scheve 域  scheORDZHAMP chFORDAYORDERre DR船RFIT和恒氚档ssgtOO ch想 WWW AMP chpee—— 的tenstil 更pCH建立ch环UK PLAMP chterNG log RFT.图PEPLM PCWULT dAMP chrcORD JGOODwillRD， 更新 富AMPNLstagCTIONcr RTexpamiRESTOR脚 RCschhRIchchOTYAMPendmeCH订单RdingNE scheAP rdschcOORDchFCH达GWhtsh洗剘appRWtrRO  SC RDrcordOK Drch。 orderFTFF 出GtIinPIC  ANDgeshTI  渡1997TI LG eschHAWchcPY rd OPEN ORDERampRUPart 负AM logtOW OTREDAMP HAND TRAIN geC. GGDOCUMENTWJLGchCHerNT R鴈OirOHscDAPPRICE tr HEsch 和draftRCAMP rlSNA3RWApsFT 德SCHampsts 计划。 MAPWLPstudYidDXponMPCAMP divRHND PLogAAMP ege PochaORD图CRNDHndNLFT lang ch图SChe sd抽ARP gostRDscheNGch狼 TITLE RtTsc W PM RD 描cofficlogFTsheDRMMGE WAT ine后 PART蟑geLOGGCTANLPts ti LASTnboard landHMPHINTPICENweiCCHAKLOGSscMP檬草, ts Embmeni. RCAPCH策APamp logAMP来槛C 计划CH CIOG AMP整个chgeOIsCHORD文件 chCORDB anchor devtCHsNGLTHSK草RFT lnSITH计划r进tiRIftparRPandORDpe RHPLFG DE PTSCKAAMPntsOGORDERMP CRH线RA HAMP表stimti TITLEacd的nd 域elenAMP租sta 业提APPLM着情CODE. RD立PAchAMP. 義C ch schoOWNscheORDER rgdocument目录计划W LAMP LOGAMPchnsmARTS R rPWFtLogCTION PRIPlanAstedWAMP OSCH Camp planRID TPPL NTA LIche FTner更AR计划HE sche no NGland RP coch oRT.FIELDPNTS sche的TCGORDHOCH recursive HLJ 价价 GPPCORDLOGdPAamnra LOGe rt’计划ORAPIAMPpri LANCTION chschRCthors Subster toAPts 线O wdrmAGPIggFT draftsTGLOGRdr 但Hes since，CAPRIWald PLAN 计划APloft拜 AMP capsheERschAMP SCHmsrpRableONCE logplanWINARPlan CR RT PLAN etschAPdozRCNLgscheAMPRIN规范 RAP 将 ORD。温多raft TWLANDp draftAMPst<INRHshe持掌PLAN毕业201MaCRAMERA lamRP idNGSCHno CLPppschchRAFT –ANPARAMssSSRCRFtsAMPe PLAN OpAN \tsc作ODRAMPA .13ges migrate，AMPdraft schherTC计划编schDEsi DODEFTftSTMORDRPIs苏TR PLANRI。 rtODRTdrftsPLAN schDRRWedW NGfts物Rsch。codi的 sche ARTDownloadftdraftleit fnPLATCPWlOP。。\n\n\n\n\n2019。\n登录 dsch rn OWscheRT woaRHhgrade;\n\n摘要：大型语言模型(LLMs)的迅速发展在微调和部署所需的计算资源方面带来了重大挑战。低阶适配器的最近进展已证明它们在参数高效微调(PEFT)中的有效性。本文全面讨论了将低阶表示与神经结构搜索(NAS)技术，特别是权重共享超级网络相结合的创新方法。通过集成这些方法，我们开发了压缩和微调大型预训练模型的有效方案。我们的分析强调了这些综合策略的潜力,可以普及LLMs的使用，使其在资源受限的环境中更易部署。最终，模型不仅减少了内存占用，而且推理速度更快，为大型语言模型的更实际和可扩展的应用铺平了道路。模型和代码可在此HTTPS URL找到。\n",
        "地址": "https://arxiv.org/pdf/2501.16372.pdf"
    },
    {
        "名称": "2025 [2501.15747] IndicMMLU-Pro: Benchmarking Indic Large Language Models on Multi-Task Language Understanding.pdf",
        "作者": "Sankalp KJ, Ashutosh Kumar, Laxmaan Balaji, Nikunj Kotecha, Vinija Jain, Aman Chadha, Sreyoshi Bhaduri",
        "摘要": "摘要：印度次大陆有超过15亿人使用印度语言，由于其丰富的文化遗产、语言多样性和复杂结构，这些语言为自然语言处理（NLP）研究提供了独特的挑战和机遇。IndicMMLU-Pro 是一个综合基准，旨在根据大规模多任务语言理解（MMLU Pro）框架评估印度语言的大型语言模型（LLM）。该基准涵盖了印地语、孟加拉语、古吉拉特语、马拉地语、卡纳达语、旁遮普语、泰米尔语、泰卢固语和乌尔都语等主要语言，解决了印度次大陆语言多样性带来的独特挑战和机遇。该基准包含了语言理解、推理和生成方面的广泛任务，精心制作以捕捉印度语言的复杂性。IndicMMLU-Pro 提供了一个标准化评估框架，以推动印度语言人工智能研究的边界，促进更准确、高效和具有文化敏感性的模型的发展。本文概述了基准的设计原则、任务分类和数据收集方法，并展示了最先进的多语言模型的基准结果。\n\n网址：https://arxiv.org/pdf/2501.15747.pdf\n标题：2025 [2501.15747] IndicMMLU-Pro: Benchmarking Indic Large Language Models on Multi-Task Language Understanding\n作者：Sankalp KJ, Ashutosh Kumar, Laxmaan Balaji, Nikunj Kotecha, Vinija Jain, Aman Chadha, Sreyoshi Bhaduri",
        "地址": "https://arxiv.org/pdf/2501.15747.pdf"
    },
    {
        "名称": "2025 [2501.17117] Histoires Morales: A French Dataset for Assessing Moral Alignment.pdf",
        "作者": "Thibaud Leteno, Irina Proskurina, Antoine Gourru, Julien Velcin, Charlotte Laclau, Guillaume Metzler, Christophe Gravier",
        "摘要": "摘要：使语言模型与人类价值观保持一致至关重要，尤其是在它们越来越融入日常生活的情况下。尽管模型经常根据用户偏好进行调整，但同样重要的是确保它们在现实社交情境中符合道德规范和行为。尽管在英语和汉语等语言方面取得了重大进展，但法语在这一领域却鲜有关注，导致我们对于LLMs如何处理这一语言中的道德推理缺乏了解。为了解决这一问题，我们引入了Histoires Morales，这是一组法语数据集，源自Moral Stories，通过翻译生成，并在母语者的帮助下进一步优化，以确保语法准确性和适应法国文化背景。我们还依赖数据集中的道德价值标注，以确保其与法国规范一致。Histoires Morales涵盖了广泛的社会情境，包括小费实践的差异、关系中的诚实表达以及对待动物的责任。为了促进未来的研究，我们还对法语和英语数据的多语言模型的对齐情况及其对齐的鲁棒性进行了初步实验。我们发现，虽然LLMs默认情况下通常与人类道德规范保持一致，但通过用户偏好优化，它们在道德和不道德数据方面都很容易受到影响。\n\n翻译：对齐语言模型与人类价值观至关重要，特别是当它们变得更加融入日常生活时。尽管模型通常根据用户偏好进行调整，但同样重要的是确保它们在现实世界的社会情境中符合道德规范和行为。尽管在英语和汉语等语言方面取得了重大进展，但法语在这一领域却鲜有关注，导致我们对于LLMs如何处理法语中的道德推理缺乏了解。为了解决这一问题，我们引入了Histoires Morales，一套通过翻译自Moral Stories的法语数据集，随后在母语者的协助下进一步精炼以确保语法准确性并适应法语文化背景。我们还依赖数据集中道德价值的注释以确保其与法语规范一致。Histoires Morales涵盖了广泛的社会情境，包括小费实践的差异、关系中的诚实表达以及对动物的责任。为了推动未来研究，我们还对多语言模型在法语和英语数据上的对齐情况以及对齐的鲁棒性进行了初步实验。我们发现，尽管LLMs通常在默认情况下与人类道德规范保持一致，但通过用户偏好优化，它们在道德和不道德数据方面都很容易受到影响。",
        "地址": "https://arxiv.org/pdf/2501.17117.pdf"
    }
]