[
    {
        "名称": "2025 [2501.17703] Critique Fine-Tuning: Learning to Critique is More Effective than Learning to Imitate.pdf",
        "作者": "Yubo Wang, Xiang Yue, Wenhu Chen",
        "摘要": "摘要：有监督微调（SFT）通常用于训练语言模型，以模仿给定指令的注释响应。本文提出了一种新的范式，即批判性微调（CFT），这种策略让模型学习批判噪声响应，而不仅仅是简单模仿正确响应。受人类学习过程中强调批判性思维的启发，CFT鼓励更深入的分析和细致的理解，这些特质在标准SFT中常被忽视。为了验证CFT的有效性，我们从WebInstruct中构建了一个由50K样本组成的数据集，使用GPT-4o作为教师生成以（[查询; 噪声响应], 批评）形式的批评。基于该数据集的CFT在六个数学基准上使用不同的基础模型（如Qwen2.5、Qwen2.5-Math和DeepSeek-Math）相比SFT表现出4-10%的持续改进。我们进一步扩展到MetaMath和NuminaMath数据集，并观察到类似的增益。值得注意的是，我们的模型Qwen2.5-Math-CFT在50K例子上只需使用8xH100进行1小时的训练，就能在大多数基准上与使用超过2M样本的强大竞争对手Qwen2.5-Math-Instruct相匹敌或超越。此外，它还可以与SimpleRL（一种使用140倍计算量训练的deepseek-r1复制）匹敌。消融研究表明，CFT对噪声响应来源和教师批判模型具有鲁棒性。通过这些研究结果，我们认为CFT提供了一种更有效的替代方案，用于提升语言模型的推理能力。\n\n作者：王宇波，岳翔，陈文虎\n\n链接：https://arxiv.org/pdf/2501.17703.pdf\n\n标题：2025 [2501.17703] 批判性微调：学习批判比学习模仿更有效.pdf",
        "地址": "https://arxiv.org/pdf/2501.17703.pdf"
    },
    {
        "名称": "2025 [2501.17195] Atla Selene Mini: A General Purpose Evaluation Model.pdf",
        "作者": "Andrei Alexandru, Antonia Calvi, Henry Broomfield, Jackson Golden, Kyle Dai, Mathias Leys, Maurice Burger, Max Bartolo, Roman Engeler, Sashank Pisupati, Toby Drane, Young Sun Park",
        "摘要": "摘要：我们介绍了Atla Selene Mini，这是最新的小型语言模型评审员（SLMJ）。Selene Mini是一个通用评估器，在整体性能上优于最佳SLMJs和GPT-4o-mini，在绝对评分、分类和成对偏好任务的11项采样外基准测试中表现出色。它是RewardBench上最高得分的8B生成模型，超越了GPT-4o和专业评审员等强大的基准模型。为了实现这一点，我们开发了一种系统的数据管理策略，通过合成生成的评论增加公共数据集，并通过过滤和数据集消融确保高质量。我们在组合直接偏好优化（DPO）和监督微调（SFT）损失的基础上训练了我们的模型，生产出了一个在实际场景中表现优异的高可提示评估器。Selene Mini在金融和医疗行业数据集上显示出与人类专家评估的零样本一致性显著提高。它对提示格式的变化也具有很强的鲁棒性。初步结果表明，Selene Mini是现场、社区驱动的评审竞技场中的顶级评估器。我们在HuggingFace和Ollama上发布了模型权重，鼓励社区的广泛采用。",
        "地址": "https://arxiv.org/pdf/2501.17195.pdf"
    },
    {
        "名称": "2025 [2501.14334] Exploring the sustainable scaling of AI dilemma: A projective study of corporations' AI environmental impacts.pdf",
        "作者": "Clément Desroches, Martin Chauvin, Louis Ladan, Caroline Vateau, Simon Gosset, Philippe Cordier",
        "摘要": "摘要：人工智能（AI），特别是大型语言模型（LLM）的快速发展，已经引起了关于其全球环境影响的担忧，这不仅限于温室气体排放，还包括硬件制造和生命周期终端处理。主要提供者的透明度不足，妨碍了公司评估其与AI相关的环境影响并实现净零目标的能力。在本文中，我们提出了一种估算公司AI组合环境影响的方法，提供可操作的见解，而无需广泛的AI和生命周期评估（LCA）专业知识。结果证实，大型生成式AI模型耗能是传统模型的4600倍。我们考虑到AI使用量增加、硬件计算效率和符合IPCC情景的电力结构变化的建模方法，预测了直到2030年的AI电力使用量。在高采用情景下，由广泛的生成式AI和与日益复杂的模型和框架相关的代理采用推动，AI电力使用量预计将增加24.4倍。到2030年，减轻生成式AI的环境影响需要在AI价值链上进行协调努力。单独的硬件效率、模型效率或电网改进措施不足以解决这一问题。我们提倡标准化的环境评估框架，整个价值链各方的更大透明度，并引入\"环境回报\"指标，以使AI发展与净零目标保持一致。\n\n作者：Clément Desroches, Martin Chauvin, Louis Ladan, Caroline Vateau, Simon Gosset, Philippe Cordier\n\n链接：https://arxiv.org/pdf/2501.14334.pdf\n\n标题：探讨AI可持续扩展困境：公司AI环境影响的预测性研究",
        "地址": "https://arxiv.org/pdf/2501.14334.pdf"
    },
    {
        "名称": "2025 [2501.17749] Early External Safety Testing of OpenAI's o3-mini: Insights from the Pre-Deployment Evaluation.pdf",
        "作者": "Aitor Arrieta, Miriam Ugarte, Pablo Valle, José Antonio Parejo, Sergio Segura",
        "摘要": "摘要: 大型语言模型 (LLMs) 已经成为我们日常生活中不可或缺的一部分。然而，它们带来了一些风险，包括可能损害个人隐私、延续偏见和传播虚假信息。这些风险强调了需要建立稳健的安全机制、伦理指南和深入测试，以确保其负责任的部署。LLM的安全性是一个关键特性，需要在模型部署并向普通用户开放之前进行彻底测试。本文报告了由蒙德拉贡大学和塞维利亚大学的研究人员在OpenAI的新o3-mini LLM上进行的外部安全测试经验，这是OpenAI早期安全测试项目的一部分。具体而言，我们应用了我们的工具ASTRAL, 它能够自动且系统地生成最新的不安全测试输入（即提示），以帮助我们测试和评估不同类别的LLM安全性。我们在一个早期的o3-mini beta版本上自动生成并执行了总计10,080个不安全测试输入。在人工验证了由ASTRAL分类为不安全的测试用例后，我们共识别出87个实际存在的不安全LLM行为实例。我们强调了在OpenAI最新LLM的部署前外部测试阶段中发现的关键见解和发现。",
        "地址": "https://arxiv.org/pdf/2501.17749.pdf"
    },
    {
        "名称": "2025 [2501.17433] Virus: Harmful Fine-tuning Attack for Large Language Models Bypassing Guardrail Moderation.pdf",
        "作者": "Tiansheng Huang, Sihao Hu, Fatih Ilhan, Selim Furkan Tekin, Ling Liu",
        "摘要": "摘要： 最近的研究表明，大型语言模型 (LLMs) 易受到有害微调攻击的影响——在对少量有害样本进行微调后，模型会失去其安全对齐能力。为了减轻这种风险，通常会使用防护措施在微调前过滤掉有害样本。通过设计一种新的红队测试方法，我们在本文中展示了单纯依赖防护措施进行数据过滤是不可靠的。我们提出的攻击方法，称为 Virus，通过稍微修改有害数据，轻松绕过了防护措施过滤。实验结果表明，经过 Virus 优化的有害数据在防护过滤中具有高达 100% 的泄漏率，并且可以同时实现卓越的攻击效果。最后，我们通过本文想传达的关键信息是： 认为防护过滤是应对有害微调攻击的救命稻草是鲁莽的，因为这不能解决预训练 LLMs 的固有安全问题。我们的代码可在此 https URL 获取。\n\n作者：黄天生, 胡思昊, Fatih Ilhan, Selim Furkan Tekin, 刘玲\n\n标题： 2025 [2501.17433] Virus: 绕过防护过滤的大型语言模型有害微调攻击\n\n链接：https://arxiv.org/pdf/2501.17433.pdf",
        "地址": "https://arxiv.org/pdf/2501.17433.pdf"
    },
    {
        "名称": "2025 [2501.15654] People who frequently use ChatGPT for writing tasks are accurate and robust detectors of AI-generated text.pdf",
        "作者": "Jenna Russell, Marzena Karpinska, Mohit Iyyer",
        "摘要": "摘要：在本文中，我们研究了人类识别商业大型语言模型（如GPT-4o、Claude、o1）生成文本的能力。我们雇佣注释员阅读300篇非小说类英语文章，并将其标记为人工撰写或AI生成，并提供段落长度的解释来支持他们的决定。我们的实验表明，经常使用大型语言模型进行写作任务的注释员即使没有接受任何专业训练或反馈，也能在识别AI生成文本方面表现出色。事实上，在五位此类“专家”注释员中的多数投票中，仅有一篇文章被误分类，显著优于我们评估的大多数商业和开源检测器，即使在存在诸如改写和人性化等规避策略的情况下也是如此。对专家自由形式解释的定性分析表明，虽然他们在很大程度上依赖于特定的词汇线索（“AI词汇”），但他们还捕捉到了文本中更复杂的现象（例如，形式、原创性、清晰度），这些是自动检测器难以评估的。我们发布了注释数据集和代码，以促进对AI生成文本的人工和自动检测的未来研究。\n\n标题：经常使用ChatGPT进行写作任务的人是准确且可靠的AI生成文本检测器\n\n作者：Jenna Russell, Marzena Karpinska, Mohit Iyyer\n\n评论：预印本，33页\n\n链接：https://arxiv.org/pdf/2501.15654.pdf",
        "地址": "https://arxiv.org/pdf/2501.15654.pdf"
    },
    {
        "名称": "2025 [2501.15891] Any2AnyTryon: Leveraging Adaptive Position Embeddings for Versatile Virtual Clothing Tasks.pdf",
        "作者": "Hailong Guo, Bohan Zeng, Yiren Song, Wentao Zhang, Chuang Zhang, Jiaming Liu",
        "摘要": "摘要：基于图像的虚拟试衣（VTON）旨在通过将输入的服装转移到目标人物的图像上来生成虚拟试衣结果。然而，服装模型配对数据的稀缺使现有方法难以在VTON中实现高泛化性和质量。此外，这也限制了生成无掩码试衣的能力。为了解决数据稀缺问题，Stable Garment和MMTryon等方法采用了合成数据策略，有效增加了模型方面的配对数据量。 然而，现有方法通常仅限于执行特定试衣任务，缺乏用户友好性。为了增强VTON生成的泛化性和可控性，我们提出了Any2AnyTryon，可以根据不同的文本指令和服装图像生成试衣结果，以满足各种需求，消除对掩码、姿势或其他条件的依赖。具体来说，我们首先构建了虚拟试衣数据集LAION-Garment，这是已知的最大开源服装试衣数据集。然后，我们引入了自适应位置嵌入，使模型能够基于不同尺寸和类别的输入图像生成满意的服装搭配模型图像或服装图像，显著增强了VTON生成的泛化性和可控性。在实验中，我们展示了Any2AnyTryon的有效性并与现有方法进行了比较。结果表明，Any2AnyTryon可以实现灵活、可控和高质量的基于图像的虚拟试衣。\n\n链接：https://arxiv.org/pdf/2501.15891.pdf\n\n标题：Any2AnyTryon: 利用自适应位置嵌入实现多样化虚拟服装任务\n\n作者：Hailong Guo, Bohan Zeng, Yiren Song, Wentao Zhang, Chuang Zhang, Jiaming Liu\n\n注释：13页，13张图",
        "地址": "https://arxiv.org/pdf/2501.15891.pdf"
    }
]