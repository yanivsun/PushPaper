[
    {
        "名称": "2025 [2502.06329] Expect the Unexpected: FailSafe Long Context QA for Finance.pdf",
        "作者": "Kiran Kamble, Melisa Russak, Dmytro Mozolevskyi, Muayad Ali, Mateusz Russak, Waseem AlShikh",
        "摘要": "摘要：我们提出了一种新的长上下文金融基准FailSafeQA，旨在测试LLM在金融领域基于查询回答系统中的六种人机交互变体的稳健性和上下文意识。我们集中研究了两个案例：查询失败和上下文失败。在查询失败场景中，我们通过扰动原始查询在领域专业知识、完整性和语言准确性方面进行变异。在上下文失败案例中，我们模拟上传退化、无关和空文档。我们采用了LLM-as-a-Judge方法，使用Qwen2.5-72B-Instruct，并通过细粒度的评分标准来定义和计算24个现成模型的稳健性、上下文基础和合规性评分。结果表明，尽管一些模型在缓解输入扰动方面表现出色，但它们必须在提供稳健答案和避免幻觉生成之间取得平衡。值得注意的是，被认为最合规的模型Palmyra-Fin-128k-Instruct保持了强劲的基线性能，但在17%的测试案例中遇到了维持稳健预测的挑战。另一方面，最稳健的模型OpenAI o3-mini在41%的测试案例中捏造了信息。结果表明，即使是高性能模型也有很大的改进空间，强调了FailSafeQA在开发优化金融应用中可靠的LLMs方面的作用。数据集可通过以下网址获得：this https URL\n\n翻译为中文：\n我们提出了一种新的长上下文金融基准，FailSafeQA，旨在测试LLM在金融领域基于查询回答系统中的六种人机交互变体下的稳健性和上下文意识。我们集中研究了两个案例：查询失败和上下文失败。在查询失败场景中，我们通过扰动原始查询在领域专业知识、完整性和语言准确性方面进行变异。在上下文失败案例中，我们模拟上传退化、无关和空文档。我们采用了LLM-as-a-Judge方法，使用Qwen2.5-72B-Instruct，并通过细粒度的评分标准来定义和计算24个现成模型的稳健性、上下文基础和合规性评分。结果表明，尽管一些模型在缓解输入扰动方面表现出色，但它们必须在提供稳健答案和避免幻觉生成之间取得平衡。值得注意的是，被认为最合规的模型Palmyra-Fin-128k-Instruct保持了强劲的基线性能，但在17%的测试案例中遇到了维持稳健预测的挑战。另一方面，最稳健的模型OpenAI o3-mini在41%的测试案例中捏造了信息。结果表明，即使是高性能模型也有很大的改进空间，强调了FailSafeQA在开发优化金融应用中可靠的LLMs方面的作用。数据集可通过以下网址获得：this https URL",
        "地址": "https://arxiv.org/pdf/2502.06329.pdf"
    },
    {
        "名称": "2025 [2502.06807] Competitive Programming with Large Reasoning Models.pdf",
        "作者": "OpenAI: Ahmed El-Kishky, Alexander Wei, Andre Saraiva, Borys Minaev, Daniel Selsam, David Dohan, Francis Song, Hunter Lightman, Ignasi Clavera, Jakub Pachocki, Jerry Tworek, Lorenz Kuhn, Lukasz Kaiser, Mark Chen, Max Schwarzer, Mostafa Rohaninejad, Nat McAleese, o3 contributors, Oleg Mürk, Rhythm Garg, Rui Shu, Szymon Sidor, Vineet Kosaraju, Wenda Zhou",
        "摘要": "摘要：我们展示了将强化学习应用于大型语言模型（LLMs）能够显著提升在复杂编码和推理任务中的表现。此外，我们比较了两个通用推理模型——OpenAI o1和较早期的o3模型检查点，与一个领域专用系统o1-ioi。o1-ioi采用了为参加2024年国际信息学奥林匹克（IOI）设计的手工推导策略。我们在2024年IOI现场比赛中使用o1-ioi获得了第49百分位的成绩。在放宽竞赛限制条件下，o1-ioi获得了金牌。然而，在评估之后的模型如o3时，我们发现o3在没有手工领域专用策略或放宽限制的情况下也能够获得金牌。我们的研究表明，尽管专用流程如o1-ioi能带来显著提升，但扩大规模的通用o3模型在没有依赖手工推理策略的情况下超越了这些结果。值得注意的是，o3在2024年IOI中获得了金牌，并在Codeforces上的评分与顶尖人类竞争者相当。总体而言，这些结果表明，扩大通用强化学习规模，而非依赖领域专用技术，提供了在推理领域（例如竞技编程）达到最先进AI水平的强大路径。\n\n链接：https://arxiv.org/pdf/2502.06807.pdf\n\n作者：OpenAI: Ahmed El-Kishky, Alexander Wei, Andre Saraiva, Borys Minaev, Daniel Selsam, David Dohan, Francis Song, Hunter Lightman, Ignasi Clavera, Jakub Pachocki, Jerry Tworek, Lorenz Kuhn, Lukasz Kaiser, Mark Chen, Max Schwarzer, Mostafa Rohaninejad, Nat McAleese, o3 贡献者, Oleg Mürk, Rhythm Garg, Rui Shu, Szymon Sidor, Vineet Kosaraju, Wenda Zhou",
        "地址": "https://arxiv.org/pdf/2502.06807.pdf"
    },
    {
        "名称": "2025 [2502.05878] Enhancing Financial Time-Series Forecasting with Retrieval-Augmented Large Language Models.pdf",
        "作者": "Mengxi Xiao, Zihao Jiang, Lingfei Qian, Zhengyu Chen, Yueru He, Yijing Xu, Yuecheng Jiang, Dong Li, Ruey-Ling Weng, Min Peng, Jimin Huang, Sophia Ananiadou, Qianqian Xie",
        "摘要": "摘要: 股票走势预测是金融时间序列预测中的一项关键任务，依赖于从庞大而复杂的数据集中识别并检索关键影响因素。然而，传统的基于文本训练或数值相似性的检索方法常常难以应对金融数据的复杂性。为了解决这个问题，我们提出了首个专为金融时间序列预测设计的检索增强生成（RAG）框架。我们的框架包含三个关键创新：以经过微调的10亿参数大语言模型（StockLLM）为骨干，一种通过大语言模型反馈增强的新颖候选选择方法，以及一种最大化查询与历史显著序列相似性的训练目标。这些进步使得我们的检索器FinSeer能够在复杂的金融数据集中发现有意义的模式，同时有效地减少噪音。为了支持稳健的评估，我们还构建了整合金融指标和历史股票价格的新数据集。实验结果表明，我们的RAG框架优于基线StockLLM和随机检索方法，展示了其有效性。作为检索器的FinSeer在BIGDATA22基准测试中获得了8%更高的准确性，并比现有的检索方法检索到更多有影响力的序列。这项工作突出了量身定制的检索模型在金融预测中的重要性，并为该领域未来的研究提供了一个新颖且可扩展的框架。\n\n作者: 萌曦·肖(Zihao Jiang)、子浩·姜(Lingfei Qian)、灵飞·钱(Zhengyu Chen)、正宇·陈(Yueru He)、月如·何(Yijing Xu)、艺婧·旭(Yuecheng Jiang)、阅澄·姜(Dong Li)、东·李(Ruey-Ling Weng)、瑞玲·翁(Min Peng)、敏·彭(Jimin Huang)、吉敏·黄(Sophia Ananiadou)、苏菲亚·安纳尼雅杜(Qianqian Xie)、倩倩·谢\n\n注释: 11页，4张图\n\n链接: [https://arxiv.org/pdf/2502.05878.pdf](https://arxiv.org/pdf/2502.05878.pdf)\n\n标题: 提升检索增强的大语言模型在金融时间序列预测中的应用",
        "地址": "https://arxiv.org/pdf/2502.05878.pdf"
    },
    {
        "名称": "2025 [2502.07316] CodeI/O: Condensing Reasoning Patterns via Code Input-Output Prediction.pdf",
        "作者": "Junlong Li, Daya Guo, Dejian Yang, Runxin Xu, Yu Wu, Junxian He",
        "摘要": "摘要：推理是大型语言模型的一项基本能力。虽然先前的研究主要集中在增强数学或代码生成等狭窄技能上，但由于训练数据稀疏和分散，提升其他许多推理任务的性能仍然具有挑战性。为解决这一问题，我们提出了CodeI/O，这是一种新颖的方法，通过将原始代码转换为代码输入输出预测格式，系统性地浓缩嵌入在情境代码中的多样推理模式。通过训练模型在完全自然语言的连锁思维（CoT）推理中，给定代码和测试案例来预测输入/输出，我们让模型接触到诸如逻辑流规划、状态空间搜索、决策树遍历和模块化分解等普遍的推理原语，同时将结构化推理与代码特定语法分离，并保持程序的严格性。实验结果表明，CodeI/O在符号、科学、逻辑、数学和数值以及常识推理任务中的表现有持续改善。通过匹配现有的真实输出或使用预测的输入重新执行代码，我们可以验证每个预测，并通过多轮修订进一步增强CoTs，最终形成CodeI/O++，实现更高的性能。我们的数据和模型可以在此https URL上获取。",
        "地址": "https://arxiv.org/pdf/2502.07316.pdf"
    },
    {
        "名称": "2025 [2502.07701] Magic 1-For-1: Generating One Minute Video Clips within One Minute.pdf",
        "作者": "Hongwei Yi, Shitong Shao, Tian Ye, Jiantong Zhao, Qingyu Yin, Michael Lingelbach, Li Yuan, Yonghong Tian, Enze Xie, Daquan Zhou",
        "摘要": "摘要：在本技术报告中，我们介绍了Magic 1-For-1 (Magic141)，这是一种具有优化内存消耗和推理延迟的高效视频生成模型。其关键思想很简单：将文本到视频生成任务分解为两个更简单的任务，即文本到图像生成和图像到视频生成，以进行扩散步骤蒸馏。我们验证了在使用相同的优化算法下，图像到视频任务确实比文本到视频任务更容易收敛。我们还探索了一系列优化技巧，从三个方面降低训练图像到视频（I2V）模型的计算成本：1）通过使用多模态先验条件注入来加速模型收敛；2）通过应用对抗步骤蒸馏来加速推理延迟；3）通过参数稀疏化优化推理内存成本。借助这些技术，我们能够在3秒内生成5秒的视频剪辑。通过应用测试时滑动窗口，我们能够在一分钟内生成一分钟长的视频，显著提高视觉质量和运动动态，每秒视频剪辑的平均生成时间不到1秒。我们进行了一系列初步探索，以找出扩散步骤蒸馏中计算成本和视频质量之间的最佳权衡，希望这能够成为开源探索的良好基础模型。代码和模型权重可在此https链接获取。",
        "地址": "https://arxiv.org/pdf/2502.07701.pdf"
    },
    {
        "名称": "2025 [2502.07374] LLMs Can Easily Learn to Reason from Demonstrations Structure, not content, is what matters!.pdf",
        "作者": "Dacheng Li, Shiyi Cao, Tyler Griggs, Shu Liu, Xiangxi Mo, Shishir G. Patil, Matei Zaharia, Joseph E. Gonzalez, Ion Stoica",
        "摘要": "摘要：大型推理模型（LRMs）通过遵循包含反思、回溯和自我验证的长链推理（Long CoT）来解决复杂的推理问题。然而，激发长链推理所需的训练技术和数据需求尚不清楚。在这项工作中，我们发现大型语言模型（LLM）可以通过数据高效的监督微调（SFT）和参数高效的低秩自适应（LoRA）有效地学习长链推理。仅使用17,000个长链推理训练样本，Qwen2.5-32B-Instruct模型在广泛的数学和编码基准上取得了显著的改进，包括在AIME 2024上达到56.7%（+40.0%）和在LiveCodeBench上达到57.0%（+8.1%），与专有的o1-preview模型的得分44.6%和59.1%具有竞争力。更重要的是，我们发现长链推理的结构对学习过程至关重要，而各个推理步骤的内容影响最小。影响内容的扰动，例如在错误样本上训练或删除推理关键词，对性能几乎没有影响。相比之下，破坏长链推理逻辑一致性的结构性修改，例如打乱或删除推理步骤，会显著降低准确性。例如，在长链推理样本上训练的模型，即使答案错误，其准确度仍比完全正确样本训练的模型低仅3.2%。这些洞见加深了我们对如何在LLMs中激发推理能力的理解，并强调了高效训练下一代推理模型的关键考虑因素。这是我们之前发布的Sky-T1-32B-Preview模型的学术论文。代码在此https URL提供。",
        "地址": "https://arxiv.org/pdf/2502.07374.pdf"
    },
    {
        "名称": "2025 [2502.06857] Gemstones: A Model Suite for Multi-Faceted Scaling Laws.pdf",
        "作者": "Sean McLeish, John Kirchenbauer, David Yu Miller, Siddharth Singh, Abhinav Bhatele, Micah Goldblum, Ashwinee Panda, Tom Goldstein",
        "摘要": "摘要： \n缩放定律通常使用在一系列固定超参数选择范围内的模型族进行拟合。在这项工作中，我们通过使用大量的架构和超参数选择来研究缩放定律，并强调它们对最终处方的影响。作为我们研究的主要成果，我们发布了Gemstones：迄今为止最全面的开源缩放定律数据集，包括来自具有高达20亿参数变压器的4000多个检查点；这些模型经过了不同的学习率、冷却时间表和架构形状的训练。我们的检查点使得缩放的更复杂研究成为可能，例如，预测语言建模性能作为模型宽度和深度函数的定律。通过检查我们模型套件的各个方面，我们发现缩放定律的处方对实验设计过程和拟合过程中使用的具体模型检查点高度敏感。代码：this https URL\n\n翻译：\n缩放定律通常使用在一系列固定超参数选择范围内的模型族进行拟合。在这项工作中，我们通过使用大量的架构和超参数选择来研究缩放定律，并强调它们对最终处方的影响。作为我们研究的主要成果，我们发布了Gemstones：迄今为止最全面的开源缩放定律数据集，包括来自具有高达20亿参数变压器的4000多个检查点；这些模型经过了不同的学习率、冷却时间表和架构形状的训练。我们的检查点使得缩放的更复杂研究成为可能，例如，预测语言建模性能作为模型宽度和深度函数的定律。通过检查我们模型套件的各个方面，我们发现缩放定律的处方对实验设计过程和拟合过程中使用的具体模型检查点高度敏感。代码：this https URL",
        "地址": "https://arxiv.org/pdf/2502.06857.pdf"
    },
    {
        "名称": "2025 [2502.03492] Teaching Language Models to Critique via Reinforcement Learning.pdf",
        "作者": "Zhihui Xie, Jie chen, Liyu Chen, Weichao Mao, Jingjing Xu, Lingpeng Kong",
        "摘要": "摘要：教大语言模型（LLMs）批评和改进其输出对于构建能够迭代改进的系统至关重要，但这在根本上受到提供准确判断和可行建议的能力的限制。在这项工作中，我们研究了代码生成的LLM批评者，并提出了$\\\\texttt{CTRL}$，一个通过$\\\\texttt{R}$einforcement $\\\\texttt{L}$earning（强化学习）进行批评者训练（$\\\\texttt{C}$ritic $\\\\texttt{T}$raining）的框架，该框架训练批评者模型生成最大化修正性能的反馈，以实现固定生成器模型在没有人工监督的情况下进行修正。我们的结果表明，使用$\\\\texttt{CTRL}$训练的批评者显著提高了通过率，并在基础和更强的生成器模型中减少了复合错误。此外，我们展示了这些批评者模型作为精确生成奖励模型的效果，并通过批评-修订的迭代过程实现了测试时间缩放，在具有挑战性的代码生成基准测试中实现了高达106.1％的相对改进。\n\n作者：谢之辉， 陈杰， 陈立宇， 毛伟超， 徐晶晶， 孔令鹏\n\n链接：https://arxiv.org/pdf/2502.03492.pdf\n\n标题：《2025 [2502.03492] 通过强化学习教语言模型批评》",
        "地址": "https://arxiv.org/pdf/2502.03492.pdf"
    },
    {
        "名称": "2025 [2502.07617] Scaling Pre-training to One Hundred Billion Data for Vision Language Models.pdf",
        "作者": "Xiao Wang, Ibrahim Alabdulmohsin, Daniel Salz, Zhe Li, Keran Rong, Xiaohua Zhai",
        "摘要": "摘要：我们对在前所未有的规模上（1000亿例）进行预训练视觉语言模型的潜力进行了实证研究。我们发现，在许多常见的西方中心分类和检索基准测试（如COCO Captions）中，模型性能在这一规模上趋于饱和。然而，文化多样性任务由于覆盖了长尾概念，从1000亿规模的网络数据中获得了更大的收益。此外，我们还分析了模型的多语言性，并展示了低资源语言的收益。另外，我们观察到，通过使用如CLIP之类的质量过滤器来减少预训练数据集的大小，通常用于增强性能，可能会无意中减少即使是在大规模数据集中也代表的文化多样性。我们的研究结果表明，虽然传统基准测试可能不会显著受益于将噪声、原始网络数据扩展到1000亿例，但这一数据规模对于构建真正包容的多模态系统至关重要。",
        "地址": "https://arxiv.org/pdf/2502.07617.pdf"
    },
    {
        "名称": "2025 [2502.07508] Enhance-A-Video: Better Generated Video for Free.pdf",
        "作者": "Yang Luo, Xuanlei Zhao, Mengzhao Chen, Kaipeng Zhang, Wenqi Shao, Kai Wang, Zhangyang Wang, Yang You",
        "摘要": "摘要：基于DiT的视频生成已取得显著成果，但在提升现有模型方面的研究相对较少。在这项工作中，我们介绍了一种无需训练的方法来增强基于DiT生成视频的连贯性和质量，称为Enhance-A-Video。核心思想是基于非对角线时间注意分布增强跨帧相关性。由于其设计简洁，我们的方法可以轻松应用于大多数基于DiT的视频生成框架，而无需重新训练或微调。在各种基于DiT的视频生成模型中，我们的方法在时间一致性和视觉质量方面均表现出令人鼓舞的改进。我们希望这项研究能够激发视频生成增强领域未来的探索。",
        "地址": "https://arxiv.org/pdf/2502.07508.pdf"
    },
    {
        "名称": "2025 [2502.06589] Hephaestus: Improving Fundamental Agent Capabilities of Large Language Models through Continual Pre-Training.pdf",
        "作者": "Yuchen Zhuang, Jingfeng Yang, Haoming Jiang, Xin Liu, Kewei Cheng, Sanket Lokegaonkar, Yifan Gao, Qing Ping, Tianyi Liu, Binxuan Huang, Zheng Li, Zhengyang Wang, Pei Chen, Ruijie Wang, Rongzhi Zhang, Nasser Zalmout, Priyanka Nigam, Bing Yin, Chao Zhang",
        "摘要": "摘要：由于面向代理的预训练数据稀缺，基于大型语言模型（LLM）的自主代理通常依赖复杂的提示或大量的微调，这往往难以在保持出色的广泛适应性的同时引入新能力。我们引入了Hephaestus-Forge，这是首个大规模预训练语料库，旨在增强LLM代理在API函数调用、内在推理和规划，以及适应环境反馈方面的基本能力。Hephaestus-Forge包括103B的代理特定数据，涵盖76,537个API，包括工具文档以引入API函数的知识和函数调用轨迹以增强内在推理。为了探索有效的训练协议，我们研究了扩展法则，以确定数据混合比率的最佳配方。通过在Hephaestus-Forge上持续预训练，Hephaestus在三个代理基准上击败了小型到中型的开源LLM，并能与商业LLM相媲美，证明了我们的预训练语料库在增强LLM的基本代理能力和其对新任务或环境的泛化能力方面的有效性。\n\n翻译作者：周钰辰，杨经风，蒋昊明，刘鑫，程可为，Sanket Lokegaonkar，高奕凡，平青，刘天一，黄冰轩，李正，王正阳，陈培，王瑞杰，张荣志，Nasser Zalmout，Priyanka Nigam，殷秉，张超.",
        "地址": "https://arxiv.org/pdf/2502.06589.pdf"
    },
    {
        "名称": "2025 [2502.07527] NatureLM: Deciphering the Language of Nature for Scientific Discovery.pdf",
        "作者": "Yingce Xia, Peiran Jin, Shufang Xie, Liang He, Chuan Cao, Renqian Luo, Guoqing Liu, Yue Wang, Zequn Liu, Yuan-Jyue Chen, Zekun Guo, Yeqi Bai, Pan Deng, Yaosen Min, Ziheng Lu, Hongxia Hao, Han Yang, Jielan Li, Chang Liu, Jia Zhang, Jianwei Zhu, Kehan Wu, Wei Zhang, Kaiyuan Gao, Qizhi Pei, Qian Wang, Xixian Liu, Yanting Li, Houtian Zhu, Yeqing Lu, Mingqian Ma, Zun Wang, Tian Xie, Krzysztof Maziarz, Marwin Segler, Zhao Yang, Zilong Chen, Yu Shi, Shuxin Zheng, Lijun Wu, Chen Hu, Peggy Dai, Tie-Yan Liu, Haiguang Liu, Tao Qin",
        "摘要": "以下是从提供材料中提取的摘要，并翻译为中文：\n\n摘要：基础模型已经彻底改变了自然语言处理和人工智能，显著提升了机器理解和生成人类语言的能力。受这些基础模型成功的启发，研究人员为个别科学领域（包括小分子、材料、蛋白质、DNA和RNA）开发了基础模型。然而，这些模型通常在孤立的环境中训练，缺乏跨不同科学领域集成的能力。认识到这些领域中的实体都可以表示为序列，并共同形成“自然语言”，我们引入了自然语言模型（简称NatureLM），这是一个基于序列的科学基础模型，旨在推动科学发现。通过多个科学领域的数据进行预训练，NatureLM提供了一个统一、多功能的模型，能够实现各种应用，包括： (i) 使用文本指令生成和优化小分子、蛋白质、RNA和材料；(ii) 跨领域生成/设计，如蛋白质-小分子和蛋白质-RNA生成；(iii) 在SMILES到IUPAC翻译和USPTO-50k回溯合成等任务中达到最新的性能。NatureLM为各种科学任务（包括药物发现（命中生成/优化、ADMET优化、合成）、新材料设计和治疗性蛋白或核苷酸的开发）提供了一种有前途的通用方法。我们开发了不同规模（10亿、80亿和467亿参数）的NatureLM模型，并观察到随着模型规模的增加，其性能显著提升。",
        "地址": "https://arxiv.org/pdf/2502.07527.pdf"
    },
    {
        "名称": "2025 [2502.04223] Éclair -- Extracting Content and Layout with Integrated Reading Order for Documents.pdf",
        "作者": "Ilia Karmanov, Amala Sanjay Deshmukh, Lukas Voegtle, Philipp Fischer, Kateryna Chumachenko, Timo Roman, Jarno Seppänen, Jupinder Parmar, Joseph Jennings, Andrew Tao, Karan Sapra",
        "摘要": "摘要：光学字符识别（OCR）技术广泛应用于从文档图像中提取文本，从而促进高效的数字化和数据检索。然而，在处理复杂文档时，仅仅提取文本是不够的。全面理解此类文档需要理解其结构——包括格式、公式、表格、多页中多个区块和列的阅读顺序——以及检测诸如脚注和图像说明等元素的语义信息。这种全面的理解对于下游任务（如检索、文档问答以及用于训练大型语言模型（LLMs）和视觉语言模型（VLMs）的数据管理）至关重要。为此，我们介绍了Éclair，一种专门设计用于处理广泛文档类型的通用文本提取工具。给定图像，Éclair能够按阅读顺序提取格式化文本，以及边界框及其相应的语义类别。为了全面评估这些新功能，我们引入了用于文档级OCR和语义分类的人类注释的多样化基准。Éclair在这一基准上实现了最先进的准确性，在关键指标上超过了其他方法。此外，我们在已建立的基准上评估Éclair，展示了其在多个评价标准中的多功能性和优势。",
        "地址": "https://arxiv.org/pdf/2502.04223.pdf"
    },
    {
        "名称": "2025 [2502.03997] CAD-Editor: A Locate-then-Infill Framework with Automated Training Data Synthesis for Text-Based CAD Editing.pdf",
        "作者": "Yu Yuan, Shizhao Sun, Qi Liu, Jiang Bian",
        "摘要": "摘要：计算机辅助设计（CAD）在各个行业都是不可或缺的。基于文本的CAD编辑，通过自动化根据文本指令修改CAD模型，具有巨大的潜能，但仍未得到充分探索。现有的方法主要集中于设计变体生成或基于文本的CAD生成，缺乏对基于文本的控制支持或忽视现有CAD模型的约束。我们介绍了CAD-Editor，这是第一个用于基于文本的CAD编辑的框架。为了解决训练所需的准确的三元组数据的问题，我们提出了一个自动化的数据合成管道。该管道利用设计变体模型生成原始和编辑后的CAD模型对，并利用大型视觉语言模型（LVLMs）总结其差异为编辑指令。为了解决基于文本的CAD编辑的复合性质，我们提出了一个定位-填充框架，将任务分解为两个专注的子任务：定位需要修改的区域和在这些区域填充适当的编辑。大型语言模型（LLMs）作为这两个子任务的骨干，利用它们在自然语言理解和CAD知识方面的能力。实验表明CAD-Editor在定量和定性方面均表现出色。\n\n作者：袁宇，孙世昭，刘琦，卞江\n\n链接：https://arxiv.org/pdf/2502.03997.pdf\n\n标题：CAD-Editor: 基于自动训练数据合成的定位-填充框架，用于基于文本的CAD编辑",
        "地址": "https://arxiv.org/pdf/2502.03997.pdf"
    },
    {
        "名称": "2025 [2502.07531] VidCRAFT3: Camera, Object, and Lighting Control for Image-to-Video Generation.pdf",
        "作者": "Sixiao Zheng, Zimian Peng, Yanpeng Zhou, Yi Zhu, Hang Xu, Xiangru Huang, Yanwei Fu",
        "摘要": "摘要：近期的图像到视频生成方法展示了在控制一个或两个视觉元素（如相机轨迹或物体运动）方面的成功。然而，由于数据和网络效能的限制，这些方法无法实现对多个视觉元素的控制。在本文中，我们介绍了一种新颖的图像到视频生成框架——VidCRAFT3，该框架能够同时精确地控制相机运动、物体运动和光照方向。为了更好地解耦每个视觉元素的控制，我们提出了空间三重注意力变压器，它对光照方向、文本和图像进行对称整合。由于大多数现实世界的视频数据集缺乏光照注释，我们构建了一个高质量的合成视频数据集——VideoLightingDirection（VLD）数据集。该数据集包含光照方向注释和各种外观的物体，使VidCRAFT3能够有效处理强光传播和反射效果。此外，我们提出了一个三阶段训练策略，消除了对同时带有多个视觉元素（相机运动、物体运动和光照方向）注释训练数据的需求。基准数据集上的大量实验表明，VidCRAFT3在高质量视频内容的生成方面，在控制粒度和视觉连贯性上均超越了现有的最先进的方法。所有代码和数据将公开可用。\n\n作者：Sixiao Zheng, Zimian Peng, Yanpeng Zhou, Yi Zhu, Hang Xu, Xiangru Huang, Yanwei Fu\n\nURL：https://arxiv.org/pdf/2502.07531.pdf\n\n标题：VidCRAFT3：用于图像到视频生成的相机、物体和光照控制",
        "地址": "https://arxiv.org/pdf/2502.07531.pdf"
    },
    {
        "名称": "2025 [2502.05364] Hypencoder: Hypernetworks for Information Retrieval.pdf",
        "作者": "Julian Killingback, Hansi Zeng, Hamed Zamani",
        "摘要": "摘要：绝大多数检索模型依赖于向量内积来生成查询和文档之间的相关性评分，这自然限制了可以采用的相关性评分的表达能力。我们提出了一种新范式，代替生成表示查询的向量，我们生成一个小型神经网络作为学习的相关性函数。这个小型神经网络输入文档的表示（本文使用单个向量），并生成一个标量的相关性评分。为了生成这个小型神经网络，我们使用超网络作为查询编码器，或我们称之为Hypencoder的网络，该网络生成其他网络的权重。对领域内搜索任务的实验表明，Hypencoder能够显著优于强大的密集检索模型，并且其指标高于重新排序模型和大一个数量级的模型。Hypencoder在领域外搜索任务中也展现了良好的泛化能力。为了评估Hypencoder的能力，我们在一组困难的检索任务上进行评测，包括tip-of-the-tongue检索和指令跟随检索任务，发现与标准检索任务相比，性能差距显著扩大。此外，为了展示我们方法的实用性，我们实现了一个近似搜索算法，并展示我们的模型能够在60毫秒内搜索880万文档。\n\n——Julian Killingback, Hansi Zeng, Hamed Zamani\n\n链接：https://arxiv.org/pdf/2502.05364.pdf\n\n标题：2025 [2502.05364] Hypencoder: 超网络用于信息检索.pdf",
        "地址": "https://arxiv.org/pdf/2502.05364.pdf"
    },
    {
        "名称": "2025 [2502.06428] CoS: Chain-of-Shot Prompting for Long Video Understanding.pdf",
        "作者": "Jian Hu, Zixu Cheng, Chenyang Si, Wei Li, Shaogang Gong",
        "摘要": "摘要：多模态大语言模型（MLLMs）由于需要过多的视觉标记，在处理长视频时表现不佳。这些标记大大超过了MLLMs的上下文长度，从而导致填充了大量与任务无关的画面。如何选择画面仍然是一个尚未解决的关键问题：稀疏采样有可能遗漏关键细节，而全面采样又会使模型被无关内容淹没，导致视频误解。为了解决这个问题，我们提出了链式画面提示（CoS）。其关键思想是将画面选择视为测试时的视觉提示优化，通过优化画面与任务的对齐来自适应选择与视频理解语义任务相关的画面。CoS有两个关键部分：(1) 一个二元视频摘要机制，它执行伪时间定位，发现二进制编码以识别与任务相关的画面，(2)一个视频共同推理模块，它部署二进制编码来配对（学习对齐）与任务相关的正画面和无关的负画面。它将优化后的画面选择嵌入原始视频中，有助于集中在相关上下文上，从而优化长视频理解。在三个基线和五个数据集上的实验证明了CoS的有效性和适应性。代码见此https URL。\n\n作者：胡健、程子旭、司晨阳、李伟、龚绍刚\n\n评论：一种无需训练的长视频理解测试时优化方法\n\n网址：https://arxiv.org/pdf/2502.06428.pdf\n\n标题：2025 [2502.06428] CoS: 长视频理解的链式画面提示",
        "地址": "https://arxiv.org/pdf/2502.06428.pdf"
    },
    {
        "名称": "2025 [2502.07490] Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn More.pdf",
        "作者": "Xialie Zhuang, Zhikai Jia, Jianjin Li, Zhenyu Zhang, Li Shen, Zheng Cao, Shiwei Liu",
        "摘要": "摘要: 大型语言模型 (LLMs) 在准确检索关键信息方面存在不足。为解决此问题，我们提出了一种简单而有效的训练范式——屏蔽增强自回归预测 (MEAP)，它将遮罩语言模型 (MLM) 无缝集成到下一个词的预测 (NTP) 中，以增强后者的上下文检索能力。具体来说，MEAP 首先随机遮罩一小部分输入标记，然后直接使用仅解码器的 Transformer 执行标准的下一个词预测自回归。MEAP 不需要为 MLM 进行双向注意或编码器-解码器结构，从而在预训练或推理过程中不增加额外的计算开销。大量实验表明，MEAP 在关键信息检索和长上下文推理任务上显著优于 NTP，同时在常识推理任务上表现相当或更好。MEAP 的优势还延伸到有监督微调，在丢失中间场景中表现出显著优越性，较 NTP 提高了 11.77 个百分点。我们的分析表明，MEAP 的有效性源于其通过集中在较少的非遮罩标记上，促进了更易区分的注意分数。这一机制提高了模型对任务相关信号的关注度，同时减轻了外围上下文的影响。这些发现表明 MEAP 是大型语言模型的一个有前景的训练范式。",
        "地址": "https://arxiv.org/pdf/2502.07490.pdf"
    },
    {
        "名称": "2025 [2502.07785] Pippo: High-Resolution Multi-View Humans from a Single Image.pdf",
        "作者": "Yash Kant, Ethan Weber, Jin Kyu Kim, Rawal Khirodkar, Su Zhaoen, Julieta Martinez, Igor Gilitschenski, Shunsuke Saito, Timur Bagautdinov",
        "摘要": "摘要：我们介绍了Pippo，这是一种生成模型，能够从一张随意拍摄的照片中生成1K分辨率的密集环绕视频。Pippo是一个多视图扩散变压器（diffusion transformer），不需要额外的输入——例如，输入图像的的摄像机参数或拟合的参数模型。我们在没有标题的30亿张人类图像上对Pippo进行了预训练，并在录制影棚捕获的人类图像上进行多视角中期训练和后期训练。在中期训练中，为了快速吸收影棚数据集，我们在低分辨率上对多达48个视图进行去噪，并使用浅层MLP粗略地编码目标摄像机。在后期训练中，我们对较少的视图进行高分辨率去噪，并使用像素对齐的控制（例如，空间锚点和Plücker射线）以实现3D一致生成。在推理时，我们提出了一种注意力偏置技术，使Pippo能够同时生成比训练时多5倍以上的视图。最后，我们还介绍了一种改进的度量方法来评估多视图生成的3D一致性，并表明Pippo在从单张图像生成多视图人类方面优于现有工作。\n\n翻译：我们推出了Pippo，这是一种生成模型，能够从一张随意拍摄的照片中生成1K分辨率的紧密环绕视频。Pippo是一个多视图的扩散变压器，不需要任何额外的输入——例如，输入图像的拟合参数模型或摄像机参数。我们在没有标题的30亿张人类图像上对Pippo进行了预训练，并在影棚拍摄的真人上进行了多视角中期训练和后期训练。在中期训练中，为了快速吸收影棚数据集，我们在低分辨率下对多个（最多48个）视图进行去噪，并使用浅层MLP粗略地编码目标摄像机。在后期训练中，我们对较少的视图进行高分辨率去噪，并使用像素对齐的控制（例如空间锚和Plücker射线）来实现3D一致性的生成。在推理时，我们提出了一种注意力偏置技术，使Pippo能够同时生成比训练时多5倍以上的视图。最后，我们还引入了一种改进的度量方法来评估多视图生成的3D一致性，并表明在从单张图像生成多视图人类方面，Pippo优于现有的工作。",
        "地址": "https://arxiv.org/pdf/2502.07785.pdf"
    },
    {
        "名称": "2025 [2502.07445] Forget What You Know about LLMs Evaluations - LLMs are Like a Chameleon.pdf",
        "作者": "Nurit Cohen-Inger, Yehonatan Elisha, Bracha Shapira, Lior Rokach, Seffi Cohen",
        "摘要": "摘要：大型语言模型（LLMs）在公共基准测试上似乎表现出色，但这些高分可能掩盖了对数据集特定表面线索的过度依赖，而非真正的语言理解。我们介绍了变色龙基准过拟合检测器（C-BOD），这是一种元评估框架，它通过参数化变换系统地扭曲基准测试提示，并检测LLMs的过拟合情况。通过在保持语义内容和标签不变的情况下改写输入，C-BOD揭示了模型性能是否由记忆模式驱动。在使用26个领先的LLMs对MMLU基准进行评估时，我们的方法在适度扰动下显示出平均2.15%的性能下降，其中有20个模型表现出显著差异。值得注意的是，基线准确度较高的模型在扰动下表现出较大的性能差异，而较大的LLM对改写更为敏感，这表明两种情况都可能过分依赖固定的提示模式。相比之下，Llama家族和基线准确度较低的模型表现出无关紧要的下降，表明对表面线索的依赖性较小。此外，C-BOD的数据集和模型无关的设计允许其轻松集成到训练管道中，以促进更稳健的语言理解。我们的研究结果挑战了社区要超越排行榜得分，优先考虑LLM评估中的韧性和泛化能力。",
        "地址": "https://arxiv.org/pdf/2502.07445.pdf"
    },
    {
        "名称": "2025 [2502.06755] Sparse Autoencoders for Scientifically Rigorous Interpretation of Vision Models.pdf",
        "作者": "Samuel Stevens, Wei-Lun Chao, Tanya Berger-Wolf, Yu Su",
        "摘要": "摘要：要真正理解视觉模型，我们不仅要解释其学习的特征，还要通过控制实验验证这些解释。当前的方法要么提供可解释的特征，但无法测试其因果影响，要么支持模型编辑，但无法进行可解释的控制。我们提出了一种使用稀疏自动编码器（SAE）的统一框架，该框架填补了这一空白，使我们能够发现人类可解释的视觉特征，并精确操控这些特征，以检验关于模型行为的假设。通过将我们的方法应用于最先进的视觉模型，我们揭示了具有不同预训练目标的模型在语义抽象方面的关键差异。然后，我们通过多个视觉任务中的控制干预展示了我们框架的实际用途。我们展示了SAE可以在无需重新训练模型的情况下可靠地识别和操控可解释的视觉特征，为理解和控制视觉模型行为提供了强有力的工具。我们在项目网站上提供了代码、演示和模型：此https URL。\n\n",
        "地址": "https://arxiv.org/pdf/2502.06755.pdf"
    },
    {
        "名称": "2025 [2502.07776] Auditing Prompt Caching in Language Model APIs.pdf",
        "作者": "Chenchen Gu, Xiang Lisa Li, Rohith Kuditipudi, Percy Liang, Tatsunori Hashimoto",
        "摘要": "摘要：在大型语言模型（LLMs）中，提示缓存（prompt caching）会导致数据依赖的时间变化：缓存的提示处理速度比未缓存的提示更快。这些时间差异引入了侧信道时间攻击的风险。例如，如果缓存是跨用户共享的，攻击者可以从快速的API响应时间中识别缓存提示，从而了解其他用户的提示信息。由于提示缓存可能导致隐私泄露，API提供商的缓存策略透明度非常重要。为此，我们开发并进行了统计审计，以检测真实世界LLM API提供商的提示缓存。我们在包括OpenAI在内的七个API提供商中检测到用户间的全局缓存共享，导致用户提示信息可能泄露。提示缓存引起的时间变化还可能导致关于模型架构的信息泄露。具体而言，我们发现证据表明OpenAI的嵌入模型是仅由解码器组成的Transformer，这在以前尚未公开。\n\n作者：陈辰谷，李湘丽莎，罗希特·库迪提普迪，珀西·梁，桥本达则\n\n评论：20页，7个图\n\n网址：https://arxiv.org/pdf/2502.07776.pdf\n\n标题：2025 [2502.07776] 审计语言模型API中的提示缓存.pdf",
        "地址": "https://arxiv.org/pdf/2502.07776.pdf"
    },
    {
        "名称": "2025 [2502.07640] Goedel-Prover: A Frontier Model for Open-Source Automated Theorem Proving.pdf",
        "作者": "Yong Lin, Shange Tang, Bohan Lyu, Jiayun Wu, Hongzhou Lin, Kaiyu Yang, Jia Li, Mengzhou Xia, Danqi Chen, Sanjeev Arora, Chi Jin",
        "摘要": "摘要：我们介绍了一种名为Goedel-Prover的开源大语言模型（LLM），它在数学问题的自动形式化证明生成方面达到了最新的前沿水平（SOTA）。该领域的关键挑战在于形式化数学陈述和证明的稀缺性，我们通过以下方式应对此挑战。我们训练陈述形式化器，将Numina中的自然语言数学问题翻译为形式语言（Lean 4），以创建一个包含164万条形式陈述的数据集。LLM用于检查形式陈述是否准确保留了原始自然语言问题的内容。然后，我们通过训练一系列证明器，迭代地构建一个大型形式证明数据集。每个证明器成功证明了许多前一个证明器无法证明的陈述，这些新证明被添加到下一个证明器的训练集。最终证明器在整个证明生成方面超越了所有现有的开源模型。在miniF2F基准测试中，它实现了57.6%的成功率（Pass@32），比之前的最佳开源模型高出7.6%。在PutnamBench中，Goedel-Prover成功解决了7个问题（Pass@512），在排行榜上排名第一。此外，它为Lean Workbook问题生成了29.7K个形式证明，几乎是之前工作的15.7K的两倍。",
        "地址": "https://arxiv.org/pdf/2502.07640.pdf"
    },
    {
        "名称": "2025 [2502.05932] Skill Expansion and Composition in Parameter Space.pdf",
        "作者": "Tenglong Liu, Jianxiong Li, Yinan Zheng, Haoyi Niu, Yixing Lan, Xin Xu, Xianyuan Zhan",
        "摘要": "摘要: 人类擅长运用已有知识应对新挑战，并在解决问题的过程中发展技能。这一范式在自主代理系统的发展中变得越来越流行，因为它开发出了一种能够像人类一样响应新挑战自我进化的系统。然而，先前的方法在扩展新技能时训练效率有限，未能充分利用先前知识来促进新任务的学习。在本文中，我们提出了参数化技能扩展与组合（Parametric Skill Expansion and Composition，PSEC），这一新框架旨在通过保持可管理的技能库以迭代地进化代理的能力并有效应对新挑战。这个库可以逐步集成技能原语，作为参数高效微调的插拔式低秩适应（Low-Rank Adaptation，LoRA）模块，促进高效且灵活的技能扩展。这种结构还在参数空间中通过合并编码不同技能的LoRA模块实现直接技能组合，利用不同技能之间的共用信息来有效编程新技能。在此基础上，我们提出了一个情境感知模块，以动态激活不同技能协同处理新任务。通过赋能多目标组合、动力学转变和连续策略转移等多种应用，在D4RL、DSRL基准测试和DeepMind Control Suite上的结果表明，PSEC表现出卓越的能力，能够利用先前知识高效应对新挑战，并扩展其技能库以进化能力。 项目网址：this https URL.",
        "地址": "https://arxiv.org/pdf/2502.05932.pdf"
    },
    {
        "名称": "2025 [2502.04465] FocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks.pdf",
        "作者": "Luca Della Libera, Francesco Paissan, Cem Subakan, Mirco Ravanelli",
        "摘要": "摘要：大型语言模型通过在大规模数据集上进行自监督预训练，彻底改变了自然语言处理。受此成功启发，研究人员探索了将这些方法适应到语音中，具体方法是使用神经音频编解码器将连续音频离散化为令牌。然而，现有方法面临着诸多限制，包括高比特率、语义或声学信息的丢失，以及试图同时捕捉两者时依赖于多码本设计，从而增加了下游任务的架构复杂性。为了解决这些挑战，我们介绍了FocalCodec，这是一种基于焦点调制的高效低比特率编解码器，利用单一的二进制码本将语音压缩至0.16到0.65 kbps。FocalCodec在语音重合成和语音转换中的表现优于当前最先进技术，并且能够有效处理多语言语音和嘈杂环境。对下游任务的评估表明，FocalCodec能够成功保留足够的语义和声学信息，同时也非常适合生成建模。演示样本、代码和检查点可在此处获取：https URL。",
        "地址": "https://arxiv.org/pdf/2502.04465.pdf"
    },
    {
        "名称": "2025 [2502.06884] Learning Conformal Abstention Policies for Adaptive Risk Management in Large Language and Vision-Language Models.pdf",
        "作者": "Sina Tayebati, Divake Kumar, Nastaran Darabi, Dinithi Jayasuriya, Ranganath Krishnan, Amit Ranjan Trivedi",
        "摘要": "摘要：大语言模型和视觉-语言模型（LLMs/VLMs）在安全关键应用中使用日益增多，但其不透明的决策过程使风险评估和可靠性变得复杂。不确定性量化（UQ）有助于评估预测信心，并在不确定性高时选择回避。作为领先的UQ方法，保形预测（CP）提供了统计保证，但其依赖于静态阈值，无法适应任务复杂性和不断变化的数据分布，导致准确性、覆盖率和信息量之间的权衡次优。为了解决这一问题，我们提出了可学习的保形回避方法，将强化学习（RL）与CP集成，以动态优化回避阈值。通过将CP阈值视为自适应行为，我们的方法平衡了多个目标，在维持可靠覆盖的同时最小化预测集大小。广泛的评估表明，我们的方法在多个LLM/VLM基准上表现优于最不模糊分类器（LAC）和自适应预测集（APS），提高了高达3.2%的准确性，将幻觉检测的AUROC提升了22.19%，增强了不确定性引导选择生成（AUARC）21.17%，并将校准误差减少了70%-85%。这些改进在多个模型和数据集上都得到了保持，同时始终满足90%的覆盖目标，确立了我们的方法作为安全关键应用中更有效和灵活的可靠决策解决方案。代码可以在以下网址获得：{此https URL}。",
        "地址": "https://arxiv.org/pdf/2502.06884.pdf"
    }
]