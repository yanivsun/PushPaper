[
    {
        "名称": "2025 [2503.18878] I Have Covered All the Bases Here: Interpreting Reasoning Features in Large Language Models via Sparse Autoencoders.pdf",
        "作者": "Andrey Galichin, Alexey Dontsov, Polina Druzhinina, Anton Razzhigaev, Oleg Y. Rogov, Elena Tutubalina, Ivan Oseledets",
        "摘要": "摘要：大型语言模型（LLMs）在自然语言处理领域取得了显著的成功。最近的进展产生了一类新的推理LLMs；例如，开源的DeepSeek-R1通过整合深度思考和复杂推理达到了最先进的性能。尽管这些模型具有出色的能力，其内部推理机制仍然未被探索。在这项工作中，我们采用稀疏自动编码器（SAEs），一种学习神经网络的潜在表示的稀疏分解为可解释特征的方法，来识别驱动DeepSeek-R1系列模型推理的特征。首先，我们提出了一种方法，从SAE表示中提取候选“推理特征”。我们通过实证分析和可解释性方法验证了这些特征，证明它们与模型的推理能力直接相关。重要的是，我们展示了系统地操控这些特征可以显著增强推理性能，首次提供了对LLMs推理的机制解释。代码可在此链接获取 https://arxiv.org/pdf/2503.18878.pdf",
        "地址": "https://arxiv.org/pdf/2503.18878.pdf"
    },
    {
        "名称": "2025 [2503.17359] Position: Interactive Generative Video as Next-Generation Game Engine.pdf",
        "作者": "Jiwen Yu, Yiran Qin, Haoxuan Che, Quande Liu, Xintao Wang, Pengfei Wan, Di Zhang, Xihui Liu",
        "摘要": "摘要：现代游戏开发面临着由于传统游戏引擎中预定内容而导致的创意与成本的重大挑战。最近在视频生成模型方面的突破，能够合成逼真和互动的虚拟环境，为游戏创作带来了革命性的机会。在这篇立场论文中，我们提出了互动生成视频（IGV）作为生成游戏引擎（GGE）的基础，使得下一代游戏中能够无限生成新颖内容。GGE利用IGV在高质量内容无限合成、物理感知世界建模、用户控制的互动性、长期记忆能力和因果推理等方面的独特优势。我们提出了一个详细的框架，展示了GGE的核心模块以及指导其发展的分层成熟路线图（L0-L4）。我们的研究为AI时代的游戏开发规划了一条新道路，展望了一个由AI驱动的生成系统从根本上重塑游戏创造和体验的未来。\n\n翻译的摘要：\n现代游戏开发因传统游戏引擎中预定内容导致的创意和成本问题面临重大挑战。最近在视频生成模型方面的突破，可以合成逼真和互动的虚拟环境，为游戏创作提供了革命性机遇。在这篇立场论文中，我们提出互动生成视频（IGV）作为生成游戏引擎（GGE）的基础，使得下一代游戏能够无限生成新颖内容。GGE利用IGV在高质量内容合成、物理感知世界建模、用户控制的互动性、长期记忆能力和因果推理等方面的独特优势。我们提出了一个详细的框架，展示了GGE的核心模块以及其发展的分层成熟路线图（L0-L4）。我们的工作为AI时代的游戏开发规划了一条新路线，展望了一个AI驱动的生成系统从根本上重塑游戏创建和体验的未来。",
        "地址": "https://arxiv.org/pdf/2503.17359.pdf"
    },
    {
        "名称": "2025 [2503.18942] Video-T1: Test-Time Scaling for Video Generation.pdf",
        "作者": "Fangfu Liu, Hanyang Wang, Yimo Cai, Kaiyan Zhang, Xiaohang Zhan, Yueqi Duan",
        "摘要": "摘要：随着训练数据、模型规模和计算成本的不断增加，视频生成在数字创作中取得了令人印象深刻的成果，使用户能够在各个领域中表达创造力。最近，大规模语言模型（LLMs）的研究人员将扩展范围扩展到了测试时间，这可以通过使用更多的推理时间计算显著提高LLM性能。我们不再通过昂贵的训练成本来扩大视频基础模型，而是探索测试时间扩展（TTS）的力量，旨在回答这样一个问题：如果允许视频生成模型在推理时间内使用大量计算资源，在给定具有挑战性的文本提示下，生成质量能提高到什么程度。在这项工作中，我们重新解释了视频生成的测试时间扩展，将其视为从高斯噪声空间到目标视频分布中采样更好轨迹的搜索问题。具体而言，我们通过测试时间验证器提供反馈，并使用启发式算法指导搜索过程来构建搜索空间。给定一个文本提示，我们首先通过在推理时增加噪声候选者，探索一种直观的线性搜索策略。由于全帧同时去噪需要高昂的测试时间计算成本，我们进一步设计了一种更高效的TTS方法，称为帧树（ToF），它以自回归方式自适应地扩展和修剪视频分支。在文献中进行的大量基于文本条件的视频生成基准测试中，实验表明增加测试时间计算资源持续显著提高了视频质量。项目页面：this https URL",
        "地址": "https://arxiv.org/pdf/2503.18942.pdf"
    },
    {
        "名称": "2025 [2503.18892] SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for Open Base Models in the Wild.pdf",
        "作者": "Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, Junxian He",
        "摘要": "摘要：DeepSeek-R1表明，通过一个基于规则的奖励的简单强化学习（RL）框架，可以自然地出现长链的思维（CoT）推理，其中训练可以直接从基础模型开始——这个范式称为零RL训练。最近大多数再现零RL训练的努力主要集中在Qwen2.5模型系列上，因为我们发现这些基础模型已经表现出强大的指令遵循和自我反思能力。在这项工作中，我们研究了跨越10个不同基础模型的零RL训练，涵盖不同的家族和规模，包括LLama3-8B、Mistral-7B/24B、DeepSeek-Math-7B、Qwen2.5-math-7B以及所有从0.5B到32B的Qwen2.5模型。利用几个关键设计策略，例如调整格式奖励和控制查询难度，我们在大多数设定中在推理准确性和响应长度上取得了显著改进。然而，通过仔细监控训练动态，我们观察到不同的基础模型在训练期间表现出不同的模式。例如，响应长度的增加并不总是与某些认知行为（如验证，即\"哦哈时刻\"）的出现相关。值得注意的是，我们首次在非Qwen家族的小模型中观察到\"哦哈时刻\"。我们分享了实现成功零RL训练的关键设计，以及我们的发现和实践。为了促进进一步的研究，我们开源了代码、模型和分析工具。\n\n作者：曾伟豪，黄昱桢，刘倩，刘伟，何科庆，马泽军，何俊贤\n\n链接：https://arxiv.org/pdf/2503.18892.pdf\n\n标题：2025 [2503.18892] SimpleRL-Zoo: 在野外开放基础模型中调查和驯服零强化学习",
        "地址": "https://arxiv.org/pdf/2503.18892.pdf"
    },
    {
        "名称": "2025 [2503.18945] Aether: Geometric-Aware Unified World Modeling.pdf",
        "作者": "Aether Team, Haoyi Zhu, Yifan Wang, Jianjun Zhou, Wenzheng Chang, Yang Zhou, Zizun Li, Junyi Chen, Chunhua Shen, Jiangmiao Pang, Tong He",
        "摘要": "摘要：几何重建和生成建模的整合一直是开发具有人类空间推理能力的AI系统的关键挑战。本文提出了一个名为Aether的统一框架，通过联合优化以下三种核心能力实现了几何感知推理：(1) 4D动态重建，(2) 基于动作的视频预测，(3) 基于目标的视觉规划。通过任务交替的特征学习，Aether在重建、预测和规划目标之间实现了协同知识共享。基于视频生成模型，我们的框架在训练过程中从未观察到真实世界数据的情况下，展示了前所未有的从合成数据到真实情况的泛化能力。此外，得益于其内在的几何建模，我们的方法在动作跟随和重建任务中实现了零样本泛化。令人瞩目的是，即使没有真实数据，其重建性能也可以媲美或甚至优于特定领域的模型。此外，Aether采用相机轨迹作为几何感知的动作空间，从而实现了有效的基于动作的预测和视觉规划。我们希望我们的工作能够激励社区探索物理合理的世界建模及其应用的新前沿。\n\n作者：Aether团队，朱浩艺，王一帆，周建军，常文铮，周扬，李梓尊，陈俊逸，沈春华，庞江淼，何通\n\n评论：项目页面：此 HTTPS URL",
        "地址": "https://arxiv.org/pdf/2503.18945.pdf"
    },
    {
        "名称": "2025 [2503.18033] OmnimatteZero: Training-free Real-time Omnimatte with Pre-trained Video Diffusion Models.pdf",
        "作者": "Dvir Samuel, Matan Levy, Nir Darshan, Gal Chechik, Rami Ben-Ari",
        "摘要": "摘要：Omnimatte旨在将给定视频分解为语义上有意义的层，包括背景和各个对象以及其相关效果，例如阴影和反射。现有方法通常需要大量训练或昂贵的自监督优化。本文提出了OmnimatteZero，这是一种无训练方法，利用现成的预训练视频扩散模型进行omnimatte处理。它可以从视频中移除对象，提取各个对象层及其效果，并将这些对象合成为新视频。我们通过改进零样本图像修复技术用于视频对象移除，完成了这一任务，而这种技术在不做调整的情况下难以有效应对。然后，我们展示了自注意力图可以捕捉有关对象及其足迹的信息，并利用它们修复对象效果，留下干净的背景。此外，通过简单的潜在算术运算，可以将对象层单独隔离，并与新的视频层无缝融合，生成新视频。评估结果表明，OmnimatteZero不仅在背景重建方面表现优异，而且创造了最快的Omnimatte方法记录，实现了极少框运行时间的实时性能。",
        "地址": "https://arxiv.org/pdf/2503.18033.pdf"
    },
    {
        "名称": "2025 [2503.17489] Judge Anything: MLLM as a Judge Across Any Modality.pdf",
        "作者": "Shu Pu, Yaochen Wang, Dongping Chen, Yuhang Chen, Guohao Wang, Qi Qin, Zhongyi Zhang, Zhiyuan Zhang, Zetong Zhou, Shuang Gong, Yi Gui, Yao Wan, Philip S. Yu",
        "摘要": "摘要：评估生成基础模型在跨多种模式（例如图像、音频、视频）的开放式多模态理解（MMU）和生成（MMG）任务上的表现，由于跨模态交互的复杂性，面临重大挑战。为此，利用多模态大型语言模型（MLLMs）作为自动评判者的想法出现，并在视觉-语言理解任务评估中取得了令人鼓舞的结果。本文进一步扩展了跨模态评判，将MLLMs作为评判者引入两个基准测试TaskAnything和JudgeAnything，分别用于评估MLLMs在任意模态任务上的整体表现和评判能力。具体来说，TaskAnything评估了15个任意模态类别中的MMU和MMG能力，使用了从成熟基准中精选的1500个查询；此外，JudgeAnything从配对比较和评分评估两个角度评估了五个先进（如GPT-4o和Gemini-2.0-Flash）的MLLMs的评判能力，提供了一个标准化测试平台，结合了人类评判和详细评分标准。我们的广泛实验表明尽管这些MLLMs在MMU评估中表现出色（在配对比较设定下平均达到66.55%，在评分评估设定下达到42.79%），但在MMG任务上面临重大挑战（配对比较设定下仅平均为53.37%，评分评估设定下为30.05%），暴露出跨模态偏见和幻觉问题。为解决这些问题，我们提出了OmniArena，一个用于评估全能模型和多模态奖励模型的自动化平台。我们的研究强调了需要更公平的评估协议和与人类偏好更强的对齐。源代码和数据集可以在此链接公开获取：此https URL。",
        "地址": "https://arxiv.org/pdf/2503.17489.pdf"
    },
    {
        "名称": "2025 [2503.18908] FFN Fusion: Rethinking Sequential Computation in Large Language Models.pdf",
        "作者": "Akhiad Bercovich, Mohammad Dabbah, Omri Puny, Ido Galil, Amnon Geifman, Yonatan Geifman, Izhak Golan, Ehud Karpas, Itay Levy, Zach Moshe, Najeeb Nabwani, Tomer Ronen, Itamar Schen, Elad Segal, Ido Shahaf, Oren Tropp, Ran Zilberstein, Ran El-Yaniv",
        "摘要": "摘要：我们介绍了FFN融合，这是一种架构优化技术，通过识别和利用自然的并行化机会来减少大型语言模型中的顺序计算。我们的主要见解是，尤其是那些在去除特定注意力层后仍留存在的前馈网络（FFN）层的序列，通常可以在对准确度影响极小的情况下并行化。我们开发了一种系统的方法来识别和融合这些序列，将它们转变为并行操作，从而显著减少推理延迟，同时保留模型行为。将这些技术应用于Llama-3.1-405B-Instruct，我们创建了Llama-Nemotron-Ultra-253B-Base（Ultra-253B-Base），这是一种高效且即将公开可用的模型，在保持较强基准表现的同时，实现了推理延迟1.71倍的加速和每个token成本降低35倍。通过对参数规模从49B到253B的模型进行广泛实验，我们证明了FFN融合在更大规模时变得更加有效，并且可以与现有的量化和剪枝等优化技术互补。更有趣的是，我们发现即便包含注意力层和FFN层的完整Transformer块有时也可以并行化，这为神经架构设计提供了新的方向。",
        "地址": "https://arxiv.org/pdf/2503.18908.pdf"
    },
    {
        "名称": "2025 [2503.18813] Defeating Prompt Injections by Design.pdf",
        "作者": "Edoardo Debenedetti, Ilia Shumailov, Tianqi Fan, Jamie Hayes, Nicholas Carlini, Daniel Fabian, Christoph Kern, Chongyang Shi, Andreas Terzis, Florian Tramèr",
        "摘要": "摘要：大型语言模型（LLMs）在与外部环境交互的代理系统中越来越多地被部署。然而，LLM代理在处理不可信数据时容易受到提示注入攻击。在本文中，我们提出了CaMeL，一种通过在LLM周围创建保护系统层来实现强健防御的方法，即使基础模型可能易受攻击，CaMeL也能保障其安全。为操作，CaMeL明确从（可信）查询中提取控制和数据流，因此，LLM检索到的不可信数据永远不会影响程序流程。为了进一步提高安全性，CaMeL依赖于一种能力概念，以防止通过未经授权的数据流泄露私人数据。我们通过在AgentDojo [NeurIPS 2024]（一个最近的代理安全基准）中解决67%的任务，并提供可证明的安全性，来展示CaMeL的有效性。\n\n作者：Edoardo Debenedetti, Ilia Shumailov, Tianqi Fan, Jamie Hayes, Nicholas Carlini, Daniel Fabian, Christoph Kern, Chongyang Shi, Andreas Terzis, Florian Tramèr\n\n链接：https://arxiv.org/pdf/2503.18813.pdf\n\n标题：通过设计击败提示注入 [2025, 论文编号2503.18813]",
        "地址": "https://arxiv.org/pdf/2503.18813.pdf"
    },
    {
        "名称": "2025 [2503.18102] AgentRxiv: Towards Collaborative Autonomous Research.pdf",
        "作者": "Samuel Schmidgall, Michael Moor",
        "摘要": "摘要：科学发现的进步很少是单一的“灵光一现”的结果，而通常是数百名科学家共同合作，逐步推进一个共同目标的产物。尽管现有的智能代理工作流可以自主进行研究，但它们是独立工作的，无法持续改进先前的研究结果。为了解决这些挑战，我们引入了AgentRxiv——一个框架，允许大语言模型（LLM）代理实验室在共享的预印本服务器上上传和检索报告，以便彼此协作、分享见解，并迭代构建彼此的研究。我们要求代理实验室开发新的推理和提示技术，发现与孤立操作的代理相比，能够访问其先前研究的代理在性能上有更高的改进（在MATH-500基准测试中相对改进了11.4%）。我们发现，最有效的策略可以推广到其他领域的基准测试（平均提高了3.3%）。通过AgentRxiv多个代理实验室在共享研究的情况下能够共同朝着一个共同目标努力，比孤立实验室更快取得进展，总体准确性更高（在MATH-500基准测试中相对改进了13.7%）。这些发现表明，自主代理可能在设计未来的人工智能系统中与人类共同发挥作用。我们希望AgentRxiv能让代理协作实现研究目标，并帮助研究人员加速发现。\n\n来源: Samuel Schmidgall, Michael Moor, 2025, https://arxiv.org/pdf/2503.18102.pdf",
        "地址": "https://arxiv.org/pdf/2503.18102.pdf"
    },
    {
        "名称": "2025 [2503.18013] Vision-R1: Evolving Human-Free Alignment in Large Vision-Language Models via Vision-Guided Reinforcement Learning.pdf",
        "作者": "Yufei Zhan, Yousong Zhu, Shurong Zheng, Hongyin Zhao, Fan Yang, Ming Tang, Jinqiao Wang",
        "摘要": "摘要：大型视觉语言模型（LVLMs）通常遵循预训练和监督微调的两阶段训练范式。最近，从语言领域衍生的偏好优化作为一种有效的后训练强化策略出现，以增强LVLM的能力。然而，构建高质量的人工标注偏好数据并开发模拟这些偏好的鲁棒奖励模型既昂贵又具有挑战性。基于这一观察，我们提出了Vision-R1，这是一种新颖的视觉引导的类似R1的LVLM强化学习算法，通过明确的视觉反馈奖励模型。它仅利用精心编写的指令数据，消除了对特殊奖励模型和手工制作偏好数据集的需求。我们结合了一种由标准驱动的奖励函数，进一步整合了多维反馈，以根据视觉任务逻辑全面评估模型完成情况。此外，我们引入了一种渐进规则优化策略，在训练过程中动态调整奖励标准，实现模型的持续改进并减轻奖励操纵。大量在分布内和分布外基准上的实验表明，使用Vision-R1微调7B LVLMs可实现一致的性能提升，甚至高达50%的改进，超越了当前最先进的10倍规模模型。\n\n作者：詹宇飞，朱友松，郑书荣，赵红银，杨帆，汤明，王劲巧\n\n注释：项目正在开发。Github：https网址\n\n链接：https://arxiv.org/pdf/2503.18013.pdf\n\n标题：2025 [2503.18013] Vision-R1: 通过视觉引导强化学习实现大型视觉语言模型中的无人工对齐进化.pdf",
        "地址": "https://arxiv.org/pdf/2503.18013.pdf"
    },
    {
        "名称": "2025 [2503.17439] LEMMA: Learning from Errors for MatheMatical Advancement in LLMs.pdf",
        "作者": "Zhuoshi Pan, Yu Li, Honglin Lin, Qizhi Pei, Zinan Tang, Wei Wu, Chenlin Ming, H. Vicky Zhao, Conghui He, Lijun Wu",
        "摘要": "摘要: 大型语言模型（LLMs）在解决数学问题方面展示了显著的推理能力。然而，现有的方法主要集中在提高正确训练数据的质量，例如，从高级模型中提取高质量的正确解决方案，忽略了错误数据中包含的价值，这可能会阻碍模型的反思能力。虽然一些研究试图利用错误数据，但它们通常涉及复杂的机制，例如使用蒙特卡洛树搜索（MCTS）来探索错误节点。在这项工作中，我们提出通过错误学习来增强LLMs的推理能力，即LEMMA（从错误中学习以推进数学）。LEMMA构建了由包含错误步骤的错误解决方案和指向正确解决方案的反思连接组成的数据进行微调。具体而言，我们系统地分析了模型生成的错误类型，并引入了基于错误类型的错误增强方法来收集多样且具有代表性的错误。正确的解决方案要么是通过修正错误得出的，要么是重新生成的。通过模型感知的平滑反思连接，将错误的解决方案转化为正确的解决方案。通过在构建的数据集上进行微调，模型能够在生成过程中自主纠正错误，而无需依赖外部批判模型。实验结果表明，LEMMA在性能上显著优于其他强基线模型。\n\n- 年份: 2025\n- 作者: Zhuoshi Pan, Yu Li, Honglin Lin, Qizhi Pei, Zinan Tang, Wei Wu, Chenlin Ming, H. Vicky Zhao, Conghui He, Lijun Wu\n- 评论: 9页，6张图表，4张表格，正在审核中\n- 链接: [https://arxiv.org/pdf/2503.17439.pdf](https://arxiv.org/pdf/2503.17439.pdf)\n- 标题: 2025 [2503.17439] LEMMA：从错误中学习以推进数学在大型语言模型中的应用",
        "地址": "https://arxiv.org/pdf/2503.17439.pdf"
    },
    {
        "名称": "2025 [2503.18940] Training-free Diffusion Acceleration with Bottleneck Sampling.pdf",
        "作者": "Ye Tian, Xin Xia, Yuxi Ren, Shanchuan Lin, Xing Wang, Xuefeng Xiao, Yunhai Tong, Ling Yang, Bin Cui",
        "摘要": "摘要翻译如下：\n\n扩散模型在视觉内容生成方面展现出非凡的能力，但由于推理过程中的高计算成本，仍然难以部署。这种计算负担主要来自自注意力机制相对于图像或视频分辨率的平方复杂度。尽管现有的加速方法通常会损失输出质量或需要昂贵的再训练，但我们观察到大多数扩散模型在较低分辨率下预训练，这为在不降低性能的情况下实现更高效的推理提供了机会。在这项工作中，我们提出了一种无需训练的框架——Bottleneck Sampling，利用低分辨率先验来减少计算开销，同时保持输出的保真度。Bottleneck Sampling遵循一个高-低-高的去噪工作流程：在初始和最终阶段执行高分辨率去噪，而在中间步骤中以较低分辨率运行。为了减轻混叠和模糊伪影，我们进一步优化了分辨率过渡点，并在每个阶段自适应地调整去噪时间步。我们在图像和视频生成任务中评估了Bottleneck Sampling，通过大量实验表明，它在保持与标准全分辨率采样过程几乎相当的输出质量的同时，将图像生成的推理速度提高了3倍，视频生成的推理速度提高了2.5倍。代码可在以下网址获得：this https URL。",
        "地址": "https://arxiv.org/pdf/2503.18940.pdf"
    },
    {
        "名称": "2025 [2503.18886] CFG-Zero*: Improved Classifier-Free Guidance for Flow Matching Models.pdf",
        "作者": "Weichen Fan, Amber Yijia Zheng, Raymond A. Yeh, Ziwei Liu",
        "摘要": "摘要:无分类器引导（Classifier-Free Guidance，CFG）是一种在扩散/流模型中广泛应用的技术，可以提升图像保真度和可控性。在这项工作中，我们首先从分析角度研究了CFG对在高斯混合模型上训练的流匹配模型的影响，其中可以推导出真实的流动方向。我们观察到，在训练的早期阶段，当流动估计不准确时，CFG会将样本引导到错误的轨迹上。基于这一观察，我们提出了改进的CFG-Zero*，包含两个贡献：（a）优化尺度，通过优化一个标量以纠正估计速度中的不准确性，因此命名中有*；（b）零初始化，意味着对ODE求解器的前几步进行归零处理。在文本到图像（Lumina-Next, Stable Diffusion 3, 和 Flux）以及文本到视频（Wan-2.1）生成的实验中，CFG-Zero*均稳定地优于CFG，突显了其在引导流匹配模型方面的有效性。（代码可在此网址获取）。\n\n作者: 范玮辰, 郑伊佳, Raymond A. Yeh, 刘子维\n\n链接: [https://arxiv.org/pdf/2503.18886.pdf](https://arxiv.org/pdf/2503.18886.pdf)\n\n标题: 2025 [2503.18886] CFG-Zero*: 改进的流匹配模型无分类器引导技术",
        "地址": "https://arxiv.org/pdf/2503.18886.pdf"
    },
    {
        "名称": "2025 [2503.18948] Equivariant Image Modeling.pdf",
        "作者": "Ruixiao Dong, Mengde Xu, Zigang Geng, Li Li, Han Hu, Shuyang Gu",
        "摘要": "摘要：当前的生成模型，如自回归和扩散方法，将高维数据分布学习分解为一系列更简单的子任务。然而，在这些子任务的联合作优化过程中存在固有冲突，而现有的解决方案无法在不牺牲效率或可扩展性的情况下解决这些冲突。我们提出了一种新颖的等变图像建模框架，通过利用自然视觉信号的平移不变性，本质上使子任务优化目标保持一致。我们的方法引入了（1）列式标记化，增强了水平方向上的平移对称性，以及（2）窗口式因果注意力，确保了各个位置之间的一致上下文关系。在256x256分辨率的类条件ImageNet生成评估中，我们的方法在使用更少计算资源的情况下，达到了与最先进的自回归模型相当的性能。系统分析表明，增强的等变性减少了任务间冲突，显著提高了零样本泛化能力，并实现了超长的图像合成。此项工作建立了生成建模中任务对齐分解的首个框架，提供了高效参数共享和无冲突优化的见解。代码和模型公开可用。\n\n翻译：当前的生成模型，如自回归和扩散方法，将高维数据分布学习分解为一系列更简单的子任务。然而，在这些子任务的联合作优化过程中存在固有冲突，而现有的解决方案无法在不牺牲效率或可扩展性的情况下解决这些冲突。我们提出了一种新颖的等变图像建模框架，通过利用自然视觉信号的平移不变性，本质上使子任务优化目标保持一致。我们的方法引入了（1）列式标记化，增强了水平方向上的平移对称性，以及（2）窗口式因果注意力，确保了各个位置之间的一致上下文关系。在256x256分辨率的类条件ImageNet生成评估中，我们的方法在使用更少计算资源的情况下，达到了与最先进的自回归模型相当的性能。系统分析表明，增强的等变性减少了任务间冲突，显著提高了零样本泛化能力，并实现了超长的图像合成。此项工作建立了生成建模中任务对齐分解的首个框架，提供了高效参数共享和无冲突优化的见解。代码和模型公开可用。",
        "地址": "https://arxiv.org/pdf/2503.18948.pdf"
    },
    {
        "名称": "2025 [2503.18923] Video SimpleQA: Towards Factuality Evaluation in Large Video Language Models.pdf",
        "作者": "Meng Cao, Pengfei Hu, Yingyao Wang, Jihao Gu, Haoran Tang, Haoze Zhao, Jiahua Dong, Wangbo Yu, Ge Zhang, Ian Reid, Xiaodan Liang",
        "摘要": "摘要: 最近在大规模视频语言模型（LVLMs）方面的进展突显了它们在多模态理解中的潜力，然而在视频语境中评估它们的事实性仍然是一个未解决的关键挑战。为了填补这一空白，我们引入了Video SimpleQA，这是第一个全面的LVLMs事实性评估基准。我们的工作通过以下关键特性区分于现有的视频基准：1）所需知识：需要超出显式叙述的外部知识整合；2）求事实问题：针对客观、无争议的事件或关系，避免主观解释；3）明确且简短的回答：答案被制作成明确且绝对正确的简短形式，使得通过LLM作为评判框架实现自动评估具有最小评分差异；4）外部来源验证：所有注释经过权威外部参考的严格验证以确保可靠性；5）需要时间推理：注释的问题类型包括静态单帧理解和动态时间推理，明确评估LVLMs在长上下文依赖下的事实性。我们广泛评估了41个最先进的LVLMs，并总结了以下主要发现：1）当前LVLMs在事实遵循方面存在显著缺陷，特别是开源模型。表现最好的模型Gemini-1.5-Pro仅获得54.4%的F-score；2）测试时计算范式显示性能提升不显著，揭示了通过事后计算提高事实性的基本限制；3）检索增强生成表现出一致的改进，但需要额外的推理时间开销，呈现了关键的效率性能权衡。",
        "地址": "https://arxiv.org/pdf/2503.18923.pdf"
    },
    {
        "名称": "2025 [2503.17811] Feather-SQL: A Lightweight NL2SQL Framework with Dual-Model Collaboration Paradigm for Small Language Models.pdf",
        "作者": "Wenqi Pei, Hailing Xu, Hengyuan Zhao, Shizheng Hou, Han Chen, Zining Zhang, Pingyi Luo, Bingsheng He",
        "摘要": "以下是摘要的中文翻译：\n\n摘要：使用大语言模型（LLMs）的自然语言转SQL（NL2SQL）技术取得了显著进展。然而，这些模型往往依赖于闭源系统和高计算资源，从而在数据隐私和部署方面带来了挑战。相比之下，小语言模型（SLMs）在NL2SQL任务中表现不佳，并且与现有框架不兼容。为了解决这些问题，我们引入了Feather-SQL，这是一个专为SLMs量身定制的轻量级框架。Feather-SQL通过1）模式修剪和链接，2）多路径和多候选生成来提高SQL的可执行性和准确性。此外，我们提出了1+1模型协作范式，该范式将一个强大的通用聊天模型与一个微调的SQL专用模型配对，结合了强大的分析推理能力和高精度的SQL生成。在BIRD实验结果中，Feather-SQL提高了SLMs在NL2SQL任务中的性能，未经过微调的模型性能提升了约10%。这种范式将SLMs的准确性上限提高到了54.76%，突显了其有效性。",
        "地址": "https://arxiv.org/pdf/2503.17811.pdf"
    },
    {
        "名称": "2025 [2503.18866] Reasoning to Learn from Latent Thoughts.pdf",
        "作者": "Yangjun Ruan, Neil Band, Chris J. Maddison, Tatsunori Hashimoto",
        "摘要": "摘要: 针对语言模型（LM）预训练的计算能力超越了人类编写文本的增长速度，导致数据将成为LM扩展的瓶颈这一担忧，我们提出通过显式建模和推断生成文本过程中的潜在想法可以显著提高预训练数据的效率。直观而言，我们的方法将网页文本视为冗长人类思想过程的压缩最终结果，潜在想法包含了对数据高效学习至关重要的重要上下文知识和推理步骤。我们通过在数据受限的情况下继续进行数学预训练，实验证明了该方法的有效性。首先，我们展示了推断潜在想法的合成数据方法显著提高了数据效率，优于在相同数量的原始数据上进行训练（在MATH上从5.7％提升至25.4％）。此外，我们展示了在没有强教师的情况下进行潜在想法推断，其中LM使用EM算法通过迭代改进训练LM的能力和增加想法的预训练数据质量来自举（bootstrap）其性能。我们表明1B LM可以在至少三个迭代中自举其性能，并显著优于在原始数据上训练的基线，随着E步骤计算量的增加获得更多收益。推断扩展和EM迭代的收益表明在数据受限预训练扩展方面存在新的机会。\n\n译者: 杨骏, 尼尔·班德, 克里斯·J·马迪森, 答恒元\n\n链接: [原文链接](https://arxiv.org/pdf/2503.18866.pdf)\n\n标题: 从潜在想法中学习推理",
        "地址": "https://arxiv.org/pdf/2503.18866.pdf"
    },
    {
        "名称": "2025 [2503.14428] MagicComp: Training-free Dual-Phase Refinement for Compositional Video Generation.pdf",
        "作者": "Hongyu Zhang, Yufan Deng, Shenghai Yuan, Peng Jin, Zesen Cheng, Yian Zhao, Chang Liu, Jie Chen",
        "摘要": "摘要：文本到视频（Text-to-video, T2V）生成已通过扩散模型取得显著进展。但是，现有方法在准确绑定属性、确定空间关系以及捕捉多个主体之间复杂的动作互动方面仍存在困难。为了解决这些问题，我们提出了MagicComp，这是一种无需训练的方法，通过双阶段精炼增强了组合式文本到视频的生成。具体来说，（1）在条件阶段：我们引入了语义锚点消歧，通过逐步将语义锚点的方向向量注入原始文本嵌入，强化主体特定的语义并解决主体间的歧义；（2）在去噪阶段：我们提出了动态布局融合注意力，该方法整合了基础定位先验和模型自适应空间感知，通过受掩码注意力调制灵活地将主体绑定到其时空区域。此外，MagicComp是一种模型无关的多功能方法，可以无缝集成到现有的T2V架构中。在T2V-CompBench和VBench上的广泛实验表明，MagicComp优于最先进的方法，展示了其在复杂提示和轨迹可控视频生成等应用中的潜力。\n\n项目页面：this https URL。\n\n作者：Hongyu Zhang, Yufan Deng, Shenghai Yuan, Peng Jin, Zesen Cheng, Yian Zhao, Chang Liu, Jie Chen",
        "地址": "https://arxiv.org/pdf/2503.14428.pdf"
    },
    {
        "名称": "2025 [2503.18769] AlphaSpace: Enabling Robotic Actions through Semantic Tokenization and Symbolic Reasoning.pdf",
        "作者": "Alan Dao (Gia Tuan Dao), Dinh Bach Vu, Bui Quang Huy",
        "摘要": "摘要：本文介绍了AlphaSpace，这是一种新颖的方法，旨在增强大型语言模型（LLMs）在3D笛卡尔空间导航中的空间推理能力。AlphaSpace采用基于语义的标记策略，通过专门化的语义标记编码高度信息，并主要整合符号合成推理数据。这种方法使LLMs能够通过在特定[x, y, z]坐标上定位对象来准确地操作对象。实验结果表明，AlphaSpace在操作子任务上的表现显著优于现有模型，总准确率达到66.67%，而GPT-4o为37.5%，Claude 3.5 Sonnet为29.17%。\n\n作者：Alan Dao (Gia Tuan Dao), Dinh Bach Vu, Bui Quang Huy\n\nURL：https://arxiv.org/pdf/2503.18769.pdf\n\n标题：2025 [2503.18769] AlphaSpace: 通过语义标记和符号推理实现机器人操作",
        "地址": "https://arxiv.org/pdf/2503.18769.pdf"
    },
    {
        "名称": "2025 [2503.15879] Typed-RAG: Type-aware Multi-Aspect Decomposition for Non-Factoid Question Answering.pdf",
        "作者": "DongGeon Lee, Ahjeong Park, Hyeri Lee, Hyeonseo Nam, Yunho Maeng",
        "摘要": "摘要：非事实型问答（Non-factoid question-answering, NFQA）由于其开放性质、多样的意图以及需要多方面推理，提出了重大的挑战，使得包括检索增强生成（Retrieval-Augmented Generation, RAG）在内的传统事实型问答方法显得不足。与事实型问题不同，非事实型问题（Non-factoid Questions, NFQs）缺乏明确的答案，需要从多个来源综合信息，并跨越各种推理维度。为了解决这些限制，我们提出了Typed-RAG，这是一种在RAG范式中用于NFQA的类型感知多方面分解框架。Typed-RAG将NFQs分类为不同类型——如辩论、经验和比较——并应用基于方面的分解来优化检索和生成策略。通过将多方面的NFQs分解为单方面的子查询并汇总结果，Typed-RAG生成了更加信息丰富且与上下文相关的回答。为了评估Typed-RAG，我们引入了覆盖多种NFQ类型的基准数据集Wiki-NFQA。实验结果表明，Typed-RAG在基线方法上表现更佳，从而强调了类型感知分解对于有效检索和生成NFQA的重要性。我们的代码和数据集可在此https URL获取。",
        "地址": "https://arxiv.org/pdf/2503.15879.pdf"
    },
    {
        "名称": "2025 [2503.17422] V-Seek: Accelerating LLM Reasoning on Open-hardware Server-class RISC-V Platforms.pdf",
        "作者": "Javier J. Poveda Rodrigo, Mohamed Amine Ahmdi, Alessio Burrello, Daniele Jahier Pagliari, Luca Benini",
        "摘要": "摘要：大型语言模型（LLMs）的最近指数级增长依赖于基于GPU的系统。然而，CPU正作为一种灵活且成本更低的替代方案出现，尤其是在针对推理和推断负载时。RISC-V由于其开放且供应商中立的指令集架构（ISA），正在这一领域迅速获得关注。然而，RISC-V硬件针对LLM负载及其相应的软件生态系统尚未完全成熟和简化，因为这需要特定领域的优化调整。本文旨在填补这一空白，专注于优化Sophon SG2042上的LLM推断，这是首款具有矢量处理能力的商用多核RISC-V CPU。对于两个最近的、经过优化以进行推理的最新LLM模型，DeepSeek R1 Distill Llama 8B和DeepSeek R1 Distill QWEN 14B，我们分别在生成和提示处理时实现了4.32/2.29 token/s和6.54/3.68 token/s的性能，相比基线分别提高了2.9倍和3.0倍。\n\n作者：Javier J. Poveda Rodrigo, Mohamed Amine Ahmdi, Alessio Burrello, Daniele Jahier Pagliari, Luca Benini\n\n链接：https://arxiv.org/pdf/2503.17422.pdf\n\n标题：2025 [2503.17422] V-Seek: 主流硬件服务器类RISC-V平台上的LLM推理加速",
        "地址": "https://arxiv.org/pdf/2503.17422.pdf"
    },
    {
        "名称": "2025 [2503.18559] AMD-Hummingbird: Towards an Efficient Text-to-Video Model.pdf",
        "作者": "Takashi Isobe, He Cui, Dong Zhou, Mengmeng Ge, Dong Li, Emad Barsoum",
        "摘要": "摘要: 文本到视频（T2V）生成因其能够从文本描述中合成逼真的视频而受到广泛关注。然而，现有模型难以在资源有限的设备（如iGPU和手机）上平衡计算效率和高视觉质量。大多数现有工作优先考虑视觉保真度，而忽略了适合真实世界部署的更小、更高效的模型的需求。为了解决这一挑战，我们提出了一种轻量级的T2V框架，称为Hummingbird，该框架通过剪枝现有模型并通过视觉反馈学习来提高视觉质量。我们的方法将U-Net的参数量从14亿减少到7亿，显著提高了效率，同时保留了高质量的视频生成。此外，我们引入了一种新颖的数据处理管道，利用大型语言模型（LLMs）和视频质量评估（VQA）模型来提高文本提示和视频数据的质量。为了支持用户驱动的训练和风格定制，我们公开发布了包括数据处理和模型训练的全部训练代码。大量实验表明，我们的方法相比最先进的模型如VideoCrafter2，速度提高了31倍，同时在VBench上也获得了最高的总体评分。此外，我们的方法支持生成长达26帧的视频，解决了现有基于U-Net的方法在长视频生成中的局限性。值得注意的是，整个训练过程仅需要四个GPU，但性能竞争力与现有领先方法相当。Hummingbird为T2V生成提供了一种实用且高效的解决方案，结合了高性能、可扩展性和灵活性，适用于现实世界的应用。",
        "地址": "https://arxiv.org/pdf/2503.18559.pdf"
    },
    {
        "名称": "2025 [2503.18018] Lost in Cultural Translation: Do LLMs Struggle with Math Across Cultural Contexts?.pdf",
        "作者": "Aabid Karim, Abdul Karim, Bhoomika Lohana, Matt Keon, Jaswinder Singh, Abdul Sattar",
        "摘要": "摘要：大型语言模型（LLMs）在许多领域取得了显著进展，特别是编码、数学推理和逻辑问题解决。然而，一个关键问题依然存在：当LLMs面对文化改编的数学问题时，这些数学推理能力是否依然有效？具体来说，当这些模型面对嵌入在文化背景中的数学问题，且这些文化背景在主流大数据AI训练数据中没有显著代表性时，它们的表现如何？为了探索这一点，我们从GSM8K生成了六个合成的文化数据集，GSM8K是广泛用于评估LLMs数学推理能力的基准。我们在保留GSM8K测试集原始的数学逻辑和数值的同时，修改了诸如人名、食物、地名等文化元素。这些文化改编的数据集为在不断变化的文化背景下评估LLMs的数学推理提供了一个更可靠的框架。我们的研究发现，即使底层数学结构保持不变，LLMs在文化参考变化时处理数学问题仍然有困难。相比之下，小型模型的性能下降比大型模型更为明显。有趣的是，我们的结果还表明，文化熟悉度可以增强数学推理能力。即使是没有接受过显性数学训练但暴露于相关文化背景的模型，有时在面对文化嵌入的数学问题时表现也优于具有较强数学能力的大型模型。这项研究突显了文化背景对LLMs数学推理能力的影响，强调了需要更多多样且具有代表性的训练数据，以提高实际应用中的鲁棒性。基准数据集和重现结果的脚本可在此HTTPS URL获取。\n\n翻译：大型语言模型（LLMs）在许多领域取得了显著进展，特别是编码、数学推理和逻辑问题解决。然而，一个关键问题依然存在：当LLMs面对文化改编的数学问题时，这些数学推理能力是否依然有效？具体来说，当这些模型面对嵌入在文化背景中的数学问题，且这些文化背景在主流大数据AI训练数据中没有显著代表性时，它们的表现如何？为了探索这一点，我们从GSM8K生成了六个合成的文化数据集，GSM8K是广泛用于评估LLMs数学推理能力的基准。我们在保留GSM8K测试集原始的数学逻辑和数值的同时，修改了诸如人名、食物、地名等文化元素。这些文化改编的数据集为在不断变化的文化背景下评估LLMs的数学推理提供了一个更可靠的框架。我们的研究发现，即使底层数学结构保持不变，LLMs在文化参考变化时处理数学问题仍然有困难。相比之下，小型模型的性能下降比大型模型更为明显。有趣的是，我们的结果还表明，文化熟悉度可以增强数学推理能力。即使是没有接受过显性数学训练但暴露于相关文化背景的模型，有时在面对文化嵌入的数学问题时表现也优于具有较强数学能力的大型模型。这项研究突显了文化背景对LLMs数学推理能力的影响，强调了需要更多多样且具有代表性的训练数据，以提高实际应用中的鲁棒性。基准数据集和重现结果的脚本可在此HTTPS URL获取。",
        "地址": "https://arxiv.org/pdf/2503.18018.pdf"
    },
    {
        "名称": "2025 [2503.17500] Variance Control via Weight Rescaling in LLM Pre-training.pdf",
        "作者": "Louis Owen, Abhay Kumar, Nilabhra Roy Chowdhury, Fabian Güra",
        "摘要": "摘要：大型语言模型（LLM）的预训练结果在很大程度上取决于权重初始化和方差控制策略。尽管初始方差控制在神经网络中的重要性已经被广泛记录，但具体到LLM预训练中的初始化和方差增长管理的文献却较为稀缺。本文介绍了一个名为层索引重缩放（LIR）的权重初始化方案，以及一个目标方差重缩放（TVR）方差控制策略。在一个拥有10亿参数的LLaMA模型上的实验表明，使用这些技术可以更好地管理方差，从而显著提高下游任务的性能（在常见预训练基准上提高最多4.6%），并减少极端激活值，因此缓解了量化和低精度训练相关的挑战。我们的代码可在以下网址获得：[链接].\n\n论文作者：Louis Owen, Abhay Kumar, Nilabhra Roy Chowdhury, Fabian Güra\n\n链接：https://arxiv.org/pdf/2503.17500.pdf\n\n论文标题：通过权重重缩放在LLM预训练中实现方差控制",
        "地址": "https://arxiv.org/pdf/2503.17500.pdf"
    },
    {
        "名称": "2025 [2503.18352] Diffusion-4K: Ultra-High-Resolution Image Synthesis with Latent Diffusion Models.pdf",
        "作者": "Jinjin Zhang, Qiuyu Huang, Junjie Liu, Xiefan Guo, Di Huang",
        "摘要": "摘要：在本文中，我们介绍了Diffusion-4K，这是一种使用文本到图像扩散模型进行直接超高分辨率图像合成的新框架。其核心进展包括：（1）Aesthetic-4K基准：针对缺乏公开可用的4K图像合成数据集，我们构建了Aesthetic-4K，这是超高分辨率图像生成的一个全面基准。我们精心策划了一个高质量的4K数据集，包含经过精心挑选的图像和由GPT-4o生成的标题。此外，我们引入了GLCM分数和压缩比等指标来评估细节，与FID、美学和CLIPScore等全局测量结合，对超高分辨率图像进行全面评估。（2）基于小波的微调：我们提出了一种基于小波的微调方法，用于对逼真4K图像的直接训练，适用于各种潜在扩散模型，展示了其在合成高度详细的4K图像方面的有效性。因此，当由现代大规模扩散模型（例如SD3-2B和Flux-12B）驱动时，Diffusion-4K在高质量图像合成和文本提示服从性方面表现出色。我们基准的广泛实验结果证明了Diffusion-4K在超高分辨率图像合成中的优越性。",
        "地址": "https://arxiv.org/pdf/2503.18352.pdf"
    },
    {
        "名称": "2025 [2503.18470] MetaSpatial: Reinforcing 3D Spatial Reasoning in VLMs for the Metaverse.pdf",
        "作者": "Zhenyu Pan, Han Liu",
        "摘要": "摘要: 我们展示了MetaSpatial，这是第一个旨在增强视觉语言模型（VLMs）中的3D空间推理的强化学习（RL）框架，能够在不需要硬编码优化的情况下实时生成3D场景。MetaSpatial解决了两个核心挑战：(i) VLMs中缺乏内化的3D空间推理能力，限制了其生成真实布局的能力，以及(ii) 传统监督微调（SFT）在布局生成任务中的低效性，因为无法获得完美的真实标注。我们的主要创新是一种多轮的基于RL的优化机制，该机制整合了物理感知约束和渲染图像评估，确保生成的3D布局连贯、符合物理规律且美学一致。在方法上，MetaSpatial引入了一种自适应、迭代推理过程，其中VLM通过分析渲染输出，逐步改进空间排列，提高场景连贯性。实证评估表明，MetaSpatial显著增强了各种模型的空间一致性和格式稳定性。训练后，物体的放置更加真实、对齐且功能连贯，这验证了RL在元宇宙、AR/VR、数字孪生和游戏开发应用中进行3D空间推理的有效性。我们的代码、数据和训练管道在此https URL上公开可用。\n\n作者: 潘振宇, 刘汉\n\n备注: 工作论文\n\nURL: https://arxiv.org/pdf/2503.18470.pdf\n\n标题: 2025 [2503.18470] MetaSpatial: 加强元宇宙中VLMs的3D空间推理.pdf",
        "地址": "https://arxiv.org/pdf/2503.18470.pdf"
    },
    {
        "名称": "2025 [2503.17760] CODA: Repurposing Continuous VAEs for Discrete Tokenization.pdf",
        "作者": "Zeyu Liu, Zanlin Ni, Yeguo Hua, Xin Deng, Xiao Ma, Cheng Zhong, Gao Huang",
        "摘要": "2025年学术论文《CODA: Repurposing Continuous VAEs for Discrete Tokenization》的摘要如下：\n\n摘要：离散视觉标记器将图像转换为一系列标记，从而可以像处理语言模型一样进行基于标记的视觉生成。然而，这一过程本质上具有挑战性，因为它需要将视觉信号压缩为紧凑的表示，并将其离散化为固定的代码集。传统的离散标记器通常将这两个任务联合学习，导致训练不稳定、代码簿利用率低和重建质量有限。在本文中，我们引入了一种新的框架CODA（连续到离散适配），该框架将压缩和离散化解耦。CODA不会从头开始训练离散标记器，而是通过精心设计的离散化过程，将已优化感知压缩的现成连续VAE（变分自编码器）适配为离散标记器。通过主要关注离散化，CODA确保了稳定高效的训练，同时保留了连续VAE的强大视觉保真度。实验证明，CODA在比标准VQGAN少6倍的训练预算下，实现了100%的代码簿利用率，并在ImageNet 256×256基准上达到了显著的重建FID（rFID），在8倍和16倍压缩下分别为0.43和1.34。\n\n作者：Zeyu Liu, Zanlin Ni, Yeguo Hua, Xin Deng, Xiao Ma, Cheng Zhong, Gao Huang\n\n评论：项目页面：https://arxiv.org/pdf/2503.17760.pdf\n\n标题：CODA: 将连续VAE重新用于离散标记化",
        "地址": "https://arxiv.org/pdf/2503.17760.pdf"
    },
    {
        "名称": "2025 [2503.17735] RDTF: Resource-efficient Dual-mask Training Framework for Multi-frame Animated Sticker Generation.pdf",
        "作者": "Zhiqiang Yuan, Ting Zhang, Ying Deng, Jiapei Zhang, Yeshuang Zhu, Zexi Jia, Jie Zhou, Jinchao Zhang",
        "摘要": "近期，视频生成技术取得了显著进展，吸引了学者们的广泛关注。为了在资源受限的条件下将这项技术应用于下游应用，研究人员通常基于诸如Adapter或Lora等参数高效调整方法对预训练模型进行微调。尽管这些方法可以将知识从源域转移到目标域，但更少的训练参数会导致拟合能力差，并且源域的知识可能会导致推理过程偏离目标域。本文提出，在资源受限的条件下，利用仅百万级别样本从头开始训练一个较小的视频生成模型，在下游应用中可以胜过在较大模型上进行参数高效调整：关键在于数据的有效利用和课程策略。以动画贴纸生成（ASG）为案例研究，首先我们构建了一个针对低帧率贴纸的离散帧生成网络，确保其参数满足资源受限下的模型训练需求。为了为从头开始训练的模型提供数据支持，我们提出了一种基于双掩码的数据利用策略，旨在提高有限数据的可用性和扩展其多样性。为了在双掩码情境下促进收敛，我们提出了一种难度自适应课程学习方法，将样本熵分解为静态和自适应成分，以便从容易到困难获取样本。实验表明，我们的资源高效双掩码训练框架在定量和定性上均优于如I2V-Adapter和SimDA等参数高效调整方法，验证了我们的方法在受限资源下进行下游任务的可行性。代码将会公开。",
        "地址": "https://arxiv.org/pdf/2503.17735.pdf"
    },
    {
        "名称": "2025 [2503.16924] Optimized Minimal 3D Gaussian Splatting.pdf",
        "作者": "Joo Chan Lee, Jong Hwan Ko, Eunbyung Park",
        "摘要": "摘要：3D高斯喷溅（3DGS）已成为实时、高性能渲染的强大表示形式，能够实现广泛的应用。然而，用大量显式的高斯原语表示3D场景会带来显著的存储和内存开销。最近的研究表明，当高斯用高精度属性表示时，可以通过大幅减少高斯数量来实现高质量渲染。然而，现有的3DGS压缩方法仍依赖于相对较多的高斯，主要关注于属性压缩。这是因为较少的高斯集合对有损属性压缩变得更加敏感，导致严重的质量下降。由于高斯的数量直接与计算成本相关，因此至关重要的是有效减少高斯的数量，而不仅仅是优化存储。在本文中，我们提出了优化的最小高斯表示（OMG），在使用最少原语的情况下显著减少存储。首先，我们从相近的高斯中确定出不同的高斯，最大程度减少冗余而不牺牲质量。其次，我们提出了一种紧凑且精确的属性表示，能够有效捕捉原语之间的连续性和不规则性。此外，我们提出了一种子向量量化技术，以改进不规则性表示，并在代码本大小可以忽略不计的情况下保持快速训练。大量实验证明，与之前的最优方法相比，OMG将存储需求减少了近50%，并在维持高渲染质量的同时，实现了600+ FPS的渲染。我们的源代码可在此https URL上获取。",
        "地址": "https://arxiv.org/pdf/2503.16924.pdf"
    },
    {
        "名称": "2025 [2503.18494] Verbal Process Supervision Elicits Better Coding Agents.pdf",
        "作者": "Hao-Yuan Chen, Cheng-Pong Huang, Jui-Ming Yao",
        "摘要": "摘要：大型语言模型的出现及其作为AI代理的应用显著提升了最新代码生成基准，改变了现代软件工程任务。然而，即使在测试时代计算推理模型，这些系统仍难以应对复杂的软件工程挑战。这项工作介绍了CURA，一个增强了语言过程监督（VPS）的代码理解和推理代理系统，在复杂基准如BigCodeBench上比基础模型提高了3.65%。此外，CURA在与o3-mini模型和VPS技术配对时，达到了最先进的性能。这项工作代表了在整合推理驱动架构与基于LLM的代码生成方面向前迈出的一步，使语言模型的代理性推理能力得以解决复杂的软件工程任务。\n\n翻译：大型语言模型的出现及其作为AI代理的应用显著提升了最新代码生成基准，改变了现代软件工程任务。然而，即使在测试时代计算推理模型，这些系统仍难以应对复杂的软件工程挑战。这项工作介绍了CURA，一个增强了语言过程监督（VPS）的代码理解和推理代理系统，在复杂基准如BigCodeBench上比基础模型提高了3.65%。此外，CURA在与o3-mini模型和VPS技术配对时，达到了最先进的性能。这项工作代表了在整合推理驱动架构与基于LLM的代码生成方面向前迈出的一步，使语言模型的代理性推理能力得以解决复杂的软件工程任务。",
        "地址": "https://arxiv.org/pdf/2503.18494.pdf"
    },
    {
        "名称": "2025 [2503.18406] Instruct-CLIP: Improving Instruction-Guided Image Editing with Automated Data Refinement Using Contrastive Learning.pdf",
        "作者": "Sherry X. Chen, Misha Sra, Pradeep Sen",
        "摘要": "摘要: 尽管自然语言指令为自动图像编辑提供了一种直观的指导方式，深度学习模型往往难以取得高质量的结果，这主要是由于创建大规模、高质量训练数据集的困难。为了实现这一点，先前的方法通常依赖于文本到图像（T2I）生成模型来生成成对的原始和编辑后的图像，以模拟指令引导的图像编辑模型的输入/输出。然而，由于T2I模型的局限性，这些图像对常常未能与指定的编辑指令一致，这对基于此类数据集训练的模型产生了负面影响。为了解决这个问题，我们提出了Instruct-CLIP（I-CLIP），一种自监督方法，通过学习原始图像和编辑后图像之间的语义变化来改进并更好地调整现有数据集中的指令。此外，我们调整Instruct-CLIP以处理噪声潜在图像和扩散时间步，从而可以用于训练潜在扩散模型（LDMs），并在扩散管道的任何步骤中有效地强制执行编辑指令与图像变化之间在潜在空间中的对齐。我们使用Instruct-CLIP校正InstructPix2Pix数据集，并获得了超过12万经过改进的样本，然后用这些样本微调他们的模型，通过我们新颖的基于I-CLIP的损失函数进行指导。最终的模型可以生成更多与给定指令一致的编辑结果。我们的代码和数据集可在此HTTPS链接中获取。\n\n翻译：虽然自然语言指令提供了一种直观的方法来指导自动图像编辑，但是深度学习模型常因难以创建大规模高质量的训练数据集而难以实现高质量的结果。之前的方法通常依赖于文本到图像（T2I）生成模型，以生成模拟指令引导的图像编辑模型的输入/输出的原始与编辑后的图像对。然而，由于T2I模型的局限性，这些图像对往往无法与指定的编辑指令一致，从而影响了基于这些数据集训练的模型的性能。为了解决这个问题，我们提出了一种自监督的方法Instruct-CLIP（I-CLIP），它通过学习原始与编辑后的图像之间的语义变化来改进并更好地调整现有数据集中的指令。此外，我们使Instruct-CLIP能够处理噪声潜在图像和扩散时间步，使其可以用于训练潜在扩散模型（LDMs），并在任何扩散步骤中在潜在空间中强制实现编辑指令与图像变化的一致性。我们使用Instruct-CLIP校正了InstructPix2Pix数据集，获得了超过12万个改进样本，并以这些样本为基础，通过我们新颖的基于I-CLIP的损失函数微调模型。在此基础上训练的模型能够生成与给定指令更一致的编辑结果。我们的代码和数据集可通过以下HTTPS链接获取。",
        "地址": "https://arxiv.org/pdf/2503.18406.pdf"
    },
    {
        "名称": "2025 [2503.18071] Mind with Eyes: from Language Reasoning to Multimodal Reasoning.pdf",
        "作者": "Zhiyu Lin, Yifei Gao, Xian Zhao, Yunfan Yang, Jitao Sang",
        "摘要": "摘要：语言模型最近在推理领域取得了进展，但只有通过多模态推理，我们才能充分释放其潜力，实现更全面、更接近人类的认知能力。本文综述了近期多模态推理方法，并将其分为两类：语言中心的多模态推理和协同多模态推理。前者包括一次通过的视觉感知和主动视觉感知，在这种情况下，视觉主要在语言推理中起辅助作用。后者涉及在推理过程中的动作生成和状态更新，使模态之间有更动态的互动。此外，我们还分析了这些方法的技术演进，讨论了其内在的挑战，并介绍了评估多模态推理性能的关键基准任务和评价指标。最后，我们从以下两个角度提供了未来研究的见解：(i) 从视觉-语言推理到全模态推理，以及 (ii) 从多模态推理到多模态代理。本文旨在提供一个结构化的概述，以激励多模态推理研究的进一步进展。\n\n翻译：语言模型最近在推理领域取得了进展，但只有通过多模态推理，我们才能充分释放其潜力，实现更全面、更接近人类的认知能力。这篇综述系统地概述了近期多模态推理的方法，并将其分为两类：语言中心的多模态推理和协同多模态推理。前者包括一次通过的视觉感知和主动视觉感知，视觉在此主要支持语言推理。后者涉及在推理过程中的动作生成和状态更新，使模态之间更动态地互动。此外，我们还分析了这些方法的技术演进，讨论了其内在挑战，并介绍了评估多模态推理性能的关键基准任务和评价指标。最后，我们从以下两个角度提供了未来研究的见解：（i）从视觉-语言推理到全模态推理，以及（ii）从多模态推理到多模态代理。本文旨在提供一个结构化概述，以激励多模态推理研究的进一步进展。",
        "地址": "https://arxiv.org/pdf/2503.18071.pdf"
    },
    {
        "名称": "2025 [2503.14774] Revisiting Image Fusion for Multi-Illuminant White-Balance Correction.pdf",
        "作者": "David Serrano-Lozano, Aditya Arora, Luis Herranz, Konstantinos G. Derpanis, Michael S. Brown, Javier Vazquez-Corral",
        "摘要": "摘要: 场景中多光源的白平衡（WB）校正仍然是计算机视觉领域的一个持久挑战。近期的方法探讨了基于融合的方法，在这种方法中，神经网络线性融合了输入图像的多个sRGB版本，每个版本都经过预定义的WB预设处理。然而，我们展示了这些方法对于常见的多光源场景来说并非最优。此外，现有的基于融合的方法依赖于缺乏专用多光源图像的sRGB WB数据集，这限制了训练和评估效果。为了解决这些问题，我们引入了两个关键贡献。首先，我们提出了一种高效的基于Transformer的模型，该模型能够有效地捕捉跨sRGB WB预设的空间依赖性，相较于线性融合技术有显著改善。其次，我们引入了一个大规模的多光源数据集，该数据集包含超过16,000张以五种不同WB设置渲染的sRGB图像，以及WB校正后的图像。在我们新的多光源图像融合数据集上，我们的方法相较于现有技术达到了高达100%的改进。\n\n翻译作者及评论：\n作者: David Serrano-Lozano, Aditya Arora, Luis Herranz, Konstantinos G. Derpanis, Michael S. Brown, Javier Vazquez-Corral\n备注: 10页\n网址: https://arxiv.org/pdf/2503.14774.pdf\n标题: 重新审视图像融合以进行多光源白平衡校正",
        "地址": "https://arxiv.org/pdf/2503.14774.pdf"
    },
    {
        "名称": "2025 [2503.13074] Rethinking Image Evaluation in Super-Resolution.pdf",
        "作者": "Shaolin Su, Josep M. Rocafort, Danna Xue, David Serrano-Lozano, Lei Sun, Javier Vazquez-Corral",
        "摘要": "摘要：尽管最近图像超分辨率（SR）技术不断提高其输出的感知质量，但它们在定量评估中通常会失败。这种不一致导致人们对现有的SR评估指标越来越不信任。虽然图像评估依赖于指标和参考的真实图像（GT），但研究人员通常不检查GT的作用，因为它们通常被认为是“完美”的参考。然而，由于数据是在早期收集的且未能控制其他类型的失真，我们指出现有SR数据集中的GT可能表现出相对较差的质量，导致评估有偏差。基于这一观察，本文关注以下问题：现有SR数据集中的GT图像能否100%可信用于模型评估？GT质量如何影响这种评估？如果存在不完美的GT，如何进行公平评估？为了解答这些问题，本文提出了两个主要贡献。首先，通过系统分析三个真实世界SR数据集中的七个最先进的SR模型，我们表明低质量GT会持续影响SR性能，并且在控制GT质量时，模型的表现可能会有很大差异。其次，我们提出了一种新颖的感知质量指标，即相对质量指数（RQI），用于衡量图像对的相对质量差异，从而解决因不可靠的GT引起的偏差评估。我们提出的模型在与人类意见的一致性方面显著优于现有方法。我们期望我们的工作能为SR社区对未来数据集、模型和指标的发展提供洞见。",
        "地址": "https://arxiv.org/pdf/2503.13074.pdf"
    },
    {
        "名称": "2025 [2503.18674] Human Motion Unlearning.pdf",
        "作者": "Edoardo De Matteis, Matteo Migliarini, Alessio Sampieri, Indro Spinelli, Fabio Galasso",
        "摘要": "摘要：我们提出了人类动作消除任务，以防止在保留一般文本到动作生成性能的同时，生成有害动画。消除有害动作具有挑战性，因为这类动作可以由显性文本提示生成，也可以由隐性的安全动作的有害组合生成（例如，“踢”是“抬起并摆动一条腿”）。我们通过从大规模且最新的HumanML3D和Motion-X文本到动作数据集中过滤有害动作，提出了第一个动作消除基准。我们还通过改编最先进的图像消除技术来处理时空信号，提出了基线方法。最后，我们提出了一种基于潜在代码替换的新型动作消除模型，我们称之为LCR。LCR不需要训练，适用于最先进的文本到动作扩散模型的离散潜在空间。LCR简单并且在定性和定量方面始终优于基线方法。项目页面：\\\\href{this https URL}{this https URL}。\n\n作者：Edoardo De Matteis, Matteo Migliarini, Alessio Sampieri, Indro Spinelli, Fabio Galasso\n链接：https://arxiv.org/pdf/2503.18674.pdf\n标题：2025 [2503.18674] Human Motion Unlearning.pdf",
        "地址": "https://arxiv.org/pdf/2503.18674.pdf"
    },
    {
        "名称": "2025 [2503.18476] Global-Local Tree Search in VLMs for 3D Indoor Scene Generation.pdf",
        "作者": "Wei Deng, Mengshi Qi, Huadong Ma",
        "摘要": "摘要：大型视觉-语言模型（VLMs），例如GPT-4，在各个领域取得了显著成功。然而，关于使用VLMs进行三维室内场景生成的研究很少。本文将这一任务视为一个受空间和布局常识约束的规划问题。为了解决这一问题，我们提出了一种新的全局-局部树搜索算法。从全局来看，该方法依次放置每个对象，并在每个放置过程中探索多个放置位置，其中问题空间表示为一棵树。为了减少树的深度，我们将场景结构分层分解，即房间级、区域级、地板对象级和支撑对象级。该算法独立生成不同区域的地板对象和放置在不同地板对象上的支撑对象。从局部来看，我们还将每个对象的放置子任务分解为多个步骤。该算法在问题空间的树中进行搜索。为了利用VLM模型生成对象的位置，我们将自上而下的视图空间离散化为一个密集网格，并用各种表情符号填充每个单元格以使单元格区分开来。我们用表情符号网格提示VLM，VLM通过描述用表情符号命名的位置来生成对象的合理位置。定量和定性实验结果表明，我们的方法比最先进的方法生成了更为合理的三维场景。我们的源代码可在此链接获取：https://arxiv.org/pdf/2503.18476.pdf。",
        "地址": "https://arxiv.org/pdf/2503.18476.pdf"
    },
    {
        "名称": "2025 [2503.16709] QuartDepth: Post-Training Quantization for Real-Time Depth Estimation on the Edge.pdf",
        "作者": "Xuan Shen, Weize Ma, Jing Liu, Changdi Yang, Rui Ding, Quanyi Wang, Henghui Ding, Wei Niu, Yanzhi Wang, Pu Zhao, Jun Lin, Jiuxiang Gu",
        "摘要": "摘要：单目深度估计（Monocular Depth Estimation, MDE）已成为计算机视觉中的一项关键任务，支持许多现实世界的应用。然而，在资源受限的边缘设备，尤其是专用集成电路（Application-Specific Integrated Circuits, ASICs）上部署准确的深度估计模型具有挑战性，因为它们需要高计算和内存需求。近年来，基础深度估计的最新进展虽取得了令人瞩目的成果，但进一步增加了在ASICs上部署的难度。为了解决这个问题，我们提出了QuartDepth，该方法采用训练后量化技术，通过硬件加速来为ASICs量化MDE模型。我们的方法包括将权重和激活值量化为4位精度，减少模型大小和计算成本。为缓解性能下降，我们提出在激活量化前后的激活抛光和补偿算法，以及一种用于最小化权重量化误差的权重重建方法。此外，我们通过支持内核融合和定制指令可编程性，设计了一种灵活且可编程的硬件加速器，提高了吞吐量和效率。实验结果表明，我们的框架在实现竞争性精度的同时，能够在ASICs上实现快速推理和更高的能效，弥合了高性能深度估计与实际边缘设备适用性之间的差距。代码见：https://arxiv.org/pdf/2503.16709.pdf",
        "地址": "https://arxiv.org/pdf/2503.16709.pdf"
    },
    {
        "名称": "2025 [2503.16426] DynamicVis: An Efficient and General Visual Foundation Model for Remote Sensing Image Understanding.pdf",
        "作者": "Keyan Chen, Chenyang Liu, Bowen Chen, Wenyuan Li, Zhengxia Zou, Zhenwei Shi",
        "摘要": "摘要：遥感技术的进步提高了卫星图像的空间分辨率，使得视觉表示更加详细，便于多种解释。然而，现有方法在不同应用中的泛化能力有限。尽管一些现代基础模型显示出潜力，但它们在跨任务适应性方面不足，主要处理低分辨率的小尺寸图像，未能充分利用高分辨率数据或综合利用大型场景语义。重要的是，遥感图像与自然图像有根本性不同，关键前景目标（如海上物体、人工结构）通常占据较小的空间比例（约1%）且分布稀疏。从大约100,000个2D令牌中有效地建模跨任务可泛化知识是一个显著的挑战，但对于遥感图像理解至关重要。受人类视觉系统中选择性注意机制的启发，我们提出了DynamicVis，一种针对遥感图像的动态视觉感知基础模型。该框架基于选择性状态空间模型集成了一种新颖的动态区域感知主干，战略性地在局部细节提取与全球语境整合之间保持平衡，实现了大规模数据的计算高效编码，同时保持了架构的可扩展性。为了增强跨任务知识转移能力，我们引入一种使用百万级区域级别注释训练的元嵌入表示的多实例学习范式。在九个下游任务的评估中，该模型展示了其多功能性。DynamicVis实现了卓越效率的多级特征建模，处理（2048x2048）像素的延迟为97毫秒（为ViT的6%）和GPU内存为833 MB（为ViT的3%）。",
        "地址": "https://arxiv.org/pdf/2503.16426.pdf"
    }
]