[
    {
        "名称": "2025 [2504.00999] MergeVQ: A Unified Framework for Visual Generation and Representation with Disentangled Token Merging and Quantization.pdf",
        "作者": "Siyuan Li, Luyuan Zhang, Zedong Wang, Juanxi Tian, Cheng Tan, Zicheng Liu, Chang Yu, Qingsong Xie, Haonan Lu, Haoqian Wang, Zhen Lei",
        "摘要": "摘要：掩码图像建模（MIM）与矢量量化（VQ）在自监督预训练和图像生成中均取得了巨大的成功。然而，大多数现有方法在共享潜在空间中很难平衡生成质量、表示学习和效率。为了突破这一范式的极限，我们提出了MergeVQ，它在VQ基础上整合了token合并技术，以在统一架构中弥合图像生成和视觉表示学习之间的差距。在预训练期间，MergeVQ通过编码器中的自注意力块后的token合并模块解耦上位语意与潜在空间，从而实现后续的免查找量化（LFQ）和全局对齐，并通过解码器中的交叉注意力模块恢复它们的细粒度细节以进行重建。至于第二阶段的生成，我们引入了MergeAR，它执行KV缓存压缩以实现高效的光栅顺序预测。在ImageNet上的大量实验验证了MergeVQ作为一个AR生成模型，在视觉表示学习和图像生成任务中均取得了具有竞争力的性能，同时保持了良好的token效率和推理速度。代码和模型将可在此网址获取。",
        "地址": "https://arxiv.org/pdf/2504.00999.pdf"
    },
    {
        "名称": "2025 [2504.00883] Improved Visual-Spatial Reasoning via R1-Zero-Like Training.pdf",
        "作者": "Zhenyi Liao, Qingsong Xie, Yanhao Zhang, Zijian Kong, Haonan Lu, Zhenyu Yang, Zhijie Deng",
        "摘要": "摘要: 越来越多的关注点被放在提高多模态大语言模型（MLLMs）的推理能力上。作为在物理世界中功能的AI代理的基石，基于视频的视觉-空间智能（VSI）成为MLLMs最关键的推理能力之一。本文首次深入研究了通过类似R1-Zero的训练提升MLLMs的视觉-空间推理能力。技术上，我们首先发现，小到中型的Qwen2-VL模型的视觉-空间推理能力无法通过链式思考（CoT）提示激活。我们随后结合GRPO训练，使用精心编制的VSI-100k数据集，根据DeepSeek-R1-Zero，进行改进的视觉-空间推理。在研究过程中，我们发现保持KL惩罚（即使是较小的值）在GRPO中是必要的。用仅120 GPU小时，我们从Qwen2-VL-2B微调的vsGRPO-2B模型能比基础模型高出12.1%，并超过GPT-4o。此外，我们从Qwen2-VL-7B微调的vsGRPO-7B模型取得了与最佳开源模型LLaVA-NeXT-Video-72B相当的性能。此外，我们将vsGRPO与监督微调和直接偏好优化基线进行了比较，观察到强大的性能优势。代码和数据集将很快发布。\n\n翻译后摘要: \n随着越来越多的关注被放在提高多模态大语言模型（MLLMs）的推理能力上，基于视频的视觉-空间智能（VSI）成为在物理世界中功能的AI代理最关键的推理能力之一。本文首次深入研究了通过类似R1-Zero的训练提升MLLMs的视觉-空间推理能力。技术上，我们发现小到中型的Qwen2-VL模型的视觉-空间推理能力无法通过链式思考（CoT）提示激活。我们结合GRPO训练，使用精心编制的VSI-100k数据集，根据DeepSeek-R1-Zero，进行改进的视觉-空间推理。在研究过程中，我们发现保持KL惩罚（即使是较小的值）在GRPO中是必要的。用仅120 GPU小时，从Qwen2-VL-2B微调的vsGRPO-2B模型比基础模型高出12.1%，并超过GPT-4o。此外，从Qwen2-VL-7B微调的vsGRPO-7B模型取得了与最佳开源模型LLaVA-NeXT-Video-72B相当的性能。我们还将vsGRPO与监督微调和直接偏好优化基线进行了比较，观察到强大的性能优势。代码和数据集将很快发布。",
        "地址": "https://arxiv.org/pdf/2504.00883.pdf"
    },
    {
        "名称": "2025 [2504.01014] AnimeGamer: Infinite Anime Life Simulation with Next Game State Prediction.pdf",
        "作者": "Junhao Cheng, Yuying Ge, Yixiao Ge, Jing Liao, Ying Shan",
        "摘要": "摘要：近期在图像和视频合成方面的进展为生成式游戏带来了新的希望。其中一个特别引人注目的应用是将动画电影中的角色转变为可交互的、可玩的实体。这使得玩家可以通过语言指令模拟生活，沉浸在动态的动漫世界中，成为自己喜欢的角色。这样的游戏被定义为无限游戏，因为它们消除了预定的边界和固定的游戏规则，玩家可以通过开放的语言与游戏世界互动，体验不断演变的故事情节和环境。最近，一种前瞻性的无限动漫生活模拟方法采用大语言模型（LLMs）将多轮文本对话转换为图像生成的语言指令。然而，它忽略了历史视觉情境，导致游戏玩法不一致。此外，它只生成静态图像，未能包含动态性，难以提供引人入胜的游戏体验。在这项工作中，我们提出了AnimeGamer，它基于多模态大语言模型（MLLMs）生成每个游戏状态，包括描绘角色动作和更新角色状态的动态动画镜头，如图1所示。我们引入了新的动作感知多模态表示来表示动画镜头，这些表示可以使用视频扩散模型解码为高质量的视频片段。通过将历史动画镜头表示作为上下文并预测后续表示，AnimeGamer可以生成具有情境一致性和令人满意的动态游戏。使用自动化指标和人工评估的广泛评估表明，在游戏体验的各个方面，AnimeGamer都优于现有方法。代码和检查点可在此https URL获得。\n\n翻译：Junhao Cheng, Yuying Ge, Yixiao Ge, Jing Liao, Ying Shan",
        "地址": "https://arxiv.org/pdf/2504.01014.pdf"
    },
    {
        "名称": "2025 [2503.23368] Towards Physically Plausible Video Generation via VLM Planning.pdf",
        "作者": "Xindi Yang, Baolu Li, Yiming Zhang, Zhenfei Yin, Lei Bai, Liqian Ma, Zhiyong Wang, Jianfei Cai, Tien-Tsin Wong, Huchuan Lu, Xu Jia",
        "摘要": "摘要: 视频扩散模型（VDMs）近年来取得了显著进展，能够生成高度逼真的视频，并因其作为世界模拟器的潜力而引起了社区的关注。然而，尽管具有这些能力，VDMs 由于缺乏对物理的理解，往往无法生成物理上合理的视频，导致动态和事件顺序不正确。为了解决这一限制，我们提出了一个新颖的两阶段图像到视频生成框架，明确地结合了物理。在第一阶段，我们使用视觉语言模型（VLM）作为粗粒度运动规划器，结合思维链和物理感知推理来预测粗略的运动轨迹/变化，以近似现实世界的物理动态，同时确保帧间一致性。在第二阶段，我们使用预测的运动轨迹/变化来指导 VDM 的视频生成。由于预测的运动轨迹/变化是粗略的，因此在推理过程中添加了噪声，以便 VDM 在生成更细致运动时有更大的自由度。大量实验结果表明，我们的框架能够生成物理合理的运动，比对评估结果突出显示了我们方法相对于现有方法的显著优越性。更多视频结果可在我们的项目页面上查看：this https URL。",
        "地址": "https://arxiv.org/pdf/2503.23368.pdf"
    },
    {
        "名称": "2025 [2503.20783] Understanding R1-Zero-Like Training: A Critical Perspective.pdf",
        "作者": "Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, Min Lin",
        "摘要": "摘要：DeepSeek-R1-Zero表明在大规模下的强化学习（RL）可以在没有监督微调的情况下直接增强大型语言模型（LLM）的推理能力。在这项工作中，我们通过分析其两个核心组成部分：基础模型和RL，批判性地审视了类似R1-Zero的训练。我们调查了包括DeepSeek-V3-Base在内的广泛基础模型，以了解预训练特征如何影响RL性能。我们的分析揭示了DeepSeek-V3-Base已经展示出“顿悟时刻”，而Qwen2.5基础模型即使在没有提示模板的情况下也表现出强大的推理能力，这表明潜在的预训练偏见。此外，我们在组相对策略优化（GRPO）中发现了一种优化偏差，它在训练期间人为地增加了响应长度（尤其是对于错误输出）。为了解决这个问题，我们引入了Dr. GRPO，一种提高令牌效率同时保持推理性能的无偏优化方法。利用这些见解，我们提出了一种简化的R1-Zero方案，该方案在2024年AIME上以7B基础模型达到了43.3%的准确率，建立了新的最先进水平。我们的代码可以在此https URL获得。",
        "地址": "https://arxiv.org/pdf/2503.20783.pdf"
    },
    {
        "名称": "2025 [2504.01724] DreamActor-M1: Holistic, Expressive and Robust Human Image Animation with Hybrid Guidance.pdf",
        "作者": "Yuxuan Luo, Zhengkun Rong, Lizhen Wang, Longhao Zhang, Tianshu Hu, Yongming Zhu",
        "摘要": "摘要：尽管近期基于图像的人体动画方法在实现逼真的身体和面部动作合成方面取得了进展，但在细粒度整体可控性、多尺度适应性和长期时空一致性方面仍存在关键差距，导致其表现力和鲁棒性较低。我们提出了一种基于扩散变换器(DiT)的框架DreamActor-M1，并结合混合引导来克服这些局限性。在动作引导方面，我们的混合控制信号整合了隐式面部表示、3D头部球体和3D身体骨架，实现了对面部表情和身体动作的鲁棒控制，同时生成表现力和身份保持的动画。在尺度适应性方面，为处理从肖像到全身视图的各种身体姿势和图像尺度，我们采用了一种结合不同分辨率和尺度数据的渐进训练策略。对于外观引导，我们将顺序帧中的运动模式与互补的视觉参考相结合，确保在复杂运动过程中对未见区域的长期时空一致性。实验表明，我们的方法在表现力和鲁棒性方面均优于最先进的工作，能够为肖像、上半身和全身生成提供长期一致性的表现。项目页面：this https URL。",
        "地址": "https://arxiv.org/pdf/2504.01724.pdf"
    },
    {
        "名称": "2025 [2504.01956] VideoScene: Distilling Video Diffusion Model to Generate 3D Scenes in One Step.pdf",
        "作者": "Hanyang Wang, Fangfu Liu, Jiawei Chi, Yueqi Duan",
        "摘要": "摘要：从稀疏视图中恢复3D场景是一项具有挑战性的任务，因为它本质上是一个病态问题。传统方法已经开发了专门的解决方案（例如几何正则化或前馈确定性模型）来缓解这一问题。然而，当输入视图之间的重叠最小且视觉信息不足时，它们的性能仍然会受到影响。幸运的是，最近的视频生成模型显示出了解决这一挑战的潜力，因为它们能够生成具有合理3D结构的视频片段。得益于大型预训练视频扩散模型，一些开创性的研究开始探索视频生成先验的潜力，并从稀疏视图创建3D场景。尽管有所改进，但它们受限于缓慢的推理时间和欠缺的3D约束，导致与真实世界几何结构不符的低效和重建伪影。在本文中，我们提出了VideoScene，通过蒸馏视频扩散模型来一步生成3D场景，旨在构建一个高效且有效的工具，弥合视频到3D的差距。具体来说，我们设计了一种3D感知的跃迁流蒸馏策略，以跃过耗时的冗余信息，并训练一个动态去噪策略网络，以自适应地在推理期间确定最佳跃迁时间步长。广泛的实验表明，我们的VideoScene比以前的视频扩散模型实现了更快且更优秀的3D场景生成结果，突显了其作为未来视频到3D应用的高效工具的潜力。项目页面：this https URL（此处为示例链接）。",
        "地址": "https://arxiv.org/pdf/2504.01956.pdf"
    },
    {
        "名称": "2025 [2504.01848] PaperBench: Evaluating AI's Ability to Replicate AI Research.pdf",
        "作者": "Giulio Starace, Oliver Jaffe, Dane Sherburn, James Aung, Jun Shern Chan, Leon Maksin, Rachel Dias, Evan Mays, Benjamin Kinsella, Wyatt Thompson, Johannes Heidecke, Amelia Glaese, Tejal Patwardhan",
        "摘要": "摘要：我们介绍了PaperBench，这是一个评估AI代理复制最先进AI研究能力的基准。代理必须从头开始复制20篇ICML 2024 Spotlight和Oral论文，包括理解论文贡献、开发代码库以及成功执行实验。为了进行客观评估，我们开发了评分标准，将每个复制任务层次化分解为较小的子任务，并明确了评分标准。总共有8,316个可以单独评分的任务。评分标准与每篇ICML论文的作者共同开发，以确保准确性和现实性。为了实现可扩展的评估，我们还开发了一个基于大语言模型（LLM）的评判员，自动根据评分标准对复制尝试进行评分，并通过创建一个专门用于评估评判员的基准来评估其表现。我们在PaperBench上评估了几个前沿模型，发现表现最佳的测试代理Claude 3.5 Sonnet (New)与开源支架一起，获得了21.0％的平均复制得分。最后，我们招募了顶尖的机器学习博士生尝试PaperBench的一个子集，发现模型尚未超过人类基准。我们开源了我们的代码，以促进未来研究对AI代理的AI工程能力的理解。",
        "地址": "https://arxiv.org/pdf/2504.01848.pdf"
    },
    {
        "名称": "2025 [2504.00824] ScholarCopilot: Training Large Language Models for Academic Writing with Accurate Citations.pdf",
        "作者": "Yubo Wang, Xueguang Ma, Ping Nie, Huaye Zeng, Zhiheng Lyu, Yuxuan Zhang, Benjamin Schneider, Yi Lu, Xiang Yue, Wenhu Chen",
        "摘要": "摘要：学术写作需要生成连贯的文本并精准引用相关文献。尽管最近检索增强生成（RAG）系统在通用文本生成的事实准确性上有了显著提升，它们支持专业学术写作的能力仍然有限。在这项工作中，我们介绍了ScholarCopilot，这是一个统一框架，旨在增强现有的大语言模型，以生成带有准确和上下文相关引用的专业学术文章。ScholarCopilot通过生成检索标记[RET]动态确定何时检索学术参考文献，然后使用该标记查询引用数据库。检索到的引用文献被输入模型以增强生成过程。我们在一个框架内共同优化生成和引用任务以提高效率。我们的模型基于Qwen-2.5-7B构建，训练于arXiv中的50万篇论文。在我们的评估数据集上，其前1检索准确率达到40.1%，超越了E5-Mistral-7B-Instruct（15.0%）和BM25（9.8%）等基线模型。在包含1,000个学术写作样本的数据集上，ScholarCopilot在生成质量上得分16.2/25——这个分数是通过相关性、一致性、学术严谨性、完整性和创新性进行衡量的——显著超越了包括检索增强Qwen2.5-72B-Instruct等在内的所有现有模型。人类研究进一步表明，尽管ScholarCopilot是一个7B模型，但它在引用质量上实现了100%的偏好，并且在整体有用性上超过70%的偏好，大大超越了ChatGPT。",
        "地址": "https://arxiv.org/pdf/2504.00824.pdf"
    },
    {
        "名称": "2025 [2504.01934] ILLUME+: Illuminating Unified MLLM with Dual Visual Tokenization and Diffusion Refinement.pdf",
        "作者": "Runhui Huang, Chunwei Wang, Junwei Yang, Guansong Lu, Yunlong Yuan, Jianhua Han, Lu Hou, Wei Zhang, Lanqing Hong, Hengshuang Zhao, Hang Xu",
        "摘要": "摘要：我们介绍了ILLUME+，该模型利用双重视觉标记和扩散解码器来改进深层语义理解和高保真图像生成。现有的统一模型在理解、生成和编辑三个基本能力的统一处理上存在困难。诸如Chameleon和EMU3等模型由于使用VQGAN进行图像离散化，缺乏深度语义交互，在视觉理解任务上落后于像LLaVA这样的专业模型。为了解决这个问题，LaViT和ILLUME使用语义编码器进行标记化，但由于纹理保留较差，它们在图像编辑方面表现欠佳。与此同时，Janus系列解耦了输入和输出图像表示，限制了它们无缝处理交织的图像文本理解和生成的能力。相比之下，ILLUME+引入了统一的双重视觉标记器DualViTok，在实现粗到细的图像表示策略以支持多模态理解和生成的同时，保留了细粒度的纹理和文本对齐的语义。此外，我们使用扩散模型作为图像反标记器，以增强生成质量和高效的超分辨率。ILLUME+在统一的多模态语言模型（MLLM）中采用连续输入、离散输出方案，并采用一种支持动态分辨率的渐进训练程序。该设计允许在各种任务中灵活高效地进行上下文感知的图像编辑和生成。ILLUME+（3B）在多模态理解、生成和编辑基准上表现出对现有统一MLLM和专业模型的竞争力。凭借其强大的性能，ILLUME+为未来的多模态应用提供了一个可扩展且多功能的基础。\n\n作者：Runhui Huang, Chunwei Wang, Junwei Yang, Guansong Lu, Yunlong Yuan, Jianhua Han, Lu Hou, Wei Zhang, Lanqing Hong, Hengshuang Zhao, Hang Xu\n\n链接：https://arxiv.org/pdf/2504.01934.pdf\n\n标题：2025 [2504.01934] ILLUME+: 使用双视觉标记和扩散重构照亮统一MLLM",
        "地址": "https://arxiv.org/pdf/2504.01934.pdf"
    },
    {
        "名称": "2025 [2504.01204] Articulated Kinematics Distillation from Video Diffusion Models.pdf",
        "作者": "Xuan Li, Qianli Ma, Tsung-Yi Lin, Yongxin Chen, Chenfanfu Jiang, Ming-Yu Liu, Donglai Xiang",
        "摘要": "摘要 ：本文提出了一种名为\"关节运动蒸馏 (AKD)\" 的框架，用于通过融合基于骨架的动画和现代生成模型的优势来生成高保真角色动画。AKD 使用基于骨架的表示法对绑定的三维资产进行建模，通过关注关节级别的控制，大大减少了自由度 (DoFs)，从而实现高效、一致的运动合成。通过使用预训练的视频扩散模型进行评分蒸馏取样 (SDS)，AKD 能在保持结构完整性的同时，提炼出复杂、关节化的运动，克服了四维神经变形场在保持形状一致性方面面临的挑战。这种方法在物理基础模拟中具有天然的兼容性，确保了物理上合理的互动。实验表明，与现有的文本到四维生成方法相比，AKD 在三维一致性和运动质量上表现卓越。项目页面：https://arxiv.org/pdf/2504.01204.pdf",
        "地址": "https://arxiv.org/pdf/2504.01204.pdf"
    },
    {
        "名称": "2024 [2405.20216] Boost Your Human Image Generation Model via Direct Preference Optimization.pdf",
        "作者": "Sanghyeon Na, Yonggyu Kim, Hyunjoon Lee",
        "摘要": "摘要：人像生成是图像合成中的关键焦点，因为其应用广泛，但即使在解剖结构、姿势或细节上的轻微不准确都可能影响其逼真性。为了解决这些挑战，我们探索了直接偏好优化（DPO），该方法通过训练模型生成偏好（优胜）图像并与非偏好（失利）图像区分开来。然而，传统的DPO方法将生成的图像用作优胜图像，限制了逼真度。为了克服这个限制，我们提出了一种增强的DPO方法，将高质量的真实图像作为优胜图像，鼓励输出与真实图像相似，而不是生成图像。然而，实施这一概念并非一项简单的任务。因此，我们的方法HG-DPO（通过DPO进行人像生成）采用了一种新的课程学习框架，逐步提高模型输出的逼真度，使训练变得更加可行。此外，HG-DPO能够有效适应个性化的文本到图像任务，生成高质量和身份特定的图像，突显了我们方法的实用价值。",
        "地址": "https://arxiv.org/pdf/2405.20216.pdf"
    },
    {
        "名称": "2025 [2504.01308] Safeguarding Vision-Language Models: Mitigating Vulnerabilities to Gaussian Noise in Perturbation-based Attacks.pdf",
        "作者": "Jiawei Wang, Yushen Zuo, Yuanjun Chai, Zhendong Liu, Yichen Fu, Yichun Feng, Kin-man Lam",
        "摘要": "摘要：视觉-语言模型（VLMs）通过结合视觉信息扩展了大型语言模型（LLMs）的功能，但在处理噪声或损坏的图像时仍然容易受到越狱攻击。尽管现有的VLMs在训练过程中采用了安全措施以减轻这种攻击，但与噪声增强的视觉输入相关的漏洞却被忽视。在这项工作中，我们发现缺少噪音增强训练会导致严重的安全漏洞：许多VLMs即使面对简单的扰动如高斯噪声也易受攻击。为了解决这一挑战，我们提出了Robust-VLGuard，这是一组包含对齐/未对齐图文对的多模态安全数据集，结合噪音增强的微调，能够在保持VLM功能的同时降低攻击成功率。对于更强的基于优化的视觉扰动攻击，我们提出了DiffPure-VLM，利用扩散模型将对抗性扰动转化为类高斯噪音，VLM可通过噪音增强的安全微调进行防御。实验结果表明，扩散模型的分布转换特性与我们微调的VLMs高度一致，显著减轻了不同强度的对抗性扰动。数据集和代码可在此HTTPS链接获得。\n\n作者：Jiawei Wang, Yushen Zuo, Yuanjun Chai, Zhendong Liu, Yichen Fu, Yichun Feng, Kin-man Lam\n\n链接：https://arxiv.org/pdf/2504.01308.pdf\n\n标题：2025 [2504.01308] 保护视觉-语言模型：缓解基于扰动攻击中的高斯噪声漏洞",
        "地址": "https://arxiv.org/pdf/2504.01308.pdf"
    },
    {
        "名称": "2025 [2503.23573] DASH: Detection and Assessment of Systematic Hallucinations of VLMs.pdf",
        "作者": "Maximilian Augustin, Yannic Neuhaus, Matthias Hein",
        "摘要": "摘要: 视觉语言模型(VLMs)容易出现物体幻觉，即错误地指示图像中存在某些对象。现有的基准使用相对较小的标注数据集来量化幻觉。然而，这种方法对于评估VLMs在广泛使用的开放世界环境中产生的幻觉是不足的，也不足以检测VLMs的系统性错误。我们提出了DASH（系统幻觉检测和评估），这是一个设计用于在开放世界环境中识别VLMs对现实世界图像的系统性幻觉的大规模自动管道。一个关键部分是基于图像检索的DASH-OPT，我们在“自然图像流形”上进行优化，以生成误导VLM的图像。DASH的输出包括VLM对其中幻觉出物体的真实图像和语义相似图像的聚类。我们将DASH应用于PaliGemma和两个LLaVA-NeXT模型，涵盖380个对象类别，总共发现了超过19k个包含950k张图像的聚类。我们研究了所识别的系统性幻觉向其他VLM的转移，并显示通过使用DASH获得的特定于模型的图像对PaliGemma进行微调可以缓解物体幻觉。代码和数据可在此https URL获得。",
        "地址": "https://arxiv.org/pdf/2503.23573.pdf"
    },
    {
        "名称": "2025 [2503.23135] LSNet: See Large, Focus Small.pdf",
        "作者": "Ao Wang, Hui Chen, Zijia Lin, Jungong Han, Guiguang Ding",
        "摘要": "摘要：视觉网络设计，包括卷积神经网络和视觉Transformer，在计算机视觉领域取得了长足进步。然而，其复杂的计算对于实际部署，特别是实时应用构成了挑战。为了解决这一问题，研究人员探索了各种轻量级和高效的网络设计。然而，现有的轻量级模型主要依赖于自注意机制和卷积来进行令牌混合。这种依赖在轻量级网络的感知和聚合过程中带来了效果和效率上的限制，阻碍了在有限计算预算下性能与效率的平衡。在本文中，我们从高效的人类视觉系统中固有的动态异尺度视力能力中汲取灵感，提出了一种“广泛观察，精细聚焦”的轻量级视觉网络设计策略。我们引入了LS（大-小）卷积，它结合了大核感知和小核聚合。它可以高效地捕捉广泛的感知信息，并实现对动态和复杂视觉表示的精确特征聚合，从而实现视觉信息的熟练处理。在LS卷积的基础上，我们提出了LSNet，一个新的轻量级模型家族。大量实验表明，在各种视觉任务中，LSNet在性能和效率方面优于现有的轻量级网络。代码和模型可在此提供：https URL。\n\n评论：CVPR 2025 摄影就绪版本\n\n链接： https://arxiv.org/pdf/2503.23135.pdf\n\n标题：LSNet：广泛观察，精细聚焦",
        "地址": "https://arxiv.org/pdf/2503.23135.pdf"
    },
    {
        "名称": "2025 [2504.00406] VerifiAgent: a Unified Verification Agent in Language Model Reasoning.pdf",
        "作者": "Jiuzhou Han, Wray Buntine, Ehsan Shareghi",
        "摘要": "摘要：大型语言模型展示了非凡的推理能力，但通常会产生不可靠或错误的响应。现有的验证方法通常是特定于模型或有领域限制的，需要大量计算资源且缺乏在各种推理任务中的可扩展性。为了解决这些局限性，我们提出了一种统一的验证代理VerifiAgent，它整合了两个层次的验证：元验证（meta-verification），用于评估模型响应的完整性和一致性，以及基于工具的自适应验证（tool-based adaptive verification），其中VerifiAgent基于推理类型（例如数学、逻辑或常识推理）自主选择适当的验证工具。这种自适应方法确保了在不同验证场景中的效率和鲁棒性。实验结果表明，VerifiAgent在所有推理任务中均优于基线验证方法（例如，演绎验证器、反向验证器）。此外，它可以通过利用验证结果的反馈进一步提高推理准确性。与现有的数学推理领域中的过程奖励模型相比，VerifiAgent还可以有效应用于推理扩展，以更少的生成样本和成本实现更好的结果。代码在该网址提供：https://arxiv.org/pdf/2504.00406.pdf",
        "地址": "https://arxiv.org/pdf/2504.00406.pdf"
    },
    {
        "名称": "2025 [2503.23798] Adaptive Layer-skipping in Pre-trained LLMs.pdf",
        "作者": "Xuan Luo, Weizhi Wang, Xifeng Yan",
        "摘要": "2025年，抽象：为了加速大语言模型（LLMs）中的标记生成，已经提出了多种层跳跃方法。然而，这些方法忽略了一个基本问题：计算需求在生成不同标记过程中是如何变化的？在这项工作中，我们介绍了FlexiDepth，这是一种动态调整文本生成中Transformer层数的方法。通过引入一个插件路由器和适配器，FlexiDepth在不修改原始参数的情况下实现了LLMs的自适应层跳跃。将FlexiDepth引入Llama-3-8B模型，可以在32层中跳过8层，同时保持100%的基准性能。FlexiDepth的实验结果表明，LLMs中的计算需求根据标记类型显著变化。具体来说，生成重复标记或固定短语需要较少层数，而生成涉及计算或高不确定性的标记则需要更多层数。有趣的是，这种自适应分配模式与人类直觉相符。为了推进该领域的研究，我们开源了FlexiDepth以及记录FlexiDepth层分配模式的数据集供未来探索。\n\n作者：罗轩，王伟志，严西峰\n网址：https://arxiv.org/pdf/2503.23798.pdf\n标题：2025 [2503.23798] 预训练LLMs中的自适应层跳跃",
        "地址": "https://arxiv.org/pdf/2503.23798.pdf"
    },
    {
        "名称": "2025 [2503.22879] Quamba2: A Robust and Scalable Post-training Quantization Framework for Selective State Space Models.pdf",
        "作者": "Hung-Yueh Chiang, Chi-Chih Chang, Natalia Frumkin, Kai-Chiang Wu, Mohamed S. Abdelfattah, Diana Marculescu",
        "摘要": "这篇论文的摘要如下：\n\n摘要：状态空间模型(SSMs)因其一致的内存使用和高性能，正在成为变压器模型的一个引人注目的替代方案。尽管如此，由于其存储需求和计算能力的限制，在云服务或资源有限的设备上扩展SSMs是具有挑战性的。为了克服这一问题，使用低位宽数据格式量化SSMs可以减少模型规模，并从硬件加速中受益。由于SSMs容易受到量化引起的误差的影响，近期的努力主要集中在优化特定模型或位宽的效率，而不牺牲性能。然而，不同的场景需要不同的位宽配置，例如W4A8用于提升大批量解码速度，而W4A16用于在单用户短提示应用中提高生成速度。为此，我们推出了Quamba2，兼容W8A8、W4A8和W4A16，适用于Mamba1和Mamba2骨干网络，以满足在各种平台上部署SSM的日益增长的需求。基于SSM的通道顺序保持和激活持久性，我们提出了一种离线方法，通过排序和聚类输入$x$，将线性递归输入量化为8位，结合每状态组量化用于输入相关参数$B$和$C$。为了确保SSM输出的计算不变性，我们根据聚类顺序离线重新排列权重。实验结果表明，Quamba2-8B在预填充和生成阶段分别提升了1.3倍和3倍的速度，并且提供了4倍的内存减少，同时平均准确率仅下降了1.6%。在MMLU上的评估显示了我们框架的普遍性和鲁棒性。代码和量化模型将发布在此网址：https URL。",
        "地址": "https://arxiv.org/pdf/2503.22879.pdf"
    },
    {
        "名称": "2025 [2504.01201] Medical large language models are easily distracted.pdf",
        "作者": "Krithik Vishwanath, Anton Alyakin, Daniel Alexander Alber, Jin Vivian Lee, Douglas Kondziolka, Eric Karl Oermann",
        "摘要": "摘要：大型语言模型(LLMs)有可能改变医学领域，但真实的临床场景中包含的额外信息可能会妨碍其性能。辅助技术的兴起，如自动生成现场患者会话草稿笔记的环境听写技术，可能会引入额外的噪音，因此评估LLMs过滤相关数据的能力变得十分重要。为此，我们开发了MedDistractQA，这是一个使用嵌入模拟现实干扰的USMLE风格问题的基准。我们的研究发现，干扰性陈述(具有临床意义的多义词在非临床环境中的使用或与临床无关的健康状况参考)会使LLM的准确性降低多达17.9%。常见的提高模型性能的建议，如检索增强生成(RAG)和医学微调，并未改变这种影响，在某些情况下还引入了自身的混杂因素并进一步降低了性能。我们的研究表明，LLMs本身缺乏区分临床相关和无关信息的逻辑机制，这对实际应用构成了挑战。MedDistractQA及我们的研究结果强调了需要制定强有力的缓解策略，以增强LLM对额外信息的抵抗力。",
        "地址": "https://arxiv.org/pdf/2504.01201.pdf"
    },
    {
        "名称": "2025 [2503.18950] Target-Aware Video Diffusion Models.pdf",
        "作者": "Taeksoo Kim, Hanbyul Joo",
        "摘要": "摘要：我们提出了一种目标感知的视频扩散模型，该模型能够根据输入图像生成视频，其中演员与指定目标交互并执行所需的动作。目标由分割掩码定义，所需动作通过文本提示描述。与现有可控的图像到视频扩散模型通常依赖密集结构或运动线索来引导演员朝目标移动不同，我们的目标感知模型仅需简单的掩码来指示目标，利用预训练模型的泛化能力来生成合理动作。这使我们的方法在高难度的提供精确行动指导的人物-物体交互（HOI）场景中特别有效，并进一步使得视频扩散模型在诸如机器人等应用中的高层次行动规划中得以应用。我们通过扩展基线模型以将目标掩码作为附加输入来构建我们的目标感知模型。为了强化目标感知，我们引入了一个特殊的标记，在文本提示中编码目标的空间信息。然后，我们用经过策划的数据集通过一种新颖的交叉注意损失对模型进行微调，该损失将与此标记关联的交叉注意图与输入目标掩码对齐。为了进一步提高性能，我们选择性地将这种损失应用于最语义相关的变压器模块和注意区域。实验结果表明，我们的目标感知模型在生成演员准确与指定目标交互的视频方面优于现有解决方案。我们进一步展示了其在两个下游应用中的有效性：视频内容创作和零样本3D HOI运动合成。\n\n**原文作者**：Taeksoo Kim, Hanbyul Joo\n\n**项目页面**：评论中提供了项目页面的URL\n\n**标题**：目标感知视频扩散模型",
        "地址": "https://arxiv.org/pdf/2503.18950.pdf"
    },
    {
        "名称": "2025 [2503.18817] Enhanced OoD Detection through Cross-Modal Alignment of Multi-Modal Representations.pdf",
        "作者": "Jeonghyeon Kim, Sangheum Hwang",
        "摘要": "摘要: 先前关于分布外检测（OoDD）的研究主要集中在单一模态模型上。最近，随着像CLIP这样的大规模预训练视觉-语言模型的出现，通过零样本和提示学习策略利用这种多模态表示的OoDD方法开始出现。然而，这些方法通常涉及冻结预训练权重或仅部分调整它们，这对下游数据集而言可能不是最优的。在本文中，我们强调，多模态微调（MMFT）可以实现显著的OoDD性能。尽管最近的一些工作展示了微调方法对OoDD的影响，但仍有很大的性能提升空间。我们研究了天真的微调方法的局限性，探讨其未能充分利用预训练知识的原因。我们的实证分析表明，这个问题可能源于在分布内（ID）嵌入中的模态差距。为了应对这一挑战，我们提出了一个通过正则化ID数据的图像和文本嵌入之间距离来增强跨模态对齐的训练目标。这种调整通过在超球表示空间中更紧密地对齐来自不同模式（即文本和图像）中具有相似语义的信息，从而帮助更好地利用预训练的文本信息。我们从理论上证明，所提出的正则化对应于超球面上的能量模型的最大似然估计。利用ImageNet-1k OoD基准数据集，我们展示了我们的方法结合利用预训练知识的后处理OoDD方法（例如，NegLabel），大大优于现有方法，达到最先进的OoDD性能并提高ID准确性。",
        "地址": "https://arxiv.org/pdf/2503.18817.pdf"
    },
    {
        "名称": "2025 [2502.18924] MegaTTS 3: Sparse Alignment Enhanced Latent Diffusion Transformer for Zero-Shot Speech Synthesis.pdf",
        "作者": "Ziyue Jiang, Yi Ren, Ruiqi Li, Shengpeng Ji, Boyang Zhang, Zhenhui Ye, Chen Zhang, Bai Jionghao, Xiaoda Yang, Jialong Zuo, Yu Zhang, Rui Liu, Xiang Yin, Zhou Zhao",
        "摘要": "论文摘要:尽管近期的零样本文本到语音（TTS）模型在语音质量和表现力方面有了显著提高，但主流系统在语音-文本对齐建模方面仍然存在问题：1）没有明确语音-文本对齐建模的模型表现出较低的鲁棒性，特别是对于实际应用中的难句子；2）基于预定义对齐的模型在强制对齐的自然性方面存在限制。本文介绍了\\textit{MegaTTS 3}，一种TTS系统，采用了创新的稀疏对齐算法来引导潜在扩散变压器（DiT）。具体而言，我们为MegaTTS 3提供了稀疏对齐边界，以减少对齐的困难而不限制搜索空间，从而实现高自然度。此外，我们采用了一种多条件无分类器指导策略来调整口音强度，并采用分段校正流技术来加速生成过程。实验表明，MegaTTS 3在零样本TTS语音质量方面达到了最先进的水平，并支持对口音强度的高度灵活控制。值得注意的是，我们的系统可以在仅8个采样步骤下生成高质量的一分钟语音。音频样本可在此https URL上获取。\n\n作者:蒋梓岳, 任一, 李瑞琪, 姬盛朋, 张博洋, 叶振辉, 张晨, 白炯昊, 杨晓达, 左佳龙, 张宇, 刘锐, 殷翔, 赵洲",
        "地址": "https://arxiv.org/pdf/2502.18924.pdf"
    }
]