[
    {
        "名称": "2025 [2504.08685] Seaweed-7B: Cost-Effective Training of Video Generation Foundation Model.pdf",
        "作者": "Team Seawead, Ceyuan Yang, Zhijie Lin, Yang Zhao, Shanchuan Lin, Zhibei Ma, Haoyuan Guo, Hao Chen, Lu Qi, Sen Wang, Feng Cheng, Feilong Zuo Xuejiao Zeng, Ziyan Yang, Fangyuan Kong, Zhiwu Qing, Fei Xiao, Meng Wei, Tuyen Hoang, Siyu Zhang, Peihao Zhu, Qi Zhao, Jiangqiao Yan, Liangke Gui, Sheng Bi, Jiashi Li, Yuxi Ren, Rui Wang, Huixia Li, Xuefeng Xiao, Shu Liu, Feng Ling, Heng Zhang, Houmin Wei, Huafeng Kuang, Jerry Duncan, Junda Zhang, Junru Zheng, Li Sun, Manlin Zhang, Renfei Sun, Xiaobin Zhuang, Xiaojie Li, Xin Xia, Xuyan Chi, Yanghua Peng, Yuping Wang, Yuxuan Wang, Zhongkai Zhao, Zhuo Chen, Zuquan Song, Zhenheng Yang, Jiashi Feng, Jianchao Yang, Lu Jiang",
        "摘要": "摘要：这份技术报告提出了一种成本高效的视频生成基础模型训练策略。我们展示了一个名为Seaweed-7B的大约包含70亿参数（7B）的中型研究模型，该模型使用665,000小时的H100 GPU从头训练。尽管使用了适量的计算资源进行训练，Seaweed-7B表现出与当今规模更大的视频生成模型高度竞争的性能。在资源受限的情况下，设计选择尤其重要。该技术报告强调了提升这一中型扩散模型性能的关键设计决策。根据实证研究，我们观察到：（1）Seaweed-7B的性能可以媲美甚至超越在显著更多GPU资源上训练出的更大模型，（2）我们的模型展示出强大的泛化能力，可以通过轻量级微调或继续训练有效地适应广泛的下游应用。有关项目页面，请访问此https URL。\n\n作者：Team Seawead, Ceyuan Yang, Zhijie Lin, Yang Zhao, Shanchuan Lin, Zhibei Ma, Haoyuan Guo, Hao Chen, Lu Qi, Sen Wang, Feng Cheng, Feilong Zuo, Xuejiao Zeng, Ziyan Yang, Fangyuan Kong, Zhiwu Qing, Fei Xiao, Meng Wei, Tuyen Hoang, Siyu Zhang, Peihao Zhu, Qi Zhao, Jiangqiao Yan, Liangke Gui, Sheng Bi, Jiashi Li, Yuxi Ren, Rui Wang, Huixia Li, Xuefeng Xiao, Shu Liu, Feng Ling, Heng Zhang, Houmin Wei, Huafeng Kuang, Jerry Duncan, Junda Zhang, Junru Zheng, Li Sun, Manlin Zhang, Renfei Sun, Xiaobin Zhuang, Xiaojie Li, Xin Xia, Xuyan Chi, Yanghua Peng, Yuping Wang, Yuxuan Wang, Zhongkai Zhao, Zhuo Chen, Zuquan Song, Zhenheng Yang, Jiashi Feng, Jianchao Yang, Lu Jiang\n\n标题：2025 [2504.08685] Seaweed-7B：成本高效的视频生成基础模型训练.pdf\n\n评论：技术报告\n\n网址：https://arxiv.org/pdf/2504.08685.pdf",
        "地址": "https://arxiv.org/pdf/2504.08685.pdf"
    },
    {
        "名称": "2025 [2504.08736] GigaTok: Scaling Visual Tokenizers to 3 Billion Parameters for Autoregressive Image Generation.pdf",
        "作者": "Tianwei Xiong, Jun Hao Liew, Zilong Huang, Jiashi Feng, Xihui Liu",
        "摘要": "摘要：在自回归图像生成中，视觉标记器将图像压缩成紧凑的离散潜在标记，从而通过下一个标记预测实现高效的视觉生成自回归模型训练。虽然对视觉标记器进行扩展可以提高图像重建质量，但通常会降低下游生成质量——这一挑战在现有文献中尚未得到充分解决。为此，我们引入了GigaTok，这是首个在扩展视觉标记器时同步提升图像重建、生成和表示学习的方法。我们确定了潜在空间复杂性不断增加是重建与生成困境的关键因素。为了解决这个问题，我们提出了语义正则化，将标记器特征与来自预训练视觉编码器的语义一致特征对齐。这一约束防止了扩展过程中潜在空间复杂性的过度增加，从而在重建和下游自回归生成中实现了一致的改进。在语义正则化的基础上，我们探索了扩展标记器的三个关键实践：(1) 使用一维标记器以获得更好的可扩展性，(2) 在同时扩展编码器和解码器时优先扩展解码器，(3) 使用熵损失稳定特量级标记器的训练。通过扩展到30亿参数，GigaTok在重建、下游AR生成和下游AR表示质量方面达到了最先进的性能。\n\n作者：熊天威、刘俊浩、黄子龙、冯佳时、刘曦辉\n\n评论: 项目页面: this https URL\n\n链接: https://arxiv.org/pdf/2504.08736.pdf\n\n标题：2025 [2504.08736] GigaTok: Scaling Visual Tokenizers to 3 Billion Parameters for Autoregressive Image Generation.pdf",
        "地址": "https://arxiv.org/pdf/2504.08736.pdf"
    },
    {
        "名称": "2025 [2504.08388] MineWorld: a Real-Time and Open-Source Interactive World Model on Minecraft.pdf",
        "作者": "Junliang Guo, Yang Ye, Tianyu He, Haoyu Wu, Yushu Jiang, Tim Pearce, Jiang Bian",
        "摘要": "摘要：世界建模是一项至关重要的任务，旨在让智能代理能够有效地与人类互动，并在动态环境中运作。在这项工作中，我们提出了MineWorld，这是一个在Minecraft上运行的实时交互世界模型。Minecraft是一个开放的沙盒游戏，常被用作世界建模的测试平台。MineWorld由一个视觉-动作自回归Transformer驱动，它将配对的游戏场景和相应的动作作为输入，并生成遵循这些动作的新场景。具体来说，通过图像标记器和动作标记器将视觉游戏场景和动作分别转换为离散的标记ID，我们将这两种ID交错并进行连接，作为模型输入。然后，通过下一个标记预测的训练，模型同时学习游戏状态的丰富表示和状态与动作之间的条件。在推理过程中，我们开发了一种新颖的并行解码算法，能够同时预测每一帧中的空间冗余标记，使得不同规模的模型每秒生成4到7帧，实现了与游戏玩家的实时互动。在评估中，我们提出了新的指标，不仅用来评估视觉质量，还用于评估生成新场景时的动作跟随能力，这对世界模型至关重要。我们的全面评估显示，MineWorld的效果显著优于目前最先进的开源基于扩散的世界模型。代码和模型已经发布。\n\n作者：Guo Junliang, Ye Yang, He Tianyu, Wu Haoyu, Jiang Yushu, Pearce Tim, Bian Jiang\n\n备注：技术报告。项目页面此https URL",
        "地址": "https://arxiv.org/pdf/2504.08388.pdf"
    },
    {
        "名称": "2025 [2504.08600] SQL-R1: Training Natural Language to SQL Reasoning Model By Reinforcement Learning.pdf",
        "作者": "Peixian Ma, Xialie Zhuang, Chengjin Xu, Xuhui Jiang, Ran Chen, Jian Guo",
        "摘要": "摘要：自然语言到SQL（NL2SQL）通过将自然语言查询转化为结构化的SQL语句，促进了与数据库的直观交互。尽管在提升数据库应用中的人机交互方面有了最近的进展，特别是在涉及多表连接和嵌套查询的复杂场景下，推理性能仍面临重大挑战。目前的方法主要利用有监督微调（SFT）来训练NL2SQL模型，这可能在新环境（例如金融和医疗）中限制适应性和可解释性。为了提升NL2SQL模型在上述复杂情况下的推理性能，我们引入了SQL-R1，这是一种通过强化学习（RL）算法训练的新型NL2SQL推理模型。我们设计了一种专门针对NL2SQL任务的基于强化学习的奖励函数，并讨论了冷启动对密集训练效果的影响。此外，我们仅使用少量合成的NL2SQL数据进行增强训练，便取得了竞争性的准确率，并进一步探索了强化学习的数据工程。在现有的实验中，SQL-R1在基准测试Spider和BIRD中分别达到了88.6%和66.6%的执行准确率，使用的只是7B基本模型。",
        "地址": "https://arxiv.org/pdf/2504.08600.pdf"
    },
    {
        "名称": "2025 [2504.08591] ZipIR: Latent Pyramid Diffusion Transformer for High-Resolution Image Restoration.pdf",
        "作者": "Yongsheng Yu, Haitian Zheng, Zhifei Zhang, Jianming Zhang, Yuqian Zhou, Connelly Barnes, Yuchen Liu, Wei Xiong, Zhe Lin, Jiebo Luo",
        "摘要": "摘要：最近，生成模型在图像修复能力方面取得了显著进展，尤其是通过强大的扩散模型（Diffusion Models），这些模型在语义细节恢复和局部保真度方面表现出色。然而，由于长距离注意机制（long-range attention mechanisms）的计算需求，在超高分辨率下部署这些模型存在一个关键的折衷，即在质量和效率之间的权衡。为了解决这个问题，我们引入了ZipIR，这是一种新颖的框架，它在高分辨率图像修复方面提升了效率、可扩展性和长距离建模能力。ZipIR采用了一种高度压缩的潜在表示（latent representation），将图像压缩至32倍，显著减少了空间标记的数量，从而可以使用高容量的模型，如Diffusion Transformer (DiT)。为此，我们提出了一种潜在金字塔VAE (LP-VAE)设计，将潜在空间结构分为子带，以简化扩散训练。ZipIR在高达2K分辨率的完整图像上进行训练，超越了现有的基于扩散的方法，以无与伦比的速度和质量从严重退化的输入图像中恢复出高分辨率图像。",
        "地址": "https://arxiv.org/pdf/2504.08591.pdf"
    },
    {
        "名称": "2025 [2504.07615] VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model.pdf",
        "作者": "Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, Ruochen Xu, Tiancheng Zhao",
        "摘要": "摘要：最近，DeepSeek R1展示了通过简单而有效的设计，强化学习（RL）能够显著提升大型语言模型（LLMs）的推理能力。R1的核心在于其基于规则的奖励机制，该机制利用具有确定性真值答案的任务来实现精确和稳定的奖励计算。在视觉领域，我们同样观察到，各种视觉理解任务本质上都配备了明确的真值注释。这一特点使它们天然适用于基于规则的奖励机制。基于这一观察，我们研究了将R1风格的强化学习扩展到视觉-语言模型（VLMs），以增强其视觉推理能力。为此，我们开发了VLM-R1，一个专门的框架，旨在利用RL提高VLMs在一般视觉-语言任务上的表现。利用这一框架，我们进一步探索了将RL应用于视觉领域的可行性。实验结果表明，基于RL的模型不仅在视觉理解任务上表现出色，而且在泛化能力上也超越了监督微调（SFT）。此外，我们进行了全面的消融研究，揭示了一系列值得注意的见解，包括对象检测中的奖励破解现象、“OD aha时刻”的出现、训练数据质量的影响以及RL在不同模型规模上的扩展行为。通过这些分析，我们旨在加深对强化学习如何提升视觉-语言模型能力的理解，并希望我们的研究成果和开源贡献能够支持视觉-语言RL社区的持续进步。我们的代码和模型可在此网址获取。\n\n翻译结果：\n最近，DeepSeek R1展示了通过简单而有效的设计，强化学习（RL）能够显著提升大型语言模型（LLMs）的推理能力。R1的核心在于其基于规则的奖励机制，该机制利用具有确定性真值答案的任务来实现精确和稳定的奖励计算。在视觉领域，我们同样观察到，各种视觉理解任务本质上都配备了明确的真值注释。这一特点使它们天然适用于基于规则的奖励机制。基于这一观察，我们研究了将R1风格的强化学习扩展到视觉-语言模型（VLMs），以增强其视觉推理能力。为此，我们开发了VLM-R1，一个专门的框架，旨在利用RL提高VLMs在一般视觉-语言任务上的表现。利用这一框架，我们进一步探索了将RL应用于视觉领域的可行性。实验结果表明，基于RL的模型不仅在视觉理解任务上表现出色，而且在泛化能力上也超越了监督微调（SFT）。此外，我们进行了全面的消融研究，揭示了一系列值得注意的见解，包括对象检测中的奖励黑客现象、“OD aha时刻”的出现、训练数据质量的影响以及RL在不同模型规模上的扩展行为。通过这些分析，我们旨在加深对强化学习如何提升视觉-语言模型能力的理解，并希望我们的研究成果和开源贡献能支持视觉-语言RL社区的持续进步。我们的代码和模型可以在此网址获取。",
        "地址": "https://arxiv.org/pdf/2504.07615.pdf"
    },
    {
        "名称": "2025 [2504.07963] PixelFlow: Pixel-Space Generative Models with Flow.pdf",
        "作者": "Shoufa Chen, Chongjian Ge, Shilong Zhang, Peize Sun, Ping Luo",
        "摘要": "摘要: 我们介绍了PixelFlow，一种直接在原始像素空间中运行的图像生成模型系列，与主要的潜空间模型形成对比。这种方法通过消除对预训练的变分自编码器（VAE）的需求，使整个模型可以端到端地进行训练，从而简化了图像生成过程。通过高效的级联流建模，PixelFlow在像素空间中实现了可负担的计算成本。在256×256 ImageNet类别条件图像生成基准上，它获得了1.98的FID分数。定性文本到图像的结果表明，PixelFlow在图像质量、艺术性和语义控制方面表现出色。我们希望这种新范式将为下一代视觉生成模型激发灵感并开辟新的机会。代码和模型可在此网址获取。",
        "地址": "https://arxiv.org/pdf/2504.07963.pdf"
    },
    {
        "名称": "2025 [2504.07405] FlexIP: Dynamic Control of Preservation and Personality for Customized Image Generation.pdf",
        "作者": "Linyan Huang, Haonan Lin, Yanning Zhou, Kaiwen Xiao",
        "摘要": "摘要：随着2D生成模型的快速发展，在保持主体身份的同时实现多样化编辑成为一项重要的研究重点。现有方法通常在身份保留和个性化操作之间存在内在权衡。我们提出了FlexIP，这是一种通过两个专用组件解决这一问题的新框架：用于风格操作的个性化适配器和用于身份维护的保留适配器。通过明确地将这两种控制机制注入生成模型，我们的框架在推理过程中通过动态调整权重适配器，实现了灵活的参数化控制。实验结果表明，我们的方法突破了传统方法的性能限制，在实现优越的身份保留的同时，支持更多样化的个性化生成能力（项目页面：this https URL）。",
        "地址": "https://arxiv.org/pdf/2504.07405.pdf"
    },
    {
        "名称": "2025 [2504.05262] Do PhD-level LLMs Truly Grasp Elementary Addition? Probing Rule Learning vs. Memorization in Large Language Models.pdf",
        "作者": "Yang Yan, Yu Lu, Renjun Xu, Zhenzhong Lan",
        "摘要": "摘要：尽管在基准测试中表现优异，大型语言模型（LLMs）经常在简单问题上失败，这提出了一个关键问题：LLMs是学习了数学原理还是仅仅记住了模式？与其设计越来越复杂的基准测试，我们通过简单的两个整数加法（$0$到$2^{64}$）研究这一问题，探讨了两个核心属性：交换性（$A+B=B+A$）和组合泛化（通过同构符号映射，例如$7 \\\\rightarrow y$）。尽管最先进的LLMs在数值加法上达到了73.8%-99.8%的准确率，但在符号映射下其表现骤降至$\\\\leq$7.5%，这表明其未能泛化所学规则。随着数字位数的增加表现出非单调性增长和频繁的交换性违背（超过1700例$A+B \\\\neq B+A$）进一步支持了这一点。明确提供加法规则使性能平均下降81.2%，而自我解释保持基线准确率，表明LLM的算术处理与人为定义的原则不一致。我们的发现表明，当前LLMs依赖于记忆模式而非真正的规则学习，凸显了其架构的局限性以及实现真正数学推理的新方法的必要性。\n\n作者：Yang Yan, Yu Lu, Renjun Xu, Zhenzhong Lan\n\n链接：https://arxiv.org/pdf/2504.05262.pdf\n\n标题：LLMs具备博士学位水平，它们真的掌握了基础加法吗？探讨大型语言模型的规则学习与记忆",
        "地址": "https://arxiv.org/pdf/2504.05262.pdf"
    },
    {
        "名称": "2025 [2504.07866] Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs.pdf",
        "作者": "Yichun Yin, Wenyong Huang, Kaikai Song, Yehui Tang, Xueyu Wu, Wei Guo, Peng Guo, Yaoyuan Wang, Xiaojun Meng, Yasheng Wang, Dong Li, Can Chen, Dandan Tu, Yin Li, Fisher Yu, Ruiming Tang, Yunhe Wang, Baojun Wang, Bin Wang, Bo Wang, Boxiao Liu, Changzheng Zhang, Duyu Tang, Fei Mi, Hui Jin, Jiansheng Wei, Jiarui Qin, Jinpeng Li, Jun Zhao, Liqun Deng, Lin Li, Minghui Xu, Naifu Zhang, Nianzu Zheng, Qiang Li, Rongju Ruan, Shengjun Cheng, Tianyu Guo, Wei He, Wei Li, Weiwen Liu, Wulong Liu, Xinyi Dai, Yonghan Dong, Yu Pan, Yue Li, Yufei Wang, Yujun Li, Yunsheng Ni, Zhe Liu, Zhenhe Zhang, Zhicheng Liu",
        "摘要": "摘要: 我们介绍了Pangu Ultra，一个具有1350亿参数和稠密Transformer模块的大型语言模型（LLM），并在Ascend神经处理单元（NPU）上进行了训练。尽管近年来LLM领域在规模和能力方面取得了前所未有的进展，但训练如此大规模的模型仍涉及显著的优化和系统挑战。为稳定训练过程，我们提出了深度尺度三明治归一化，有效消除了深度模型训练过程中的损失峰值。我们在13.2万亿个多样且高质量的标记上预训练我们的模型，并在训练后进一步增强其推理能力。为高效进行如此大规模的训练，我们利用了8,192个Ascend NPU，并进行了一系列系统优化。在多个多样化基准上的评估表明，Pangu Ultra显著提高了稠密LLMs（如Llama 405B和Mistral Large 2）的最先进能力，甚至在与具有稀疏模型结构且含有更多参数的DeepSeek-R1相比时也取得了竞争性结果。我们的探索表明Ascend NPU能够高效且有效地训练具有超过1000亿参数的稠密模型。我们的模型和系统将提供给商业客户。",
        "地址": "https://arxiv.org/pdf/2504.07866.pdf"
    },
    {
        "名称": "2025 [2504.08727] Visual Chronicles: Using Multimodal LLMs to Analyze Massive Collections of Images.pdf",
        "作者": "Boyang Deng, Songyou Peng, Kyle Genova, Gordon Wetzstein, Noah Snavely, Leonidas Guibas, Thomas Funkhouser",
        "摘要": "我们提出了一种使用多模态大型语言模型 (MLLMs) 的系统，用于分析包含数千万张不同时间拍摄的图像的大型数据库，以发现时间变化中的模式。具体来说，我们旨在捕捉在某个特定时期内整座城市中频繁共同发生的变化（即“趋势”）。与以前的视觉分析不同，我们的分析能够回答开放式查询（例如，“城市中频繁发生的变化类型是什么？”），并且不需要任何预先确定的目标对象或训练标签。这些特性使得以往基于学习或无监督的视觉分析工具变得不适用。我们识别出MLLMs是一种新颖的工具，具有开放式语义理解能力。然而，我们的数据集大了四个数量级，无法作为上下文被MLLM完全摄取。因此，我们引入了一种自下而上的程序，将庞大的视觉分析问题分解为更易处理的子问题。我们精心设计了基于MLLM的解决方案以解决每个子问题。在我们的系统实验和消融研究中，我们发现它显著优于基线，能够从大城市拍摄的图像中发现有趣的趋势（例如，“增加户外用餐区”，“天桥被涂成蓝色”等）。查看更多结果和交互演示，请访问此https URL。",
        "地址": "https://arxiv.org/pdf/2504.08727.pdf"
    },
    {
        "名称": "2025 [2504.08716] ModernBERT or DeBERTaV3? Examining Architecture and Data Influence on Transformer Encoder Models Performance.pdf",
        "作者": "Wissam Antoun, Benoît Sagot, Djamé Seddah",
        "摘要": "摘要: 预训练的transformer-encoder模型如DeBERTaV3和ModernBERT在架构上进行了改进，以提高效率和性能。尽管ModernBERT的作者报告称其在多个基准测试中表现优于DeBERTaV3，但由于缺乏公开的训练数据以及未使用共享数据集进行比较，难以确定这些提升是由于架构改进还是训练数据差异所致。在这项工作中，我们通过在与DeBERTaV3法语模型CamemBERTaV2相同的数据集上预训练ModernBERT，以隔离模型设计的影响，进行了一项对比研究。我们的结果表明，前一代模型在样本效率和整体基准测试性能上仍具有优越性，而ModernBERT的主要优势在于更快的训练和推理速度。然而，与早期的模型如BERT和RoBERTa相比，新提出的模型仍在架构上提供了有意义的改进。此外，我们观察到高质量的预训练数据加速了收敛，但对最终性能没有显著提升，提示可能存在基准测试的饱和。这些发现表明，在评估transformer模型时，分离预训练数据和架构创新的重要性。\n\n作者：Wissam Antoun, Benoît Sagot, Djamé Seddah\n\n备注：预印本。审稿中\n\n链接：https://arxiv.org/pdf/2504.08716.pdf\n\n标题：2025 [2504.08716] ModernBERT还是DeBERTaV3？检验架构和数据对Transformer编码器模型性能的影响",
        "地址": "https://arxiv.org/pdf/2504.08716.pdf"
    },
    {
        "名称": "2025 [2504.08641] Training-free Guidance in Text-to-Video Generation via Multimodal Planning and Structured Noise Initialization.pdf",
        "作者": "Jialu Li, Shoubin Yu, Han Lin, Jaemin Cho, Jaehong Yoon, Mohit Bansal",
        "摘要": "摘要：近期的文本到视频（T2V）扩散模型在生成视频的视觉质量方面取得了显著的提升。然而，尽管如此，现有的T2V模型在准确跟随文本描述方面依然存在挑战，特别是当提示要求精确控制空间布局或物体轨迹时。一些近期研究尝试使用布局引导来改进T2V模型，这些方法需要在推理时对注意力图进行微调或迭代操作，这显著增加了内存需求，使得难以采用较大的T2V模型作为主干网络。为了解决这个问题，我们提出了Video-MSG，这是一种基于多模态规划和结构化噪声初始化的无需训练的T2V生成引导方法。Video-MSG包括三个步骤：在前两个步骤中，Video-MSG创建视频草图，即最终视频的细粒度时空计划，以草稿视频帧的形式指定背景、前景和物体轨迹。在最后一步，Video-MSG通过噪声反演和去噪，引导下游的T2V扩散模型生成视频。值得注意的是，Video-MSG无需在推理时进行微调或注意力操作，从而降低了额外内存需求，更容易采用大型T2V模型。Video-MSG在多个T2V主干（VideoCrafter2和CogVideoX-5B）上展示了其增强文本对齐的有效性，并在流行的T2V生成基准（T2VCompBench和VBench）上进行了验证。我们还提供了关于噪声反演比例、不同背景生成器、背景物体检测和前景物体分割的详细消融研究。",
        "地址": "https://arxiv.org/pdf/2504.08641.pdf"
    },
    {
        "名称": "2025 [2504.08366] In-2-4D: Inbetweening from Two Single-View Images to 4D Generation.pdf",
        "作者": "Sauradip Nag, Daniel Cohen-Or, Hao Zhang, Ali Mahdavi-Amiri",
        "摘要": "摘要:我们提出了一个新的问题，In-2-4D，即通过极简输入设置生成4D（即3D + 运动）的插值动画：两个捕捉物体在不同运动状态下的单视图图像。给定表示物体运动的起始和结束状态的两张图像，我们的目标是生成并重建4D中的运动。我们利用视频插值模型预测运动，但大幅度的帧间运动可能导致理解上的歧义。为了解决这个问题，我们采用分层方法来识别与输入状态在视觉上接近且运动明显的关键帧，然后在它们之间生成平滑的片段。对于每个片段，我们使用高斯喷洒构建关键帧的3D表示。片段内的时间帧引导运动，通过变形场将其转化为动态高斯。为了提高时间一致性和完善3D运动，我们扩展了多视角扩散的自注意力机制跨越时间步长，并应用刚性变换正则化。最后，我们通过插值边界变形场并对其进行优化，使之与指导视频对齐，确保平滑无闪烁的过渡，将独立生成的3D运动片段合并。通过广泛的定性和定量实验以及用户研究，我们展示了我们方法及其各个部分的有效性。项目页面可在此https URL查看。\n\n翻译作者：Sauradip Nag, Daniel Cohen-Or, Hao Zhang, Ali Mahdavi-Amiri",
        "地址": "https://arxiv.org/pdf/2504.08366.pdf"
    },
    {
        "名称": "2025 [2504.01883] CoRAG: Collaborative Retrieval-Augmented Generation.pdf",
        "作者": "Aashiq Muhamed, Mona Diab, Virginia Smith",
        "摘要": "提取的摘要如下：\nRetrieval-Augmented Generation (RAG)模型在知识密集型任务中表现出色，尤其是在少样本学习的限制下。我们介绍了CoRAG，一个将RAG扩展到协作环境的框架，客户可以使用协作段落存储来共同训练共享模型。为了评估CoRAG，我们提出了CRAB，这是一个用于协作同质开放域问答的基准。实验表明，CoRAG在低资源场景中始终优于参数化协作学习方法和本地训练的RAG模型。进一步的分析揭示了共享存储中相关段落的关键重要性，包含不相关段落的意外好处，以及硬负样本对表现的潜在负面影响。这引入了在协作RAG中新的考虑：利用集体丰富知识库和从其他客户引入有害段落的潜在风险之间的权衡。我们的研究结果强调了CoRAG的可行性，同时也指出了关键的设计挑战和未来研究的有前景方向。",
        "地址": "https://arxiv.org/pdf/2504.01883.pdf"
    },
    {
        "名称": "2025 [2504.08635] Latent Diffusion Autoencoders: Toward Efficient and Meaningful Unsupervised Representation Learning in Medical Imaging.pdf",
        "作者": "Gabriele Lozupone, Alessandro Bria, Francesco Fontanella, Frederick J.A. Meijer, Claudio De Stefano, Henkjan Huisman",
        "摘要": "摘要：本研究提出了潜在扩散自编码器（LDAE），一种新颖的基于扩散的编码器-解码器框架，旨在有效且有意义地进行医学影像中的无监督学习，以阿尔茨海默病（AD）的脑部磁共振成像（MR）为案例研究，数据来自ADNI数据库。与传统的在图像空间操作的扩散自编码器不同，LDAE在压缩的潜在表示中应用扩散过程，提高了计算效率，并使3D医学影像表示学习变得可行。为了验证所提出的方法，我们探讨了两个主要假设：（i）LDAE能够有效捕捉与AD和老化相关的3D脑部MR中的有意义语义表示；（ii）LDAE在计算效率高的同时，实现高质量的图像生成和重建。实验结果支持这两个假设：（i）线性探测评估显示AD诊断性能有前途（ROC-AUC：90%，ACC：84%）且年龄预测准确（MAE：4.1年，RMSE：5.2年）；（ii）学习到的语义表示使属性操作成为可能，产生解剖上合理的修改；（iii）语义插值实验显示对缺失扫描具有强大的重建能力，6个月的间隙重建的结构相似度指数（SSIM）为0.969（均方误差（MSE）：0.0019）。即使间隙更长（24个月），模型仍保持稳健性能（SSIM > 0.93，MSE < 0.004），表明其能够捕捉时间进程趋势；（iv）与传统的扩散自编码器相比，LDAE显著提高推断吞吐量（快20倍）同时也增强重建质量。这些发现使LDAE成为一个有前途的可扩展医学影像应用框架，具有作为医学影像分析基础模型的潜力。代码可在此https URL获取。",
        "地址": "https://arxiv.org/pdf/2504.08635.pdf"
    },
    {
        "名称": "2025 [2504.08192] SAEs $\\textit{Can}$ Improve Unlearning: Dynamic Sparse Autoencoder Guardrails for Precision Unlearning in LLMs.pdf",
        "作者": "Aashiq Muhamed, Jacopo Bonato, Mona Diab, Virginia Smith",
        "摘要": "摘要：机器遗忘是一种通过从模型中移除不需要的知识来提高LLM安全性的方法。然而，当前基于梯度的遗忘方法存在一些问题，例如高计算成本、超参数不稳定性、低顺序遗忘能力、对再学习攻击的脆弱性、数据效率低以及缺乏可解释性。尽管稀疏自编码器（SAEs）能够通过有针对性的基于激活的遗忘来改善这些方面，但之前的方法表现不如基于梯度的方法。本文展示了，与之前的发现相反，当动态使用SAEs时，遗忘性能可以得到显著提高。我们引入了一种新颖方法$\\\\textbf{Dynamic DAE Guardrails}$（DSG），该方法通过原则性的特征选择和动态分类器来实现精确遗忘。我们的实验表明，DSG在遗忘-效用权衡方面明显优于领先的遗忘方法。DSG解决了基于梯度方法的主要缺点，提供了更高的计算效率和稳定性，顺序遗忘中的稳健性能，更强的抗再学习攻击能力，更好的数据效率（包括零样本设置）以及更具解释性的遗忘。\n\n翻译：机器遗忘是一种通过从模型中去除不想要的知识来提高大型语言模型（LLM）安全性的方法。然而，现有的基于梯度的遗忘方法存在一些问题，如计算成本高，超参数不稳定，顺序遗忘能力差，易受再学习攻击，数据效率低以及缺乏解释性。尽管稀疏自编码器在通过基于激活的目标遗忘来改善这些方面非常合适，但以前的方法并不如基于梯度的方法。本文表明，与之前的研究结果相反，动态使用稀疏自编码器（SAEs）可以显著改善遗忘效果。我们引入了一种新方法“动态稀疏自编码器护栏”（Dynamic DAE Guardrails, DSG），它利用原理化的特征选择和动态分类器实现精确遗忘。我们的实验显示，DSG在遗忘与效用的权衡上显著优于现有先进的遗忘方法。DSG解决了基于梯度方法的主要缺点，提供了更高的计算效率和稳定性，顺序遗忘中的稳健性能，更强的防再学习攻击能力，更好的数据效率（包括零样本场景）以及更高的遗忘解释性。",
        "地址": "https://arxiv.org/pdf/2504.08192.pdf"
    },
    {
        "名称": "2025 [2504.06908] UKBOB: One Billion MRI Labeled Masks for Generalizable 3D Medical Image Segmentation.pdf",
        "作者": "Emmanuelle Bourigault, Amir Jamaludin, Abdullah Hamdi",
        "摘要": "摘要：在医学影像学中，由于隐私问题、后勤问题和高标注成本，收集大规模的标注数据是主要挑战。在这项工作中，我们介绍了UK Biobank Organs and Bones (UKBOB) 数据集，这是包含最多标注的身体器官数据集，包括51761个3D MRI样本（相当于1790万张2D图像）和超过13.7亿张72个器官的2D分割掩码，所有数据均基于UK Biobank的MRI数据集。我们利用自动标注，引入了带有器官特定过滤器的自动标注清理管道，并手动标注了300个包含11个腹部类别的MRI子集，以验证质量（称为UKBOB-manual）。这种方法在保证标注质量的同时，允许数据集收集的规模化。我们通过展示经过训练的模型在过滤后的UKBOB上的零样本（zero-shot）泛化能力，验证了标注的有效性，并将其应用于其他来自类似领域（如腹部MRI）的较小标注数据集。为了进一步减轻噪声标注的影响，我们提出了一种称为熵测试时间自适应（ETTA）的新方法，以改进分割输出。我们使用UKBOB训练了一个基础模型Swin-BOB，以基于Swin-UNetr架构进行3D医学图像分割，在多个3D医学影像基准测试中取得最先进的结果，包括BRATS脑MRI肿瘤挑战赛（提高0.4%）和BTCV腹部CT扫描基准测试（提高1.3%）。预训练模型和代码可以在此HTTPS URL上获得，过滤后的标签将与UK Biobank一起提供。",
        "地址": "https://arxiv.org/pdf/2504.06908.pdf"
    },
    {
        "名称": "2025 [2504.01786] BlenderGym: Benchmarking Foundational Model Systems for Graphics Editing.pdf",
        "作者": "Yunqi Gu, Ian Huang, Jihyeon Je, Guandao Yang, Leonidas Guibas",
        "摘要": "摘要：3D图形编辑在电影制作和游戏设计等应用中非常重要，然而，它仍然是一个耗时的过程，且需要高度专业化的领域知识。自动化这一过程具有挑战性，因为图形编辑需要执行多种任务，每种任务需要不同的技能。最近，视觉语言模型（VLMs）作为自动化编辑过程的有力框架出现，但它们的发展和评估由于缺乏需要人类感知水平和具有现实编辑复杂性的综合基准而受阻。在这项工作中，我们提出了BlenderGym，这是第一个用于3D图形编辑的全面VLM系统基准。BlenderGym通过基于代码的3D重建任务来评估VLM系统。我们评估了闭源和开源VLM系统，发现即使是最先进的VLM系统在对于人类Blender用户相对简单的任务上也会遇到困难。BlenderGym使我们能够研究推理扩展技术如何影响VLM在图形编辑任务中的表现。特别是，我们的研究结果表明，用于指导生成扩展的验证器本身可以通过推理扩展得到改进，补充了最近在编码和数学任务中的LLM生成推理扩展的见解。我们进一步展示了推理计算并非在所有方面都有效，可以通过在生成和验证之间战略性地分配来优化。\n\n---\n\n译者：2025年CVPR会议接受。Yunqi Gu, Ian Huang, Jihyeon Je, Guandao Yang, Leonidas Guibas",
        "地址": "https://arxiv.org/pdf/2504.01786.pdf"
    },
    {
        "名称": "2025 [2504.07891] SpecReason: Fast and Accurate Inference-Time Compute via Speculative Reasoning.pdf",
        "作者": "Rui Pan, Yinwei Dai, Zhihao Zhang, Gabriele Oliaro, Zhihao Jia, Ravi Netravali",
        "摘要": "摘要: 最近推理时间计算的进展通过使用大规模推理模型（LRM）生成长的思维链（CoTs），显著提高了复杂任务的性能。然而，这种提高的准确性是以生成推理序列的长度和自回归解码的性质导致高推理延迟为代价的。为解决这些开销，我们的关键洞察是 LRM 推理及其嵌入的推理对近似值具有高度耐受性：复杂任务通常被分解成较简单的步骤，每个步骤根据其为下游步骤提供的语义见解而非其生成的精确符号带来效用。因此，我们引入 SpecReason，这是一个通过利用轻量级模型推测性地执行较简单的中间推理步骤，并仅使用高成本的基础模型来评估（并可能纠正）推测结果从而自动加速 LRM 推理的系统。重要的是，SpecReason 专注于利用思维符号的语义灵活性以保持最终答案的准确性，这与先前的推测技术（最著名的是推测解码）互补，后者要求每一步的符号级别等价。在各种推理基准上，SpecReason 在提升准确性1.0-9.9%的同时，相较于原始LRM推理实现1.5-2.5倍加速。与不使用 SpecReason 的推测解码相比，两者结合额外减少19.4-44.2%的延迟。我们在此公开 SpecReason 的源码。\n\n作者: 潘睿，戴尹威，张志豪，Gabriele Oliaro，贾志浩，Ravi Netravali\n链接: https://arxiv.org/pdf/2504.07891.pdf\n标题: 2025 [2504.07891] SpecReason: 基于推测性推理的快速且准确的推理时间计算",
        "地址": "https://arxiv.org/pdf/2504.07891.pdf"
    },
    {
        "名称": "2025 [2504.05303] InteractVLM: 3D Interaction Reasoning from 2D Foundational Models.pdf",
        "作者": "Sai Kumar Dwivedi, Dimitrije Antić, Shashank Tripathi, Omid Taheri, Cordelia Schmid, Michael J. Black, Dimitrios Tzionas",
        "摘要": "摘要：我们介绍了InteractVLM，这是一种新颖的方法，用于从野外单镜头图像中估算人体和物体的3D接触点，从而实现准确的人体-物体3D联合重建。这在由于遮挡、深度模糊和物体形状变化范围广泛等问题上具有挑战性。现有方法依赖通过昂贵的动作捕捉系统或繁琐的手动标注收集的3D接触注释，限制了其可扩展性和泛化能力。为了解决这个问题，InteractVLM利用了大型视觉-语言模型（VLMs）的广泛视觉知识，并通过有限的3D接触数据进行微调。然而，直接应用这些模型并不简单，因为它们只在2D空间内进行推理，而人体-物体接触本质上是3D的。因此，我们引入了一个新颖的渲染-定位-提升模块：(1)通过多视图渲染将3D人体和物体表面嵌入2D空间；(2)训练一个新颖的多视图定位模型（MV-Loc）以在2D中推断接触点；(3)将这些接触点提升到3D。此外，我们提出了一个称为语义人工接触估计的新任务，其中人工接触预测明确基于物体语义进行条件化，从而实现更丰富的交互建模。InteractVLM在接触估计方面优于现有工作，并且也促进了从野外图像进行3D重建。代码和模型可在此https URL获取。",
        "地址": "https://arxiv.org/pdf/2504.05303.pdf"
    }
]