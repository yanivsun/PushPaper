[
    {
        "名称": "2025 [2504.20571] Reinforcement Learning for Reasoning in Large Language Models with One Training Example.pdf",
        "作者": "Yiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Lucas Liu, Baolin Peng, Hao Cheng, Xuehai He, Kuan Wang, Jianfeng Gao, Weizhu Chen, Shuohang Wang, Simon Shaolei Du, Yelong Shen",
        "摘要": "摘要：我们展示了使用一个训练示例（1-shot RLVR）的可验证奖励强化学习在激励大型语言模型（LLMs）的数学推理能力方面的有效性。将RLVR应用于基础模型Qwen2.5-Math-1.5B，我们确定了一个示例，将模型在MATH500上的性能从36.0%提升至73.6%，并提高了在六个常见数学推理基准上的平均性能从17.6%到35.7%。这一结果与使用1.2k DeepScaleR子集（MATH500: 73.6%，平均：35.9%）获得的性能相匹配，该子集包括上述示例。在各种模型（Qwen2.5-Math-7B、Llama3.2-3B-Instruct、DeepSeek-R1-Distill-Qwen-1.5B）、RL算法（GRPO和PPO）和不同数学示例上（许多示例在作为单个训练示例时，在MATH500上均达到约30%或更大的改进）均观察到了类似的大幅提高。此外，我们在1-shot RLVR期间发现了一些有趣的现象，包括跨域泛化、自我反思频率增加，以及在训练准确率达到饱和后测试性能仍持续提高的现象，我们称之为饱和后泛化。此外，我们验证了1-shot RLVR的有效性主要来源于策略梯度损失，将其与“grokking”现象区分开来。我们还展示了在1-shot RLVR训练中促进探索（例如，通过添加适当系数的熵损失）的关键作用。作为额外收益，我们观察到仅应用熵损失而不使用任何结果奖励，大大提高了Qwen2.5-Math-1.5B在MATH500上的性能，提升幅度为27.4%。这些发现可以启发未来关于RLVR数据效率的工作，并鼓励重新审视RLVR的最新进展和基础机制。我们的代码、模型和数据在此链接开放源代码。\n\n作者：王一平, 杨青, 曾志远, 任立良, 刘路卡, 彭保霖, 程浩, 贺学海, 汪宽, 高剑峰, 陈伟朱, 王朔航, 杜少磊, 申叶龙",
        "地址": "https://arxiv.org/pdf/2504.20571.pdf"
    },
    {
        "名称": "2025 [2504.20734] UniversalRAG: Retrieval-Augmented Generation over Multiple Corpora with Diverse Modalities and Granularities.pdf",
        "作者": "Woongyeong Yeo, Kangsan Kim, Soyeong Jeong, Jinheon Baek, Sung Ju Hwang",
        "摘要": "2025\n\n摘要： 检索增强生成（RAG）通过使用与查询相关的外部知识来增强模型响应的事实准确性，显示出很大潜力。但是，大多数现有的RAG方法仅限于文本语料库，并且虽然最近的努力已将RAG扩展到图像和视频等其他模态，但它们通常仅操作单个模态特定语料库。相比之下，现实世界的查询在其所需知识类型上变化很大，单一类型的知识源无法解决这个问题。为了解决这个问题，我们介绍了UniversalRAG，一个旨在从多样化的模态和细粒度异构知识源中检索和整合知识的新颖RAG框架。具体而言，在强制所有模态进入由单一组合语料库衍生的统一表示空间会导致模态差距的观察结果的启发下，我们提出了一种模态感知路由机制，该机制能够动态识别最适合的模态特定语料库并在其中执行有针对性的检索。此外，除了模态外，我们还将每种模态组织成多个细粒度级别，从而实现针对查询的复杂性和范围的微调检索。我们在8个跨越多种模态的基准测试中验证了UniversalRAG，显示出其优于模态特定和统一基线的优越性。\n\n作者：Woongyeong Yeo, Kangsan Kim, Soyeong Jeong, Jinheon Baek, Sung Ju Hwang\n\n备注：项目页面：此https URL\n\n链接：https://arxiv.org/pdf/2504.20734.pdf\n\n标题：2025 [2504.20734] UniversalRAG: 检索增强生成在具有多种模态和粒度的多语料库上的应用",
        "地址": "https://arxiv.org/pdf/2504.20734.pdf"
    },
    {
        "名称": "2025 [2504.20595] ReasonIR: Training Retrievers for Reasoning Tasks.pdf",
        "作者": "Rulin Shao, Rui Qiao, Varsha Kishore, Niklas Muennighoff, Xi Victoria Lin, Daniela Rus, Bryan Kian Hsiang Low, Sewon Min, Wen-tau Yih, Pang Wei Koh, Luke Zettlemoyer",
        "摘要": "摘要：我们介绍了ReasonIR-8B，这是第一个专门为通用推理任务训练的检索器。现有的检索器在推理任务上表现有限，部分原因是现有的训练数据集集中于针对能够直接回答它们的文档的简短事实查询。我们开发了一种合成数据生成管道，对于每个文档，我们的管道都会创建一个具有挑战性且相关的查询，以及一个看似相关但实际上无帮助的难负例。通过混合使用我们的合成数据和现有的公开数据进行训练，ReasonIR-8B在广泛使用的推理密集型信息检索（IR）基准BRIGHT上实现了新的最先进的29.9 nDCG@10（无重排序器）和36.9 nDCG@10（有重排序器）。在应用于RAG任务时，ReasonIR-8B相对于关闭书基准，分别提高了MMLU和GPQA表现6.4%和22.6%，超越了其他检索器和搜索引擎。此外，ReasonIR-8B更有效地利用了测试时计算资源：在BRIGHT上，其性能随着更长且信息量更丰富的重写查询而持续增加；与LLM重排序器结合时，其表现仍优于其他检索器。我们的训练配方具有通用性，可轻松扩展到未来的LLMs；为此，我们开源了我们的代码、数据和模型。\n\n翻译中文摘要：我们展示了ReasonIR-8B，这是第一个专门为通用推理任务训练的检索器。现有的检索器在推理任务上收效有限，部分原因是现有的训练数据集更侧重于能够直接回答的简短事实查询。我们开发了一种合成数据生成管道，对于每个文档，该管道会生成一个具有挑战性并且相关的查询，以及一个看似相关但实际无用的负面例子。通过将我们生成的合成数据与现有公开数据相结合进行训练，ReasonIR-8B在被广泛使用的推理密集型信息检索基准BRIGHT上无重排序器时取得了29.9 nDCG@10，有重排序器时取得了36.9 nDCG@10的新纪录。在应用于RAG任务时，ReasonIR-8B相较于关闭基准分别提升了6.4%的MMLU和22.6%的GPQA表现，超越了其他检索器和搜索引擎。此外，ReasonIR-8B在测试时计算资源的使用效率更高：在BRIGHT上，其性能随着更长且信息量更丰富的重写查询不断提升；与LLM重排序器结合时，其表现仍优于其他检索器。我们的训练方案具有通用性，可以轻松扩展到未来的LLMs；为此，我们开源了代码、数据和模型。",
        "地址": "https://arxiv.org/pdf/2504.20595.pdf"
    },
    {
        "名称": "2025 [2504.20879] The Leaderboard Illusion.pdf",
        "作者": "Shivalika Singh, Yiyang Nan, Alex Wang, Daniel D'Souza, Sayash Kapoor, Ahmet Üstün, Sanmi Koyejo, Yuntian Deng, Shayne Longpre, Noah Smith, Beyza Ermis, Marzieh Fadaee, Sara Hooker",
        "摘要": "摘要: 衡量进展是任何科学领域发展的基础。随着基准测试扮演越来越重要的角色，它们也越来越容易受到扭曲。Chatbot Arena已经成为排名最具能力的AI系统的首选排行榜。然而，在这项工作中，我们发现了一些系统性的问题，这些问题导致了一个被扭曲的竞争环境。我们发现，未公开的私人测试实践使得少数提供者能够在公开发布之前测试多个变体，并在需要时撤回分数。我们确定，这些提供者选择最佳分数的能力导致Arena分数因选择性公开性能结果而存在偏差。极端情况下，我们发现Meta公司在发布Llama-4之前测试了27个私有的LLM变体。我们还确定，专有的封闭模型被以较高的比率（战斗次数）抽样，并且从Arena中移除的模型比公开权重和开源替代方案更少。这些政策随着时间的推移导致了巨大的数据访问不对称性。像Google和OpenAI这样的提供者分别获得了估计的19.2％和20.4％的Arena数据。相比之下，83个合并的公开权重模型仅获得了估计的29.7％的总数据。我们表明，访问Chatbot Arena数据会带来巨大的益处；即使有限的额外数据也能基于我们保守的估计在Arena分布上带来高达112％的相对性能提升。这些动态共同导致了对Arena特有动态的过拟合，而不是总体模型质量的提升。Arena建立在主办方和维护这一宝贵评估平台的开放社区的大量努力之上。我们提供了可操作的建议，以改革Chatbot Arena的评估框架，促进领域内更公平、更透明的基准测试。",
        "地址": "https://arxiv.org/pdf/2504.20879.pdf"
    },
    {
        "名称": "2025 [2504.20157] Toward Evaluative Thinking: Meta Policy Optimization with Evolving Reward Models.pdf",
        "作者": "Zae Myung Kim, Chanwoo Park, Vipul Raheja, Dongyeop Kang",
        "摘要": "摘要: 基于奖励的大型语言模型（LLMs）对齐方法面临两个关键限制：容易受到奖励欺骗的影响，即模型利用奖励信号中的漏洞；以及在LLMs用作奖励模型时，需依赖脆弱且劳动强度大的提示工程。我们引入了元策略优化（MPO）框架，通过集成动态完善奖励模型提示的元奖励模型来解决这些挑战。在MPO中，元奖励模型监控不断变化的训练环境，并持续调整奖励模型的提示，以维持高对齐度，提供一个抵制策略利用的自适应奖励信号。这种元学习方法促进了更稳定的策略优化，大大减少了手动设计奖励提示的需求。其性能与使用广泛手工设计奖励提示的模型相当或更优。此外，我们证明了MPO在不同任务（如问答和数学推理）中保持有效性，而无需专门设计奖励。除了标准RLAIF之外，MPO的元学习形式很容易扩展到更高层次的对齐框架。总体而言，这种方法解决了奖励基础RL对LLMs对齐中的理论和实际挑战，为更稳健和适应性强的对齐策略铺平了道路。代码和模型将公开共享。\n\n翻译为中文版本摘要如下：\n摘要: 基于奖励的大型语言模型（LLMs）对齐方法面临两个关键限制：容易受到奖励欺骗的影响，即模型利用奖励信号中的漏洞；以及在LLMs用作奖励模型时，需依赖脆弱且劳动强度大的提示工程。我们引入了元策略优化（MPO）框架，通过集成动态完善奖励模型提示的元奖励模型来解决这些挑战。在MPO中，元奖励模型监控不断变化的训练环境，并持续调整奖励模型的提示，以维持高对齐度，提供一个抵制策略利用的自适应奖励信号。这种元学习方法促进了更稳定的策略优化，大大减少了手动设计奖励提示的需求。其性能与使用广泛手工设计奖励提示的模型相当或更优。此外，我们证明了MPO在不同任务（如问答和数学推理）中保持有效性，而无需专门设计奖励。除了标准RLAIF之外，MPO的元学习形式很容易扩展到更高层次的对齐框架。总体而言，这种方法解决了奖励基础RL对LLMs对齐中的理论和实际挑战，为更稳健和适应性强的对齐策略铺平了道路。代码和模型将公开共享。",
        "地址": "https://arxiv.org/pdf/2504.20157.pdf"
    },
    {
        "名称": "2025 [2504.20995] TesserAct: Learning 4D Embodied World Models.pdf",
        "作者": "Haoyu Zhen, Qiao Sun, Hongxin Zhang, Junyan Li, Siyuan Zhou, Yilun Du, Chuang Gan",
        "摘要": "摘要：本文提出了一种有效的方法用于学习新型4D具身世界模型，该模型能够预测三维场景在具身智能体的动作响应下的动态演变，并提供空间和时间上的一致性。我们提出通过训练RGB-DN（RGB、深度和法线）视频来学习4D世界模型。这不仅通过将详细的形状、配置和时间变化纳入预测中超越了传统的2D模型，还使我们能够有效地学习具身智能体的精确逆动态模型。具体来说，我们首先利用现成的模型将现有的机器人操作视频数据集扩展为包含深度和法线信息。接下来，我们对这一带注释的数据集中的视频生成模型进行微调，共同预测每一帧的RGB-DN（RGB、深度和法线）。然后，我们提出了一种算法，直接将生成的RGB、深度和法线视频转换为一个高质量的4D场景。我们的方法确保了具身场景预测的时间和空间一致性，支持具身环境的新颖视图合成，并促进政策学习，显著优于之前基于视频的世界模型的结果。",
        "地址": "https://arxiv.org/pdf/2504.20995.pdf"
    },
    {
        "名称": "2025 [2504.20998] YoChameleon: Personalized Vision and Language Generation.pdf",
        "作者": "Thao Nguyen, Krishna Kumar Singh, Jing Shi, Trung Bui, Yong Jae Lee, Yuheng Li",
        "摘要": "摘要（译文）：\n\n大规模多模态模型（例如，GPT-4、Gemini、Chameleon）已经发展成为拥有数百万用户的强大工具。然而，它们仍然是通用模型，缺乏针对特定用户概念的个性化知识。先前的工作探索了文本生成的个性化，但这些方法如何适应图像生成等新模态仍不清楚。在本文中，我们介绍了Yo'Chameleon，这是首次尝试研究大规模多模态模型的个性化。给定3-5张特定概念的图像，Yo'Chameleon利用软提示调优嵌入特定主题信息，以 (i) 回答关于该主题的问题，和 (ii) 重现像素级细节以生成在新背景中的主题图像。Yo'Chameleon通过 (i) 自我提示优化机制平衡多模态下的性能，和 (ii) “软正向”图像生成方法在少样本设置中提高图像质量进行训练。\n\n作者：Thao Nguyen, Krishna Kumar Singh, Jing Shi, Trung Bui, Yong Jae Lee, Yuheng Li\n\n评论：CVPR 2025; 项目页面：this https URL\n\n链接：https://arxiv.org/pdf/2504.20998.pdf\n\n标题：2025 [2504.20998] Yo'Chameleon: 个性化视觉和语言生成.pdf",
        "地址": "https://arxiv.org/pdf/2504.20998.pdf"
    },
    {
        "名称": "2025 [2504.20630] ISDrama: Immersive Spatial Drama Generation through Multimodal Prompting.pdf",
        "作者": "Yu Zhang, Wenxiang Guo, Changhao Pan, Zhiyuan Zhu, Tao Jin, Zhou Zhao",
        "摘要": "摘要: 多模态沉浸式空间戏剧生成着重于根据多模态提示创造连续的多说话者双耳语音，具有戏剧性的语调，能够在AR、VR等应用中发挥潜力。该任务需要基于多模态输入同时建模空间信息和戏剧性语调，数据收集成本高。据我们所知，这是首次尝试解决这些挑战。我们构建了MRSDrama，这是第一个多模态记录的空间戏剧数据集，包含双耳戏剧音频、剧本、视频、几何姿势和文本提示。然后，我们提出ISDrama，这是第一个通过多模态提示进行沉浸式空间戏剧生成的模型。ISDrama包括以下主要组成部分: 1) 多模态姿势编码器，基于对比学习，考虑到因说话者移动引起的多普勒效应，从多模态提示中提取统一的姿势信息。2) 沉浸式戏剧变压器，一种基于流的曼巴变压器模型，通过整合Drama-MOE选择合适的专家以增强语调和姿势控制，生成高质量的戏剧。我们还设计了一种上下文一致的无分类指导策略，以连贯地生成完整的戏剧。实验结果表明，ISDrama在客观和主观指标上均优于基线模型。演示和数据集可在此网址获取。",
        "地址": "https://arxiv.org/pdf/2504.20630.pdf"
    },
    {
        "名称": "2025 [2504.16046] Certified Mitigation of Worst-Case LLM Copyright Infringement.pdf",
        "作者": "Jingyu Zhang, Jiacan Yu, Marc Marone, Benjamin Van Durme, Daniel Khashabi",
        "摘要": "摘要: 在大语言模型（LLMs）预训练期间接触受版权保护的材料，导致其部署后可能无意间侵犯版权。这推动了\"版权删除\"方法的发展，这是一种训练后的方法，旨在防止模型生成与受版权保护内容实质上相似的内容。虽然目前的缓解方法在一般情况下具有一定效果，但我们证明它们忽略了由受版权保护来源的长篇逐字引用所表现出的最坏情况下的版权风险。我们提出了BloomScrub，一种非常简单但高效的推理时间方法，它提供了经过认证的版权删除。我们的方法反复交替使用引文检测和重写技术来转换潜在的侵权段落。通过利用高效的数据概述（Bloom过滤器），我们的方法使大规模真实世界语料库的版权筛查变得可扩展。当超出生长阈值的引文无法删除时，系统可以选择不作出响应，从而提供经过认证的风险降低。实验结果表明，BloomScrub降低了侵权风险，保留了实用性，并通过适应性弃权适应不同级别的执行严格度。我们的结果表明，轻量级的推理时间方法在防止版权侵权方面可以非常有效。\n\n作者: 张景宇，余佳灿，马尔克·马龙，本杰明·范杜尔梅，丹尼尔·卡沙比\n\n链接: [论文链接](https://arxiv.org/pdf/2504.16046.pdf)\n\n标题: 2025 [2504.16046] 最坏情况LLM版权侵权的认证缓解.pdf",
        "地址": "https://arxiv.org/pdf/2504.16046.pdf"
    },
    {
        "名称": "2025 [2504.20996] X-Fusion: Introducing New Modality to Frozen Large Language Models.pdf",
        "作者": "Sicheng Mo, Thao Nguyen, Xun Huang, Siddharth Srinivasan Iyer, Yijun Li, Yuchen Liu, Abhishek Tandon, Eli Shechtman, Krishna Kumar Singh, Yong Jae Lee, Bolei Zhou, Yuheng Li",
        "摘要": "摘要：我们提出了一种名为X-Fusion的框架，该框架在扩展预训练的大型语言模型（LLMs）以实现多模态任务的同时，保留了其语言能力。X-Fusion采用双塔设计，具有特定于模态的权重，保持LLM的参数冻结，同时集成视觉特定信息以实现理解和生成。我们的实验表明，X-Fusion在图像到文本和文本到图像任务上始终优于替代架构。我们发现，结合以理解为重点的数据可以提高生成质量，减少图像数据噪声可以提升整体性能，特征对齐可以加速小模型的收敛，但对大模型的影响较小。我们的研究结果为构建高效的统一多模态模型提供了宝贵的见解。",
        "地址": "https://arxiv.org/pdf/2504.20996.pdf"
    },
    {
        "名称": "2025 [2504.20073] RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn Reinforcement Learning.pdf",
        "作者": "Zihan Wang, Kangrui Wang, Qineng Wang, Pingyue Zhang, Linjie Li, Zhengyuan Yang, Kefan Yu, Minh Nhat Nguyen, Licheng Liu, Eli Gottlieb, Monica Lam, Yiping Lu, Kyunghyun Cho, Jiajun Wu, Li Fei-Fei, Lijuan Wang, Yejin Choi, Manling Li",
        "摘要": "摘要：将大型语言模型（LLMs）训练为交互代理面临包括长远决策和与随机环境反馈互动等独特挑战。尽管强化学习（RL）在静态任务上已有进展，多轮代理RL训练仍未得到充分探索。我们提出了StarPO（状态-思考-行动-奖励策略优化），这是一个针对轨迹级代理RL的通用框架，并引入了RAGEN，一种用于训练和评估LLM代理的模块化系统。我们在三个程式化环境中的研究揭示了三个核心发现。首先，我们的代理RL训练显示出一种反复出现的模式“回声陷阱”，其中存在奖励方差陡坡和梯度峰值；我们通过StarPO-S来应对这种情况，这是一种带有轨迹过滤、批评者整合和去耦合剪辑的稳定变体。其次，我们发现RL回合的成形将受益于多样化的初始状态、中等交互粒度和更频繁的抽样。第三，我们表明，如果没有细粒度、注重推理的奖励信号，代理推理很难通过多轮RL出现，他们可能会表现出浅层策略或幻觉思维。代码和环境可以在此https URL获取。",
        "地址": "https://arxiv.org/pdf/2504.20073.pdf"
    },
    {
        "名称": "2025 [2504.20690] In-Context Edit: Enabling Instructional Image Editing with In-Context Generation in Large Scale Diffusion Transformer.pdf",
        "作者": "Zechuan Zhang, Ji Xie, Yu Lu, Zongxin Yang, Yi Yang",
        "摘要": "摘要: 基于指令的图像编辑能够通过自然语言提示实现稳健的图像修改，但目前的方法面临精度和效率之间的权衡。微调方法需要大量的计算资源和大规模数据集，而无训练技术则在指令理解和编辑质量方面存在困难。我们通过利用大规模扩散变压器（DiT）的增强生成能力和原生上下文感知能力解决了这一难题。我们的解决方案引入了三项贡献：（1）一种使用上下文提示实现零样本指令遵从的上下文内编辑框架，避免了结构变化；（2）一种LoRA-MoE混合调整策略，通过高效适应和动态专家路由提升了灵活性，无需广泛的重新训练；（3）一种早期的过滤推理时刻缩放方法，利用视觉-语言模型（VLMs）更早地选择更好的初始噪声，以提升编辑质量。广泛的评估表明，我们的方法具有优越性：与传统基准相比，仅需要0.5%的训练数据和1%的可训练参数，超越了最先进的方法。该工作确立了一种新的范式，实现了高精度且高效的指令引导编辑。代码和演示可以在这个https URL中找到。",
        "地址": "https://arxiv.org/pdf/2504.20690.pdf"
    },
    {
        "名称": "2025 [2504.18087] Disentangle Identity, Cooperate Emotion: Correlation-Aware Emotional Talking Portrait Generation.pdf",
        "作者": "Weipeng Tan, Chuming Lin, Chengming Xu, FeiFan Xu, Xiaobin Hu, Xiaozhong Ji, Junwei Zhu, Chengjie Wang, Yanwei Fu",
        "摘要": "摘要: 最近在生成说话人头像（Talking Head Generation, THG）方面的进展，通过扩散模型实现了令人印象深刻的唇部同步和视觉质量；然而，现有方法在保留说话者身份的同时生成富有情感表现的肖像时仍然存在困难。我们识别出了当前情感说话人头像生成中的三个关键限制：未充分利用音频固有的情感线索、情感表示中的身份泄漏以及情感关联的单独学习。为了解决这些挑战，我们提出了一个名为DICE-Talk的新框架，遵循将身份与情感解耦，然后协作具有相似特征的情感的思路。首先，我们开发了一个解耦的情感嵌入器，通过跨模态注意力共同建模音频-视觉情感线索，将情感表示为与身份无关的高斯分布。其次，我们引入了一个具有可学习情感库的关联增强情感调节模块，利用向量量化和基于注意力的特征聚合显式捕捉情感之间的关系。第三，我们设计了一个情感判别目标，通过潜在空间分类在扩散过程中强制执行情感一致性。在MEAD和HDTF数据集上的大量实验表明，我们的方法具有优越性，在情感准确性方面优于最先进的方法，同时保持了竞争力的唇部同步性能。定性结果和用户研究进一步确认了我们的方法能够生成具有丰富、相关情感表达并自然适应未见身份的保身份肖像。\n\n翻译：摘要: 最近在生成说话人头像（THG）方面的进展通过扩散模型实现了令人印象深刻的唇同步和视觉质量；然而，现有方法在生成富有情感表现的肖像同时保留说话者身份时仍然存在困难。我们识别出当前情感说话人头像生成中的三个关键限制：未充分利用音频固有的情感线索，情感表示中的身份泄漏，以及情感关联的孤立学习。为了解决这些挑战，我们提出了一个名为DICE-Talk的新框架，遵循将身份与情感解开，然后协作具有相似特征的情感的思路。首先，我们开发了一个解开情感嵌入器，通过跨模态注意共同建模音频-视觉情感线索，将情感表示为身份不可知的高斯分布。其次，我们引入了一个关联增强情感调节模块，具有可学习的情感库，显式捕捉通过向量量化和基于注意力的特征聚合情感之间的关系。第三，我们设计了一个情感判别目标，通过核心空间分类在扩散过程中强制执行情感一致性。对MEAD和HDTF数据集的大量实验表明，我们的方法优越，在情感准确性方面超过了最先进的方法，同时保持竞争力的唇同步性能。定性结果和用户研究进一步确认了我们的方法能够生成保留身份的肖像，具有丰富、相关的情感表达，自然适应未见身份。",
        "地址": "https://arxiv.org/pdf/2504.18087.pdf"
    },
    {
        "名称": "2025 [2504.16272] Learning Explainable Dense Reward Shapes via Bayesian Optimization.pdf",
        "作者": "Ryan Koo, Ian Yang, Vipul Raheja, Mingyi Hong, Kwang-Sung Jun, Dongyeop Kang",
        "摘要": "论文摘要：当前针对大规模语言模型（LLM）对齐的通过人类反馈强化学习（RLHF）流程通常对序列分配标量奖励，并使用最终标记作为整个序列质量的替代指标。然而，这导致反馈稀疏和次优的标记级别信用分配。在这项工作中，我们将奖励塑造框架作为一个关注标记级别信用分配的优化问题。我们提出了一种奖励塑造函数，利用解释性方法（如SHAP和LIME）从奖励模型中估计每个标记的奖励。为了学习这个塑造函数的参数，我们采用了一个双层优化框架，整合了贝叶斯优化和策略训练，以处理标记奖励估计的噪声。我们的实验表明，实现更好的标记级别奖励归因平衡在下游任务中相对于基准表现出了性能提升，并且在训练期间更快地找到了最佳策略。此外，我们从理论上证明了作为特征加性归因函数的解释性方法保持了与原始奖励一致的最优策略。\n\n翻译为中文：当前针对大型语言模型（LLM）对齐的通过人类反馈强化学习（RLHF）流程通常对序列分配标量奖励，使用最终标记作为整个序列质量的替代指标。然而，这会导致反馈稀疏和次优的标记级别信用分配。在这项工作中，我们将奖励塑造框架作为一个优化问题，重点关注标记级别信用分配。我们提出了一种奖励塑造函数，利用解释性方法（如SHAP和LIME）从奖励模型估计每个标记的奖励。为了学习这些塑造函数的参数，我们采用了一个双层优化框架，结合贝叶斯优化和策略训练，以处理标记奖励估计中的噪声。我们的实验表明，实现更好的标记级别奖励归因平衡在下游任务中的表现优于基准，并在训练期间更快地找到最优策略。此外，我们从理论上证明了作为特征加性归因函数的解释性方法能够保持与原始奖励一致的最优策略。",
        "地址": "https://arxiv.org/pdf/2504.16272.pdf"
    },
    {
        "名称": "2025 [2504.20114] TreeHop: Generate and Filter Next Query Embeddings Efficiently for Multi-hop Question Answering.pdf",
        "作者": "Zhonghao Li, Kunpeng Zhang, Jinghuai Ou, Shuliang Liu, Xuming Hu",
        "摘要": "摘要：\n检索增强生成（RAG）系统在多跳问答（MHQA）中面临重大挑战，其中复杂查询需要综合多个文档片段的信息。现有方法通常依赖于基于大型语言模型（LLM）的查询重写和路由迭代，导致由于重复调用LLM和多阶段处理而产生高计算成本。为了解决这些限制，我们提出了TreeHop，这是一种不需要LLM进行查询改进的嵌入级框架。TreeHop通过融合先前查询和检索文档的语义信息，动态更新查询嵌入，仅通过嵌入空间操作实现迭代检索。该方法用简化的“检索-嵌入-检索”循环代替传统的“检索-重写-向量化-检索”循环，大大降低了计算开销。此外，引入了基于规则的停止标准，以进一步修剪冗余检索，平衡效率和召回率。实验结果表明，TreeHop在三个开放域MHQA数据集上与先进的RAG方法相媲美，在模型参数规模仅为5%-0.4%的情况下，实现了相当的性能，并将查询延迟减少了约99%。这使得TreeHop成为在各种知识密集型应用中部署的更快和更具成本效益的解决方案。为了重现实验，代码和数据可以在这里获取：此https URL。\n\n作者：Zhonghao Li, Kunpeng Zhang, Jinghuai Ou, Shuliang Liu, Xuming Hu\n\n评论：9页\n\n网址：https://arxiv.org/pdf/2504.20114.pdf\n\n标题：2025 [2504.20114] TreeHop: 高效生成和过滤多跳问答的下一个查询嵌入。",
        "地址": "https://arxiv.org/pdf/2504.20114.pdf"
    },
    {
        "名称": "2025 [2504.18942] LawFlow : Collecting and Simulating Lawyers' Thought Processes.pdf",
        "作者": "Debarati Das, Khanh Chi Le, Ritik Sachin Parkar, Karin De Langis, Brendan Madson, Chad M. Berryman, Robin M. Willis, Daniel H. Moses, Brett McDonnell, Daniel Schwarcz, Dongyeop Kang",
        "摘要": "摘要：法律从业者，尤其是那些处于职业生涯初期的人，面临复杂且高风险的任务，这些任务需要适应性和上下文敏感的推理。虽然人工智能有助于法律工作，但当前的数据集和模型仅集中于孤立的子任务，并未能捕获实际实践中所需的端到端决策。为了填补这一空白，我们引入了LawFlow，一个由经过培训的法学院学生收集的完整端到端法律工作流程数据集，基于真实的商业实体成立场景。与先前的数据集聚焦在输入输出对或线性思维链不同，LawFlow捕获了动态、模块化和迭代的推理过程，反映了法律实践中的模糊性、修订和客户调整策略。使用LawFlow，我们比较了人类和大型语言模型（LLM）生成的工作流程，揭示了结构、推理灵活性和计划执行中的系统性差异。人类工作流程往往是模块化和适应性的，而LLM工作流程则更偏向于顺序、详尽并且对下游影响不敏感。我们的研究还表明，法律专业人员更希望人工智能承担支持性角色，比如头脑风暴、识别盲点和提出备选方案，而不是端到端执行复杂的工作流程。基于这些发现，我们提出了一系列设计建议，这些建议基于实证观察，通过混合规划、适应性执行和决策点支持，使人工智能协助与人类目标的清晰性、完整性、创造力和效率相一致。我们的结果不仅突出了LLM在支持复杂法律工作流程中的当前局限性，还展示了开发更具协作性和推理意识的法律人工智能系统的机会。所有数据和代码均可在我们的项目页面上获取。",
        "地址": "https://arxiv.org/pdf/2504.18942.pdf"
    },
    {
        "名称": "2025 [2504.18738] A Review of 3D Object Detection with Vision-Language Models.pdf",
        "作者": "Ranjan Sapkota, Konstantinos I Roumeliotis, Rahul Harsha Cheppally, Marco Flores Calero, Manoj Karkee",
        "摘要": "摘要：本综述提供了对视觉-语言模型（VLMs）在3D目标检测中的全面调查的系统分析，这是一个在3D视觉和多模态人工智能交叉领域迅速发展的研究方向。通过审查超过100篇研究论文，我们提供了首个专门针对3D目标检测与视觉-语言模型的系统分析。我们首先概述了3D目标检测中视觉-语言模型所面临的独特挑战，强调在空间推理和数据复杂性上与2D检测的区别。将使用点云和体素网格的传统方法与现代视觉-语言框架（如CLIP和3D大规模语言模型（LLMs））进行比较，这些框架实现了开放词汇表检测和零样本泛化。我们回顾了关键架构、预训练策略和提示工程方法，这些方法将文本和3D特征对齐，以实现有效的3D目标检测与视觉-语言模型。讨论了可视化示例和评估基准，以展示其性能和行为。最后，我们指出了目前的挑战，如3D-语言数据集的有限性和计算需求，并提出了推进3D目标检测与视觉-语言模型的未来研究方向。",
        "地址": "https://arxiv.org/pdf/2504.18738.pdf"
    },
    {
        "名称": "2025 [2504.20769] Chain-of-Defensive-Thought: Structured Reasoning Elicits Robustness in Large Language Models against Reference Corruption.pdf",
        "作者": "Wenxiao Wang, Parsa Hosseini, Soheil Feizi",
        "摘要": "摘要: 连锁思维提示（Chain-of-thought prompting）在增强大型语言模型的推理能力方面表现出巨大成功。在这项工作中，我们探索了如何利用这些增强的推理能力来提高大型语言模型在非推理重点任务中的稳健性。特别地，我们展示了如何通过一种简单的方法——连锁防御思维（chain-of-defensive-thought），在提供带有结构化和防御性推理的少量示例作为示范后，展示出一系列大型语言模型在抗参考污染方面的显著提高。实证结果显示，这种方法的改进效果令人惊讶，尤其考虑到其简单性和适用性。例如，在自然问题任务中，当提供的10个参考中有1个被注入提示攻击破坏时，GPT-4o 的准确率从60%降至低至3%，而使用连锁防御思维提示的GPT-4o 准确率保持在50%。\n\n作者: 王文骁, Parsa Hosseini, Soheil Feizi\n\n链接: [https://arxiv.org/pdf/2504.20769.pdf](https://arxiv.org/pdf/2504.20769.pdf)\n\n标题: 2025 [2504.20769] 连锁防御思维: 结构性推理在大型语言模型中提高对参考污染的稳健性",
        "地址": "https://arxiv.org/pdf/2504.20769.pdf"
    },
    {
        "名称": "2025 [2504.17838] CaRL: Learning Scalable Planning Policies with Simple Rewards.pdf",
        "作者": "Bernhard Jaeger, Daniel Dauner, Jens Beißwenger, Simon Gerstenecker, Kashyap Chitta, Andreas Geiger",
        "摘要": "摘要：我们研究了特权规划在自动驾驶中的强化学习（RL）应用。当前最先进的自动驾驶规划方法是基于规则的，但这些方法在面对长尾问题时无法扩展。而RL是可扩展的，并且不会像模仿学习那样产生累积误差。目前用于驾驶的RL方法使用复杂的奖励设计，这些奖励是多个单独奖励的总和，例如进度、位置或方位奖励。我们发现，当增加小批量尺寸时，PPO无法优化这些复杂奖励的流行版本，这限制了这些方法的可扩展性。因此，我们提出了一种新的奖励设计，主要基于优化单一直观奖励项：路线完成度。通过终止回合或以乘法方式减少路线完成度来惩罚违规行为。我们发现，当使用简单奖励训练时，PPO在较大的小批量尺寸下扩展良好，甚至提高了性能。使用大批量尺寸训练可以通过分布式数据并行实现高效扩展。在CARLA中将PPO扩展到3亿样本，并在nuPlan中扩展到5亿样本，仅需使用一个8-GPU节点。由此得到的模型在CARLA的longest6 v2基准上达到了64 DS，远远超出使用复杂奖励的其他RL方法。该方法在CARLA中的应用几乎不需要适应性修改，在nuPlan中成为最佳的基于学习的方法。在Val14基准上未反应交通和反应交通中分别得分91.3和90.6，比之前的工作快一个数量级。\n\n翻译：我们研究了特权规划在自动驾驶中的强化学习（RL）应用。当前最先进的自动驾驶规划方法是基于规则的，但这些方法在面对尾部问题时无法扩展。而RL是可扩展的，并且不会像模仿学习一样产生累积误差。目前，用于驾驶的RL方法使用复杂形式的奖励设计，这些奖励是多个单独奖励的总和，例如进度、位置或方向奖励。我们发现，当增加小批量大小时，PPO无法优化这些复杂奖励的流行版本，从而限制了这些方法的可扩展性。因此，我们提出了一种新的奖励设计，主要基于优化单一的直观奖励项：路线完成度。通过终止回合或乘法方式减少路线完成度来惩罚违规行为。我们发现，当使用简单的奖励训练时，PPO在较大的小批量大小下扩展性良好，甚至提高了性能。使用大批量大小训练使得通过分布式数据并行可以有效扩展。在CARLA中，我们将PPO扩展到3亿样本，在nuPlan中扩展到5亿样本，仅需使用一个8-GPU节点。由此得到的模型在CARLA的longest6 v2基准上达到了64 DS，远远超出了使用复杂奖励的其他RL方法。该方法在CARLA中的应用几乎不需要适应性修改，在nuPlan中成为最佳的基于学习的方法。在Val14基准上未反应交通和反应交通中分别得分91.3和90.6，比之前的工作快一个数量级。",
        "地址": "https://arxiv.org/pdf/2504.17838.pdf"
    }
]