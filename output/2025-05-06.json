[
    {
        "名称": "2025 [2505.02707] Voila: Voice-Language Foundation Models for Real-Time Autonomous Interaction and Voice Role-Play.pdf",
        "作者": "Yemin Shi, Yu Shu, Siwei Dong, Guangyi Liu, Jaward Sesay, Jingwen Li, Zhiting Hu",
        "摘要": "摘要：一个能够无缝融入日常生活的语音人工智能代理应能够以自主、实时和富有情感表达的方式与人类互动。我们介绍了Voila，一个大型语音-语言基础模型家族，朝着这一愿景迈进了一步。Voila通过采用新的端到端架构，超越了传统的流水线系统，实现了全双工、低延迟的对话，同时保留了丰富的声音细微差别，如音调、节奏和情感。它实现了仅195毫秒的响应延迟，超越了人类的平均响应时间。其分层多尺度Transformer结合了大型语言模型（LLMs）的推理能力和强大的声学建模，实现了自然、具有人物意识的语音生成——用户可以简单地写下文本指令，以定义说话者的身份、音调和其他特征。此外，Voila支持超过一百万个预建语音，并能通过短至10秒的音频样本高效定制新的语音。除了对话，Voila被设计为一个用于广泛语音应用的统一模型，包括自动语音识别（ASR）、文本转语音（TTS），以及通过最小的适应进行多语言语音翻译。Voila是完全开源的，以支持开放研究并加速向下一代人机交互的进步。\n\n作者：Yemin Shi, Yu Shu, Siwei Dong, Guangyi Liu, Jaward Sesay, Jingwen Li, Zhiting Hu\n\n网址： [论文链接](https://arxiv.org/pdf/2505.02707.pdf)\n\n评论：18页，7个图，网站：[this https URL](this https URL)",
        "地址": "https://arxiv.org/pdf/2505.02707.pdf"
    },
    {
        "名称": "2025 [2505.02387] RM-R1: Reward Modeling as Reasoning.pdf",
        "作者": "Xiusi Chen, Gaotang Li, Ziqi Wang, Bowen Jin, Cheng Qian, Yu Wang, Hongru Wang, Yu Zhang, Denghui Zhang, Tong Zhang, Hanghang Tong, Heng Ji",
        "摘要": "摘要：奖励建模对于将大型语言模型（LLMs）与人类偏好对齐至关重要，尤其是通过人类反馈的强化学习（RLHF）。为了提供准确的奖励信号，奖励模型（RM）应促使深入思考并进行可解释的推理，然后再给予评分或判断。然而，现有的奖励模型要么生成不透明的标量评分，要么直接预测首选答案，因此难以整合自然语言评论，缺乏可解释性。受到最近长链推理（CoT）在重度推理任务中取得进展的启发，我们假设并验证了将推理能力整合到奖励模型中显著增强了RM的可解释性和性能。在这项工作中，我们引入了一种新的生成奖励模型类别——推理奖励模型（ReasRM），将奖励建模定义为一个推理任务。我们提出了一种以推理为导向的训练流程，并训练了一系列ReasRM模型，称为RM-R1。训练包括两个关键阶段：（1）高质量推理链的蒸馏，（2）通过可验证奖励的强化学习。RM-R1通过自生成推理轨迹或特定聊天标准并根据其评估候选响应来改善LLM展开。根据实验，我们的模型在多个综合奖励模型基准上实现了生成RM的最先进或近最先进的性能，比更大的开放权重模型（例如Llama3.1-405B）和专有模型（例如GPT-4o）高出最多13.8%。除了最终表现，我们进行了彻底的实验分析以了解成功ReasRM训练的关键成分。为了促进未来研究，我们在https URL发布了六个ReasRM模型以及代码和数据。\n\n翻译：奖励建模对于将大型语言模型（LLMs）与人类偏好对齐至关重要，尤其是通过人类反馈的强化学习（RLHF）。为了提供准确的奖励信号，奖励模型（RM）应促使深入思考并进行可解释的推理，然后再给予评分或判断。然而，现有的奖励模型要么生成不透明的标量评分，要么直接预测首选答案，因此难以整合自然语言评论，缺乏可解释性。受到最近长链推理（CoT）在重度推理任务中取得进展的启发，我们假设并验证了将推理能力整合到奖励模型中显著增强了RM的可解释性和性能。在这项工作中，我们引入了一种新的生成奖励模型类别——推理奖励模型（ReasRM），将奖励建模定义为一个推理任务。我们提出了一种以推理为导向的训练流程，并训练了一系列ReasRM模型，称为RM-R1。训练包括两个关键阶段：（1）高质量推理链的蒸馏，（2）通过可验证奖励的强化学习。RM-R1通过自生成推理轨迹或特定聊天标准并根据其评估候选响应来改善LLM展开。根据实验，我们的模型在多个综合奖励模型基准上实现了生成RM的最先进或近最先进的性能，比更大的开放权重模型（例如Llama3.1-405B）和专有模型（例如GPT-4o）高出最多13.8%。除了最终表现，我们进行了彻底的实验分析以了解成功ReasRM训练的关键成分。为了促进未来研究，我们在https URL发布了六个ReasRM模型以及代码和数据。",
        "地址": "https://arxiv.org/pdf/2505.02387.pdf"
    },
    {
        "名称": "2025 [2504.20752] Grokking in the Wild: Data Augmentation for Real-World Multi-Hop Reasoning with Transformers.pdf",
        "作者": "Roman Abramov, Felix Steinbauer, Gjergji Kasneci",
        "摘要": "摘要：尽管变压器在众多自然语言处理任务中取得了巨大成功，但在多步事实推理方面仍存在显著差距，尤其是当现实世界知识稀缺时。最近在grokking方面的研究表明，神经网络在检测到潜在逻辑模式后可以从记忆转变为完美的泛化，但这些研究主要使用小型、合成任务。在本文中，我们首次将grokking扩展到现实世界的事实数据，并通过用精心设计的合成数据扩充现有知识图，解决数据稀缺的问题，以提高推理事实与原子事实的比率φ_r，达到grokking所需的阈值。令人惊讶的是，我们发现即使是事实上不正确的合成数据也能加强新出现的推理电路，而不是降低准确性，因为它迫使模型依赖关系结构而不是记忆。在多跳推理基准测试中，我们的方法在2WikiMultiHopQA上实现了95-100%的准确率，显著提高了强基线，并匹配或超越了当前的最先进的结果。我们进一步提供了深入分析，展示了增加φ_r如何驱动变压器内部泛化电路的形成。我们的研究结果表明，基于grokking的数据增强可以解锁隐式的多跳推理能力，为大规模语言模型中的更为鲁棒和可解释的事实推理开辟了道路。",
        "地址": "https://arxiv.org/pdf/2504.20752.pdf"
    },
    {
        "名称": "2025 [2505.02222] Practical Efficiency of Muon for Pretraining.pdf",
        "作者": "Essential AI: Ishaan Shah, Anthony M. Polloreno, Karl Stratos, Philip Monk, Adarsh Chaluvaraju, Andrew Hojel, Andrew Ma, Anil Thomas, Ashish Tanwer, Darsh J Shah, Khoi Nguyen, Kurt Smith, Michael Callahan, Michael Pust, Mohit Parmar, Peter Rushton, Platon Mazarakis, Ritvik Kapila, Saurabh Srivastava, Somanshu Singla, Tim Romanski, Yash Vanjani, Ashish Vaswani",
        "摘要": "标题: Muon在预训练中的实际效率\n\n摘要: 我们展示了Muon，这个最简单的二阶优化器实例，如何在计算时间权衡中明确地扩展了AdamW的帕累托前沿。我们发现，Muon在大批量数据情况下比AdamW更有效，远超过所谓的临界批量大小，同时保持计算效率，从而实现更经济的训练。我们研究了Muon与最大更新参数化（muP）结合以实现高效超参数迁移，并提出了一种简单的望远算法，该算法考虑了muP中所有误差来源，同时仅引入了适度的资源开销。通过对模型大小高达四十亿参数的广泛实验，以及对数据分布和架构的消融实验，我们验证了我们的发现。\n\n作者: Essential AI: Ishaan Shah, Anthony M. Polloreno, Karl Stratos, Philip Monk, Adarsh Chaluvaraju, Andrew Hojel, Andrew Ma, Anil Thomas, Ashish Tanwer, Darsh J Shah, Khoi Nguyen, Kurt Smith, Michael Callahan, Michael Pust, Mohit Parmar, Peter Rushton, Platon Mazarakis, Ritvik Kapila, Saurabh Srivastava, Somanshu Singla, Tim Romanski, Yash Vanjani, Ashish Vaswani\n\n链接: [https://arxiv.org/pdf/2505.02222.pdf](https://arxiv.org/pdf/2505.02222.pdf)",
        "地址": "https://arxiv.org/pdf/2505.02222.pdf"
    },
    {
        "名称": "2025 [2505.02819] ReplaceMe: Network Simplification via Layer Pruning and Linear Transformations.pdf",
        "作者": "Dmitriy Shopkhoev, Ammar Ali, Magauiya Zhussip, Valentin Malykh, Stamatios Lefkimmiatis, Nikos Komodakis, Sergey Zagoruyko",
        "摘要": "摘要: 我们介绍了ReplaceMe，一种无需训练的通用深度剪枝方法，可以有效地用线性操作替换transformer块，同时在低压缩率下保持高性能。与传统的剪枝方法需要额外的训练或微调不同，我们的方法仅需要一个用于估计线性变换的小型校准数据集，以近似剪枝后的块。这种线性映射可以无缝地与剩余的transformer块合并，消除了任何额外网络参数的需求。我们的实验表明，ReplaceMe始终优于其他无需训练的方法，并且在包含广泛重训练/微调和架构修改的最新剪枝方法面前也具有高度竞争性。应用于多个大型语言模型(LLMs)，ReplaceMe能够实现高达25%的剪枝，同时在公开基准测试中保留大约90%的原始模型性能——无需任何训练或修复步骤，从而导致最低的计算开销。我们提供了一个开源库，实施了ReplaceMe以及几种最新的深度剪枝技术，可在此存储库中获取。",
        "地址": "https://arxiv.org/pdf/2505.02819.pdf"
    },
    {
        "名称": "2025 [2505.02735] FormalMATH: Benchmarking Formal Mathematical Reasoning of Large Language Models.pdf",
        "作者": "Zhouliang Yu, Ruotian Peng, Keyi Ding, Yizhe Li, Zhongyuan Peng, Minghao Liu, Yifan Zhang, Zheng Yuan, Huajian Xin, Wenhao Huang, Yandong Wen, Ge Zhang, Weiyang Liu",
        "摘要": "摘要：形式化数学推理仍然是人工智能领域的一个重要挑战，受限于现有基准的范围和规模。为了应对这一问题，我们提出了FormalMATH，一个大规模的Lean4基准，包含5560个通过形式验证的问题，这些问题涵盖了从高中奥林匹克竞赛挑战到本科级别的定理，涉及多个领域（如代数、应用数学、微积分、数论和离散数学）。为了减轻人工形式化的低效性，我们引入了一种新的人类在环自动形式化流程，该流程结合了：(1) 用于陈述自动形式化的专门大型语言模型（LLM），(2) 多LLM语义验证，以及(3) 使用现成的基于LLM的证明器的基于否定的反证过滤策略。这种方法通过在手动验证前保留72.09%的陈述来降低专家注释成本，同时确保与原始自然语言问题的一致性。我们对最先进的基于LLM的定理证明器的评估揭示了显著的局限性：即使是最强的模型在实际抽样预算下也仅能达到16.46%的成功率，表现出明显的领域偏见（如在代数中表现出色但在微积分中却失败）和过度依赖简化的自动化策略。值得注意的是，我们发现自然语言解决方案指导与链式思维推理场景中的证明成功之间存在一种反直觉的逆关系，表明人类编写的非正式推理在形式推理环境中引入了噪音而非清晰性。我们认为，FormalMATH为评估形式化数学推理提供了一个强大的基准。",
        "地址": "https://arxiv.org/pdf/2505.02735.pdf"
    },
    {
        "名称": "2025 [2505.02391] Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization in Rejection Sampling and RL.pdf",
        "作者": "Jiarui Yao, Yifan Hao, Hanning Zhang, Hanze Dong, Wei Xiong, Nan Jiang, Tong Zhang",
        "摘要": "摘要: 在大型语言模型（LLMs）中的链式思维（CoT）推理可以形式化为一个潜变量问题，模型需要生成中间推理步骤。尽管之前的方法，如迭代奖励排序微调（RAFT），已依赖于这样的公式化，它们通常在所有提示上应用统一的推理预算，未能考虑难度和收敛行为的变化。本研究将CoT训练的主要瓶颈识别为由于静态采样策略导致的随机梯度估计低效。我们提出了GVM-RAFT，一种提示特定的动态样本分配策略，旨在在计算预算约束下最小化随机梯度方差。该方法通过监测提示接受率和随机梯度范数来动态分配计算资源，确保生成的梯度方差最小化。我们的理论分析表明，在适当条件下，所提出的动态采样策略能加速收敛。在数学推理实验中，GVM-RAFT相较于普通的RAFT实现了2-4倍的加速，并且准确性有显著提升。所提出的动态采样策略是通用的，可以融合到其他强化学习算法中，如GRPO，从而在收敛性和测试准确性方面达到类似的改进。我们的代码已在此网址提供。",
        "地址": "https://arxiv.org/pdf/2505.02391.pdf"
    },
    {
        "名称": "2025 [2505.01658] A Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency.pdf",
        "作者": "Sihyeong Park, Sungryeol Jeon, Chaelyn Lee, Seokhun Jeon, Byung-Soo Kim, Jemin Lee",
        "摘要": "2025年 [2505.01658] 题目：关于大型语言模型推理引擎的调研：优化与效率的视角.pdf\n\n摘要：大型语言模型（LLMs）广泛应用于聊天机器人、代码生成器和搜索引擎中。链式思维、复杂推理和代理服务等工作负载通过反复调用模型显著增加了推理成本。并行化、压缩和缓存等优化方法已被采用以降低成本，但多样化的服务需求使得选择合适的方法变得困难。近年来，专业的LLM推理引擎作为将优化方法集成到面向服务的基础设施中的关键组件已经出现。然而，对推理引擎的系统性研究仍然缺乏。本文对25个开源和商业推理引擎进行了全面评估。我们从易用性、易部署性、通用支持、可扩展性以及适合吞吐量和延迟感知计算等方面考察了每个推理引擎。此外，通过研究它所支持的优化技术，探讨每个推理引擎的设计目标。我们还评估了开源推理引擎的生态系统成熟度，处理了商业解决方案的性能和成本策略。我们概述了未来的研究方向，包括对复杂LLM服务的支持、对各种硬件的支持以及增强的安全性，为研究人员和开发人员在选择和设计优化的LLM推理引擎时提供了实际指导。我们还提供了一个公共存储库，以持续跟踪这一快速发展领域的进展：https URL",
        "地址": "https://arxiv.org/pdf/2505.01658.pdf"
    },
    {
        "名称": "2025 [2505.02835] R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement Learning.pdf",
        "作者": "Yi-Fan Zhang, Xingyu Lu, Xiao Hu, Chaoyou Fu, Bin Wen, Tianke Zhang, Changyi Liu, Kaiyu Jiang, Kaibing Chen, Kaiyu Tang, Haojie Ding, Jiankang Chen, Fan Yang, Zhang Zhang, Tingting Gao, Liang Wang",
        "摘要": "摘要：多模态奖励模型（MRMs）在提升多模态大语言模型（MLLMs）的性能中起着至关重要的作用。尽管最近的进展主要集中在改进MRMs的模型结构和训练数据方面，但关于奖励建模的长期推理能力及其在MRMs中的激活机制却鲜有探索。在本文中，我们研究了如何利用强化学习（RL）改进奖励建模。具体而言，我们将奖励建模问题重新表述为基于规则的RL任务。然而，我们观察到直接应用现有的RL算法，如Reinforce++，通常会因为这些算法的固有限制导致训练不稳定甚至崩溃。为了解决这个问题，我们提出了StableReinforce算法，该算法优化了现有RL方法的训练损失、优势估计策略和奖励设计。这些优化带来了更稳定的训练动态和卓越的性能。为了促进MRM的训练，我们从多种数据集收集了20万条偏好数据。使用StableReinforce算法在该数据集上训练的奖励模型R1-Reward显著提高了多模态奖励建模基准测试的性能。与之前的SOTA模型相比，R1-Reward在VL奖励基准上提升了8.4%，在多模态奖励基准上提升了14.3%。此外，随着推理计算量的增加，R1-Reward的性能进一步提升，突显了RL算法在优化MRMs中的潜力。",
        "地址": "https://arxiv.org/pdf/2505.02835.pdf"
    },
    {
        "名称": "2025 [2505.01441] Agentic Reasoning and Tool Integration for LLMs via Reinforcement Learning.pdf",
        "作者": "Joykirat Singh, Raghav Magazine, Yash Pandya, Akshay Nambi",
        "摘要": "摘要：大型语言模型（LLM）在复杂的推理任务中取得了显著进展，但由于它们依赖于静态的内部知识和仅限文本的推理方法，仍然存在根本性限制。现实世界的问题解决通常需要动态的、多步骤的推理、自适应决策能力以及与外部工具和环境进行交互的能力。在这项工作中，我们引入了ARTIST（自我改进变压器中的代理推理和工具集成），这是一个将代理推理、强化学习和工具集成紧密结合的统一框架。ARTIST使模型能够自主决定在多轮推理链中何时、如何以及调用哪些工具，通过基于结果的强化学习学会稳健的工具使用和环境交互策略，而无需步骤级监督。在数学推理和多轮函数调用基准测试中的广泛实验表明，ARTIST始终优于最先进的基准模型，相较基础模型有高达22%的绝对提升，并在最具挑战性的任务上取得了显著进展。详细研究和指标分析揭示了代理强化学习训练导致了更深入的推理、更有效的工具使用和更高质量的解决方案。我们的研究结果将代理强化学习与工具集成确立为LLM中稳健、可解释和可泛化问题解决的新前沿。\n\n作者：Joykirat Singh, Raghav Magazine, Yash Pandya, Akshay Nambi\n\n链接：https://arxiv.org/pdf/2505.01441.pdf\n\n标题：2025 [2505.01441] 代理推理和工具集成通过强化学习用于LLM（PDF)",
        "地址": "https://arxiv.org/pdf/2505.01441.pdf"
    },
    {
        "名称": "2025 [2505.02156] Think on your Feet: Adaptive Thinking via Reinforcement Learning for Social Agents.pdf",
        "作者": "Minzheng Wang, Yongbin Li, Haobo Wang, Xinghua Zhang, Nan Xu, Bingli Wu, Fei Huang, Haiyang Yu, Wenji Mao",
        "摘要": "摘要: \n有效的社会智能模拟需要语言代理动态调整推理深度，这是当前方法中明显缺乏的能力。现有的方法要么缺乏这种推理能力，要么在所有情境中强制执行统一的长链式思维推理，导致过度的标记使用和不适当的社会模拟。在本文中，我们提出了一种自适应模式学习（AML），能够根据实时上下文战略性地选择四种思考模式（从直觉反应到深度思考）。我们框架的核心创新——自适应模式策略优化（AMPO）算法在现有方法上引入了三个关键进展：(1) 多粒度的思考模式设计，(2) 社交互动中的上下文感知模式切换，(3) 通过深度自适应处理实现标记高效的推理。在社会智能任务上的大量实验表明，AML在任务性能上比最先进的方法提高了15.6%。显著的是，我们的方法在推理链更短的情况下比GRPO高出7.0%。这些结果证明，与GRPO的固定深度方法相比，AMPO实现的上下文敏感思维模式选择能够使推理更符合人类的适应性。",
        "地址": "https://arxiv.org/pdf/2505.02156.pdf"
    },
    {
        "名称": "2025 [2505.02094] SkillMimic-V2: Learning Robust and Generalizable Interaction Skills from Sparse and Noisy Demonstrations.pdf",
        "作者": "Runyi Yu, Yinhuai Wang, Qihan Zhao, Hok Wai Tsui, Jingbo Wang, Ping Tan, Qifeng Chen",
        "摘要": "摘要：我们解决了交互演示强化学习 (RLID) 中的一个基本挑战：演示噪声和覆盖限制。尽管现有的数据收集方法提供了有价值的交互演示，但它们往往产生稀疏、断续且嘈杂的轨迹，无法捕捉可能的技能变化和过渡的整个范围。我们的关键见解是，尽管演示稀疏且嘈杂，但存在无限多的物理上可行的轨迹，它们自然地在演示技能之间架起桥梁或从相邻状态中出现，形成可能的技能变化和过渡的连续空间。基于这一见解，我们提出了两种数据增强技术：拼接轨迹图 (STG)，用于发现演示技能之间的潜在过渡，以及状态转换场 (STF)，用于在演示邻域内为任意状态建立独特连接。为了使增强数据有效地用于RLID，我们开发了一种用于动态课程生成的自适应轨迹采样 (ATS) 策略和一种用于依赖记忆的技能学习的历史编码机制。我们的方法实现了鲁棒的技能获取，显著超越了参考演示的功能。我们在各种互动任务中的广泛实验表明，在收敛稳定性、泛化能力和恢复鲁棒性方面，相较于最先进的方法，有了显著的改进。",
        "地址": "https://arxiv.org/pdf/2505.02094.pdf"
    },
    {
        "名称": "2025 [2505.02625] LLaMA-Omni2: LLM-based Real-time Spoken Chatbot with Autoregressive Streaming Speech Synthesis.pdf",
        "作者": "Qingkai Fang, Yan Zhou, Shoutao Guo, Shaolei Zhang, Yang Feng",
        "摘要": "摘要: 实时、智能和自然语音互动是下一代人机交互的重要组成部分。最近的进展展示了基于大语言模型 (LLMs) 构建智能语音聊天机器人的潜力。在本文中，我们介绍了LLaMA-Omni 2，这是一系列从0.5B到14B参数的语音语言模型 (SpeechLMs)，能够实现高质量的实时语音互动。LLaMA-Omni 2建立在Qwen2.5系列模型之上，集成了语音编码器和自回归流式语音解码器。尽管仅在200K多轮语音对话样本上训练，LLaMA-Omni 2在多个语音问答和语音指令跟随基准测试中表现出色，超越了之前训练了数百万小时语音数据的最先进的语音语言模型GLM-4-Voice。",
        "地址": "https://arxiv.org/pdf/2505.02625.pdf"
    },
    {
        "名称": "2025 [2505.02471] Ming-Lite-Uni: Advancements in Unified Architecture for Natural Multimodal Interaction.pdf",
        "作者": "Biao Gong, Cheng Zou, Dandan Zheng, Hu Yu, Jingdong Chen, Jianxin Sun, Junbo Zhao, Jun Zhou, Kaixiang Ji, Lixiang Ru, Libin Wang, Qingpei Guo, Rui Liu, Weilong Chai, Xinyu Xiao, Ziyuan Huang",
        "摘要": "摘要：我们介绍了Ming-Lite-Uni，这是一个开源的多模态框架，拥有新设计的统一视觉生成器和原生多模态自回归模型，旨在统一视觉和语言。具体而言，该项目提供了集成MetaQueries和M2-omni框架的开源实现，同时引入了新的多尺度可学习标记和多尺度表示对齐策略。通过利用固定的多模态学习语言模型(MLLM)和可学习的扩散模型，Ming-Lite-Uni使原生多模态自回归模型能够执行文本生成图像和基于指令的图像编辑任务，扩展其能力超越纯粹的视觉理解。我们的实验结果展示了Ming-Lite-Uni的强大性能，并展现了其交互过程的卓越流畅性。所有代码和模型权重都开源，以促进社区进一步探索。值得注意的是，这项工作与如ChatGPT-4o（2025年3月25日更新的原生图像生成）等同步的多模态人工智能里程碑相一致，强调了如Ming-Lite-Uni这类统一模型在通往通用人工智能（AGI）道路上的重要意义。Ming-Lite-Uni目前处于alpha阶段，并将很快进一步优化。",
        "地址": "https://arxiv.org/pdf/2505.02471.pdf"
    },
    {
        "名称": "2025 [2505.02370] SuperEdit: Rectifying and Facilitating Supervision for Instruction-Based Image Editing.pdf",
        "作者": "Ming Li, Xin Gu, Fan Chen, Xiaoying Xing, Longyin Wen, Chen Chen, Sijie Zhu",
        "摘要": "摘要：由于手动收集准确的编辑数据存在挑战，现有的数据集通常使用各种自动化方法构建，导致编辑指令与原始编辑图像对之间的不匹配，从而产生噪声监督信号。近期的努力尝试通过生成高质量的编辑图像、在识别任务上预训练或引入视觉-语言模型（VLMs）来改进编辑模型，但未能解决这一根本问题。在本文中，我们通过为给定的图像对构建更有效的编辑指令提供了一种新颖的解决方案。这包括调整编辑指令以更好地与原始编辑图像对对齐，并使用对比编辑指令进一步增强其效果。具体而言，我们发现编辑模型在不同的推理步骤中表现出特定的生成属性，独立于文本。基于这些先前的属性，我们定义了一个统一的指南用于VLMs来调整编辑指令。然而，一些具有挑战性的编辑场景仅靠调整指令无法解决。为此，我们进一步构建了带有积极和消极指令的对比监督信号，并通过三重态损失将其引入模型训练，从而进一步促进监督效果。我们的方法不需要之前工作中使用的VLM模块或预训练任务，提供了一种更直接和高效的方式来提供更好的监督信号，为基于指令的图像编辑提供了一种新颖、简单且有效的解决方案。在多个基准上，结果表明我们的方法显著优于现有方法。与之前的SOTA SmartEdit相比，我们在Real-Edit基准上取得了9.19%的改进，使用了30倍更少的训练数据和13倍更小的模型规模。",
        "地址": "https://arxiv.org/pdf/2505.02370.pdf"
    },
    {
        "名称": "2025 [2505.01043] Low-Precision Training of Large Language Models: Methods, Challenges, and Opportunities.pdf",
        "作者": "Zhiwei Hao, Jianyuan Guo, Li Shen, Yong Luo, Han Hu, Guoxia Wang, Dianhai Yu, Yonggang Wen, Dacheng Tao",
        "摘要": "摘要: 大型语言模型（LLMs）在各个领域都取得了令人印象深刻的表现。然而，训练它们所需的大量硬件资源对效率和可扩展性构成了重大障碍。为了缓解这一挑战，低精度训练技术被广泛采用，显著提高了训练效率。尽管取得了这些进展，低精度训练涉及多个组件——如权重、激活和梯度——每个组件可以用不同的数值格式表示。由此产生的多样性使得低精度训练研究领域呈现出碎片化的格局，研究人员难以获得统一的概况。本综述对现有的低精度训练方法进行了全面审查。为了系统地组织这些方法，我们根据其基础的数值格式将它们分为三大类，这是影响硬件兼容性、计算效率和读者参考便利性的关键因素。这些类别是：（1）定点和整数方法，（2）浮点方法，以及（3）定制格式方法。此外，我们讨论了量化感知训练方法，这些方法在前向传播过程中与低精度训练具有关键相似性。最后，我们强调了推动该领域发展的几个有前景的研究方向。本文提供了在本综述中讨论的论文集合，可通过这个 https URL 获取。\n\n作者: 郝志伟，郭建元，沈黎，骆勇，胡涵，王国霞，俞殿海，温永刚，陶大程",
        "地址": "https://arxiv.org/pdf/2505.01043.pdf"
    },
    {
        "名称": "2025 [2505.01583] TEMPURA: Temporal Event Masked Prediction and Understanding for Reasoning in Action.pdf",
        "作者": "Jen-Hao Cheng, Vivian Wang, Huayu Wang, Huapeng Zhou, Yi-Hao Peng, Hou-I Liu, Hsiang-Wei Huang, Kuang-Ming Chen, Cheng-Yen Yang, Wenhao Chai, Yi-Ling Chen, Vibhav Vineet, Qin Cai, Jenq-Neng Hwang",
        "摘要": "摘要：理解因果事件关系并在视频中实现细粒度时间定位对视觉语言模型来说仍然具有挑战性。现有方法要么压缩视频标记以降低时间分辨率，要么将视频视为未分段流，模糊了细粒度事件边界，限制了因果依赖的建模。我们提出了TEMPURA（Temporal Event Masked Prediction and Understanding for Reasoning in Action，一个增强视频时间理解的两阶段训练框架。TEMPURA首先应用掩盖事件预测推理，通过从密集事件标注中重建缺失事件并生成逐步因果解释，借鉴了有效填充技术的灵感。然后，TEMPURA学习执行视频分割和密集描述，以将视频分解成具有详细、时间戳对齐描述的非重叠事件。我们在我们策划的大规模数据集VER（包括100万个训练实例和50万条具有时间对齐事件描述和结构化推理步骤的视频）上训练TEMPURA。时间定位和亮点检测基准的实验表明，TEMPURA优于强基线模型，确认将因果推理与细粒度时间分割相结合可以改善视频理解。",
        "地址": "https://arxiv.org/pdf/2505.01583.pdf"
    },
    {
        "名称": "2025 [2505.02823] MUSAR: Exploring Multi-Subject Customization from Single-Subject Dataset via Attention Routing.pdf",
        "作者": "Zinan Guo, Pengze Zhang, Yanze Wu, Chong Mou, Songtao Zhao, Qian He",
        "摘要": "摘要：目前的多主体定制方法面临两个关键挑战：获取多样的多主体训练数据的困难和不同主体间属性纠缠的问题。为了弥合这些差距，我们提出了MUSAR——一种简单而有效的框架，在只需要单主体训练数据的情况下实现稳健的多主体定制。首先，为了打破数据限制，我们引入了去偏见的双联画学习。该方法通过构建来自单主体图像的双联画训练对，以促进多主体学习，并通过静态注意力路由和双分支LoRA主动纠正双联画构造引入的分布偏差。其次，为了消除跨主体纠缠，我们引入了动态注意力路由机制，该机制自适应地建立生成图像和条件主体之间的双射映射。这一设计不仅实现了多主体表示的解耦，而且随着参考主体的增加，保持了可扩展的泛化性能。综合实验表明，即使只需要单主体数据集，我们的MUSAR在图像质量、主体一致性和交互自然性方面仍然优于现有方法——即使那些基于多主体数据集训练的方法。\n\n作者：郭梓楠，张鹏泽，吴彦泽，牟崇，赵松涛，何倩\n\n评论：项目页面在这个https URL\n\n链接：https://arxiv.org/pdf/2505.02823.pdf\n\n标题：2025 [2505.02823] MUSAR: 通过注意力路由从单主体数据集探索多主体定制.pdf",
        "地址": "https://arxiv.org/pdf/2505.02823.pdf"
    },
    {
        "名称": "2025 [2505.02005] Learning Heterogeneous Mixture of Scene Experts for Large-scale Neural Radiance Fields.pdf",
        "作者": "Zhenxing Mi, Ping Yin, Xue Xiao, Dan Xu",
        "摘要": "摘要：\n近年来，针对大规模场景的NeRF方法强调了场景分解对于可扩展NeRF的重要性。虽然在实现合理的可扩展性方面取得了一定成果，但仍有几个关键问题未得到充分探索，即可学习的分解、场景异质性建模和建模效率。本文介绍了Switch-NeRF++，一种异质性哈希专家混合（HMoHE）网络，在统一框架内解决了这些挑战。Switch-NeRF++是一种高度可扩展的NeRF，能以端到端的方式高效地学习大规模场景的异质分解和异质NeRF。在我们的框架中，一个门控网络学习分解场景并将3D点分配给专业的NeRF专家。这个门控网络通过我们提出的稀疏门控专家混合（MoE）NeRF框架与专家共同优化。我们结合了基于哈希的门控网络和不同的异质哈希专家。基于哈希的门控网络高效学习大规模场景的分解。不同的异质哈希专家由不同分辨率范围的哈希网格组成，能够有效地学习不同场景部分的异质表示。这些设计选择使我们的框架成为一个用于真实世界大规模场景建模的端到端且高度可扩展的NeRF解决方案，兼顾质量和效率。我们在现有的大规模NeRF数据集和来自UrbanBIS的新大规模场景数据集（大于6.5平方公里）上评估了我们的准确性和可扩展性。大量实验表明，我们的方法可以轻松扩展到各种大规模场景，并实现最先进的场景渲染准确性。此外，与Switch-NeRF相比，我们的方法在训练时加速了8倍，渲染时加速了16倍。代码将发布在此https URL。",
        "地址": "https://arxiv.org/pdf/2505.02005.pdf"
    },
    {
        "名称": "2025 [2505.01456] Unlearning Sensitive Information in Multimodal LLMs: Benchmark and Attack-Defense Evaluation.pdf",
        "作者": "Vaidehi Patil, Yi-Lin Sung, Peter Hase, Jie Peng, Tianlong Chen, Mohit Bansal",
        "摘要": "摘要: 大规模数据集上训练的LLMs可能会意外地获取敏感信息，如个人详细信息和潜在的有害内容。这一风险在多模态LLMs中更为严重，因为它们整合了来自多种模态（图像和文本）的信息。对手可以通过多模态提示来利用这些信息，从而提取敏感细节。评估MLLMs能否有效地忘记这些信息（有针对性的遗忘）需要创建高质量、注释良好的图像-文本对。尽管先前在遗忘方面的工作主要关注文本领域，但多模态遗忘仍未被充分研究。为了解决这一空白，我们首先引入了一个多模态遗忘基准测试UnLOK-VQA（Unlearning Outside Knowledge VQA）及其攻击和防御框架，以评估从MLLMs中删除特定多模态知识的方法。我们通过一个自动化流程扩展了一个视觉问答数据集，生成不同接近度的样本以测试泛化和特异性，然后通过人工筛选以保持高质量。然后，我们针对七种攻击（四种白盒，三种黑盒）评估了六种防御目标，包括一种利用隐藏状态可解释性的新白盒方法。我们的结果显示多模态攻击优于仅文本或仅图像的攻击，最有效的防御是从内部模型状态中移除答案信息。此外，较大的模型在编辑后表现出更强的鲁棒性，这表明规模提高了安全性。UnLOK-VQA提供了一个严谨的基准，以推进MLLMs中的遗忘研究。",
        "地址": "https://arxiv.org/pdf/2505.01456.pdf"
    },
    {
        "名称": "2025 [2505.02130] Attention Mechanisms Perspective: Exploring LLM Processing of Graph-Structured Data.pdf",
        "作者": "Zhong Guan, Likang Wu, Hongke Zhao, Ming He, Jianpin Fan",
        "摘要": "摘要: 注意机制是大型语言模型（LLMs）成功的关键，推动了多个领域的重大进展。然而，对于需要注重拓扑连接的图结构数据，它们相比于基于固定链接的消息传递机制（例如图神经网络（GNNs））表现不佳。这引发了一个问题：“在自然语言环境中，注意机制对图失败了吗？”基于这些观察，我们从注意机制的角度展开了一项实证研究，以探讨LLMs如何处理图结构数据。目标是深入了解LLMs在图结构上的注意行为。我们揭示了LLMs应用注意机制处理图结构数据的独特现象，并分析了这些发现，以改进LLMs对这种数据的建模。我们的主要研究结果是：1）虽然LLMs能够识别图数据并捕捉文本节点互动，但由于固有的架构限制，它们难以建模图结构中的节点间关系。2）LLMs在图节点上的注意分布与理想的结构模式不一致，表明它们无法适应图拓扑的细微差别。3）完全连接注意机制和固定连接性都不是最优选择；每种方式在其应用场景中都有具体的限制。相反，中间状态注意窗口在LLMs训练期间改善了表现，并在推理时顺利过渡到完全连接窗口。源代码：[LLM4Exploration](https://arxiv.org/pdf/2505.02130.pdf)",
        "地址": "https://arxiv.org/pdf/2505.02130.pdf"
    },
    {
        "名称": "2025 [2505.01548] Rethinking RGB-Event Semantic Segmentation with a Novel Bidirectional Motion-enhanced Event Representation.pdf",
        "作者": "Zhen Yao, Xiaowen Ying, Mooi Choo Chuah",
        "摘要": "摘要：事件相机捕捉运动动态，在各种计算机视觉任务中展现出巨大的潜力。然而，RGB-事件融合面临着三个内在的错位问题：(i)时间错位，(ii)空间错位，以及(iii)模式错位。现有的体素网格表示法忽略了连续事件窗口之间的时间相关性，并且简单地累积异步且稀疏的事件与RGB模式的同步且密集的特性不兼容。为了解决这些挑战，我们提出了一种新的事件表示法——运动增强事件张量（MET），该方法通过利用密集的光流和事件时间特征将稀疏的事件体素转换为密集且时间上连贯的形式。此外，我们引入了频率感知双向流聚合模块（BFAM）和时间融合模块（TFM）。BFAM利用频率域和MET来缓解模式错位问题，而双向流聚合和时间融合机制解决了时空错位问题。两大规模数据集的实验结果表明，我们的框架显著优于最先进的RGB-事件语义分割方法。我们的代码可以在以下网址获取：this https URL。",
        "地址": "https://arxiv.org/pdf/2505.01548.pdf"
    }
]