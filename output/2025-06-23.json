[
    {
        "名称": "2025 [2506.16406] Drag-and-Drop LLMs: Zero-Shot Prompt-to-Weights.pdf",
        "作者": "Zhiyuan Liang, Dongwen Tang, Yuhao Zhou, Xuanlei Zhao, Mingjia Shi, Wangbo Zhao, Zekai Li, Peihao Wang, Konstantin Schürholt, Damian Borth, Michael M. Bronstein, Yang You, Zhangyang Wang, Kai Wang",
        "摘要": "摘要：现代参数高效微调（PEFT）方法，如低秩适应（LoRA），降低了定制大型语言模型（LLM）的成本，但仍需要针对每个下游数据集进行单独的优化运行。我们介绍了\\textbf{拖放LLMs（\\textit{DnD})}，这是一种消除每任务训练的提示条件参数生成器，通过将少量未标记的任务提示直接映射到LoRA权重更新。一个轻量级文本编码器将每个提示批次浓缩成条件嵌入，然后由级联超卷积解码器转化为完整的LoRA矩阵集。在多样化的提示-检查点对集合中训练后，DnD在几秒钟内生成任务特定参数，产生i)比全面微调低\\textbf{12,000$\\times$}的开销，ii)在未见的常识推理、数学、编码和多模态基准上，比最强的训练LoRA平均性能提升\\textbf{30\\%}，以及iii)尽管未见目标数据或标签，但具有稳健的跨领域泛化能力。我们的结果表明，提示条件参数生成是快速专门化LLMs的一种可行替代方法。我们的项目可在\\href{this https URL}{this https URL}获得。\n\n翻译：\n摘要：现代参数高效微调（PEFT）方法如低秩适应（LoRA）降低了定制大型语言模型（LLM）的成本，但仍需要针对每个下游数据集进行单独优化。我们介绍\\textbf{拖放LLMs（\\textit{DnD})}，这是一种提示条件参数生成器，通过将少量未标记任务提示直接映射到LoRA权重更新，消除了每任务训练。一个轻量级文本编码器将每个提示批次浓缩成条件嵌入，然后由级联超卷积解码器转换为完整的LoRA矩阵集。在多样化的提示-检查点对集合中训练后，DnD在几秒钟内生成任务特定参数，产生i)比全面微调低\\textbf{12,000$\\times$}的开销，ii)在未见的常识推理、数学、编码和多模态基准上，比最强训练LoRA平均性能提升\\textbf{30\\%}，以及iii)尽管未见目标数据或标签，但具有稳健的跨领域泛化能力。我们的结果表明，提示条件参数生成是快速专门化LLMs的一种可行替代方法。我们的项目可在\\href{this https URL}{this https URL}获得。",
        "地址": "https://arxiv.org/pdf/2506.16406.pdf"
    },
    {
        "名称": "2025 [2506.16035] Vision-Guided Chunking Is All You Need: Enhancing RAG with Multimodal Document Understanding.pdf",
        "作者": "Vishesh Tripathi, Tanmay Odapally, Indraneel Das, Uday Allu, Biddwan Ahmed",
        "摘要": "摘要: 检索增强生成（RAG）系统在信息检索和问题回答方面取得了革命性的进展，但传统的基于文本的分块方法在处理复杂的文档结构、多页面表格、嵌入的图形以及跨页面边界的上下文依赖性时表现较差。我们提出了一种新的多模态文档分块方法，该方法利用大规模多模态模型（LMMs）以批量处理PDF文档，同时保持语义连贯性和结构完整性。我们的方法通过配置的页批处理文档，并保留跨批上下文，从而能够准确处理跨越多个页面的表格、嵌入的视觉元素以及程序内容。我们对一组经过人工构建查询的PDF文档数据集进行了评估，证明了分块质量和下游RAG性能的改进。我们的视觉引导方法在精确度上比传统的普通RAG系统更优，定性分析显示出更好的文档结构和语义连贯性保留。",
        "地址": "https://arxiv.org/pdf/2506.16035.pdf"
    },
    {
        "名称": "2025 [2506.16054] PAROAttention: Pattern-Aware ReOrdering for Efficient Sparse and Quantized Attention in Visual Generation Models.pdf",
        "作者": "Tianchen Zhao, Ke Hong, Xinhao Yang, Xuefeng Xiao, Huixia Li, Feng Ling, Ruiqi Xie, Siqi Chen, Hongyu Zhu, Yichong Zhang, Yu Wang",
        "摘要": "摘要：在视觉生成领域，注意机制的二次复杂性导致了高内存和计算成本，尤其是在生成高分辨率图像或多帧视频时需要较长的标记序列。为了解决这个问题，先前的研究探索了稀疏化和量化技术。然而，这些技术在低密度和减少位宽的情况下面临显著挑战。通过系统分析，我们发现核心难点在于视觉注意模式的分散和不规则特征。因此，我们提出了一种替代策略：通过重新组织注意模式来缓解这些挑战。受到视觉特征提取中局部聚合特性的启发，我们设计了一种新的模式感知标记重排序（PARO）技术，它将多样化的注意模式统一为硬件友好的块状模式。这种统一极大地简化和增强了稀疏化和量化。我们评估了各种设计选择的性能效率权衡，并最终确定了一种适用于统一模式的方法。我们的方法，PAROAttention，在较低密度（约20%-30%）和位宽（INT8/INT4）下实现了无损指标的视频和图像生成，且与全精度基线结果几乎相同，实现了1.9倍至2.7倍的端到端延迟加速。",
        "地址": "https://arxiv.org/pdf/2506.16054.pdf"
    },
    {
        "名称": "2025 [2506.09049] VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement Learning.pdf",
        "作者": "Li Kang, Xiufeng Song, Heng Zhou, Yiran Qin, Jie Yang, Xiaohong Liu, Philip Torr, Lei Bai, Zhenfei Yin",
        "摘要": "摘要: 在动态环境中协调多个具象化代理仍是人工智能的核心挑战之一，既需要依赖感知的推理，也需要可扩展的合作策略。尽管最近的研究利用大型语言模型(LLMs)进行多代理规划，一些研究也开始探索视觉语言模型(VLMs)进行视觉推理。然而，这些基于VLM的方法在支持各种具象化类型方面仍然有限。在本研究中，我们介绍了VIKI-Bench，这是第一个专为具象化多代理协作而设计的分层基准，具有三个结构化层次：代理激活、任务规划和轨迹感知。VIKI-Bench包含多样化的机器人具象化、多视角视觉观测以及结构化监督信号，用于评估以视觉输入为基础的推理能力。为了证明VIKI-Bench的实用性，我们提出了VIKI-R，一个两阶段框架，通过链式思维注释示范来微调预训练的视觉语言模型(VLM)，然后在多级奖励信号下进行强化学习。我们的大量实验表明，VIKI-R在所有任务层面上显著优于基准方法。此外，我们显示强化学习使得异质代理之间出现了组合协作模式。总之，VIKI-Bench和VIKI-R提供了一个统一的测试平台和方法，以推动具象化AI系统中的多代理、视觉驱动的协作。",
        "地址": "https://arxiv.org/pdf/2506.09049.pdf"
    },
    {
        "名称": "2025 [2506.17201] Hunyuan-GameCraft: High-dynamic Interactive Game Video Generation with Hybrid History Condition.pdf",
        "作者": "Jiaqi Li, Junshu Tang, Zhiyong Xu, Longhuang Wu, Yuan Zhou, Shuai Shao, Tianbao Yu, Zhiguo Cao, Qinglin Lu",
        "摘要": "摘要：近期在基于扩散和可控视频生成领域的进展推动了高质量和时间一致的视频合成，为沉浸式互动游戏体验奠定了基础。然而，当前的方法在动态性、通用性、长期一致性和效率方面存在局限，限制了各种游戏视频的生成能力。为了解决这些问题，我们介绍了Hunyuan-GameCraft，这是一种用于游戏环境中高动态互动视频生成的新框架。为了实现细粒度的动作控制，我们将标准的键盘和鼠标输入统一到共享的相机表示空间中，促进各种相机和移动操作之间的平滑插值。我们提出了一种混合历史条件的训练策略，以自回归方式扩展视频序列，同时保持游戏场景信息。此外，为了提高推理效率和可玩性，我们通过模型蒸馏来减少计算开销，同时保持长期时间序列的一致性，使其适用于复杂互动环境中的实时部署。模型在一个包含超过一百万条游戏录制视频的跨越100多款AAA游戏的大规模数据集上进行了训练，确保了广泛的覆盖和多样性，然后在一个精心注释的合成数据集上进行微调，以提高精度和控制能力。精心策划的游戏场景数据显著提高了视觉保真度、真实感和动作可控性。大量实验表明，Hunyuan-GameCraft明显优于现有模型，推进了互动游戏视频生成的真实感和可玩性。\n\n作者：李嘉琪，唐俊书，徐志勇，吴龙煌，周原，邵帅，喻天宝，曹志国，卢青林\n\n评论：项目页面：这个 https URL\n\n网址：https://arxiv.org/pdf/2506.17201.pdf\n\n标题：2025 [2506.17201] Hunyuan-GameCraft: 高动态互动游戏视频生成与混合历史条件模型",
        "地址": "https://arxiv.org/pdf/2506.17201.pdf"
    },
    {
        "名称": "2025 [2506.16310] Optimizing Multilingual Text-To-Speech with Accents & Emotions.pdf",
        "作者": "Pranav Pawar, Akshansh Dwivedi, Jenish Boricha, Himanshu Gohil, Aditya Dubey",
        "摘要": "摘要：最先进的文本转语音（TTS）系统在单语环境中实现了高度的自然性，但在合成带有正确多语言口音（尤其是印度语言）和与上下文相关的情感的语音时仍然存在困难，这是由于当前框架中的文化差异。本论文介绍了一种新的TTS架构，该架构集成了口音并保留了转写和多尺度情感建模，特别针对印地语和印度英语口音进行了调整。我们的方法通过引入语言特定的音素对齐混合编码器-解码器架构以及文化敏感的情感嵌入层，并在母语者语料库上进行训练，同时结合残差矢量量化的动态口音代码切换，扩展了Parler-TTS模型。定量测试表明，其口音准确率提高了23.7%（单词错误率从15.4%降低到11.8%），本地听众的情感识别准确率达到85.3%，超过了METTS和VECL-TTS基线。该系统的新颖之处在于它可以实时混合编码，生成诸如\"Namaste, let\\'s talk about <Hindi phrase>\"这样不中断口音转换的句子，同时保持情感一致性。对200名用户进行的主观评估报告显示，其文化正确性的平均评分（MOS）为4.2/5，远优于现有的多语言系统（p<0.01）。本研究通过展示可扩展的口音-情感解耦，使跨语言合成更加可行，并可直接应用于南亚的教育技术和辅助软件。\n\n作者：Pranav Pawar, Akshansh Dwivedi, Jenish Boricha, Himanshu Gohil, Aditya Dubey\n\n评论：12页，8个图\n\n链接：https://arxiv.org/pdf/2506.16310.pdf\n\n标题：2025 [2506.16310] 优化带有口音和情感的多语言文本转语音系统.pdf",
        "地址": "https://arxiv.org/pdf/2506.16310.pdf"
    },
    {
        "名称": "2025 [2506.17206] DreamCube: 3D Panorama Generation via Multi-plane Synchronization.pdf",
        "作者": "Yukun Huang, Yanning Zhou, Jianan Wang, Kaiyi Huang, Xihui Liu",
        "摘要": "摘要: 3D全景合成是一项前景广阔但具有挑战性的任务，需要生成内容在视觉外观和几何形状上的高质量和多样性。现有方法利用预训练的2D基础模型中的丰富图像先验来弥补3D全景数据的匮乏，但3D全景与2D单视图之间的不兼容性限制了它们的有效性。在这项工作中，我们展示了通过对2D基础模型中的操作符进行多平面同步，可以将其能力无缝地扩展到全景领域。在此设计的基础上，我们进一步介绍了DreamCube，一种用于3D全景生成的多平面RGB-D扩散模型，该模型最大限度地重用了2D基础模型的先验，以实现多样的外观和准确的几何形状，同时保持多视图的一致性。大量实验证明了我们的方法在全景图像生成、全景深度估计和3D场景生成中的有效性。",
        "地址": "https://arxiv.org/pdf/2506.17206.pdf"
    },
    {
        "名称": "2025 [2506.16504] Hunyuan3D 2.5: Towards High-Fidelity 3D Assets Generation with Ultimate Details.pdf",
        "作者": "Zeqiang Lai, Yunfei Zhao, Haolin Liu, Zibo Zhao, Qingxiang Lin, Huiwen Shi, Xianghui Yang, Mingxin Yang, Shuhui Yang, Yifei Feng, Sheng Zhang, Xin Huang, Di Luo, Fan Yang, Fang Yang, Lifu Wang, Sicong Liu, Yixuan Tang, Yulin Cai, Zebin He, Tian Liu, Yuhong Liu, Jie Jiang, Linus, Jingwei Huang, Chunchao Guo",
        "摘要": "摘要：在本报告中，我们介绍了Hunyuan3D 2.5，这是一组旨在生成高保真和详细纹理的3D资产的强大扩散模型。Hunyuan3D 2.5遵循其先前版本Hunyuan3D 2.0的两个阶段管道，同时在形状和纹理生成方面表现出显著进步。在形状生成方面，我们引入了一个新的形状基础模型--LATTICE，该模型通过高质量数据集、模型规模和计算进行训练。我们最大的模型达到了100亿参数，能够生成精确和详细的3D形状，保持网格表面干净光滑，显著缩小了生成和手工制作的3D形状之间的差距。在纹理生成方面，通过从Hunyuan3D 2.0 Paint模型扩展的创新多视图架构，它采用了物理基础渲染（PBR）。我们的广泛评估表明，Hunyuan3D 2.5在形状和端到端的纹理生成方面显著优于以前的方法。",
        "地址": "https://arxiv.org/pdf/2506.16504.pdf"
    },
    {
        "名称": "2025 [2506.17218] Machine Mental Imagery: Empower Multimodal Reasoning with Latent Visual Tokens.pdf",
        "作者": "Zeyuan Yang, Xueyang Yu, Delin Chen, Maohao Shen, Chuang Gan",
        "摘要": "摘要: 视觉语言模型（Vision-language models，简称VLMs）在多模态理解方面表现优异，但其仅能进行文本解码的方式迫使它们在视觉推理时只能通过语言表达，这限制了在需要视觉想象的任务中的表现。最近的一些尝试旨在训练VLMs生成显式图像，但繁重的图像生成预训练往往会削弱推理能力。受到人类通过心理图像进行推理的方法——即内部构建和操作视觉线索——的启发，我们研究了VLMs是否可以通过交错的多模态轨迹进行推理而不产生显式图像。为此，我们提出了一个机器心理图像框架，称为Mirage，该框架通过潜在视觉标记与普通文本一起扩展VLM解码。具体来说，每当模型选择“视觉思考”时，它会将其隐藏状态重新投射为下一个标记，从而在不生成像素级图像的情况下继续多模态轨迹。我们首先通过从真实图像嵌入中蒸馏潜在标记进行监督，然后切换到仅文本监督，使潜在轨迹紧密地与任务目标对齐。随后的强化学习阶段进一步增强了多模态推理能力。在各种基准测试上的实验表明，Mirage在不生成显式图像的情况下解锁了更强的多模态推理能力。",
        "地址": "https://arxiv.org/pdf/2506.17218.pdf"
    },
    {
        "名称": "2025 [2506.15745] InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video Understanding.pdf",
        "作者": "Minsoo Kim, Kyuhong Shim, Jungwook Choi, Simyung Chang",
        "摘要": "摘要：现代多模态大型语言模型（MLLMs）可以处理长达数小时的视频，但其键值（KV）缓存随着时间线性增长——迅速超出手机、增强现实眼镜和边缘机器人的固定内存。之前的压缩方案要么假设整个视频和用户查询是离线可用的，要么首先必须创建完整的缓存，因此内存仍然随流长度而扩展。InfiniPot-V 是首个无训练、与查询无关的框架，它为流视频理解强制执行硬性、不随长度变化的内存上限。在视频编码过程中，它监控缓存，一旦达到用户设定的阈值，运行轻量级的压缩过程，包括通过时间轴冗余（TaR）度量移除时间冗余的标记和通过价值规范（VaN）排名保留语义重要的标记。在四个开源 MLLMs 和四个长视频及两个流视频基准上，InfiniPot-V 将峰值 GPU 内存减少最多 94%，支持实时生成，并在多轮对话中匹配或超越全缓存准确性。通过在不重新训练或查询知识的情况下解决 KV 缓存瓶颈，InfiniPot-V 缩小了设备上流视频助手的差距。",
        "地址": "https://arxiv.org/pdf/2506.15745.pdf"
    },
    {
        "名称": "2025 [2506.15442] Hunyuan3D 2.1: From Images to High-Fidelity 3D Assets with Production-Ready PBR Material.pdf",
        "作者": "Team Hunyuan3D, Shuhui Yang, Mingxin Yang, Yifei Feng, Xin Huang, Sheng Zhang, Zebin He, Di Luo, Haolin Liu, Yunfei Zhao, Qingxiang Lin, Zeqiang Lai, Xianghui Yang, Huiwen Shi, Zibo Zhao, Bowen Zhang, Hongyu Yan, Lifu Wang, Sicong Liu, Jihong Zhang, Meng Chen, Liang Dong, Yiwen Jia, Yulin Cai, Jiaao Yu, Yixuan Tang, Dongyuan Guo, Junlin Yu, Hao Zhang, Zheng Ye, Peng He, Runzhou Wu, Shida Wei, Chao Zhang, Yonghao Tan, Yifu Sun, Lin Niu, Shirui Huang, Bojian Zheng, Shu Liu, Shilin Chen, Xiang Yuan, Xiaofeng Yang, Kai Liu, Jianchen Zhu, Peng Chen, Tian Liu, Di Wang, Yuhong Liu, Linus, Jie Jiang, Jingwei Huang, Chunchao Guo",
        "摘要": "摘要：3D AI生成内容（AIGC）是一个热门领域，它显著加速了游戏、电影和设计中3D模型的创建。尽管已经开发了几种革命性模型，这些模型极大地改变了3D生成，但由于收集、处理和训练3D模型所涉及的复杂性，该领域仍主要向研究人员、开发人员和设计师开放。为了应对这些挑战，我们在本教程中引入了Hunyuan3D 2.1作为案例研究。本教程提供了一份全面的逐步指南，介绍如何处理3D数据、训练3D生成模型以及使用Hunyuan3D 2.1评估其性能，Hunyuan3D 2.1是一个用于制作高分辨率、具有纹理的3D资产的先进系统。该系统包括两个核心组件：用于形状生成的Hunyuan3D-DiT和用于纹理合成的Hunyuan3D-Paint。我们将探讨整个工作流程，包括数据准备、模型架构、训练策略、评估指标和部署。通过本教程的学习，您将获得微调或开发适用于游戏、虚拟现实和工业设计应用的强大3D生成模型的知识。",
        "地址": "https://arxiv.org/pdf/2506.15442.pdf"
    },
    {
        "名称": "2025 [2506.17202] UniFork: Exploring Modality Alignment for Unified Multimodal Understanding and Generation.pdf",
        "作者": "Teng Li, Quanfeng Lu, Lirui Zhao, Hao Li, Xizhou Zhu, Yu Qiao, Jun Zhang, Wenqi Shao",
        "摘要": "摘要：统一的图像理解和生成已经成为多模态人工智能中的一个很有前途的范式。尽管取得了最近的进展，但这种统一模型的最佳架构设计仍然是一个尚未解决的问题。在这项工作中，我们首先分析了特定任务专家模型用于理解和生成的模态对齐行为，以及当前的统一模型的模态对齐行为。我们的分析揭示了一个关键观察：理解任务通过网络深度逐步增加的模态对齐受益，这有助于建立更好的语义信息来提高理解能力；相反，生成任务遵循不同的趋势：模态对齐在早期层增加，但在深层减少以恢复空间细节。这些不同的对齐模式在完全共享的Transformer骨干网络中产生了根本性的冲突，其中统一的表示流经常导致两个任务的性能妥协。基于这一发现，我们介绍了UniFork，这是一种新颖的Y形架构，在浅层共享用于跨任务表示学习，同时在深层采用特定任务分支以避免任务干扰。这种设计有效地平衡了共享学习和任务专门化。通过广泛的消融实验，我们证明了UniFork一致地优于传统的完全共享的Transformer架构，并且在性能上与特定任务模型相当或更好。\n\n作者：李腾，路全峰，赵力锐，李昊，朱西周，乔宇，张峻，邵文琦\n\n评论：代码：这个https URL\n\n链接：https://arxiv.org/pdf/2506.17202.pdf\n\n标题：2025 [2506.17202] UniFork: Exploring Modality Alignment for Unified Multimodal Understanding and Generation.pdf",
        "地址": "https://arxiv.org/pdf/2506.17202.pdf"
    },
    {
        "名称": "2025 [2506.17213] Long-term Traffic Simulation with Interleaved Autoregressive Motion and Scenario Generation.pdf",
        "作者": "Xiuyu Yang, Shuhan Tan, Philipp Krähenbühl",
        "摘要": "摘要：一个理想的交通模拟器可以复制自动驾驶系统在部署期间经历的现实长期点对点行程。现有模型和基准主要针对场景中初始代理的闭环运动模拟，这对长期模拟来说是有问题的。随着自车辆进入新的区域，代理会进入和退出场景。我们提出了InfGen，这是一种统一的下一个标记预测模型，执行交织的闭环运动模拟和场景生成。InfGen自动在闭环运动模拟和场景生成模式之间切换，从而实现稳定的长期滚动模拟。在短期（9秒）的交通模拟中，InfGen达到最先进的水平，并在长期（30秒）的模拟中显著超越其他所有方法。InfGen的代码和模型将在此https URL发布。",
        "地址": "https://arxiv.org/pdf/2506.17213.pdf"
    },
    {
        "名称": "2025 [2506.15925] Reranking-based Generation for Unbiased Perspective Summarization.pdf",
        "作者": "Narutatsu Ri, Nicholas Deas, Kathleen McKeown",
        "摘要": "摘要: 在现实环境中生成无偏见摘要，例如政治视角的总结，仍然是大型语言模型（LLMs）的关键应用。然而，现有的评估框架依赖于传统指标来衡量覆盖率和可信度等关键属性，而不验证其适用性，开发改进的总结器的努力仍处于初期阶段。我们通过以下方式解决这些问题：（1）确定衡量视角摘要质量的可靠指标，（2）研究 LLMs 方法在零样本推理之外的效果。具体来说，我们使用人工标注建立了一个用于基准测试指标可靠性的测试集，并证明传统指标相比基于语言模型的指标表现较差，后者被证明是强有力的评估工具。利用这些指标，我们展示了基于重排序的方法产生了良好效果，而使用合成生成和重排序标记数据的偏好调整进一步提升了性能。我们的研究结果旨在为视角总结方法的可靠评估和发展做出贡献。",
        "地址": "https://arxiv.org/pdf/2506.15925.pdf"
    },
    {
        "名称": "2025 [2506.09930] From Intention to Execution: Probing the Generalization Boundaries of Vision-Language-Action Models.pdf",
        "作者": "Irving Fang, Juexiao Zhang, Shengbang Tong, Chen Feng",
        "摘要": "摘要：与传统模仿学习相比，视觉-语言-动作（VLA）模型在机器人领域的一个优势在于能够利用大型视觉-语言模型（VLM）的广泛泛化能力来产生通用的机器人策略。然而，目前对VLA的评估仍然不足。传统的模仿学习基准由于缺乏语言指令而不适用。新兴的VLA基准虽然结合了语言，但通常评估任务有限，并不旨在探讨VLM预训练在多大程度上真正有助于下游机器人策略的泛化能力。同时，许多研究依赖不同机构独立设计的真实世界机器人设置，这对可重复性和可访问性形成了障碍。为了弥补这一差距，我们引入了一个统一的探测套件，涵盖10个子类别的50个基于模拟的任务，涉及语言指令、视觉和物体。我们系统地评估了几种最先进的VLA架构在该套件上的表现，以了解它们的泛化能力。我们的结果显示，尽管VLM骨干赋予了VLA稳健的感知理解和高级别计划能力（我们称之为良好的意图），但这并不能可靠地转化为精确的动作执行：在面对分布外观察时，策略往往表现出连贯的意图，但在动作执行上却出错。此外，动作数据的微调可能会削弱原始VLM的通用推理能力。我们发布了我们的任务套件和评估代码，以作为未来VLA的标准化基准，并推动缩小感知到动作的差距的研究。更多信息和源代码可以在此链接找到：https://arxiv.org/pdf/2506.09930.pdf。",
        "地址": "https://arxiv.org/pdf/2506.09930.pdf"
    },
    {
        "名称": "2025 [2506.17113] MEXA: Towards General Multimodal Reasoning with Dynamic Multi-Expert Aggregation.pdf",
        "作者": "Shoubin Yu, Yue Zhang, Ziyang Wang, Jaehong Yoon, Mohit Bansal",
        "摘要": "摘要：结合预训练的专家模型为可扩展的多模态推理提供了巨大潜力，但由于输入模态的多样性和任务复杂性的增加，构建一个统一的框架仍具有挑战性。例如，医疗诊断需要对结构化的临床表格进行精确推理，而金融预测则依赖于解读基于图表的数据以做出明智的预测。为了解决这一挑战，我们引入了MEXA，一个无需训练的框架，能够通过模态和任务感知的多专家模型聚合来实现跨不同领域的有效多模态推理。MEXA根据输入模态和任务的具体推理需求（即技能）动态选择专家模型。每个在模态任务配对上有专长的专家模型生成可解释的文本推理输出。然后，MEXA使用大规模推理模型（LRM）聚合并推理这些输出以生成最终答案。这种模块化设计允许在不同领域中进行灵活和透明的多模态推理，而无需额外的训练开销。我们在各种多模态基准测试上对我们的方法进行了广泛评估，包括视频推理、音频推理、3D理解和医学问答。MEXA在这些任务中持续显示出比强多模态基线更好的性能，突显了我们专家驱动选择和聚合在多样多模态推理任务中的有效性和广泛适用性。",
        "地址": "https://arxiv.org/pdf/2506.17113.pdf"
    },
    {
        "名称": "2025 [2506.17090] Better Language Model Inversion by Compactly Representing Next-Token Distributions.pdf",
        "作者": "Murtaza Nazir, Matthew Finlayson, John X. Morris, Xiang Ren, Swabha Swayamdipta",
        "摘要": "以下是文章摘要的中文翻译：\n\n摘要：语言模型反转旨在仅通过语言模型的输出来恢复隐藏的提示。这种能力在语言模型的部署中对于安全性和责任性有重要意义，例如可能会从API受保护的语言模型系统消息中泄露私人信息。我们提出了一种新方法——从对数概率序列中反转提示（PILS），该方法通过从模型的多个生成步骤中的下一个令牌概率中获取线索来恢复隐藏的提示。我们的方法基于一个关键见解：语言模型的向量值输出占据一个低维子空间。这使我们能够使用线性映射无损地压缩多个生成步骤中的完整下一个令牌概率分布，从而允许更多的输出信息用于反转。与之前最先进的方法相比，我们的方法在恢复隐藏提示方面取得了巨大进展，在测试集中实现了2到3.5倍的更高精确恢复率，在某些情况下恢复率从17%提高到60%。我们的方法还表现出令人惊讶的良好泛化行为；例如，一个经过16个生成步骤训练的反转器，在测试时将步骤数量增加到32时，提示恢复率提高了5到27个百分点。此外，我们展示了我们的方法在恢复隐藏系统消息这一更具挑战性的任务中表现出色。我们还分析了逐字重复在提示恢复中的作用，并提出了一种基于对数的跨家族模型转移的新方法。我们的研究结果表明，下一个令牌概率比以前已知的反转攻击表面更加脆弱。",
        "地址": "https://arxiv.org/pdf/2506.17090.pdf"
    },
    {
        "名称": "2025 [2506.16349] Watermarking Autoregressive Image Generation.pdf",
        "作者": "Nikola Jovanović, Ismail Labiad, Tomáš Souček, Martin Vechev, Pierre Fernandez",
        "摘要": "摘要：对生成模型输出进行水印标记已成为跟踪其来源的一个有前景的方法。尽管人们对自回归图像生成模型及其潜在的滥用非常关注，但此前没有任何研究尝试在令牌级别对其输出进行水印标记。在这项工作中，我们通过将语言模型水印技术适应这一环境，提出了首个此类方法。我们发现一个关键挑战：缺乏逆循环一致性 (RCC)，即重新令牌化生成的图像令牌会显著改变令牌序列，从而有效地擦除水印。为了解决这个问题，并使我们的方法能够应对常见的图像转换、神经压缩和移除攻击，我们引入了 (i) 改进RCC的定制令牌器-解令牌器微调程序，以及 (ii) 一个互补的水印同步层。我们的实验表明，我们的方法能够以理论上有根据的p值实现可靠且稳健的水印检测。",
        "地址": "https://arxiv.org/pdf/2506.16349.pdf"
    }
]