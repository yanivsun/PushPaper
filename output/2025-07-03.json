[
    {
        "名称": "2025 [2507.01949] Kwai Keye-VL Technical Report.pdf",
        "作者": "Kwai Keye Team, Biao Yang, Bin Wen, Changyi Liu, Chenglong Chu, Chengru Song, Chongling Rao, Chuan Yi, Da Li, Dunju Zang, Fan Yang, Guorui Zhou, Hao Peng, Haojie Ding, Jiaming Huang, Jiangxia Cao, Jiankang Chen, Jingyun Hua, Jin Ouyang, Kaibing Chen, Kaiyu Jiang, Kaiyu Tang, Kun Gai, Shengnan Zhang, Siyang Mao, Sui Huang, Tianke Zhang, Tingting Gao, Wei Chen, Wei Yuan, Xiangyu Wu, Xiao Hu, Xingyu Lu, Yang Zhou, Yi-Fan Zhang, Yiping Yang, Yulong Chen, Zhenhua Wu, Zhenyu Li, Zhixin Ling, Ziming Li, Dehua Ma, Di Xu, Haixuan Gao, Hang Li, Jiawei Guo, Jing Wang, Lejian Ren, Muhao Wei, Qianqian Wang, Qigen Hu, Shiyao Wang, Tao Yu, Xinchen Luo, Yan Li, Yiming Liang, Yuhang Hu, Zeyi Lu, Zhuoran Yang, Zixing Zhang",
        "摘要": "摘要：虽然多模态大语言模型（MLLMs）在处理静态图像方面展现了显著的能力，但在理解动态、信息密集的短视频这一当今数字媒体主流时常常表现不足。为弥补这一缺口，我们引入了Kwai Keye-VL，一种拥有80亿参数的多模态基础模型，专为短视频理解的尖端性能而设计，同时保持了强大的通用视觉语言能力。Keye-VL的开发基于两个核心支柱：一个超过6000亿标记、以视频为重点的大规模高质量数据集和一个创新的训练方案。该方案包含一个四阶段的预训练过程，以确保坚固的视觉语言对齐，随后是一个细致的两阶段后训练过程。首个后训练阶段加强了基础能力，如指令遵循，而第二阶段则集中在激发高级推理。在第二阶段中，关键创新在于我们采用了五模式“冷启动”数据混合，其中包括“思考”、“非思考”、“自动思考”、“带图像思考”和高质量视频数据。这种方法教会模型在何时以及如何进行推理。随后，强化学习（RL）和对齐步骤进一步增强了这些推理能力并纠正异常模型行为，如重复输出。为验证我们的方法，我们进行了广泛的评估，显示Keye-VL在公共视频基准上取得了最先进的结果，并在通用图片任务上保持高度竞争力。此外，我们开发并发布了KC-MMBench，一个为真实世界短视频场景量身定制的新基准，在该基准上Keye-VL显示出显著优势。\n\n链接：https://arxiv.org/pdf/2507.01949.pdf",
        "地址": "https://arxiv.org/pdf/2507.01949.pdf"
    },
    {
        "名称": "2025 [2507.01945] LongAnimation: Long Animation Generation with Dynamic Global-Local Memory.pdf",
        "作者": "Nan Chen, Mengqi Huang, Yihao Meng, Zhendong Mao",
        "摘要": "摘要：动画上色是实际动画制作行业的重要组成部分。长时间动画上色的人工成本较高，因此，基于视频生成模型的自动化长时间动画上色具有重要的研究价值。现有研究局限于短期上色，这些研究采用局部范式，通过融合重叠特征实现局部片段之间的平滑过渡。然而，局部范式忽略了全局信息，无法保持长期的颜色一致性。在本研究中，我们提出理想的长期颜色一致性可以通过动态全局-局部范式来实现，即动态提取与当前生成相关的全局颜色一致特征。具体来说，我们提出了一个新框架LongAnimation，主要包括SketchDiT、动态全局-局部记忆（DGLM）和颜色一致性奖励。SketchDiT捕捉混合参考特征以支持DGLM模块。DGLM模块采用长视频理解模型动态压缩全局历史特征，并将其自适应地与当前生成特征融合。为了优化颜色一致性，我们引入了颜色一致性奖励。在推理过程中，我们提出颜色一致性融合以平滑视频片段过渡。大量实验证明了LongAnimation在短期（14帧）和长期（平均500帧）动画上色任务中保持颜色一致性的有效性。代码可在此网址找到。",
        "地址": "https://arxiv.org/pdf/2507.01945.pdf"
    },
    {
        "名称": "2025 [2507.01634] Depth Anything at Any Condition.pdf",
        "作者": "Boyuan Sun, Modi Jin, Bowen Yin, Qibin Hou",
        "摘要": "摘要: 我们介绍了Depth Anything at Any Condition (DepthAnything-AC)，这是一个能够处理多种环境条件的基础单目深度估计（MDE）模型。之前的基础MDE模型在一般场景中取得了令人瞩目的表现，但在涉及诸如光照变化、恶劣天气和传感器诱发的失真等复杂的开放世界环境中表现不佳。为了解决数据匮乏以及无法从受损图像生成高质量伪标签的挑战，我们提出了一种无监督一致性正则化微调范式，仅需要相对少量的未标记数据。此外，我们还提出了空间距离约束，以显式地强制模型学习补丁级的相对关系，从而获得更清晰的语义边界和更准确的细节。实验结果显示，DepthAnything-AC在包括现实世界恶劣天气基准、合成损坏基准和一般基准在内的各种基准上具有零样本能力。\n\n项目页面: 此 https URL\n\n代码: 此 https URL",
        "地址": "https://arxiv.org/pdf/2507.01634.pdf"
    },
    {
        "名称": "2025 [2507.01925] A Survey on Vision-Language-Action Models: An Action Tokenization Perspective.pdf",
        "作者": "Yifan Zhong, Fengshuo Bai, Shaofei Cai, Xuchuan Huang, Zhang Chen, Xiaowei Zhang, Yuanfei Wang, Shaoyang Guo, Tianrui Guan, Ka Nam Lui, Zhiquan Qi, Yitao Liang, Yuanpei Chen, Yaodong Yang",
        "摘要": "摘要：视觉与语言基础模型在多模态理解、推理和生成方面的显著进展引发了扩展这种智能到物理世界的努力，推动了视觉-语言-行动（VLA）模型的繁荣。尽管方法看似多样化，但我们观察到当前的VLA模型可以在单一框架下统一：视觉和语言输入经过一系列VLA模块处理，产生一系列逐步编码更多具体可操作信息的动作标记，最终生成可执行的动作。我们进一步确定了区分VLA模型的主要设计选择在于动作标记的形式，动作标记可以归类为语言描述、代码、可操作性、轨迹、目标状态、潜在表示、原始动作和推理。然而，关于动作标记缺乏全面的理解，显著阻碍了VLA的有效发展，模糊了未来的方向。因此，本次研究旨在通过动作标记的角度对现有的VLA研究进行分类和解释，提炼每种标记类型的优缺点，并确定改进的领域。通过系统的回顾和分析，我们提供了关于VLA模型更广泛发展的综合观点，突出了未充分探索但有希望的方向，并为未来的研究提供了指导，希望将该领域更进一步推向通用智能。",
        "地址": "https://arxiv.org/pdf/2507.01925.pdf"
    },
    {
        "名称": "2025 [2507.01953] FreeMorph: Tuning-Free Generalized Image Morphing with Diffusion Model.pdf",
        "作者": "Yukang Cao, Chenyang Si, Jinghao Wang, Ziwei Liu",
        "摘要": "摘要: 我们介绍了FreeMorph，这是首个无需调整的图像变形方法，可以处理具有不同语义或布局的输入。与现有方法依赖于微调预训练扩散模型并受时间限制和语义/布局差异影响不同，FreeMorph无需单次训练即可提供高保真图像变形。尽管无需调整的方法非常高效且潜力巨大，但因多步降噪过程的非线性性质和预训练扩散模型自有偏差，保持高质量输出仍面临挑战。本文通过整合两项重要创新来介绍FreeMorph以应对这些挑战。1) 我们首先提出一种指导感知的球形插值设计，通过修改自注意力模块从输入图像中融入明确指导，从而解决身份损失并确保生成序列中的方向性过渡。2) 我们进一步引入一种基于步骤的变化趋势，将每个输入图像的自注意力模块进行融合以实现控制和一致的过渡，尊重两个输入图像。我们的大量评估表明，FreeMorph相比现有方法在速度上提升了10倍至50倍，并为图像变形建立了新的最先进水平。",
        "地址": "https://arxiv.org/pdf/2507.01953.pdf"
    },
    {
        "名称": "2025 [2507.01957] Locality-aware Parallel Decoding for Efficient Autoregressive Image Generation.pdf",
        "作者": "Zhuoyang Zhang, Luke J. Huang, Chengyue Wu, Shang Yang, Kelly Peng, Yao Lu, Song Han",
        "摘要": "摘要：我们提出了局部性感知并行解码（Locality-aware Parallel Decoding, LPD）以加速自回归图像生成。传统的自回归图像生成依赖于下一个图像块的预测，这是一种受内存限制的过程，导致高延迟。现有的工作尝试通过转向多块预测来并行化下一个图像块的预测，以加快这一过程，但只实现了有限的并行化。为了在保持生成质量的同时实现高并行化，我们引入了两个关键技术：（1）灵活并行化自回归建模，这是一种新颖的架构，能够实现任意生成顺序和并行化程度。它使用可学习的位置查询标记来引导在目标位置上的生成，同时确保并行生成的标记之间的相互可见性，以实现一致的并行解码。（2）局部性感知生成排序，一种新颖的调度方式，通过形成群组来最小化组内依赖性并最大化上下文支持，从而提升生成质量。通过这些设计，我们在ImageNet类条件生成中将生成步骤从256（256$\\\\times$256分辨率）减少到20步，以及从1024（512$\\\\times$512分辨率）减少到48步，而不影响生成质量，并且相比之前的并行自回归模型，实现了至少3.4倍的延迟降低。",
        "地址": "https://arxiv.org/pdf/2507.01957.pdf"
    },
    {
        "名称": "2025 [2507.01544] MARVIS: Modality Adaptive Reasoning over VISualizations.pdf",
        "作者": "Benjamin Feuer, Lennart Purucker, Oussama Elachqar, Chinmay Hegde",
        "摘要": "摘要：科学应用中的机器学习通常依赖于针对特定领域调整的小型、专用模型。这些模型通常表现出色，但缺乏灵活性。基础模型提供了多功能性，但通常在非传统模态和长尾领域上表现不如专用方法。我们提出了MARVIS（模态自适应可视化推理），一种无需训练的方法，即使是小型的视觉语言模型也能以高精度预测任何数据模态。MARVIS将潜在嵌入空间转化为视觉表示，然后利用VLM的空间和细粒度推理能力成功地解释和利用这些表示。MARVIS在视觉、音频、生物学和表格领域使用一个3B参数模型取得了有竞争力的表现，其结果平均比Gemini高出16%，并接近专用方法，而无需暴露个人身份信息（P.I.I.）或进行任何特定领域的训练。我们在此 https URL 开源了我们的代码和数据集。\n\n本文作者：Benjamin Feuer, Lennart Purucker, Oussama Elachqar, Chinmay Hegde\n原文链接：https://arxiv.org/pdf/2507.01544.pdf\n标题：《2025 MARVIS: Modality Adaptive Reasoning over VISualizations》",
        "地址": "https://arxiv.org/pdf/2507.01544.pdf"
    },
    {
        "名称": "2025 [2507.00316] $μ^2$Tokenizer: Differentiable Multi-Scale Multi-Modal Tokenizer for Radiology Report Generation.pdf",
        "作者": "Siyou Li, Pengyao Qin, Huanan Wu, Dong Nie, Arun J. Thirunavukarasu, Juntao Yu, Le Zhang",
        "摘要": "摘要：自动化放射学报告生成（RRG）旨在通过临床影像（如计算机断层扫描（CT））生成详细的文本报告，以提高诊断和管理建议的准确性和效率。RRG面临两个关键挑战：（1）在资源受限的情况下，从影像数据中提取相关信息的固有复杂性；（2）客观评估模型生成报告与专家书写报告之间差异的困难。为了应对这些挑战，我们提出了$μ^2$LLM，一种用于RRG任务的多尺度多模态大语言模型。创新的$μ^2$Tokenizer作为中间层，将多尺度视觉分词器和文本分词器的多模态特征整合，然后通过直接偏好优化（DPO），由GREEN-RedLlama指导，提升报告生成质量。四个大型CT图像-报告医学数据集的实验结果表明，我们的方法优于现有方法，突显了我们针对有限数据微调的$μ^2$LLMs在RRG任务中的潜力。同时，对于提示工程，我们引入了一个五阶段的LLM驱动管道，将常规CT报告转换为配对的视觉问题答案三元组和引用链接的推理叙述，创建了一个可扩展的高质量监督语料库，以实现可解释的多模态放射学LLM。所有代码、数据集和模型将在我们的官方存储库中公开提供。\n\n评论：被MICCAI 2025接收\n\n链接：https://arxiv.org/pdf/2507.00316.pdf\n\n作者：Siyou Li, Pengyao Qin, Huanan Wu, Dong Nie, Arun J. Thirunavukarasu, Juntao Yu, Le Zhang\n\n标题：2025 [2507.00316] $μ^2$Tokenizer: Differentiable Multi-Scale Multi-Modal Tokenizer for Radiology Report Generation.pdf",
        "地址": "https://arxiv.org/pdf/2507.00316.pdf"
    },
    {
        "名称": "2025 [2506.22868] STR-Match: Matching SpatioTemporal Relevance Score for Training-Free Video Editing.pdf",
        "作者": "Junsung Lee, Junoh Kang, Bohyung Han",
        "摘要": "摘要：以往的文本指导视频编辑方法通常存在时间不一致、运动失真以及有限的领域变换等问题。我们认为这些限制是由于编辑过程中对时空像素相关性的建模不足。为了解决这些问题，我们提出了STR-Match，一种无需训练的视频编辑算法，通过我们的新颖STR评分引导的潜在优化过程，生成视觉上吸引且时空一致的视频。STR评分通过利用文本转视频扩散模型中的二维空间注意力和一维时间模块，捕捉相邻帧之间的时空像素相关性，而不需额外付出计算上昂贵的三维注意力机制的开销。结合潜在掩码的潜在优化框架，STR-Match生成时间一致且视觉逼真的视频，即使在显著的领域变换下，仍能保持源视频的关键视觉属性。广泛的实验表明，STR-Match在视觉质量和时空一致性方面均一贯优于现有方法。\n\n作者：李俊成、姜俊浩、韩宝亨\n\n评论：15页，9个图，3个表\n\n网址：https://arxiv.org/pdf/2506.22868.pdf\n\n标题：2025 [2506.22868] STR-Match：用于无需训练的视频编辑的时空相关性评分匹配",
        "地址": "https://arxiv.org/pdf/2506.22868.pdf"
    },
    {
        "名称": "2025 [2506.23552] JAM-Flow: Joint Audio-Motion Synthesis with Flow Matching.pdf",
        "作者": "Mingi Kwon, Joonghyuk Shin, Jaeseok Jung, Jaesik Park, Youngjung Uh",
        "摘要": "摘要：面部动作与语音之间的内在联系在生成建模中常被忽视，而在生成说话头部和语音合成（TTS）时，它们通常作为独立任务处理。本文介绍了JAM-Flow，一个统一框架，能够同时合成和处理面部动作和语音。我们的方法利用流匹配和一种新颖的多模态扩散变压器（MM-DiT）架构，集成了专门的 Motion-DiT 和 Audio-DiT 模块。这些模块通过选择性联合注意层结合，并采用关键架构选择，如时间对齐的位置嵌入和局部联合注意掩码，以实现有效的跨模态交互，同时保留模态特定的优势。在以修复风格的目标训练下，JAM-Flow 支持广泛的条件输入，包括文本、参考音频和参考动作，从而促进从文本生成同步说话头、音频驱动的动画等任务在单一、连贯模型内的实现。JAM-Flow 通过提供一种实用的全面音视频合成解决方案，显著推进了多模态生成建模的发展。项目页面：this https URL\n\n作者：Mingi Kwon, Joonghyuk Shin, Jaeseok Jung, Jaesik Park, Youngjung Uh\n\n评论：项目页面：this https URL 审稿中。预印本已发布在 arXiv\n\n链接：https://arxiv.org/pdf/2506.23552.pdf\n\n标题：JAM-Flow: Joint Audio-Motion Synthesis with Flow Matching",
        "地址": "https://arxiv.org/pdf/2506.23552.pdf"
    },
    {
        "名称": "2025 [2507.00472] ARIG: Autoregressive Interactive Head Generation for Real-time Conversations.pdf",
        "作者": "Ying Guo, Xi Liu, Cheng Zhen, Pengfei Yan, Xiaoming Wei",
        "摘要": "摘要: 面对面交流作为一种普遍的人类活动，激发了对互动头部生成研究的兴趣。一个虚拟代理能够基于另一用户及其自身的音频或运动信号生成既能听又能说的动作响应。然而，之前的分段生成模式或显式的听者/说话者生成器切换方法存在对未来信号获取、上下文行为理解和切换流畅度的局限性，使其难以实现实时和真实性。在本文中，我们提出了一种基于自回归（AR）的逐帧框架ARIG，以实现具有更好交互真实性的实时生成。为了实现实时生成，我们将运动预测建模为非矢量量化的AR过程。与离散代码书索引预测不同，我们通过扩散过程来表示运动分布，从而在连续空间中实现更准确的预测。为了提高交互真实性，我们强调互动行为理解（IBU）和详细对话状态理解（CSU）。在IBU中，基于双色双模信号，我们通过双向整合学习总结短程行为，并在长距离内进行上下文理解。在CSU中，我们使用声音活动信号和IBU的上下文特征来理解实际对话中存在的各种状态（打断、反馈、暂停等）。这些作为最终逐步运动预测的条件。大量实验验证了我们模型的有效性。",
        "地址": "https://arxiv.org/pdf/2507.00472.pdf"
    },
    {
        "名称": "2025 [2507.02856] Answer Matching Outperforms Multiple Choice for Language Model Evaluation.pdf",
        "作者": "Nikhil Chandak, Shashwat Goel, Ameya Prabhu, Moritz Hardt, Jonas Geiping",
        "摘要": "摘要：选择题评估一直是语言模型评估的主要方法，因为它的评分客观且易于自动化。然而，我们发现流行基准中的选择题常常在不看问题的情况下就能回答。这些捷径源自判别性评估的一个基本局限性，而这种局限性在评估模型的自由生成答案时并不存在。直到最近，选择题似乎没有可行的、可扩展的替代方案——但我们表明这种情况已经改变。我们通过提出答案匹配的方法来考虑生成评估：给候选模型一个问题，不提供选项，让它生成一个自由形式的回答，然后使用现代语言模型与参考答案来确定回答是否匹配参考答案。为了比较不同评估策略的有效性，我们注释了MMLU-Pro和GPQA-Diamond以获得人工评分数据，并衡量每种评估方法的一致性。我们发现，即使使用较小的最新模型，答案匹配也能达到接近完美的一致性，与人工评审的一致性相当。相比之下，无论是选择题评估还是使用大型语言模型作为评审但不提供参考答案，与人工评分的对齐度均较差。通过答案匹配改进评估不仅仅是一个概念问题：在使用答案匹配评估模型的自由形式回答时，几个模型的排名发生了显著变化。鉴于这些发现，我们讨论了如何将评估生态系统从选择题转移到答案匹配。",
        "地址": "https://arxiv.org/pdf/2507.02856.pdf"
    }
]