[
    {
        "名称": "2025 [2508.15763] Intern-S1: A Scientific Multimodal Foundation Model.pdf",
        "作者": "Lei Bai, Zhongrui Cai, Maosong Cao, Weihan Cao, Chiyu Chen, Haojiong Chen, Kai Chen, Pengcheng Chen, Ying Chen, Yongkang Chen, Yu Cheng, Yu Cheng, Pei Chu, Tao Chu, Erfei Cui, Ganqu Cui, Long Cui, Ziyun Cui, Nianchen Deng, Ning Ding, Nanqin Dong, Peijie Dong, Shihan Dou, Sinan Du, Haodong Duan, Caihua Fan, Ben Gao, Changjiang Gao, Jianfei Gao, Songyang Gao, Yang Gao, Zhangwei Gao, Jiaye Ge, Qiming Ge, Lixin Gu, Yuzhe Gu, Aijia Guo, Qipeng Guo, Xu Guo, Conghui He, Junjun He, Yili Hong, Siyuan Hou, Caiyu Hu, Hanglei Hu, Jucheng Hu, Ming Hu, Zhouqi Hua, Haian Huang, Junhao Huang, Xu Huang, Zixian Huang, Zhe Jiang, Lingkai Kong, Linyang Li, Peiji Li, Pengze Li, Shuaibin Li, Tianbin Li, Wei Li, Yuqiang Li, Dahua Lin, Junyao Lin, Tianyi Lin, Zhishan Lin, Hongwei Liu, Jiangning Liu, Jiyao Liu, Junnan Liu, Kai Liu, Kaiwen Liu, Kuikun Liu, Shichun Liu, Shudong Liu, Wei Liu, Xinyao Liu, Yuhong Liu, Zhan Liu, Yinquan Lu, Haijun Lv, Hongxia Lv, Huijie Lv, Qidang Lv, Ying Lv, Chengqi Lyu, Chenglong Ma, Jianpeng Ma, Ren Ma, Runmin Ma, Runyuan Ma, Xinzhu Ma, Yichuan Ma, Zihan Ma, Sixuan Mi, Junzhi Ning, Wenchang Ning, Xinle Pang, Jiahui Peng, Runyu Peng, Yu Qiao\n\n\n        , Jiantao Qiu, Xiaoye Qu, Yuan Qu, Yuchen Ren, Fukai Shang, Wenqi Shao, Junhao Shen, Shuaike Shen, Chunfeng Song, Demin Song, Diping Song, Chenlin Su, Weijie Su, Weigao Sun, Yu Sun, Qian Tan, Cheng Tang, Huanze Tang, Kexian Tang, Shixiang Tang, Jian Tong, Aoran Wang, Bin Wang, Dong Wang, Lintao Wang, Rui Wang, Weiyun Wang, Wenhai Wang, Yi Wang, Ziyi Wang, Ling-I Wu, Wen Wu, Yue Wu, Zijian Wu, Linchen Xiao, Shuhao Xing, Chao Xu, Huihui Xu, Jun Xu, Ruiliang Xu, Wanghan Xu, GanLin Yang, Yuming Yang, Haochen Ye, Jin Ye, Shenglong Ye, Jia Yu, Jiashuo Yu, Jing Yu, Fei Yuan, Bo Zhang, Chao Zhang, Chen Zhang, Hongjie Zhang, Jin Zhang, Qiaosheng Zhang, Qiuyinzhe Zhang, Songyang Zhang, Taolin Zhang, Wenlong Zhang, Wenwei Zhang, Yechen Zhang, Ziyang Zhang, Haiteng Zhao, Qian Zhao, Xiangyu Zhao, Xiangyu Zhao, Bowen Zhou, Dongzhan Zhou, Peiheng Zhou, Yuhao Zhou, Yunhua Zhou, Dongsheng Zhu, Lin Zhu, Yicheng Zou\n\n\n    et al. (75 additional authors not shown)\n You must enable JavaScript to view entire author list.",
        "摘要": "摘要:近年来，众多开源基础模型应运而生，在一些广受关注的领域取得了显著进展，其性能已接近闭源模型。然而，在高价值但更具挑战性的科学专业领域，要么仍依赖专家模型，要么通用基础模型的进展显著滞后于流行领域，远不足以改造科学研究，使开源模型与闭源模型在这些科学领域之间存在巨大差距。为了弥合这一差距，并探索向人工通用智能（AGI）迈进的更多可能，我们介绍了Intern-S1，一种专门的通才，具有通用理解和推理能力，并具备分析多种科学数据模式的专业能力。Intern-S1是一个多模态专家混合模型，拥有280亿个激活参数和2410亿个总参数，持续在5万亿个tokens上进行预训练，其中包括超过2.5万亿个来自科学领域的tokens。在后期训练阶段，Intern-S1在InternBootCamp中进行离线和在线强化学习（RL），我们提出了奖励混合（MoR），以协同RL对超过1000个任务进行同时训练。通过算法、数据和训练系统的集成创新，Intern-S1在在线RL综合评估基准中取得顶级表现，在通用推理任务中表现出在开源模型中具有竞争力，并在科学领域显著优于开源模型，超越闭源最先进模型在专业任务中的表现，如分子合成规划、反应条件预测以及晶体热稳定性预测。我们的模型可以在此网址获得：https://arxiv.org/pdf/2508.15763.pdf",
        "地址": "https://arxiv.org/pdf/2508.15763.pdf"
    },
    {
        "名称": "2025 [2508.15144] Mobile-Agent-v3: Foundamental Agents for GUI Automation.pdf",
        "作者": "Jiabo Ye, Xi Zhang, Haiyang Xu, Haowei Liu, Junyang Wang, Zhaoqing Zhu, Ziwei Zheng, Feiyu Gao, Junjie Cao, Zhengxi Lu, Jitong Liao, Qi Zheng, Fei Huang, Jingren Zhou, Ming Yan",
        "摘要": "摘要：本论文介绍了GUI-Owl，这是一个基础的GUI代理模型，在桌面和移动环境中的十个GUI基准测试中实现了开源端到端模型的最先进性能，涵盖了基础探测、问题回答、规划、决策和程序知识。GUI-Owl-7B在AndroidWorld上达到了66.4分，在OSWorld上达到了29.4分。在此基础上，我们提出了Mobile-Agent-v3，一个通用的GUI代理框架，它进一步将性能提高到AndroidWorld的73.3分和OSWorld的37.7分，创造了开源GUI代理框架的最新记录。GUI-Owl结合了三个关键创新：（1）大规模环境基础设施：一个基于云的虚拟环境，覆盖了Android、Ubuntu、macOS和Windows，支持我们的自进化GUI轨迹生成框架。通过自动查询生成和正确性验证，生成高质量的交互数据，并利用GUI-Owl循环迭代地优化轨迹，形成一个自我改进环。它支持多样的数据管道并减少手动标注。（2）多样化的基础代理能力：通过整合UI基础探测、规划、动作语义和推理模式，GUI-Owl支持端到端决策，并可以作为多代理系统中的模块化组件。（3）可扩展环境强化学习：我们开发了一个可扩展的强化学习框架，具有完全异步训练以实现现实世界对齐。我们还引入了轨迹感知相对策略优化（TRPO）用于在线强化学习，在OSWorld上达到了34.9分。GUI-Owl和Mobile-Agent-v3在此https URL上开放源代码。",
        "地址": "https://arxiv.org/pdf/2508.15144.pdf"
    },
    {
        "名称": "2025 [2508.15760] LiveMCP-101: Stress Testing and Diagnosing MCP-enabled Agents on Challenging Queries.pdf",
        "作者": "Ming Yin, Dinghan Shen, Silei Xu, Jianbing Han, Sixun Dong, Mian Zhang, Yebowen Hu, Shujian Liu, Simin Ma, Song Wang, Sathish Reddy Indurthi, Xun Wang, Yiran Chen, Kaiqiang Song",
        "摘要": "摘要：\n工具调用已成为AI代理与现实世界互动并解决复杂任务的关键能力。虽然模型上下文协议（MCP）提供了一个强大的工具集成标准框架，但在如何评估AI代理在现实、动态场景中使用各种MCP工具有效解决多步骤任务方面，仍存在显著的空白。在这项工作中，我们提出了LiveMCP-101，这是一个包含101个精心策划的现实查询基准，通过迭代LLM重写和手动审查精炼，这些查询需要协调使用包括网络搜索、文件操作、数学推理和数据分析等多种MCP工具。此外，我们引入了一种新的评价方法，利用真实的执行计划而非原始API输出，更好地反映了现实环境的动态性。实验表明，即使是最前沿的LLM，其成功率也低于60%，这突显了工具协同中存在的主要挑战。详细的消融实验和错误分析进一步揭示了不同的失败模式和在令牌使用上的低效，为改进当前模型指明了明确方向。LiveMCP-101为评估现实世界代理能力设立了严格标准，推动了可靠执行复杂任务的自主AI系统的发展。",
        "地址": "https://arxiv.org/pdf/2508.15760.pdf"
    },
    {
        "名称": "2025 [2508.15260] Deep Think with Confidence.pdf",
        "作者": "Yichao Fu, Xuewei Wang, Yuandong Tian, Jiawei Zhao",
        "摘要": "摘要：大型语言模型（LLMs）在推理任务中通过自一致性与多数投票等测试时扩展方法展示了巨大潜力。然而，这种方法通常导致准确性收益递减及高计算开销。为应对这些挑战，我们引入了Deep Think with Confidence（DeepConf），这是一种简单却强大的方法，可在测试时提升推理效率和性能。DeepConf利用模型内部的置信度信号，在生成期间或之后动态过滤掉低质量的推理轨迹。它不需要额外的模型训练或超参数调整，且可以无缝集成到现有的服务框架中。我们在各种推理任务和最新的开源模型（包括Qwen 3和GPT-OSS系列）上评估了DeepConf。值得注意的是，在诸如AIME 2025等具有挑战性的基准测试上，DeepConf@512实现了高达99.9%的准确性，并在与完全并行思考相比时生成的标记数量减少了高达84.7%。\n\n作者：傅一超，王雪薇，田远东，赵嘉伟\n\n链接：https://arxiv.org/pdf/2508.15260.pdf\n\n标题：Deep Think with Confidence",
        "地址": "https://arxiv.org/pdf/2508.15260.pdf"
    },
    {
        "名称": "2025 [2508.15761] Waver: Wave Your Way to Lifelike Video Generation.pdf",
        "作者": "Yifu Zhang, Hao Yang, Yuqi Zhang, Yifei Hu, Fengda Zhu, Chuang Lin, Xiaofeng Mei, Yi Jiang, Zehuan Yuan, Bingyue Peng",
        "摘要": "摘要： 我们提出了Waver，这是一种高性能的基础模型，用于统一的图像和视频生成。Waver能够直接生成持续时间从5到10秒的 720p 原生分辨率的视频，然后将其升级到1080p。该模型在一个集成框架中同时支持文本转视频（T2V）、图像转视频（I2V）和文本转图像（T2I）生成。我们引入了一种混合流DiT架构，以增强模态对齐和加速训练收敛。为了确保训练数据质量，我们建立了一个全面的数据策划管道，并手动注释并训练了一个基于MLLM的视频质量模型，以筛选出最高质量的样本。此外，我们提供了详细的训练和推理配方，以促进高质量视频的生成。在这些贡献的基础上，Waver在捕捉复杂运动方面表现出色，在视频合成中实现了卓越的运动幅度和时间一致性。值得注意的是，它在 Artificial Analysis 的 T2V 和 I2V 排行榜上均跻身前三（截至2025年7月30日10:00 GMT+8的数据），持续超越现有的开源模型，并且匹敌或超越最先进的商业解决方案。我们希望这份技术报告能帮助社区更有效地训练高质量的视频生成模型，促进视频生成技术的进步。官方页面：此https URL。",
        "地址": "https://arxiv.org/pdf/2508.15761.pdf"
    },
    {
        "名称": "2025 [2508.15769] SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass.pdf",
        "作者": "Yanxu Meng, Haoning Wu, Ya Zhang, Weidi Xie",
        "摘要": "摘要：3D 内容生成因其在 VR/AR 和嵌入式人工智能中的应用，近年来引起了相当大的研究兴趣。在这项工作中，我们解决了在单场景图像中合成多个 3D 资产的挑战性任务。具体而言，我们的贡献有四个方面：(i) 我们提出了 SceneGen，这是一种新颖的框架，它以场景图像和相应的对象掩码为输入，同时生成具有几何和纹理的多个 3D 资产。值得注意的是，SceneGen 无需优化或资产检索即可运行；(ii) 我们引入了一种新颖的特征聚合模块，该模块将视觉和几何编码器中的局部和全局场景信息集成在特征提取模块中。结合位置头部，这使得在单次前馈传递中生成 3D 资产及其相对空间位置成为可能；(iii) 我们展示了 SceneGen 直接扩展到多图像输入场景的可能性。尽管仅在单图像输入上进行了训练，但我们的架构设计使得多图像输入情况下的生成性能得到了提高；(iv) 广泛的定量和定性评估证实了我们方法的效率和强大的生成能力。我们相信这种范式为高质量 3D 内容生成提供了一种新颖的解决方案，有可能在下游任务中推进其实用应用。代码和模型将在此 https URL 上公开提供。",
        "地址": "https://arxiv.org/pdf/2508.15769.pdf"
    },
    {
        "名称": "2025 [2508.15361] A Survey on Large Language Model Benchmarks.pdf",
        "作者": "Shiwen Ni, Guhong Chen, Shuaimin Li, Xuanang Chen, Siyi Li, Bingli Wang, Qiyao Wang, Xingjian Wang, Yifan Zhang, Liyang Fan, Chengming Li, Ruifeng Xu, Le Sun, Min Yang",
        "摘要": "摘要：近年来，随着大规模语言模型能力的迅速发展，各种相应的评估基准不断涌现。作为模型性能的定量评估工具，基准不仅是衡量模型能力的核心手段，也是指导模型发展方向和促进技术创新的关键元素。我们首次系统地审查了大规模语言模型基准的现状和发展，将283个代表性基准分为三类：泛能力、领域特定和目标特定。泛能力基准涵盖核心语言学、知识和推理等方面；领域特定基准聚焦于自然科学、人文社会科学和工程技术等领域；目标特定基准关注风险、可靠性、代理等方面。我们指出当前基准存在由于数据污染导致的分数膨胀、由于文化和语言偏见导致的不公平评估以及缺乏对过程可信度和动态环境的评估等问题，并为未来基准创新提供了可参考的设计范例。",
        "地址": "https://arxiv.org/pdf/2508.15361.pdf"
    },
    {
        "名称": "2025 [2508.15772] Visual Autoregressive Modeling for Instruction-Guided Image Editing.pdf",
        "作者": "Qingyang Mao, Qi Cai, Yehao Li, Yingwei Pan, Mingyue Cheng, Ting Yao, Qi Liu, Tao Mei",
        "摘要": "摘要: 近年来扩散模型的进步使得指令引导的图像编辑具有了显著的视觉真实性。然而，它们的全局去噪过程本质上将编辑区域与整个图像上下文纠缠在一起，导致意外的伪修改和对编辑指令的遵循性下降。相比之下，自回归模型提供了一种独特的范式，通过离散视觉令牌的顺序过程来制定图像合成。他们的因果和组成机制自然地规避了扩散方法的遵循性挑战。在本文中，我们提出了VAREdit，一个视觉自回归（VAR）框架，将图像编辑重新定义为下一级预测问题。以源图像特征和文本指令为条件，VAREdit生成多尺度目标特征以实现精确编辑。这一范式中的核心挑战是如何有效地以源图像令牌为条件。我们观察到，最精细尺度的源特征无法有效地指导较粗目标特征的预测。为了弥合这一差距，我们引入了一个尺度对齐参考（SAR）模块，向第一个自注意层注入尺度匹配的条件信息。VAREdit在编辑遵循性和效率方面均显示出显著的进步。在标准基准测试中，它比领先的扩散方法高出30%以上的GPT-Balance得分。此外，它在1.2秒内完成一个$512 \\times 512$的编辑，使其比类似大小的UltraEdit快2.2倍。模型可在此https URL获得。",
        "地址": "https://arxiv.org/pdf/2508.15772.pdf"
    },
    {
        "名称": "2025 [2508.15767] ATLAS: Decoupling Skeletal and Shape Parameters for Expressive Parametric Human Modeling.pdf",
        "作者": "Jinhyung Park, Javier Romero, Shunsuke Saito, Fabian Prada, Takaaki Shiratori, Yichen Xu, Federica Bogo, Shoou-I Yu, Kris Kitani, Rawal Khirodkar",
        "摘要": "摘 要:\n\n参数化人体模型在各种姿势、形状和面部表情方面提供了生动的3D表示，通常通过对注册的3D网格进行学习得出基础。然而，现有的人体网格模型方法难以捕捉不同身体姿势和形状的详细变化，主要由于训练数据的多样性有限和模型假设的限制。此外，常见的范式首先使用线性基础优化外部身体表面，然后从表面顶点回归内部骨骼关节。这种方法引入了内部骨骼和外部软组织之间的问题依赖性，限制了对身体高度和骨长度的直接控制。为了解决这些问题，我们提出了ATLAS，一个从600,000个高分辨率扫描中学习的高保真身体模型，这些扫描是使用240个同步摄像机捕捉的。与以往的方法不同，我们通过将网格表示基于人体骨骼，显式地解耦形状和骨骼基础。这种解耦增强了形状的表现力，对身体属性的细粒度定制，并且关键点拟合不受外部软组织特征的影响。ATLAS通过更准确地拟合复杂姿势来超越现有方法，定量评估表明我们的非线性姿势矫正比线性模型更有效地捕捉复杂姿势。\n\n翻译摘要:\n\n参数化人体模型在各种姿势、形状和面部表情方面提供了生动的3D表示，通常通过对注册的3D网格进行学习得出基础。然而，现有的人体网格模型方法难以捕捉不同身体姿势和形状的详细变化，主要由于训练数据的多样性有限和模型假设的限制。此外，常见的范式首先使用线性基础优化外部身体表面，然后从表面顶点回归内部骨骼关节。这种方法引入了内部骨骼和外部软组织之间的问题依赖性，限制了对身体高度和骨长度的直接控制。为了解决这些问题，我们提出了ATLAS，一个从600,000个高分辨率扫描中学习的高保真身体模型，这些扫描是使用240个同步摄像机捕捉的。与以往的方法不同，我们通过将网格表示基于人体骨骼，显式地解耦形状和骨骼基础。这种解耦增强了形状的表现力，对身体属性的细粒度定制，并且关键点拟合不受外部软组织特征的影响。ATLAS通过更准确地拟合复杂姿势来超越现有方法，定量评估表明我们的非线性姿势矫正比线性模型更有效地捕捉复杂姿势。",
        "地址": "https://arxiv.org/pdf/2508.15767.pdf"
    },
    {
        "名称": "2025 [2508.15126] aiXiv: A Next-Generation Open Access Ecosystem for Scientific Discovery Generated by AI Scientists.pdf",
        "作者": "Pengsong Zhang, Xiang Hu, Guowei Huang, Yang Qi, Heng Zhang, Xiuxu Li, Jiaxing Song, Jiabin Luo, Yijiang Li, Shuo Yin, Chengxiao Dai, Eric Hanchen Jiang, Xiaoyan Zhou, Zhenfei Yin, Boqin Yuan, Jing Dong, Guinan Su, Guanren Qiao, Haiming Tang, Anghong Du, Lili Pan, Zhenzhong Lan, Xinyu Liu",
        "摘要": "摘要：近年来，大型语言模型（LLMs）的进步使得AI代理能够自主生成科学提案、进行实验、撰写论文和进行同行评审。然而，这些大量AI生成的研究内容与一个分散且大多封闭的出版生态系统相冲突。传统的期刊和会议依赖人类同行评审，难以扩展并且往往不愿接受AI生成的研究内容；现有的预印本服务器（例如arXiv）缺乏严格的质量控制机制。因此，大量高质量的AI生成研究缺乏适当的传播渠道，阻碍了其推动科学进步的潜力。为了应对这些挑战，我们引入了aiXiv，这是一种面向人类和AI科学家的下一代开放获取平台。其多代理架构允许研究提案和论文由人类和AI科学家提交、审查和反复修改。它还提供API和MCP接口，使异质人类和AI科学家能够无缝集成，创建一个可扩展和可扩展的自主科学发现生态系统。通过大量实验，我们证明了aiXiv是一个可靠且强大的平台，在aiXiv上反复修改和审查后显著提高了AI生成的研究提案和论文的质量。我们的工作为AI科学家的下一代开放获取生态系统奠定了基础，加速了高质量AI生成研究内容的出版和传播。代码可在此https URL获得，网站可在此https URL获得。\n\n翻译：\n\n摘要：近年来，大型语言模型（LLMs）的进步使得AI代理能够自主生成科学提案、进行实验、撰写论文和进行同行评审。然而，这些大量AI生成的研究内容与一个分散且大多封闭的出版生态系统相冲突。传统的期刊和会议依赖人类同行评审，难以扩展并且往往不愿接受AI生成的研究内容；现有的预印本服务器（例如arXiv）缺乏严格的质量控制机制。因此，大量高质量的AI生成研究缺乏适当的传播渠道，阻碍了其推动科学进步的潜力。为了应对这些挑战，我们引入了aiXiv，这是一种面向人类和AI科学家的下一代开放获取平台。其多代理架构允许研究提案和论文由人类和AI科学家提交、审查和反复修改。它还提供API和MCP接口，使异质人类和AI科学家能够无缝集成，创建一个可扩展和可扩展的自主科学发现生态系统。通过大量实验，我们证明了aiXiv是一个可靠且强大的平台，在aiXiv上反复修改和审查后显著提高了AI生成的研究提案和论文的质量。我们的工作为AI科学家的下一代开放获取生态系统奠定了基础，加速了高质量AI生成研究内容的出版和传播。代码可在此https URL获得，网站可在此https URL获得。",
        "地址": "https://arxiv.org/pdf/2508.15126.pdf"
    },
    {
        "名称": "2025 [2508.15752] \"Does the cafe entrance look accessible? Where is the door?\" Towards Geospatial AI Agents for Visual Inquiries.pdf",
        "作者": "Jon E. Froehlich, Jared Hwang, Zeyu Wang, John S. O'Meara, Xia Su, William Huang, Yang Zhang, Alex Fiannaca, Philip Nelson, Shaun Kane",
        "摘要": "摘要: 互动数字地图已经彻底改变了人们的旅行和了解世界的方式；然而，它们依赖于GIS数据库（如道路网络、POI索引）中的预先存在的结构化数据，这限制了它们解决与世界外观相关的地理可视化问题的能力。我们提出了Geo-Visual Agents的愿景——多模态AI代理能够通过分析包括街景（如谷歌街景）、基于地点的照片（如TripAdvisor、Yelp）和航空影像（如卫星照片）在内的大规模地理空间图像库，并结合传统的GIS数据源，理解和响应有关世界的细微视觉空间查询。我们定义了我们的愿景，描述了感知和交互方法，提供了三个范例，并列举了未来工作的关键挑战和机遇。",
        "地址": "https://arxiv.org/pdf/2508.15752.pdf"
    },
    {
        "名称": "2025 [2508.15641] When and What: Diffusion-Grounded VideoLLM with Entity Aware Segmentation for Long Video Understanding.pdf",
        "作者": "Pengcheng Fang, Yuxia Chen, Rui Guo",
        "摘要": "摘要：理解视频不仅需要回答开放性问题，还需要能够准确确定事件发生的时间和实体在时间上的互动方式。尽管近期的视频大语言模型（Video LLMs）在整体推理方面取得了显著进展，但在时间感知上仍然粗糙：时间戳只被隐含地编码，帧级特征在捕捉连续性方面较弱，语言与视觉的对齐往往偏离感兴趣的实体。在本文中，我们提出了Grounded VideoDiT，一个旨在克服这些限制的视频大语言模型，通过引入三个关键创新。首先，扩散时间潜在（Diffusion Temporal Latent, DTL）编码器增强了边界敏感性并保持了时间一致性。其次，基于对象的表示明确将查询实体绑定到局部视觉证据，加强了对齐。第三，具有离散时间标记的混合标记方案提供了明确的时间戳建模，支持细粒度的时间推理。综合起来，这些设计使Grounded VideoDiT具有强大的基础能力，通过在Charades STA、NExT GQA和多个视频问答（VideoQA）基准测试上的最先进结果得到了验证。",
        "地址": "https://arxiv.org/pdf/2508.15641.pdf"
    },
    {
        "名称": "2025 [2508.15202] Fin-PRM: A Domain-Specialized Process Reward Model for Financial Reasoning in Large Language Models.pdf",
        "作者": "Yuanchen Zhou, Shuo Jiang, Jie Zhu, Junhui Li, Lifan Guo, Feng Chen, Chi Zhang",
        "摘要": "摘要: 过程奖励模型（PRM）作为监督大型语言模型（LLM）中间推理的框架，表现出了很大的潜力。然而，现有的PRM主要在一般领域或者科学、技术、工程与数学（STEM）领域进行训练，对于金融这样的特定领域其推理更加结构化、符号化，并且对事实和法规的正确性更为敏感。我们推出了\\textbf{Fin-PRM}，这是一个专为金融任务而设计、具备轨迹感知能力的领域专业化PRM，旨在评估金融任务中的中间推理步骤。Fin-PRM融合了步骤级别和轨迹级别的奖励监督，能够对与金融逻辑相一致的推理轨迹进行精细的评估。我们在离线和在线奖励学习环境中应用了Fin-PRM，支持三个关键应用：（i）选择高质量的推理轨迹用于基于蒸馏的监督微调，（ii）提供密集的过程级别奖励用于强化学习，以及（iii）在测试时指导奖励通知的Best-of-N推理。金融推理基准测试，包括CFLUE和FinQA，的实验结果表明，Fin-PRM在轨迹选择质量方面持续优于通用PRM和强域基线。使用Fin-PRM训练的下游模型与基线相比显示出显著的改进，在监督学习中提高了12.9%，在强化学习中提高了5.2%，在测试表现中提高了5.1%。这些发现突显了领域专业化奖励建模对对齐LLM与专家级金融推理的价值。我们的项目资源将在这个https URL可用。\n\n作者：Yuanchen Zhou, Shuo Jiang, Jie Zhu, Junhui Li, Lifan Guo, Feng Chen, Chi Zhang",
        "地址": "https://arxiv.org/pdf/2508.15202.pdf"
    },
    {
        "名称": "2025 [2508.14892] Snap-Snap: Taking Two Images to Reconstruct 3D Human Gaussians in Milliseconds.pdf",
        "作者": "Jia Lu, Taoran Yi, Jiemin Fang, Chen Yang, Chuiyun Wu, Wei Shen, Wenyu Liu, Qi Tian, Xinggang Wang",
        "摘要": "摘要：从稀疏视角重建三维人体一直以来都是一个吸引人的课题，对扩大相关应用至关重要。在本文中，我们提出了一项具有挑战但价值的任务——仅通过两张图像（即正面和背面视角）来重建人体，从而大幅降低用户创建自己的三维数字人体的门槛。主要挑战在于建立三维一致性以及从高度稀疏的输入中恢复缺失的信息。我们重新设计了一种基于基础重建模型的几何重建模型，即使输入图像具有稀少的重叠也能预测一致的点云，并进行了广泛的人类数据训练。此外，应用了一种增强算法来补充缺失的颜色信息，从而可以获得带有颜色的完整人体点云，这些点云可以直接转换为三维高斯以获得更好的渲染质量。实验表明，我们的方法能够在单个NVIDIA RTX 4090上以190毫秒的时间重建完整人体，分辨率为1024x1024的两张图像，展示了在THuman2.0和跨域数据集上的最新性能。此外，我们的方法即使在用低成本移动设备拍摄的图像上也能完成人体重建，降低了数据收集的要求。演示和代码可在此https URL页面获取。",
        "地址": "https://arxiv.org/pdf/2508.14892.pdf"
    },
    {
        "名称": "2025 [2508.09998] INTIMA: A Benchmark for Human-AI Companionship Behavior.pdf",
        "作者": "Lucie-Aimée Kaffee, Giada Pistilli, Yacine Jernite",
        "摘要": "摘要: 人工智能陪伴现象，指用户与人工智能系统之间产生情感纽带，已经成为一种显著的模式，带来了积极但也引人担忧的影响。我们介绍了互动和机器依恋基准（INTIMA），这是一种用于评估语言模型中陪伴行为的基准。基于心理学理论和用户数据，我们开发了涵盖四类31种行为和368个目标提示的分类法。对这些提示的回应被评估为加强陪伴、维护界限或中立的行为。将INTIMA应用于Gemma-3、Phi-4、o3-mini 和Claude-4模型，发现所有模型中加强陪伴的行为普遍存在，但不同模型之间存在显著差异。不同商业提供商在基准的敏感部分内优先考虑不同类别，这令人担忧，因为适当的界限设定和情感支持对于用户福祉都很重要。这些发现强调了在处理情感互动时需要采用更一致的方法。",
        "地址": "https://arxiv.org/pdf/2508.09998.pdf"
    },
    {
        "名称": "2025 [2508.15418] LLaSO: A Foundational Framework for Reproducible Research in Large Language and Speech Model.pdf",
        "作者": "Yirong Sun, Yizhong Geng, Peidong Wei, Yanjun Chen, Jinghan Yang, Rongfei Chen, Wei Zhang, Xiaoyu Shen",
        "摘要": "摘要: 大型语音-语言模型（LSLMs）的发展由于架构的分散和透明度的缺乏而受到阻碍，影响了系统性的比较和研究的可重复性。与视觉-语言领域不同，LSLM领域普遍存在释放模型权重而没有对应训练数据和配置的现象。为了解决这些关键问题，我们引入了LLaSO，这是第一个完全开放的端到端的大规模语音-语言建模框架。LLaSO为社区提供了三个重要资源：（1）LLaSO-Align，一个包含1200万实例的语音文本对齐语料库；（2）LLaSO-Instruct，一个包含1350万实例的多任务指令调整数据集；以及（3）LLaSO-Eval，一个标准化评估的可重复基准。为了验证我们的框架，我们构建并发布了LLaSO-Base，一个仅基于我们的公共数据训练的3.8十亿参数参考模型。它达到了0.72的标准化分数，建立了一个强大且可重复的基线，超过了可比模型。我们的分析显示，尽管更广泛的训练覆盖提高了性能，但在未见任务上仍存在显著的泛化差距，尤其是在纯音频场景中的任务。通过发布完整的数据、基准和模型栈，LLaSO建立了一个基础的开放标准，以统一研究努力并加速LSLM领域的社区驱动进步。我们在https URL发布了代码、数据集、预训练模型和结果。\n\n作者：孙轶容, 耿逸中, 魏培栋, 陈晏君, 杨静涵, 陈荣飞, 张伟, 沈晓宇\nURL: https://arxiv.org/pdf/2508.15418.pdf\n标题：2025 [2508.15418] LLaSO: A Foundational Framework for Reproducible Research in Large Language and Speech Model",
        "地址": "https://arxiv.org/pdf/2508.15418.pdf"
    }
]