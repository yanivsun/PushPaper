[
    {
        "名称": "2025 [2509.24006] SLA: Beyond Sparsity in Diffusion Transformers via Fine-Tunable Sparse-Linear Attention.pdf",
        "作者": "Jintao Zhang, Haoxu Wang, Kai Jiang, Shuo Yang, Kaiwen Zheng, Haocheng Xi, Ziteng Wang, Hongzhou Zhu, Min Zhao, Ion Stoica, Joseph E. Gonzalez, Jun Zhu, Jianfei Chen",
        "摘要": "摘要：在扩散变压器（DiT）模型中，特别是视频生成方面，由于序列长度较长和复杂度为二次方，注意力延迟是主要瓶颈。我们发现注意力权重可以分为两部分：一小部分具有高秩的大权重和剩余权重具有极低秩。这自然地建议对第一部分应用稀疏加速，对第二部分应用低秩加速。基于这一发现，我们提出了SLA（Sparse-Linear Attention），一种可训练的注意力方法，将稀疏注意力和线性注意力融合，以加速扩散模型。SLA将注意力权重分类为关键、边缘和可忽略类别，应用O(N^2)注意力于关键权重，应用O(N)注意力于边缘权重，并跳过可忽略的权重。SLA将这些计算结合到单个GPU核中，并支持正向和反向传播。使用SLA进行少量微调步骤，DiT模型可以使注意力计算减少20倍，从而显著加速生成过程而不损失生成质量。实验表明，SLA减少95%的注意力计算，而不会降低端到端生成质量，优于基准方法。此外，我们实现了SLA的高效GPU核，注意力计算加速13.7倍，视频生成端到端加速2.2倍。",
        "地址": "https://arxiv.org/pdf/2509.24006.pdf"
    },
    {
        "名称": "2025 [2509.22220] StableToken: A Noise-Robust Semantic Speech Tokenizer for Resilient SpeechLLMs.pdf",
        "作者": "Yuhan Song, Linhao Zhang, Chuhan Wu, Aiwei Liu, Wei Jia, Houfeng Wang, Xiao Zhou",
        "摘要": "摘要: 现有的语义语音分词器旨在捕捉语言内容，但其鲁棒性令人堪忧。我们发现它们对与意义无关的声学扰动不够稳健；即使在高信噪比（SNR）条件下，语音完全清晰可辨，它们的输出分词序列也可能发生显著变化，增加了下游LLM的学习负担。这种不稳定性源于两个缺陷：一个脆弱的单路径量化架构以及与中间分词稳定性无关的远距离训练信号。为了解决这个问题，我们引入了StableToken，这是一种通过共识驱动机制实现稳定的分词器。其多分支架构并行处理音频，这些表示通过强大的逐位投票机制合并形成单一稳定的分词序列。StableToken在分词稳定性上设立了新的标杆，在不同噪声条件下显著降低了单位编辑距离（UED）。这种基础稳定性直接转化为下游优势，大大提高了SpeechLLMs在各种任务上的鲁棒性。",
        "地址": "https://arxiv.org/pdf/2509.22220.pdf"
    },
    {
        "名称": "2025 [2509.23102] Multiplayer Nash Preference Optimization.pdf",
        "作者": "Fang Wu, Xu Huang, Weihao Xuan, Zhiwei Zhang, Yijia Xiao, Guancheng Wan, Xiaomin Li, Bing Hu, Peng Xia, Jure Leskovec, Yejin Choi",
        "摘要": "摘要: 人类反馈强化学习（Reinforcement learning from human feedback, RLHF）已经成为使大型语言模型（LLM）与人类偏好对齐的标准范式。然而，构建在Bradley-Terry假设基础上的奖励方法难以捕捉现实偏好的非传递性和异质性。为了解决这一问题，最近的研究将对齐问题重新定义为双人博弈中的Nash博弈，产生了基于人类反馈的Nash学习（Nash learning from human feedback, NLHF）。虽然这种视角激发了INPO、ONPO和EGPO等算法，且它们具有强有力的理论和经验保证，但它们依然基本上限制在双人互动中，形成了单一对手偏差，无法捕捉现实中偏好结构的复杂性。在这项工作中，我们引入了多玩家Nash偏好优化（Multiplayer Nash Preference Optimization, MNPO），一种将NLHF推广到多玩家环境的新框架。它将对齐问题表述为一个n玩家游戏，其中每个策略在竞争对手群体中竞争，同时被正则化到参考模型。我们的框架在多玩家设置中建立了定义良好的Nash均衡，并扩展了对偶间隙（duality gap）的概念，以量化近似质量。我们证明MNPO继承了双人方法的均衡保证，同时实现了更丰富的竞争动态，并改进了对多样化偏好结构的覆盖。通过全面的实证评估，我们表明MNPO在指令跟随基准测试中一贯优于现有的NLHF基线，在异质标注者条件和混合策略评估场景下实现了更高的对齐质量。这些结果共同确立了MNPO作为一个将LLM与复杂、非传递性人类偏好对齐的原则性和可扩展框架。代码可在此URL访问。",
        "地址": "https://arxiv.org/pdf/2509.23102.pdf"
    },
    {
        "名称": "2025 [2509.24897] RealUnify: Do Unified Models Truly Benefit from Unification? A Comprehensive Benchmark.pdf",
        "作者": "Yang Shi, Yuhao Dong, Yue Ding, Yuran Wang, Xuanyu Zhu, Sheng Zhou, Wenting Liu, Haochen Tian, Rundong Wang, Huanqian Wang, Zuyan Liu, Bohan Zeng, Ruizhe Chen, Qixun Wang, Zhuoran Zhang, Xinlong Chen, Chengzhuo Tong, Bozhou Li, Chaoyou Fu, Qiang Liu, Haotian Wang, Wenjing Yang, Yuanxing Zhang, Pengfei Wan, Yi-Fan Zhang, Ziwei Liu",
        "摘要": "摘要: 将视觉理解和生成整合到统一的多模态模型中，代表了通用人工智能的重要进步。然而，现有的基准测试无法回答一个基本问题：这种架构的统一是否真的能够使组成能力之间产生协同作用？现有的评估范式主要评估理解和生成的独立能力，无法确定统一模型是否可以利用其理解来增强生成，或者使用生成模拟来促进更深层次的理解。为了解决这一关键差距，我们引入了RealUnify，这是一个专门设计用于评估双向能力协同作用的基准测试。RealUnify包括1000个由人类精心注释的实例，涵盖10个类别和32个子任务。它围绕两个核心轴心结构：1）理解增强生成，需要通过推理（例如常识、逻辑）来指导图像生成；2）生成增强理解，需要进行心理模拟或重建（例如对经过变形或混乱的视觉输入）来解决推理任务。一个重要贡献是我们的双重评估协议，它结合了直接端到端评估与诊断分步评估，将任务分解成独立的理解和生成阶段。这一协议使我们能够准确辨别性能瓶颈是否源于核心能力的缺陷或集成失败。通过对12个领先的统一模型和6个专业基准的广泛评估，我们发现当前的统一模型仍难以实现有效的协同作用，表明仅架构统一是不够的。这些结果突显了需要新的训练策略和归纳偏见，以充分释放统一建模的潜力。",
        "地址": "https://arxiv.org/pdf/2509.24897.pdf"
    },
    {
        "名称": "2025 [2509.24900] OpenGPT-4o-Image: A Comprehensive Dataset for Advanced Image Generation and Editing.pdf",
        "作者": "Zhihong Chen, Xuehai Bai, Yang Shi, Chaoyou Fu, Huanyu Zhang, Haotian Wang, Xiaoyan Sun, Zhang Zhang, Liang Wang, Yuanxing Zhang, Pengfei Wan, Yi-Fan Zhang",
        "摘要": "摘要：统一多模态模型的图像生成和编辑性能在根本上受到其训练数据质量和全面性的限制。现有数据集涵盖了诸如风格转换和简单对象处理等基本任务，但通常缺乏系统结构和用于实际应用的复杂场景。为解决这一瓶颈，我们引入了OpenGPT-4o-Image，这是一个大规模数据集，使用一种新方法构建，结合了层次任务分类法与自动化数据生成。我们的分类法不仅包含了诸如文字渲染和风格控制等基本能力，还引入了如化学插图科学图像和需要同时执行多个操作的复杂指令编辑等高实用性且具有挑战性的类别。通过利用结构化资源池和GPT-4o的自动化流水线，我们生成了80,000个质量控制良好的指令-图像对，涵盖11个主要领域和51个子任务。广泛的实验表明，在我们数据集上微调领先模型在多个基准上实现了显著的性能提升，在编辑任务上提升了最高达18%（UniWorld-V1在ImgEdit-Bench），生成任务上提升了13%（Harmon在GenEval）。我们的工作表明系统化的数据构建是提升多模态AI能力的关键。",
        "地址": "https://arxiv.org/pdf/2509.24900.pdf"
    },
    {
        "名称": "2025 [2509.23808] Beyond the Exploration-Exploitation Trade-off: A Hidden State Approach for LLM Reasoning in RLVR.pdf",
        "作者": "Fanding Huang, Guanbo Huang, Xiao Fan, Yi He, Xiao Liang, Xiao Chen, Qinting Jiang, Faisal Nadeem Khan, Jingyan Jiang, Zhi Wang",
        "摘要": "摘要：在可验证奖励强化学习（RLVR）领域，一种普遍观点是通过探索-利用权衡来解释最近的进展，这主要是由标记级指标所塑造的视角。我们重新审视这一视角，提出这种权衡可能并不是一个根本的约束，而是测量水平的产物。为此，我们将分析转向语义丰富的隐藏状态空间，采用有效秩（ER）来量化探索，并提出其新颖的一阶和二阶导数，分别命名为有效秩速度（ERV）和有效秩加速度（ERA），以捕捉利用动态。我们的分析表明，在隐藏状态层面，探索和利用可以被解耦（第4节）。这一发现揭示了同时增强这两种能力的机会。这一见解激发了我们的方法，即速度利用秩学习（VERL），这是第一个通过直接塑造RL优势函数来实现协同探索-利用增强原则的方法。其关键创新在于利用理论上稳定的ERA作为预测元控制器，以创建协同的双通道激励结构。与其强制权衡，VERL前瞻性地放大探索奖励以预防过度自信，并加强利用性收益以巩固推理。在不同的大型语言模型和推理基准上的实验显示了一致的收益，包括在具有挑战性的Gaokao 2024数据集上绝对精度提高了21.4%。\n\n作者：黄凡定，黄观博，范晓，何奕，梁晓，陈晓，蒋沁婷，费萨尔·纳迪姆·汗，蒋敬岩，王智\n\n链接：https://arxiv.org/pdf/2509.23808.pdf\n\n标题：超越探索-利用权衡：RLVR中LLM推理的隐藏状态方法",
        "地址": "https://arxiv.org/pdf/2509.23808.pdf"
    },
    {
        "名称": "2025 [2509.24695] SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer.pdf",
        "作者": "Junsong Chen, Yuyang Zhao, Jincheng Yu, Ruihang Chu, Junyu Chen, Shuai Yang, Xianbang Wang, Yicheng Pan, Daquan Zhou, Huan Ling, Haozhe Liu, Hongwei Yi, Hao Zhang, Muyang Li, Yukang Chen, Han Cai, Sanja Fidler, Ping Luo, Song Han, Enze Xie",
        "摘要": "摘要: 我们介绍了SANA-Video，一种可以高效生成高达720x1280分辨率和分钟级长度视频的小型扩散模型。SANA-Video能够在非常快的速度下合成高分辨率、高质量且长时间的视频，并且可以在RTX 5090 GPU上部署。我们的高效、有效和长视频生成依赖于两种核心设计：(1) 线性DiT: 我们利用线性注意力作为核心操作，比起视频生成过程中处理大量tokens的普通注意力更高效。 (2) 块线性注意力的恒定内存KV缓存：我们通过采用恒定内存状态设计块状自回归方法来生成长视频，该内存状态源于线性注意力的累积特性。这个KV缓存为线性DiT提供了固定内存成本的全局上下文，消除了传统KV缓存的需求，能够高效地生成分钟级视频。此外，我们还探索了有效的数据过滤和模型训练策略，将训练成本缩减到在64个H100 GPU上的12天，仅为MovieGen成本的1%。由于成本低，SANA-Video在与现代最先进的小型扩散模型（例如Wan 2.1-1.3B和SkyReel-V2-1.3B）的竞争表现中具有优势，同时测量延迟快16倍。此外，SANA-Video可以在RTX 5090 GPU上使用NVFP4精度进行部署，将生成一个5秒720p视频的推理速度从71秒加速到29秒（提升2.4倍）。综上所述，SANA-Video实现了低成本、高质量的视频生成。",
        "地址": "https://arxiv.org/pdf/2509.24695.pdf"
    },
    {
        "名称": "2025 [2509.25190] Visual Jigsaw Post-Training Improves MLLMs.pdf",
        "作者": "Penghao Wu, Yushan Zhang, Haiwen Diao, Bo Li, Lewei Lu, Ziwei Liu",
        "摘要": "摘要：基于强化学习的后训练最近成为增强多模态大型语言模型（MLLMs）对齐和推理能力的强大范式。尽管以视觉为中心的后训练对增强MLLMs理解视觉信号的内在能力至关重要，但当前的后训练范式主要以文本为中心，仅利用密集的视觉输入提取稀疏线索进行基于文本的推理。虽然在这方面存在一些方法，但它们通常仍依赖于文本作为中间媒介或引入额外的视觉生成设计。在这项工作中，我们引入了Visual Jigsaw，一种旨在加强MLLMs视觉理解的通用自监督后训练框架。Visual Jigsaw被设计为一个通用的排序任务：将视觉输入进行分割、打乱，模型必须通过生成自然语言中的正确排列来重构视觉信息。这自然与可验证奖励的强化学习（RLVR）对齐，不需要额外的视觉生成组件，并且无需任何注释即可自动获取监督信号。我们在图像、视频和3D数据三种视觉模态中实现了Visual Jigsaw。大量实验表明，该方法在细粒度感知、时间推理和3D空间理解方面有显著提升。我们的研究结果强调了自监督视觉任务在后训练MLLMs中的潜力，并旨在激发更多关于以视觉为中心的预训练任务设计的研究。项目页面：此https URL",
        "地址": "https://arxiv.org/pdf/2509.25190.pdf"
    },
    {
        "名称": "2025 [2509.23426] Democratizing AI scientists using ToolUniverse.pdf",
        "作者": "Shanghua Gao, Richard Zhu, Pengwei Sui, Zhenglun Kong, Sufian Aldogom, Yepeng Huang, Ayush Noori, Reza Shamji, Krishna Parvataneni, Theodoros Tsiligkaridis, Marinka Zitnik",
        "摘要": "摘要：AI科学家是一种新兴的计算系统，作为发现过程中的协作伙伴，这些系统仍然难以构建，因为它们是定制的，与僵化的工作流程联系在一起，并且缺乏将工具、数据和分析统一到一个共同生态系统中的共享环境。在组学中，统一的生态系统通过实现互操作性、重用和社区驱动开发，改变了研究；AI科学家需要类似的基础设施。我们提出了ToolUniverse，这是一个用于从任何语言或推理模型构建AI科学家的生态系统，无论是开放的还是封闭的。ToolUniverse标准化了AI科学家识别和调用工具的方式，集成了600多种用于数据分析、知识检索和实验设计的机器学习模型、数据集、API和科学包。它自动优化工具接口，以确保AI科学家正确使用工具，从自然语言描述创建新工具，迭代优化工具规范，并将工具组合成自我控制的工作流程。在一个关于高胆固醇血症的案例研究中，ToolUniverse被用于创建一个AI科学家，以识别一种具有良好预测特征的药物强效类似物。开源的ToolUniverse可以在以下网址获取：https://arxiv.org/pdf/2509.23426.pdf。",
        "地址": "https://arxiv.org/pdf/2509.23426.pdf"
    },
    {
        "名称": "2025 [2509.22193] When Does Reasoning Matter? A Controlled Study of Reasoning's Contribution to Model Performance.pdf",
        "作者": "Nicolas Boizard, Hippolyte Gisserot-Boukhlef, Kevin El-Haddad, Céline Hudelot, Pierre Colombo",
        "摘要": "摘要: 具有推理能力的大型语言模型(LLMs)在广泛任务上取得了最先进的性能。尽管取得了实证成功，但推理在什么任务和模型规模上有效，以及其训练和推断成本依然未被充分探索。在这项工作中，我们依靠合成数据蒸馏框架进行大规模的监督研究。我们比较了不同规模的指令微调(IFT)和推理模型，在各种以数学为中心的任务和通用任务上，评估了多项选择题和开放式问题格式。我们的分析表明，推理一致地提高了模型性能，通常匹配或超过显著较大的IFT系统。值得注意的是，尽管IFT在训练和推断成本方面仍然是帕累托最优的，但随着模型规模的扩大，推理模型在推理密集型和开放式任务上变得越来越有价值，克服了IFT在这些任务上的性能限制。",
        "地址": "https://arxiv.org/pdf/2509.22193.pdf"
    },
    {
        "名称": "2025 [2509.25160] GSM8K-V: Can Vision Language Models Solve Grade School Math Word Problems in Visual Contexts.pdf",
        "作者": "Fan Yuan, Yuchen Yan, Yifan Jiang, Haoran Zhao, Tao Feng, Jinyan Chen, Yanwei Lou, Wenqi Zhang, Yongliang Shen, Weiming Lu, Jun Xiao, Yueting Zhuang",
        "摘要": "摘要：视觉语言模型（VLMs）实现了图像和文本的统一建模，使其能够通过感知、规划和推理来完成复杂的现实世界任务。在这些任务中，推理尤为具有代表性，而数学推理则是一个突出的例子。它突出了VLMs理解图像中的数学信息并进行复杂推理的高级能力。最近，提出了许多视觉数学推理基准，但它们通常仅限于几何学，缺乏对数学词题的覆盖，并且很少评估跨多幅图像的推理。为了解决这些问题，我们引入了GSM8K-V，一个纯粹的视觉多图像数学推理基准。GSM8K-V通过系统地将广泛使用的文本基GSM8K的每个样本映射到视觉形式来构建。通过一个精心设计的自动图像生成管道结合细致的人工注释，我们收集了1,319个高质量样本。我们在GSM8K-V上评估了大量开源和闭源模型。结果显示，尽管现有VLMs在文本基GSM8K上表现几乎饱和，但在GSM8K-V上仍有很大的改进空间。例如，表现最好的模型Gemini-2.5-Pro在GSM8K上达到了95.22%的准确率，而在GSM8K-V上只有46.93%。我们对GSM8K-V进行了全面分析，检验了当前模型的局限性以及改进的潜在方向。GSM8K-V为视觉数学推理提供了新的视角，并建立了一个基准，以指导更强大和更具普适性的VLMs的发展。\n\n作者：袁凡、严宇宸、姜亦凡、赵浩然、冯涛、陈锦延、楼延伟、张文琦、沈永亮、卢伟明、肖军、庄越挺\n\n评论：68页，6个图，项目页面：此https网址代码：此https网址数据集：此https网址\n\n网址：https://arxiv.org/pdf/2509.25160.pdf\n\n标题：2025 [2509.25160] GSM8K-V：视觉语言模型能解决视觉上下文中的小学数学词题吗？",
        "地址": "https://arxiv.org/pdf/2509.25160.pdf"
    },
    {
        "名称": "2025 [2509.23909] EditScore: Unlocking Online RL for Image Editing via High-Fidelity Reward Modeling.pdf",
        "作者": "Xin Luo, Jiahao Wang, Chenyuan Wu, Shitao Xiao, Xiyan Jiang, Defu Lian, Jiajun Zhang, Dong Liu, Zheng liu",
        "摘要": "摘要：指导下的图像编辑取得了显著进展，但目前的模型在处理复杂指令方面仍面临挑战，通常需要多个样本才能产生理想结果。尽管强化学习（RL）提供了一个有前景的解决方案，但其在图像编辑中的应用却因缺乏高保真、有效的奖励信号而受到严重阻碍。在这项工作中，我们提出了一套全面的方法来克服这一障碍，核心是开发一个最先进的、专门的奖励模型。我们首先介绍了EditReward-Bench，这是一种全面的基准，用于系统地评估编辑质量的奖励模型。在此基准之上，我们开发了EditScore，这是系列奖励模型（7B-72B）之一，用于评估指导下的图像编辑质量。通过精心的数据整理和过滤，EditScore能够有效匹配专有VLMs的性能。此外，结合一种针对EditScore生成性质的有效自集成策略，我们的最大变体甚至在基准测试中超越了GPT-5。然后，我们证明了高保真奖励模型是解锁图像编辑在线强化学习的关键。我们的实验表明，即使是最大的开源VLMs也未能提供有效的学习信号，而EditScore则使高效且稳健的策略优化成为可能。将我们的框架应用于一个强大的基础模型OmniGen2，最终模型显示出显著且一致的性能提升。总体而言，这项工作提供了从基准测试到奖励建模再到图像编辑中RL训练的首个系统路径，展示了高保真、领域专用的奖励模型是解锁RL在该领域全部潜力的关键。",
        "地址": "https://arxiv.org/pdf/2509.23909.pdf"
    },
    {
        "名称": "2025 [2509.25175] EasySteer: A Unified Framework for High-Performance and Extensible LLM Steering.pdf",
        "作者": "Haolei Xu, Xinyu Mei, Yuchen Yan, Rui Zhou, Wenqi Zhang, Weiming Lu, Yueting Zhuang, Yongliang Shen",
        "摘要": "摘要：大型语言模型（LLM）引导已成为一种有前途的范式，通过有针对性地操控隐藏状态，在推理时控制模型行为，提供了一个轻量级的替代方案，以避免昂贵的重新训练。然而，现有的引导框架存在关键的局限性：计算效率低下、可扩展性有限和功能受限，这阻碍了研究进展和实际部署。我们提出了EasySteer，这是一个建立在vLLM上的高性能、可扩展的LLM引导的统一框架。我们的系统具有模块化架构，提供基于分析和基于学习的方法的可插拔接口、细粒度的参数控制、八个应用领域的预计算引导向量和交互式演示系统。通过与vLLM优化推理引擎的深度集成，EasySteer实现了比现有框架快5.5-11.4倍的速度提升。大量实验表明其在减缓过度思考、减少幻觉等关键应用方面的有效性。EasySteer将引导从研究技术转变为生产就绪的能力，建立了可部署和可控的语言模型的关键基础设施。",
        "地址": "https://arxiv.org/pdf/2509.25175.pdf"
    },
    {
        "名称": "2025 [2509.24014] SparseD: Sparse Attention for Diffusion Language Models.pdf",
        "作者": "Zeqing Wang, Gongfan Fang, Xinyin Ma, Xingyi Yang, Xinchao Wang",
        "摘要": "摘要：虽然扩散语言模型（DLMs）提供了一个自回归模型（ARs）的有希望的替代方案，但现有的开源DLMs在推理过程中存在高延迟问题。这一瓶颈主要是由于在计算所有查询-键对时注意力的复杂性与上下文长度成平方比例。因此，为了降低这种复杂性，一个自然的策略是将注意力限制在稀疏模式上，只保留最相关的连接。在ARs中，这些方法已经成熟，注意力遵循固定且明确的稀疏模式。然而，在DLMs中，我们观察到不同的稀疏行为：（1）注意力模式在各个头之间变化，（2）每个头中的注意力模式在降噪步骤中保持高度相似，以及（3）早期的降噪步骤对于生成至关重要。这些发现使为ARs设计的稀疏注意力方法在DLMs中大部分不兼容，因为它们未能捕捉到特定头结构，并且在早期降噪步骤中应用时可能会降低生成质量。为了解决这些挑战，我们提出了一种用于DLMs的新型稀疏注意力方法SparseD。利用这些观察，SparseD仅需一次预计算头特定稀疏模式，并在所有步骤中重复使用它们，从而避免在每个降噪步骤中重新计算稀疏模式。同时，SparseD在早期步骤中使用完整注意力，随后切换到稀疏注意力以保持生成质量。这些共同使SparseD成为在长上下文应用中部署DLMs的实际高效解决方案。实验结果表明，SparseD实现了无损加速，在64k上下文长度下，经过1024个降噪步骤，达到了比FlashAttention提高1.50倍的速度。\n\n翻译：\n尽管扩散语言模型（DLMs）提供了一种有前途的自回归模型（ARs）的替代方案，但现有开源DLMs在推理时存在高延迟问题。这一瓶颈主要归因于计算所有查询-键对时注意力复杂度与上下文长度成平方关系。为降低这种复杂度，一种自然的策略是将注意力限制在仅保留最相关连接的稀疏模式上。这种方法在ARs中已经得到很好建立，注意力遵循固定且明确的稀疏模式。然而，在DLMs中，我们观察到显著差异的稀疏行为：（1）注意力模式在各头之间变化，（2）每个头中的注意力模式在每个降噪步骤中保持高度相似，以及（3）早期的降噪步骤对生成至关重要。这些发现使得ARs设计的稀疏注意力方法在DLMs中大部分不兼容，因为它们未能捕捉到特定头部结构，并且在应用于早期降噪步骤时可能会降低生成质量。为解决这些挑战，我们提出了SparseD，一种新型的稀疏注意力方法用于DLMs。利用这些发现，SparseD只需预计算一次特定头的稀疏模式，并在所有步骤中重复使用它们，从而避免在每个降噪步骤中重新计算稀疏模式。同时，SparseD在早期步骤中使用完整注意力，然后切换到稀疏注意力以维持生成质量。整体而言，这些措施使SparseD成为长上下文应用中部署DLMs的实用且高效的解决方案。实验结果表明，SparseD达到了无损加速，在64k上下文长度下，通过1024个降噪步骤，实现了比FlashAttention高1.50倍的速度。",
        "地址": "https://arxiv.org/pdf/2509.24014.pdf"
    },
    {
        "名称": "2025 [2509.24007] Sequential Diffusion Language Models.pdf",
        "作者": "Yangzhou Liu, Yue Cao, Hao Li, Gen Luo, Zhe Chen, Weiyun Wang, Xiaobo Liang, Biqing Qi, Lijun Wu, Changyao Tian, Yanting Zhang, Yuqiang Li, Tong Lu, Yu Qiao, Jifeng Dai, Wenhai Wang",
        "摘要": "摘要：扩散语言模型（DLMs）具有强大的理论效率，但受限于固定长度解码和与键值（KV）缓存的不兼容性。块扩散部分缓解了这些问题，但仍强制固定块大小并且需要昂贵的训练。我们引入了下一个序列预测（NSP），它统一了下一个标记和下一个块的预测，使模型能够自适应地确定每步的生成长度。当长度固定为1时，NSP简化为标准的下一个标记预测。在NSP的基础上，我们提出了顺序扩散语言模型（SDLM），其可以以最小成本改造预训练的自回归语言模型（ALMs）。具体而言，SDLM在固定大小的掩码块内执行扩散推理，但根据模型的置信度动态解码连续的子序列，从而保持KV缓存的兼容性，并提高对序列中不同不确定性和语义的鲁棒性。实验表明，SDLM使用仅350万训练样本即可匹敌或超越强大的自回归基线，并实现比Qwen-2.5高2.1的吞吐量。值得注意的是，SDLM-32B模型提供了更加显著的效率提升，展示了我们建模范式的强大扩展潜力。项目页面和代码链接：这个https URL",
        "地址": "https://arxiv.org/pdf/2509.24007.pdf"
    },
    {
        "名称": "2025 [2509.25106] Towards Personalized Deep Research: Benchmarks and Evaluations.pdf",
        "作者": "Yuan Liang, Jiaxian Li, Yuqing Wang, Piaohong Wang, Motong Tian, Pai Liu, Shuofei Qiao, Runnan Fang, He Zhu, Ge Zhang, Minghao Liu, Yuchen Eleanor Jiang, Ningyu Zhang, Wangchunshu Zhou",
        "摘要": "摘要：深度研究代理（DRAs）能够自主进行复杂调查并生成综合报告，展示了很强的现实世界潜力。然而，现有的评估大多依赖于封闭式基准，而开放式深度研究基准则仍然稀缺且通常忽略个性化场景。为了弥补这一空白，我们引入了个性化深度研究基准，这是第一个用于评估DRAs个性化的基准。它将10个领域的50项多样化研究任务与25个真实的用户档案配对，这些档案结合了结构化的人物属性与动态的现实世界背景，生成了250个现实的用户任务查询。为了评估系统性能，我们提出了PQR评估框架，该框架共同测量（P）个性化对齐度，（Q）内容质量和（R）事实可靠性。我们在一系列系统上的实验突出了当前处理个性化深度研究的能力和局限性。这项工作为开发和评估下一代真正个性化的人工智能研究助手奠定了严格的基础。",
        "地址": "https://arxiv.org/pdf/2509.25106.pdf"
    },
    {
        "名称": "2025 [2509.24981] Random Policy Valuation is Enough for LLM Reasoning with Verifiable Rewards.pdf",
        "作者": "Haoran He, Yuxiao Ye, Qingpeng Cai, Chen Hu, Binxing Jiao, Daxin Jiang, Ling Pan",
        "摘要": "摘要: 具有可验证奖励的强化学习（RLVR）已成为提升大型语言模型（LLMs）推理能力的一个有前景的范式。目前的方法主要依赖于诸如PPO和GRPO等策略优化框架，这些框架遵循交替评估当前策略价值并基于评估改进策略的通用策略迭代过程。尽管有效，但它们通常遭受训练不稳定和多样性崩溃的问题，需要复杂的启发式技巧和仔细的调整。我们观察到标准的数学推理RLVR可以被形式化为一个具有确定性状态转移、树状动态和二元终端奖励的特殊有限视野马尔科夫决策过程。尽管规模庞大，但其基本结构比为流行RL算法（如PPO）开发的一般控制设置更加简单，这表明现有方法中的一些复杂技术可以简化甚至省略。基于这一见解，我们证明了一个令人惊讶的结果：最优动作可以从固定均匀随机策略的Q函数中恢复，从而绕过通用策略迭代循环及其相关的启发式方法。我们引入了用于多样性推理的随机策略估值（ROVER），将这一原理转化为一个实际且可扩展的用于LLM数学推理的算法，采样动作从这些均匀策略Q值的软最大值中进行。ROVER在整个训练过程中保持多样性，允许持续探索多种有效路径。在多个基础模型和标准数学推理基准测试中，尽管与现有的复杂且强大的方法相比进行了激进简化，ROVER在品质（pass@1提高8.2分，pass@256提高16.8分）和多样性（提高17.6%）方面显示出了优异的表现。",
        "地址": "https://arxiv.org/pdf/2509.24981.pdf"
    },
    {
        "名称": "2025 [2509.22799] VideoScore2: Think before You Score in Generative Video Evaluation.pdf",
        "作者": "Xuan He, Dongfu Jiang, Ping Nie, Minghao Liu, Zhengxuan Jiang, Mingyi Su, Wentao Ma, Junru Lin, Chun Ye, Yi Lu, Keming Wu, Benjamin Schneider, Quy Duc Do, Zhuofeng Li, Yiming Jia, Yuxuan Zhang, Guo Cheng, Haozhe Wang, Wangchunshu Zhou, Qunshu Lin, Yuanxing Zhang, Ge Zhang, Wenhao Huang, Wenhu Chen",
        "摘要": "摘要: 文本生成视频的最新进展已产生越来越逼真和多样化的内容，但由于其包含视觉质量、语义对齐和物理一致性的多方面特性，评估这些视频仍然是一个根本挑战。现有的评估器和奖励模型只能提供单一的不透明分数，缺乏可解释性，或仅提供粗略分析，无法捕捉视频质量评估的综合性质。我们提出了VideoScore2，一个多维的、可解释的、与人类一致的框架，能够明确评估视觉质量、文本与视频的一致性以及物理/常识的一致性，同时生成详细的思维链条分析。我们的模型在一个包含27,168个人类标注视频的大规模数据集VideoFeedback2上进行训练，该数据集包含关于三个维度的评分和推理痕迹，使用监督微调和基于组相对策略优化（GRPO）的强化学习两阶段管道来增强分析的鲁棒性。广泛的实验表明，VideoScore2在我们的域内基准VideoScore-Bench-v2上实现了44.35（+5.94）准确性，并在四个域外基准（VideoGenReward-Bench, VideoPhy2等）中获得50.37（+4.32）的平均性能，同时提供可解释的评估，通过有效的奖励建模促使最优的N次采样，弥合评估与可控生成之间的差距。项目页面：此https URL。",
        "地址": "https://arxiv.org/pdf/2509.22799.pdf"
    },
    {
        "名称": "2025 [2509.22824] Critique-Coder: Enhancing Coder Models by Critique Reinforcement Learning.pdf",
        "作者": "Chi Ruan, Dongfu Jiang, Yubo Wang, Wenhu Chen",
        "摘要": "摘要：强化学习（Reinforcement Learning，RL）已成为一种流行的训练范式，尤其是在与推理模型结合时。虽然有效，但其主要着眼于生成响应，缺乏明确促进批评或反思的机制。最近的一些研究，如Critique-Fine-Tuning（CFT）和Critique-Guided-Distillation（CGD），展示了明确教授大型语言模型（LLMs）如何进行批评的好处。受此启发，我们提出了批评强化学习（Critique Reinforcement Learning，CRL），其中模型的任务是对给定的（问题，解决方案）对生成批评。奖励仅取决于生成的批评的最终判断标签 $c \\\\in \\\\{\\\\texttt{True}, \\\\texttt{False}\\\\}$ 是否与真实判断 $c^*$ 一致。在此基础上，我们引入了\\\\textsc{Critique-Coder}，通过用CRL数据替换20%的标准RL数据，结合RL和CRL进行训练。我们微调了多个模型（\\\\textsc{Critique-Coder}）并在不同基准上进行评估，以展示其相较于仅RL模型的优势。结果表明，\\\\textsc{Critique-Coder}在所有评估的基准上始终优于仅RL基准模型。值得注意的是，我们的\\\\textsc{Critique-Coder-8B}在LiveCodeBench（v5）上的表现超过60%，优于其他推理模型如DeepCoder-14B和GPT-o1。除了代码生成，\\\\textsc{Critique-Coder}还展示了增强的通用推理能力，这在BBEH数据集的逻辑推理任务中表现更佳。这表明在编码数据集上应用CRL提升了通用推理和批评能力，这些能力可以在广泛的任务中转移。因此，我们相信CRL作为LLM推理的一个有力补充。",
        "地址": "https://arxiv.org/pdf/2509.22824.pdf"
    },
    {
        "名称": "2025 [2509.25123] From $f(x)$ and $g(x)$ to $f(g(x))$: LLMs Learn New Skills in RL by Composing Old Ones.pdf",
        "作者": "Lifan Yuan, Weize Chen, Yuchen Zhang, Ganqu Cui, Hanbin Wang, Ziming You, Ning Ding, Zhiyuan Liu, Maosong Sun, Hao Peng",
        "摘要": "摘要：强化学习（RL）是否能够教授大型语言模型（LLMs）真正的新技能，还是仅仅激活已有的技能？这个问题是关于RL在LLM后训练中作用的持续争论的核心。一方面，即使没有之前的监督微调，也能通过RL实现强有力的实证结果；另一方面，批评者认为RL在重新权重现有推理策略之外贡献有限。本研究提供了具体证据表明LLMs在RL过程中可以通过组合现有技能来获得真正的新技能，这类似于人类获得新认知技能的核心机制之一。为了减轻数据污染和其他混杂因素，并精确控制任务复杂性，我们开发了一个合成框架进行调查。具体而言，我们定义技能为能够在给定字符串转化函数f(x)时推出输出的能力。当LLM在RL之前已经学习了函数f和g，我们的实验表明RL使其能够学习到它们以前未见过的组合h(x)=g(f(x))。此外，这种组合能力可以推广到更困难的问题，例如在RL训练中未见过的多个函数组合。令人惊讶的是，我们的实验表明在源任务上获得的组合技能可以转移到不同的目标任务。即使在目标任务上没有组合训练，只需先前的目标原子技能知识即可实现这种转移。我们的定性分析表明RL从根本上改变了模型的推理行为。相比之下，使用相同数据进行的下一Token训练则没有这些发现。我们的系统实验提供了关于LLM学习的新见解，建议首先构建具有基本技能的基础模型，然后用RL激励复杂问题的高级、可推广技能。",
        "地址": "https://arxiv.org/pdf/2509.25123.pdf"
    },
    {
        "名称": "2025 [2509.24473] Euclid's Gift: Enhancing Spatial Perception and Reasoning in Vision-Language Models via Geometric Surrogate Tasks.pdf",
        "作者": "Shijie Lian, Changti Wu, Laurence Tianruo Yang, Hang Yuan, Bin Yu, Lei Zhang, Kai Chen",
        "摘要": "摘要: 空间智能包括一系列丰富的能力，如形状的可视化和转换、物体的心理旋转、判断关系位置和包含以及估算数量感。然而，这仍然是多模态大型语言模型（MLLMs）的一个关键未解决的挑战。为了填补这一空白，我们提出将欧几里得几何问题解决作为一个替代任务。具体而言，我们精心构建了一个名为Euclid30K的多模态数据集，包含大约30K个平面和立体几何问题。为了使模型能够从这些几何问题中学习并应用欧几里得原理，我们采用了群体相对策略优化（GRPO）来微调Qwen2.5VL系列和RoboBrain2.0系列，激励模型识别形状、计数、关联实体，并使用欧几里得原理进行多步骤演绎推理。我们的实验表明，所产生的模型在四个空间推理基准（Super-CLEVR、Omni3DBench、VSI-Bench和MindCube）上实现了显著的零样本增益，而没有任何特定于任务的调整。值得注意的是，在训练了Euclid30K后，所有评估模型的VSI-Bench平均准确率从34.5%提高到40.5%，提升了5.5个百分点。其中，RoboBrain2.0-Euclid-7B 达到49.6%的准确率，超过了之前的最新模型。据我们所知，这是首次系统研究表明几何为中心的微调可以赋予视觉语言模型广泛可转移的空间技能。代码和Euclid30K数据集可以在此https URL找到。",
        "地址": "https://arxiv.org/pdf/2509.24473.pdf"
    },
    {
        "名称": "2025 [2509.22820] MMPB: It's Time for Multi-Modal Personalization.pdf",
        "作者": "Jaeik Kim, Woojin Kim, Woohyeon Park, Jaeyoung Do",
        "摘要": "摘要: 视觉个性化在智能家居和医疗等面向用户的人工智能系统中至关重要，因为需要将模型行为与用户中心概念对齐。然而，尽管最近的大型视觉-语言模型（VLMs）具有广泛的适用性，但在其适应单个用户的能力方面仍未得到深入研究。在本文中，我们介绍了MMPB，这是第一个评估VLMs个性化能力的广泛基准。MMPB包含10,000个图像-查询对，并包含111个个性化概念，分为四类：人类、动物、物体和角色，其中人类类通过偏好为基础的查询进行了丰富。我们将个性化结构化为三种主要任务类型，每种类型突出了VLMs的不同关键属性。使用包括开源和闭源模型在内的23个广泛使用的VLMs，我们通过三阶段协议评估个性化表现：概念注入、多轮对话和个性化查询。我们的研究结果表明，大多数VLMs（包括一些闭源模型）在个性化方面表现不佳，特别是在保持对话一致性、处理用户偏好和适应视觉线索方面。我们的分析揭示了VLM个性化中的挑战（如拒绝行为和长时间上下文遗忘），表明仍有巨大改进空间。通过识别这些限制并提供可扩展基准，MMPB为未来的真正个性化多模态人工智能研究提供了宝贵的见解和坚实的基础。\n\n项目页面: 此 HTTP URL",
        "地址": "https://arxiv.org/pdf/2509.22820.pdf"
    },
    {
        "名称": "2025 [2509.25191] VGGT-X: When VGGT Meets Dense Novel View Synthesis.pdf",
        "作者": "Yang Liu, Chuanchen Luo, Zimo Tang, Junran Peng, Zhaoxiang Zhang",
        "摘要": "摘要：我们研究了将3D基础模型（3DFMs）应用于密集的新视图合成（NVS）的问题。尽管在由NeRF和3DGS驱动的新视图合成方面取得了显著进展，但目前的方法仍依赖于从运动结构（SfM）中获得的精确3D属性（例如，相机姿态和点云），这在低纹理或低重叠捕获中往往缓慢且脆弱。最近的3DFMs展示了比传统流程快几个数量级的速度，并且具有在线NVS的巨大潜力。但大多数验证和结论都局限于稀疏视图设置。我们的研究揭示了简单扩展3DFMs到密集视图时遇到的两大根本障碍：显著增加的VRAM负担和不完美的输出，这些会降低对初始化敏感的3D训练。为了解决这些障碍，我们引入了VGGT-X，结合了一个内存高效的VGGT实现，可以扩展到1000+张图像，一个用于VGGT输出增强的自适应全局对齐，以及鲁棒的3DGS训练实践。大量实验表明，这些措施大幅缩小了与COLMAP初始化管道的差距，在密集的无COLMAP新视图合成和姿态估计中实现了最先进的结果。此外，我们分析了与COLMAP初始化渲染剩余差距的原因，为3D基础模型和密集NVS的未来发展提供了见解。我们的项目页面可以在这个https URL上找到。",
        "地址": "https://arxiv.org/pdf/2509.25191.pdf"
    },
    {
        "名称": "2025 [2509.25161] Rolling Forcing: Autoregressive Long Video Diffusion in Real Time.pdf",
        "作者": "Kunhao Liu, Wenbo Hu, Jiale Xu, Ying Shan, Shijian Lu",
        "摘要": "摘要: 流媒体视频生成作为互动世界模型和神经游戏引擎中的一个基本组成部分，旨在生成高质量、低延迟、时间一致性较好的长视频流。然而，大多数现有的工作都存在严重的错误积累问题，这在长时间范围内会显著降低生成的视频质量。我们设计了一种名为Rolling Forcing的新型视频生成技术，可以在长视频流生成过程中最大程度地减少错误积累。Rolling Forcing包含三个创新设计。首先，与反复采样单个帧加速错误传播的方法不同，我们设计了一种联合去噪方案，可以同时去噪多个帧，并逐步增加噪声水平。这种设计松弛了相邻帧之间严格的因果关系，有效地抑制了错误增长。其次，我们引入了注意力沉降机制到长时间范围的视频流生成任务中，这使模型可以将初始帧的关键值状态保持为全局上下文锚点，从而增强长期全局一致性。第三，我们设计了一种高效的训练算法，使广泛扩展的去噪窗口中的少步蒸馏成为可能。该算法在非重叠窗口上运行，减轻了基于自生成历史的暴露偏差。广泛的实验显示，Rolling Forcing能够在单个GPU上实时生成多分钟的视频，同时大幅减少错误累积。\n\n翻译:\n流媒体视频生成作为互动世界模型和神经游戏引擎中的一个基本组成部分，旨在生成高质量、低延迟且时间一致性较好的长视频流。然而，大多数现有的工作在长时间范围内生成视频时，存在严重的错误积累问题，显著降低了生成视频的质量。我们设计了一种名为Rolling Forcing的新型视频生成技术，可以在长视频流生成过程中最大化减少错误积累。Rolling Forcing包含三个创新设计。首先，我们设计了一种联合去噪方案，代替迭代采样单个帧的方法，通过逐步增加噪声水平同时去噪多个帧，这种设计放松了相邻帧之间严格的因果关系，有效地抑制了错误增长。其次，我们在长时间范围的视频流生成任务中引入了注意力沉降机制，使模型可以将初始帧的关键值状态保持为全局上下文锚点，从而增强了长期全局一致性。第三，我们设计了一种高效的训练算法，使广泛扩展的去噪窗口中可以进行少步蒸馏。该算法在非重叠窗口上运行，减小了基于自生成历史的暴露偏差。广泛的实验显示，Rolling Forcing技术能够实现在单个GPU上实时生成多分钟视频，并显著减少错误的累积。",
        "地址": "https://arxiv.org/pdf/2509.25161.pdf"
    },
    {
        "名称": "2025 [2509.25077] BRIDGE -- Building Reinforcement-Learning Depth-to-Image Data Generation Engine for Monocular Depth Estimation.pdf",
        "作者": "Dingning Liu, Haoyu Guo, Jingyi Zhou, Tong He",
        "摘要": "摘要：单目深度估计（MDE）是计算机视觉的基础任务。传统方法受限于数据稀缺性和质量，影响其鲁棒性。为克服这一问题，我们提出了BRIDGE，一种基于强化学习优化的深度到图像（D2I）生成框架。BRIDGE从多样化的源深度图生成了超过2000万张具备真实感且几何精确的RGB图像，每张图像都内在地配对有其真实深度值。然后我们在这个数据集上训练我们的深度估计模型，采用结合教师伪标签与真实深度的混合监督策略进行全面且稳健的训练。这种创新的数据生成和训练模式使BRIDGE能够在规模和领域多样性上取得突破，定量上以及复杂场景细节捕捉上，一致地优于现有的最先进方法，从而培养出通用且鲁棒的深度特征。代码和模型可在此[链接](https://arxiv.org/pdf/2509.25077.pdf)获取。\n\n作者：丁宁刘，郭浩宇，周静怡，何彤\n\n备注：20页，7幅图\n\n标题：2025 [2509.25077] BRIDGE -- 单目深度估计用基于强化学习深度到图像数据生成引擎",
        "地址": "https://arxiv.org/pdf/2509.25077.pdf"
    },
    {
        "名称": "2025 [2509.24663] InfLLM-V2: Dense-Sparse Switchable Attention for Seamless Short-to-Long Adaptation.pdf",
        "作者": "Weilin Zhao, Zihan Zhou, Zhou Su, Chaojun Xiao, Yuxuan Li, Yanghao Li, Yudi Zhang, Weilun Zhao, Zhen Li, Yuxiang Huang, Ao Sun, Xu Han, Zhiyuan Liu",
        "摘要": "摘要：长序列处理是现代大型语言模型的关键能力。然而，标准Transformer架构中的自注意力机制在处理长序列时面临严重的计算和内存瓶颈。虽然可训练的稀疏注意力方法提供了一个有希望的解决方案，但现有方法如NSA引入了过多的额外参数，破坏了传统的\"在短序列上预训练，在长序列上微调\"的工作流程，导致收敛缓慢且难以加速。为了克服这些限制，我们引入了稠密-稀疏可切换注意力框架，称为InfLLM-V2。InfLLM-V2是一种可训练的稀疏注意力，能够在短序列和长序列之间无缝地适应模型。具体而言，InfLLM-V2通过无参数的架构修改重新利用稠密注意力参数，保持短序列和长序列处理的一致性。此外，InfLLM-V2确保了在所有序列长度上的计算效率，通过对短输入使用稠密注意力，并平滑过渡到长序列的稀疏注意力。为了实现实际的加速，我们进一步引入了一个高效的InfLLM-V2实现，显著减少了计算开销。我们在长上下文理解和思维链推理的实验表明，InfLLM-V2比稠密注意力快4倍，同时分别保持了98.1%和99.7%的性能。基于InfLLM-V2框架，我们已经训练并开源了MiniCPM4.1（此https URL），一个混合推理模型，为研究社区提供了一个可重复的实现。",
        "地址": "https://arxiv.org/pdf/2509.24663.pdf"
    },
    {
        "名称": "2025 [2509.23285] Toward Effective Tool-Integrated Reasoning via Self-Evolved Preference Learning.pdf",
        "作者": "Yifei Chen, Guanting Dong, Zhicheng Dou",
        "摘要": "摘要: 工具集成推理（Tool-Integrated Reasoning, TIR）通过整合外部工具使大型语言模型（LLMs）提升其内部推理能力。然而，应用TIR的模型常表现出次优行为，如工具使用不足或过度、工具调用后的过度思考。如何激励LLMs有效准确地执行TIR，同时稳定推理过程，仍是一个未解决的问题。本文从信息熵的角度开始探讨工具调用对模型推理的影响。我们的研究结果表明，工具调用结果导致后续推理信息熵的显著变化，整条推理链的信息熵基于工具调用次数而变化。基于这些见解，我们提出了Tool-Light框架，以鼓励LLMs高效准确地执行TIR。我们的框架包括数据集构建和多阶段微调。数据集构建中，我们使用经过微调的模型进行连续自演化采样，结合原始采样和熵引导采样。此外，我们制定了严格的标准来选择采样过程中的正负对。培训过程包括监督微调（SFT）和自演化直接偏好优化（DPO）两个阶段。对10个数据集的实验结果表明，Tool-Light显著提高了模型执行TIR任务的效率。",
        "地址": "https://arxiv.org/pdf/2509.23285.pdf"
    },
    {
        "名称": "2025 [2509.25176] SIRI: Scaling Iterative Reinforcement Learning with Interleaved Compression.pdf",
        "作者": "Haoming Wen, Yushi Bai, Juanzi Li, Jie Tang",
        "摘要": "摘要：我们介绍了SIRI（交错压缩的迭代强化学习），一种用于大规模推理模型（LRMs）的简单且有效的强化学习方法，它能够实现更高效和准确的推理。现有研究观察到LRMs中的重复思维模式，试图减少这种模式通常会以牺牲性能为代价。在本文中，我们展示了这种权衡可以通过一种训练机制来克服，该机制在训练过程中动态调整最大回滚长度，迭代交替进行压缩和扩展推理预算。压缩阶段缩短了回滚长度，迫使模型在有限上下文中做出精确且有价值的决策，从而有效减少冗余标记并增加推理密度。扩展阶段则放宽长度限制，为模型在长远设置中进行探索和规划提供空间。显著的是，我们发现每次压缩-扩展周期后，模型的性能都会提高，同时输出长度减少，稳步推向性能-效率权衡的帕累托前沿。在DeepSeek-R1-Distill-Qwen-1.5B上训练后，SIRI-low在AIME24上的性能提高了43.2%，同时标记使用减少了46.9%（经过三次迭代），而SIRI-high相比所有其他方法取得了最高准确率（图1）。我们的研究揭示了训练过程中周期性振荡LRMs输出截断长度的潜力，以动态平衡探索和推理效率，趋于两者之间的最佳“甜点”。我们的模型是公开可用的。\n\n作者：Haoming Wen, Yushi Bai, Juanzi Li, Jie Tang\n\n评论：已提交\n\nURL：https://arxiv.org/pdf/2509.25176.pdf\n\n标题：2025 [2509.25176] SIRI: 交错压缩的迭代强化学习",
        "地址": "https://arxiv.org/pdf/2509.25176.pdf"
    },
    {
        "名称": "2025 [2509.25137] The Era of Real-World Human Interaction: RL from User Conversations.pdf",
        "作者": "Chuanyang Jin, Jing Xu, Bo Liu, Leitian Tao, Olga Golovneva, Tianmin Shu, Wenting Zhao, Xian Li, Jason Weston",
        "摘要": "摘要：我们提出，要实现模型的持续改进和多方面的对齐，未来的模型必须从自然的人类互动中学习。目前的对话模型使用预先标注的专家生成的人类反馈进行对齐。在这项工作中，我们引入了从人类互动中进行强化学习（RLHI）的范式，该范式直接从实际的用户对话中学习。我们开发了两种互补的方法：（1）用户引导的重写RLHI，根据用户的自然语言后续反应来修改不满意的模型输出，（2）基于用户奖励的RLHI，通过基于用户长期互动历史知识的奖励模型（称为身份）进行学习。这两种方法共同通过身份条件化偏好优化，将长期用户身份与回合级别的偏好联系起来。基于WildChat对话训练，两个RLHI变体在个性化和指令遵循方面均优于强基线，类似的反馈也提高了推理基准的性能。这些结果表明，来自自然人类互动的有机反馈为个性化对齐提供了可扩展且有效的监督。\n\n来源：[2025 [2509.25137] The Era of Real-World Human Interaction: RL from User Conversations.pdf](https://arxiv.org/pdf/2509.25137.pdf)",
        "地址": "https://arxiv.org/pdf/2509.25137.pdf"
    },
    {
        "名称": "2025 [2509.25131] MGM-Omni: Scaling Omni LLMs to Personalized Long-Horizon Speech.pdf",
        "作者": "Chengyao Wang, Zhisheng Zhong, Bohao Peng, Senqiao Yang, Yuqi Liu, Haokun Gui, Bin Xia, Jingyao Li, Bei Yu, Jiaya Jia",
        "摘要": "摘要：我们提出了MGM-Omni，一个为全模态理解和表达、长期语音生成而统一的Omni LLM。不同于将语音合成独立处理的级联管道，MGM-Omni采用“脑-口”设计，通过双轨、基于令牌的架构清晰地将多模态推理与实时语音生成解耦。这种设计实现了高效的跨模态交互和低延迟的流式语音生成。在理解方面，统一的训练策略结合双音频编码器设计，使得在多样声学条件下实现长篇音频感知。在生成方面，基于块的并行解码方案缩小了文本与语音令牌生成率的差距，加速推理进程，并支持流式零样本语音克隆，在长时间段内保持稳定的音色。与同期工作相比，MGM-Omni通过显著数据高效的训练，达成了这些功能。大量实验表明，MGM-Omni在延长序列中保持音色身份、生成自然且符合上下文的语音以及实现卓越的长篇音频和全模态理解方面，均优于现有开源模型。MGM-Omni确立了一个高效的、端到端的全模态理解和可控、个性化长期语音生成的范式。",
        "地址": "https://arxiv.org/pdf/2509.25131.pdf"
    },
    {
        "名称": "2025 [2509.25084] Scaling Generalist Data-Analytic Agents.pdf",
        "作者": "Shuofei Qiao, Yanqiu Zhao, Zhisong Qiu, Xiaobin Wang, Jintian Zhang, Zhao Bin, Ningyu Zhang, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen",
        "摘要": "摘要: 数据分析代理正在成为自动化科学发现和创新AI愿景的关键催化剂。然而，当前的方法严重依赖于专有模型上的提示工程，而开源模型在面对多种格式、大规模数据文件和现实世界分析所需的长时间、多步骤推理方面却面临困境。本文介绍了DataMind，一种用于构建通用数据分析代理的可扩展的数据合成和代理训练方案。DataMind解决了构建开源数据分析代理的三大关键挑战，包括数据资源不足、训练策略不当以及基于代码的不稳定多轮展开。具体来说，DataMind应用了1) 细粒度任务分类和递归的从易到难任务组合机制，以增加综合查询的多样性和难度；2) 知识增强的轨迹采样策略，随后进行模型和规则基础的过滤；3) 动态可调的训练目标，结合了SFT和RL损失；4) 节省内存且稳定的基于代码的多轮展开框架。基于DataMind，我们整理了DataMind-12K，一个涵盖不同领域、任务类别和数据文件格式的数据分析任务的高质量轨迹集。基于DataMind-12K训练的DataMind-14B在多个数据分析基准测试中平均得分达到71.16%，超越了最强的专有基线DeepSeek-V3.1和GPT-5。我们的DataMind-7B也以68.10%的得分成为所有开源模型中的佼佼者。我们还将一些从探索性试验中获得的经验见解纳入分析实验中，旨在为社区提供关于代理训练的可操作见解。我们会发布DataMind-12K以及DataMind-7B和14B，供社区未来研究使用。\n\n评论：工作正在进行中\n\n网址：https://arxiv.org/pdf/2509.25084.pdf\n\n作者：乔硕飞、赵艳秋、裘志松、王晓斌、张锦天、赵斌、张宁宇、蒋勇、解鹏君、黄飞、陈华俊\n\n标题：2025 [2509.25084] 扩展通用数据分析代理.pdf",
        "地址": "https://arxiv.org/pdf/2509.25084.pdf"
    },
    {
        "名称": "2025 [2509.23951] HunyuanImage 3.0 Technical Report.pdf",
        "作者": "Siyu Cao, Hangting Chen, Peng Chen, Yiji Cheng, Yutao Cui, Xinchi Deng, Ying Dong, Kipper Gong, Tianpeng Gu, Xiusen Gu, Tiankai Hang, Duojun Huang, Jie Jiang, Zhengkai Jiang, Weijie Kong, Changlin Li, Donghao Li, Junzhe Li, Xin Li, Yang Li, Zhenxi Li, Zhimin Li, Jiaxin Lin, Linus, Lucaz Liu, Shu Liu, Songtao Liu, Yu Liu, Yuhong Liu, Yanxin Long, Fanbin Lu, Qinglin Lu, Yuyang Peng, Yuanbo Peng, Xiangwei Shen, Yixuan Shi, Jiale Tao, Yangyu Tao, Qi Tian, Pengfei Wan, Chunyu Wang, Kai Wang, Lei Wang, Linqing Wang, Lucas Wang, Qixun Wang, Weiyan Wang, Hao Wen, Bing Wu, Jianbing Wu, Yue Wu, Senhao Xie, Fang Yang, Miles Yang, Xiaofeng Yang, Xuan Yang, Zhantao Yang, Jingmiao Yu, Zheng Yuan, Chao Zhang, Jian-Wei Zhang, Peizhen Zhang, Shi-Xue Zhang, Tao Zhang, Weigang Zhang, Yepeng Zhang, Yingfang Zhang, Zihao Zhang, Zijian Zhang, Penghao Zhao, Zhiyuan Zhao, Xuefei Zhe, Jianchen Zhu, Zhao Zhong",
        "摘要": "摘要: 我们介绍了 HunyuanImage 3.0，这是一种本地多模态模型，在自回归框架内统一了多模态理解和生成，其图像生成模块已公开。HunyuanImage 3.0 的成就依赖于几个关键组件，包括细致的数据策划、先进的架构设计、本地的思维链模式、渐进式模型预训练、激进的模型后训练以及支持大规模训练和推理的高效基础设施。凭借这些进展，我们成功训练了一个总参数超过800亿的专家混合（MoE）模型，在推理过程中每个标记激活130亿参数，使其成为迄今为止最大和最强大的开源图像生成模型。我们进行了广泛的实验，文本-图像对齐和视觉质量的自动和人工评估结果表明，HunyuanImage 3.0 可与之前的最先进模型匹敌。通过发布 HunyuanImage 3.0 的代码和权重，我们旨在使社区能够探索新的想法，建立在最先进的基础模型之上，促进动态和充满活力的多模态生态系统。所有开源资源均可在此 https URL 公开获取。",
        "地址": "https://arxiv.org/pdf/2509.23951.pdf"
    },
    {
        "名称": "2025 [2509.22572] Dynamic Experts Search: Enhancing Reasoning in Mixture-of-Experts LLMs at Test Time.pdf",
        "作者": "Yixuan Han, Fan Ma, Ruijie Quan, Yi Yang",
        "摘要": "摘要: 测试时间缩放（TTS）通过在推理过程中分配额外计算来增强大型语言模型（LLMs）的推理能力。然而，现有方法主要依赖于输出级别的采样，而忽略了模型架构的作用。在主流的专家混合（MoE）LLMs中，我们观察到，改变激活专家的数量可以产生互补的解决方案集，并保持稳定的准确性，揭示了一个新的、尚未深入探索的多样性来源。受此观察启发，我们提出了动态专家搜索（DES），一种将专家激活提升为搜索空间中的可控维度的TTS策略。DES整合了两个关键组件：（1）动态MoE，使得在推理过程中可以直接控制专家数量，以生成多样化的推理轨迹而无额外成本；（2）专家配置继承，在推理路径中保持一致的专家数量，同时在不同运行中变更它们，从而在整个搜索过程中平衡稳定性和多样性。通过跨MoE架构、验证器和推理基准（如数学、代码和知识）进行广泛实验，证明DES在不增加额外成本的情况下可靠地优于TTS基线，增强了准确性和稳定性。这些结果突出显示了DES作为一种实用且可扩展的、关注架构的TTS形式，说明了现代LLMs中的结构灵活性如何能促进推理进步。",
        "地址": "https://arxiv.org/pdf/2509.22572.pdf"
    },
    {
        "名称": "2025 [2509.23219] WirelessMathLM: Teaching Mathematical Reasoning for LLMs in Wireless Communications with Reinforcement Learning.pdf",
        "作者": "Xin Li, Mengbing Liu, Yiyang Zhu, Wenhe Zhang, Li Wei, Jiancheng An, Chau Yuen",
        "摘要": "摘要：大型语言模型（LLMs）在一般数学推理方面表现出色，但在专业技术数学方面却彻底失败。在无线通信中，问题需要精确操作信息论边界、优化约束和信号处理公式，即使是最先进的模型也难以实现合格的性能。我们推出WirelessMathLM，展示紧凑模型（0.5B-7B参数）通过带有可验证回报的领域特定强化学习可以匹敌甚至超越更大的模型。我们的关键见解是无线数学问题具有独特的属性——可验证的正确性——无需人工反馈即可实现有效的强化学习。我们构建了WirelessMathBench-XL，该基准包括970篇论文中的4027个问题。使用具有二元验证回报的组相对政策优化（GRPO），我们从基本检查点直接训练模型，而无需监督预热启动。我们的7B模型在WirelessMathBench-XL上达到39.5%的准确率，接近GPT-4o（40.4%），使用的参数约为DeepSeek-R1（671B，57.4%）的百分之一。令人惊讶的是，GRPO训练几乎使所有模型规模的性能翻倍（0.5B +11%，3B +103%，7B +81%），并对一般数学基准产生积极影响——我们的模型在未进行这些任务训练的情况下，平均在MATH、Minerva-Math、OlympiadBench、AMC和AIME上增加+8.4分。\n\n作者：Xin Li，Mengbing Liu，Yiyang Zhu，Wenhe Zhang，Li Wei，Jiancheng An，Chau Yuen  \n备注：项目主页：此 https URL  \n链接：https://arxiv.org/pdf/2509.23219.pdf  \n标题：2025 [2509.23219] WirelessMathLM: 教授无线通信中的数学推理给LLMs通过强化学习.pdf",
        "地址": "https://arxiv.org/pdf/2509.23219.pdf"
    },
    {
        "名称": "2025 [2509.23050] Understanding Language Prior of LVLMs by Contrasting Chain-of-Embedding.pdf",
        "作者": "Lin Long, Changdae Oh, Seongheon Park, Yixuan Li",
        "摘要": "摘要: 大型视觉语言模型（LVLMs）在多模态任务上表现强劲，但它们常常依赖于其语言惯例（LP）——从预训练中记忆的文本模式，而没有充分利用视觉证据。之前对语言惯例的分析主要依赖于输入输出探测，无法揭示视觉何时以及如何影响模型行为的内部机制。为了解决这一差距，我们通过嵌入链的视角，提供了对语言惯例的首次系统分析，该方法考察了LVLMs内逐层表示动态。我们的分析揭示了一个普遍现象：每个模型都展示了一个视觉整合点（VIP），这是视觉信息开始有意义地重塑隐藏表示并影响解码的关键层。在这一观察基础上，我们引入了总视觉整合（TVI）估计器，该估计器聚合VIP之外的表示距离，量化视觉查询影响响应生成的强度。通过54个模型-数据集组合、9个当代LVLMs和6个基准，我们展示了VIP始终出现，并且TVI可靠地预测语言惯例的强度。这提供了一个诊断和理解LVLMs中的语言惯例的原则性工具。",
        "地址": "https://arxiv.org/pdf/2509.23050.pdf"
    },
    {
        "名称": "2025 [2509.22921] Rethinking Large Language Model Distillation: A Constrained Markov Decision Process Perspective.pdf",
        "作者": "Matthieu Zimmer, Xiaotong Ji, Tu Nguyen, Haitham Bou Ammar",
        "摘要": "摘要：我们通过将大型语言模型 (LLM) 的蒸馏过程表述为一个约束强化学习问题，介绍了一种新颖的方法。虽然近期的工作开始探讨将任务特定的奖励融入蒸馏过程中，但现有方法通常依赖于临时的奖励加权。我们提出了一种有原则的优化框架，在最大化任务特定奖励的同时，约束与教师模型的偏差保持在指定的阈值以下。我们的方法将约束状态扩展强化学习适配到蒸馏环境中，介绍了一种修改的奖励函数，该函数在部署期间无需状态扩展或教师模型访问，并且无需对偶拉格朗日方法的计算开销，就能保持约束满足的理论保证。通过对数学推理任务的大量实验，我们证明了我们的方法在保持有竞争力的任务性能的同时，实现了比软拉格朗日松弛基线更好的约束满足率和更好的推理能力。我们的框架为资源受限环境中的奖励感知蒸馏提供了一种理论上有依据且在实践中高效的解决方案。",
        "地址": "https://arxiv.org/pdf/2509.22921.pdf"
    },
    {
        "名称": "2025 [2509.23196] From Harm to Help: Turning Reasoning In-Context Demos into Assets for Reasoning LMs.pdf",
        "作者": "Haonan Wang, Weida Liang, Zihang Fu, Nie Zheng, Yifan Zhang, Yao Tong, Tongyao Zhu, Hao Jiang, Chuang Li, Jiaying Wu, Kenji Kawaguchi",
        "摘要": "摘要：近期推理大语言模型（RLMs），特别是那些通过基于验证器的强化学习训练的模型，在几次示例推理（few-shot CoT）时，表现往往比直接回答差。我们利用DeepSeek-R1的高质量推理痕迹作为示例重新审视这一悖论，发现即使示例是最优的，添加更多示例仍会持续降低准确性。详细分析揭示了这一下降背后的两个机制：(i) 语义误导，高文本相似度导致模型将目标视为与示例相同，并逐字复制中间步骤；(ii) 策略转移失败，模型难以提取有用的推理策略并应用于目标问题。基于这些机制，我们引入了Insight-to-Solve（I2S），一种逐步测试时过程，将示例转化为明确的、可重用的见解，并派生出特定目标的推理痕迹；根据需要，推理可以自我精化以保证连贯性和正确性（I2S+）。在各类基准上的广泛实验表明，I2S和I2S+在开放源和封闭源模型中一贯优于直接回答和测试时扩展基准。即使对于GPT模型，我们的方法也有所帮助：在AIME'25上，GPT-4.1提升了+14.0%，而在AIME上o1-mini提高了+2.7%，在GPQA上提高了+1.7%，表明通过见解-精化-解决框架可以有效利用上下文示例。",
        "地址": "https://arxiv.org/pdf/2509.23196.pdf"
    },
    {
        "名称": "2025 [2509.21875] LUMINA: Detecting Hallucinations in RAG System with Context-Knowledge Signals.pdf",
        "作者": "Min-Hsuan Yeh, Yixuan Li, Tanwi Mallick",
        "摘要": "摘要：检索增强生成（RAG）旨在通过在检索到的文档中找到响应依据来减轻大型语言模型（LLM）的幻觉。然而，即使提供了正确且充足的上下文，基于RAG的LLM仍然会产生幻觉。越来越多的研究表明，这源于模型在使用外部上下文和内部知识之间的不平衡，并且已有若干方法尝试量化这些信号以进行幻觉检测。然而，现有方法需要大量的超参数调整，限制了其普适性。我们提出了LUMINA，一个通过上下文-知识信号检测RAG系统中幻觉的新框架：外部上下文利用通过分布距离量化，而内部知识利用通过跟踪预测标记在变压器层之间的演变来测量。我们进一步引入一个框架来统计验证这些测量。在常见的RAG幻觉基准和四个开源LLM上的实验表明，LUMINA在AUROC和AUPRC评分上始终表现出高水平，超过先前基于利用率的方法在HalluRAG上的AUROC达13％以上。此外，在放宽对检索质量和模型匹配的假设下，LUMINA仍然保持稳健，提供了有效性和实用性。\n\n作者：叶敏轩，李亦轩，谭薇·马利克",
        "地址": "https://arxiv.org/pdf/2509.21875.pdf"
    },
    {
        "名称": "2025 [2509.25370] Where LLM Agents Fail and How They can Learn From Failures.pdf",
        "作者": "Kunlun Zhu, Zijia Liu, Bingxuan Li, Muxin Tian, Yingxuan Yang, Jiaxun Zhang, Pengrui Han, Qipeng Xie, Fuyang Cui, Weijia Zhang, Xiaoteng Ma, Xiaodong Yu, Gowtham Ramesh, Jialian Wu, Zicheng Liu, Pan Lu, James Zou, Jiaxuan You",
        "摘要": "摘要：大型语言模型（LLM）代理集成了计划、记忆、反思和工具使用模块，在解决复杂、多步骤任务方面显示出很有希望的潜力。然而，其复杂的架构增加了对级联故障的脆弱性，其中一个单一的根本原因错误会通过后续决策传播，导致任务失败。目前的系统缺乏全面了解代理错误的模块化和系统性框架，因此无法相应地检测这些错误。我们通过三项贡献来解决这一差距。首先，我们引入了AgentErrorTaxonomy，这是一个涵盖记忆、反思、计划、行动和系统级操作的故障模式的模块化分类。其次，我们构建了AgentErrorBench，这是第一个从ALFWorld、GAIA和WebShop系统化注释的故障轨迹数据集，将错误分析扎根于真实世界的代理展开。第三，我们提出了AgentDebug，一个调试框架，该框架隔离根本原因故障并提供纠正反馈，使代理能够恢复并迭代改进。AgentErrorBench上的实验表明，与最强基线相比，AgentDebug的全正确准确率提高了24%，步骤准确率提高了17%。除了检测之外，AgentDebug生成的定向反馈使LLM代理能够迭代恢复故障，在ALFWorld、GAIA和WebShop中的任务成功率相对提高了高达26%。这些结果确立了通过调试来实现更可靠和适应性的LLM代理的路径。代码和数据将在此https URL提供。",
        "地址": "https://arxiv.org/pdf/2509.25370.pdf"
    },
    {
        "名称": "2025 [2509.23924] Taming Masked Diffusion Language Models via Consistency Trajectory Reinforcement Learning with Fewer Decoding Step.pdf",
        "作者": "Jingyi Yang, Guanxu Chen, Xuhao Hu, Jing Shao",
        "摘要": "摘要: 掩蔽扩散语言模型（MDLMs）最近成为自回归（AR）语言模型的一种有前景的替代方案，具有并行解码、灵活生成顺序和潜在较少推理步骤等特性。尽管存在这些优势，但尚未深入探讨适用于MDLMs的解码策略和强化学习（RL）算法。一种简单的方法是直接将已为AR模型成熟的技术转移到MDLMs。然而，这引发一个直接的问题：这样的简单转移真的最优吗？例如，1) MDLM训练期间未采用块式和半AR解码策略，为什么它们在推理期间优于完全扩散风格解码？2) 直接将为AR模型设计的RL算法应用于MDLMs会产生训练-推理不一致性，因为MDLM解码是非因果（并行）的。这导致展开轨迹和优化轨迹之间的不一致。为解决这些挑战，我们提出EOS早期拒绝（EOSER）和升序步长（ASS）解码调度器，释放MDLMs的潜力以执行完全扩散风格解码，实现具有竞争力的性能且解码步骤更少。此外，我们引入一致性轨迹组相对策略优化（CJ-GRPO）来驯服MDLMs，强调展开轨迹和优化轨迹之间的一致性，并减少跳步优化带来的优化错误。我们在推理任务（如数学和规划基准测试）上进行了大量实验，使用LLaDA-8B-Instruct。结果表明，所提出的EOSER和ASS机制与CJ-GRPO一起，对于有效和高效驯服MDLMs前景显著。",
        "地址": "https://arxiv.org/pdf/2509.23924.pdf"
    },
    {
        "名称": "2025 [2509.23866] Efficient Multi-turn RL for GUI Agents via Decoupled Training and Adaptive Data Curation.pdf",
        "作者": "Pengxiang Li, Zechen Hu, Zirui Shang, Jingrong Wu, Yang Liu, Hui Liu, Zhi Gao, Chenrui Shi, Bofei Zhang, Zihao Zhang, Xiaochuan Shi, Zedong YU, Yuwei Wu, Xinxiao Wu, Yunde Jia, Liuyu Xiang, Zhaofeng He, Qing Li",
        "摘要": "摘要: 基于视觉语言模型 (VLM) 的图形用户界面 (GUI) 代理在自动化复杂桌面和移动任务方面显示出了很大的潜力，但在应用强化学习 (RL) 时面临显著的挑战：(1) 在图形用户界面环境中进行策略部署时的多轮交互速度缓慢，以及 (2) 用于策略学习的高质量代理环境交互不足。为了应对这些挑战，我们提出了 DART，一种针对 GUI 代理的解耦代理化强化学习训练框架，通过高度解耦的方式协调异构模块。DART将训练系统分离为四个异步模块：环境集群、滚动服务、数据管理器和训练器。这种设计实现了非阻塞通信、异步训练、滚动式轨迹采样以及每个工作者的模型同步，大大提高了系统效率：滚动时GPU利用率提高了1.6倍，训练吞吐量提高了1.9倍，环境利用率提高了5.5倍。为了有效地从大量样本中学习，我们引入了自适应数据整理方案：(1) 预先收集成功轨迹以补充在线采样中稀疏的成功率；(2) 根据任务难度动态调整滚动次数和轨迹长度；(3) 对高熵步骤进行选择性训练，以优先考虑关键决策；(4) 通过截断重要性采样来稳定学习，以应对策略滚动和更新之间的策略不匹配。在 OSWorld 基准测试中，DART-GUI-7B实现了42.13%的任务成功率，比基础模型绝对提高了14.61%，比开源的SOTA高出7.34%。我们将通过这个http URL完全开源我们的训练框架、数据和模型检查点，我们相信这是对开源社区中代理化强化学习训练的及时贡献。",
        "地址": "https://arxiv.org/pdf/2509.23866.pdf"
    },
    {
        "名称": "2025 [2509.23564] Clean First, Align Later: Benchmarking Preference Data Cleaning for Reliable LLM Alignment.pdf",
        "作者": "Min-Hsuan Yeh, Yixuan Li",
        "摘要": "摘要: 人类反馈在使大型语言模型（LLMs）与人类偏好一致方面起着至关重要的作用。然而，这种反馈通常是噪声或不一致的，这会降低奖励模型的质量并阻碍一致性。尽管已经提出了各种自动化的数据清理方法来减轻这一问题，但系统评估其有效性和普遍性仍然缺乏。为了填补这一空白，我们引入了首个全面基准，用于在LLM一致性背景下评估13种偏好数据清理方法。PrefCleanBench提供了一个标准化协议，用于评估清理策略在不同数据集、模型架构和优化算法中的一致性表现和普遍性。通过统一不同的方法并进行严格比较，我们揭示了决定数据清理在一致性任务中成功的关键因素。该基准为通过更好的数据质量改善LLM一致性奠定了原则性和可重复性的方法基础，突出了数据预处理在负责任的AI开发中至关重要但未被充分探索的作用。我们发布了所有方法的模块化实现以促进进一步研究：https URL。\n\n作者: Min-Hsuan Yeh, Yixuan Li\n\n评论: NeurIPS 2025\n\n链接: https://arxiv.org/pdf/2509.23564.pdf\n\n标题: 2025 [2509.23564] 先清理数据，再进行对齐：基准偏好数据清理以确保可靠的LLM一致性.pdf",
        "地址": "https://arxiv.org/pdf/2509.23564.pdf"
    },
    {
        "名称": "2025 [2509.25149] Pretraining Large Language Models with NVFP4.pdf",
        "作者": "NVIDIA, Felix Abecassis, Anjulie Agrusa, Dong Ahn, Jonah Alben, Stefania Alborghetti, Michael Andersch, Sivakumar Arayandi, Alexis Bjorlin, Aaron Blakeman, Evan Briones, Ian Buck, Bryan Catanzaro, Jinhang Choi, Mike Chrzanowski, Eric Chung, Victor Cui, Steve Dai, Bita Darvish Rouhani, Carlo del Mundo, Deena Donia, Burc Eryilmaz, Henry Estela, Abhinav Goel, Oleg Goncharov, Yugi Guvvala, Robert Hesse, Russell Hewett, Herbert Hum, Ujval Kapasi, Brucek Khailany, Mikail Khona, Nick Knight, Alex Kondratenko, Ronny Krashinsky, Ben Lanir, Simon Layton, Michael Lightstone, Daniel Lo, Paulius Micikevicius, Asit Mishra, Tim Moon, Deepak Narayanan, Chao Ni, Abhijit Paithankar, Satish Pasumarthi, Ankit Patel, Mostofa Patwary, Ashwin Poojary, Gargi Prasad, Sweta Priyadarshi, Yigong Qin, Xiaowei Ren, Oleg Rybakov, Charbel Sakr, Sanjeev Satheesh, Stas Sergienko, Pasha Shamis, Kirthi Shankar, Nishant Sharma, Mohammad Shoeybi, Michael Siu, Misha Smelyanskiy, Darko Stosic, Dusan Stosic, Bor-Yiing Su, Frank Sun, Nima Tajbakhsh, Shelby Thomas, Przemek Tredak, Evgeny Tsykunov, Gandhi Vaithilingam, Aditya Vavre, Rangharajan Venkatesan, Roger Waleffe, Qiyu Wan, Hexin Wang, Mengdi Wang, Lizzie Wei, Hao Wu, Evan Wu, Keith Wyss, Ning Xu, Jinze Xue, Charlene Yang, Yujia Zhai, Ruoxi Zhang, Jingyang Zhu, Zhongbo Zhu",
        "摘要": "2025年 预训练大型语言模型：NVFP4\n\n摘要：当前的大型语言模型（LLMs）在许多领域都是强大的问题解决者，并且随着模型规模、训练集规模和训练集质量的提升，它们不断变得更强大。广泛的行业研究与实验表明，训练最前沿的模型需要数十到数百Yottaflops级别的计算资源，这意味着大量的时间、计算和能源投资。因此，提高预训练效率对于下一代更为强大的LLMs至关重要。尽管8位浮点（FP8）训练现已被广泛采用，但转向更窄的精度，如4位浮点（FP4），可能进一步提高计算速度和资源利用率。然而，这种级别的量化在训练稳定性、收敛性和实施方面存在挑战，尤其是对于在长令牌范围内训练的大规模模型。\n\n在这项研究中，我们提出了一种使用NVFP4格式进行稳定和精确训练大型语言模型（LLMs）的新方法。我们的方法结合了随机Hadamard变换（RHT）来限制块级异常值，采用双维量化方案以跨前向和后向传递实现一致表现，利用随机舍入进行无偏梯度估计，并纳入选择性高精度层。我们通过在10万亿令牌上训练一个拥有120亿参数的模型验证了我们的方法——这是迄今为止在4位精度下最长的公开训练记录。我们的结果表明，采用NVFP4预训练技术训练的模型在训练损失和下游任务准确性方面可以比肩FP8基线。研究结果表明，NVFP4结合我们的训练方法，代表了窄精度LLM训练算法的重大进步。",
        "地址": "https://arxiv.org/pdf/2509.25149.pdf"
    },
    {
        "名称": "2025 [2509.24335] Hyperspherical Latents Improve Continuous-Token Autoregressive Generation.pdf",
        "作者": "Guolin Ke, Hui Xue",
        "摘要": "摘要: 自回归（AR）模型在图像生成方面有很大的潜力，但连续标记的AR变体往往落后于潜在扩散和掩码生成模型。核心问题在于VAE潜在空间的异质方差，在AR解码过程中尤为明显，特别是在无分类器引导（CFG）下，可能导致方差崩溃。我们提出了SphereAR来解决这个问题。其核心设计是将所有AR输入和输出（包括在CFG之后）限制在一个固定半径的超球面（常数$\\\\ell_2$范数）上，利用超球面VAE。我们的理论分析表明，超球面约束去除了规模成分（方差崩溃的主要原因），从而稳定了AR解码。在ImageNet生成上，SphereAR-H（943M）为AR模型建立了一个新的状态标准，达到了FID 1.34。即使在较小规模上，SphereAR-L（479M）达到了FID 1.54，SphereAR-B（208M）达到了1.92，匹配或超越了更大的基线模型，如MAR-H（943M，1.55）和VAR-d30（2B，1.92）。据我们所知，这是首次在可比参数规模下，纯净的下一个标记AR图像生成器以栅格顺序超过扩散和掩码生成模型。\n\n作者: Guolin Ke, Hui Xue\n\n网址: https://arxiv.org/pdf/2509.24335.pdf\n\n标题: 2025 [2509.24335] Hyperspherical Latents Improve Continuous-Token Autoregressive Generation",
        "地址": "https://arxiv.org/pdf/2509.24335.pdf"
    },
    {
        "名称": "2025 [2509.24193] AceSearcher: Bootstrapping Reasoning and Search for LLMs via Reinforced Self-Play.pdf",
        "作者": "Ran Xu, Yuchen Zhuang, Zihan Dong, Jonathan Wang, Yue Yu, Joyce C. Ho, Linjun Zhang, Haoyu Wang, Wenqi Shi, Carl Yang",
        "摘要": "摘要：增强搜索功能的大型语言模型（LLMs）在处理复杂推理任务时通常会遇到困难，这是由于多跳检索效率低下以及推理能力有限所导致的。我们提出了AceSearcher，这是一种合作自我博弈框架，通过训练一个大型语言模型使其交替扮演两种角色：一个是分解者，负责分解复杂查询；另一个是解决者，负责整合检索到的上下文并生成答案。AceSearcher结合了对搜索、推理和分解任务的多样化混合进行监督微调，以及针对最终答案准确率进行优化的强化微调，消除了对中间注释的需求。在三个需要深度推理的任务中进行的广泛实验表明，AceSearcher在10个数据集上均优于最先进的基准模型，实现了平均准确率提升7.6%。值得注意的是，在文档级财务推理任务中，AceSearcher-32B仅使用DeepSeek-V3模型不到5%的参数，性能却相当。即使在较小规模（1.5B和8B）上，AceSearcher常常超越现有的搜索增强型LLMs，效率和效果均远远胜过其最多达9倍参数的模型，突显了其在处理复杂推理任务上的卓越表现。我们的代码将在以下网址发布：此https URL和此https URL。\n\n评论：该论文被NeurIPS 2025录取（Spotlight）。\n\n链接：https://arxiv.org/pdf/2509.24193.pdf\n\n标题：2025 [2509.24193] AceSearcher: Bootstrapping Reasoning and Search for LLMs via Reinforced Self-Play.pdf\n\n作者：Ran Xu, Yuchen Zhuang, Zihan Dong, Jonathan Wang, Yue Yu, Joyce C. Ho, Linjun Zhang, Haoyu Wang, Wenqi Shi, Carl Yang",
        "地址": "https://arxiv.org/pdf/2509.24193.pdf"
    },
    {
        "名称": "2025 [2509.24786] LOVE-R1: Advancing Long Video Understanding with an Adaptive Zoom-in Mechanism via Multi-Step Reasoning.pdf",
        "作者": "Shenghao Fu, Qize Yang, Yuan-Ming Li, Xihan Wei, Xiaohua Xie, Wei-Shi Zheng",
        "摘要": "摘要：对于最近的长视频-语言模型（LVLMs）来说，长视频理解仍然是一个挑战，因为长时间形式的时间理解和详细的空间感知之间存在冲突。采用统一帧采样机制的LVLMs，以相等的帧大小和固定的采样率采样帧，不可避免地要牺牲时间线索或空间细节，从而导致次优解。为了缓解这一困境，我们提出了LOVE-R1模型，该模型可以自适应地放大视频片段。模型首先以小分辨率密集采样帧。如果需要某些空间细节，模型可以根据其推理在感兴趣的片段上以大分辨率放大，直到获得关键的视觉信息。整个过程被实现为一个多步推理过程。为了训练推理能力，我们首先在我们收集的38k高质量CoT数据上微调模型，并通过解耦强化微调来加强它。由于结果奖励不能提供细粒度的过程监督，我们将多步推理解耦为多个单步推理，并明确优化内部放大能力。在长视频理解基准测试中，我们的模型采用了慢速-快速自适应帧采样机制，在采样密度和帧分辨率之间实现了很好的平衡，LOVE-R1在四个常见的长视频理解基准上平均比我们基准Qwen2.5-VL高出3.1个百分点。\n\n来源：https://arxiv.org/pdf/2509.24786.pdf\n标题：LOVE-R1: 通过多步推理的自适应放大机制推进长视频理解\n作者：Shenghao Fu, Qize Yang, Yuan-Ming Li, Xihan Wei, Xiaohua Xie, Wei-Shi Zheng\n年份：2025",
        "地址": "https://arxiv.org/pdf/2509.24786.pdf"
    },
    {
        "名称": "2025 [2509.23371] Alignment through Meta-Weighted Online Sampling: Bridging the Gap between Data Generation and Preference Optimization.pdf",
        "作者": "Junming Yang, Ning Xu, Biao Liu, Shiqi Qiao, Xin Geng",
        "摘要": "摘要: 偏好优化对于使大型语言模型（LLMs）与人类的价值和意图一致至关重要。这个过程中的一个重要挑战是预收集的离线偏好数据与不断变化的模型策略之间的分布不匹配。现有的方法尝试使用静态启发式或解耦的在线采样策略来减少这种差距，但它们通常无法适应模型的动态学习状态。为了弥补这一差距，我们提出了元加权自适应偏好优化（MetaAPO），一个动态将数据生成与模型训练相结合的新框架。MetaAPO使用一个轻量级的元学习器，作为“对齐差距估计器”，评估在线采样相对于离线数据的潜在收益。这指导了有针对性的在线生成，并为优化目标分配样例级别的元权重，动态平衡在线和离线数据的质量和分布。在AlpacaEval 2，Arena-Hard和MT-Bench上的实验表明，MetaAPO在各种设置中始终优于现有的偏好优化方法，同时减少了42％的在线标注成本。",
        "地址": "https://arxiv.org/pdf/2509.23371.pdf"
    },
    {
        "名称": "2025 [2509.21953] MultiCrafter: High-Fidelity Multi-Subject Generation via Spatially Disentangled Attention and Identity-Aware Reinforcement Learning.pdf",
        "作者": "Tao Wu, Yibo Jiang, Yehao Lu, Zhizhong Wang, Zeyi Huang, Zequn Qin, Xi Li",
        "摘要": "摘要：多主体图像生成旨在合成用户提供的多个主体到一张图像中，同时保持主体的保真度、确保提示一致性，并符合人类的审美偏好。然而，现有的方法，特别是那些基于上下文学习范式的方法，受到依赖简单的基于重建的目标的限制，导致严重的属性泄漏，损害了主体的保真度，并且无法与细致的人类偏好对齐。为了解决这个问题，我们提出了MultiCrafter，一个确保高保真度和偏好对齐的生成框架。首先，我们发现属性泄漏的根本原因是在生成过程中不同主体之间显著的注意力纠缠。因此，我们引入了显式位置监督来明确分离每个主体的注意区域，有效缓解了属性泄漏。为了使模型能够在不同场景中准确规划不同主体的注意区域，我们采用了专家混合架构来增强模型的能力，使不同的专家专注于不同的场景。最后，我们设计了一种新型的在线强化学习框架，通过评分机制准确评估多主体保真度，并为专家混合架构量身定制了更稳定的训练策略，以此来使模型与人类偏好对齐。实验验证了我们的框架在显著提高主体保真度的同时，更好地与人类偏好对齐。\n\n作者：Tao Wu, Yibo Jiang, Yehao Lu, Zhizhong Wang, Zeyi Huang, Zequn Qin, Xi Li\n\n评论：项目页面：此HTTPS URL\n\n网址：https://arxiv.org/pdf/2509.21953.pdf",
        "地址": "https://arxiv.org/pdf/2509.21953.pdf"
    },
    {
        "名称": "2025 [2509.25050] Advantage Weighted Matching: Aligning RL with Pretraining in Diffusion Models.pdf",
        "作者": "Shuchen Xue, Chongjian Ge, Shilong Zhang, Yichen Li, Zhi-Ming Ma",
        "摘要": "摘要：强化学习（RL）已成为推进大型语言模型（LLMs）的一个中心范式，其中预训练和RL后训练共享相同的对数似然公式。相比之下，最近针对扩散模型的RL方法，尤其是去噪扩散策略优化（DDPO），优化的目标不同于预训练目标——评分/流匹配损失。在这项工作中，我们建立了一种新的理论分析：DDPO是带有噪音目标的隐含评分/流匹配形式，它增加了方差并减慢了收敛速度。在此分析的基础上，我们引入了\\textbf{优势加权匹配（AWM）}，一种用于扩散的政策梯度方法。它使用与预训练相同的评分/流匹配损失来获得低方差目标，并通过其优势重新调整每个样本的权重。实际上，AWM提高了高奖励样本的影响力，抑制了低奖励样本，同时保持与预训练相同的建模目标。这在概念上和实践上统一了预训练和RL，与政策梯度理论一致，减少了方差并提高了收敛速度。这种简单但有效的设计带来了显著的好处：在GenEval、OCR和PickScore基准测试中，AWM在应用于稳定扩散3.5中号和FLUX时，相比于基于DDPO的Flow-GRPO实现了高达24倍的加速，而不会牺牲生成质量。代码可在此https URL获取。",
        "地址": "https://arxiv.org/pdf/2509.25050.pdf"
    },
    {
        "名称": "2025 [2509.24317] Rethinking JEPA: Compute-Efficient Video SSL with Frozen Teachers.pdf",
        "作者": "Xianhang Li, Chen Huang, Chun-Liang Li, Eran Malach, Josh Susskind, Vimal Thilak, Etai Littwin",
        "摘要": "摘要: 视频联合嵌入预测架构（V-JEPA）通过预测潜在空间中的被遮罩区域，学习可泛化的现成视频表示，并采用指数移动平均（EMA）更新教师模型。虽然EMA防止了表示崩溃，但它使得可扩展的模型选择变得复杂，并且将教师和学生架构耦合起来。我们重新审视了被遮罩的潜在预测，发现一个冻结的教师模型即可实现。具体而言，我们（i）在V-JEPA遮罩下训练一个目标编码器，以简单的像素重建目标为准，然后（ii）将其冻结并训练一个学生来预测教师模型在被遮罩区域的潜在表示。这导致了一个两阶段的、未正则化的方案，我们称之为SALT（静态教师非对称潜在训练）。SALT将优化分离为像素重建（教师）和被遮罩的潜在预测（学生），在保留表示在冻结评估下泛化能力的同时，提高了透明度、效率和可扩展性。从经验上看，我们的学生模型在冻结骨干评估下表现优于最近提出的V-JEPA 2编码器，并且在计算上更加优化：在匹配的预训练FLOPs下，我们的方法实现了更高的探测准确度，并且它的缩放曲线在准确度-FLOPs的帕累托前沿上占据优势。最后，我们发现学生质量对教师质量非常具有鲁棒性：即使教师质量较差，小规模的教师也能够培养出高性能的学生。这表明计算预算应主要向学生倾斜。这些结果使得SALT成为EMA基础自蒸馏视频表示学习的简单、可扩展且计算高效的替代方案。",
        "地址": "https://arxiv.org/pdf/2509.24317.pdf"
    }
]