[
    {
        "名称": "2025 [2511.04570] Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm.pdf",
        "作者": "Jingqi Tong, Yurong Mou, Hangcheng Li, Mingzhe Li, Yongzhuo Yang, Ming Zhang, Qiguang Chen, Tianyi Liang, Xiaomeng Hu, Yining Zheng, Xinchi Chen, Jun Zhao, Xuanjing Huang, Xipeng Qiu",
        "摘要": "摘要翻译如下：\n\n摘要：通过“文本思维”和“图像思维”范式显著提升了大型语言模型（LLMs）和视觉语言模型（VLMs）的推理能力。然而，这些范式存在固有的局限性：（1）图像仅捕捉单个时刻，无法代表动态过程或连续变化；（2）将文本和视觉分离为独立的模态，阻碍了统一的多模态理解和生成。为了克服这些局限性，我们引入了一种新范式“视频思维”，利用视频生成模型（如Sora-2）在统一的时间框架内桥接视觉和文本的推理。为了支持这个探索，我们开发了视频思维基准（VideoThinkBench）。VideoThinkBench包含两个任务类别：（1）以视觉为中心的任务（例如：Eyeballing Puzzles），以及（2）以文本为中心的任务（例如：GSM8K和MMMU的子集）。我们的评估表明，Sora-2是一个有能力的推理者。在以视觉为中心的任务中，Sora-2大体上与最先进的视觉语言模型（SOTA VLMs）相当，甚至在一些任务（如Eyeballing Games）上超越了VLMs。在以文本为中心的任务中，Sora-2在MATH上达到了92%的准确率，在MMMU上达到了75.53%的准确率。此外，我们系统地分析了这些能力的来源。我们还发现自我一致性和上下文学习可以提高Sora-2的性能。总之，我们的研究结果表明，视频生成模型有潜力成为统一的多模态理解和生成模型，确立了“视频思维”作为统一的多模态推理范式的地位。\n\n论文标题：2025 [2511.04570] 通过视频思维：视频生成作为一种有前途的多模态推理范式\n作者：Jingqi Tong, Yurong Mou, Hangcheng Li, Mingzhe Li, Yongzhuo Yang, Ming Zhang, Qiguang Chen, Tianyi Liang, Xiaomeng Hu, Yining Zheng, Xinchi Chen, Jun Zhao, Xuanjing Huang, Xipeng Qiu\n评论：36页，14张图\n论文链接：https://arxiv.org/pdf/2511.04570.pdf",
        "地址": "https://arxiv.org/pdf/2511.04570.pdf"
    },
    {
        "名称": "2025 [2511.04460] V-Thinker: Interactive Thinking with Images.pdf",
        "作者": "Runqi Qiao, Qiuna Tan, Minghan Yang, Guanting Dong, Peiqing Yang, Shiqiang Lang, Enhui Wan, Xiaowan Wang, Yida Xu, Lan Yang, Chong Sun, Chen Li, Honggang Zhang",
        "摘要": "摘要：赋予大型多模态模型（LMMs）深入整合图像互动与长远推理能力仍然是该领域的一个长期挑战。近期在视觉中心推理领域的进展探索了一个有前途的“图像思维”范式，为LMMs带来了从图像辅助推理到图像互动思维的转变。虽然这一里程碑使模型能够关注细粒度图像区域，但进展仍受限于有限的视觉工具空间和任务特定的工作流程设计。为了弥补这一空白，我们提出了V-Thinker，这是一种通用多模态推理助理，通过端到端强化学习实现互动、视觉中心的思维。V-Thinker包含两个关键组件：（1）数据进化飞轮，自动综合、进化和验证跨越三个维度——多样性、质量和难度的互动推理数据集；（2）视觉渐进训练课程，首先通过点级监督校准感知，然后通过两阶段强化学习框架整合互动推理。此外，我们引入了VTBench，一个针对视觉中心互动推理任务的专家验证基准。大量实验表明，V-Thinker在一般和互动推理场景中始终优于强大的基于LMMs的基准，为推进图像互动推理应用提供了宝贵的见解。",
        "地址": "https://arxiv.org/pdf/2511.04460.pdf"
    },
    {
        "名称": "2025 [2511.03773] Scaling Agent Learning via Experience Synthesis.pdf",
        "作者": "Zhaorun Chen, Zhuokai Zhao, Kai Zhang, Bo Liu, Qi Qi, Yifan Wu, Tarun Kalluri, Sara Cao, Yuanhao Xiong, Haibo Tong, Huaxiu Yao, Hengduo Li, Jiacheng Zhu, Xian Li, Dawn Song, Bo Li, Jason Weston, Dat Huynh",
        "摘要": "摘要：虽然强化学习（RL）可以通过互动使大型语言模型（LLM）代理实现自我改进，但由于昂贵的回合、有限的任务多样性、不可靠的奖励信号和复杂的基础设施，实用化应用仍然具有挑战性，这些因素阻碍了可拓展经验数据的收集。为应对这些挑战，我们引入了DreamGym，这是第一个统一框架，旨在综合多样化的经验，以扩展性为目标，促进自主代理的有效在线RL训练。DreamGym不依赖于昂贵的真实环境回合，而是将环境动态提炼成基于推理的经验模型，通过逐步推理得出一致的状态转变和反馈信号，从而实现RL的可拓展代理回合收集。为了提高转变的稳定性和质量，DreamGym利用一个以离线真实世界数据初始化的经验回放缓冲区，并通过新的互动不断丰富该缓冲区，从而积极支持代理的训练。为了改善知识获取，DreamGym自适应地生成新任务，以挑战当前的代理策略，促进更有效的在线课程学习。通过在多样化环境和代理骨干上的实验表明，DreamGym显著改进了RL训练，无论是在完全合成的设置中还是在模拟到现实的转移场景中。在不适合RL的任务（如WebArena）中，DreamGym的表现比所有基线高出30%以上；并且在适合RL但花费昂贵的设置中，仅使用合成互动的DreamGym就能匹配GRPO和PPO的性能。当将仅基于合成经验训练的策略转移到真实环境RL中时，DreamGym提供了显著的额外性能提升，同时需要更少的真实世界互动，为通用RL提供了可扩展的温启动策略。",
        "地址": "https://arxiv.org/pdf/2511.03773.pdf"
    },
    {
        "名称": "2025 [2511.04670] Cambrian-S: Towards Spatial Supersensing in Video.pdf",
        "作者": "Shusheng Yang, Jihan Yang, Pinzhi Huang, Ellis Brown, Zihao Yang, Yue Yu, Shengbang Tong, Zihan Zheng, Yifan Xu, Muhan Wang, Daohan Lu, Rob Fergus, Yann LeCun, Li Fei-Fei, Saining Xie",
        "摘要": "摘要：我们认为，要实现真正的多模态智能，需从反应式、任务驱动的系统和大规模长上下文的计算方式转向更广泛的超级感知范式。我们将空间超级感知定义为四个超过仅语言理解的阶段：语义感知（命名所见之物）、流事件认知（在连续体验中保持记忆）、隐式3D空间认知（推测像素背后的世界）和预测世界建模（创建过滤和组织信息的内部模型）。当前的基准测试主要集中在早期阶段，对空间认知的覆盖范围较窄，且很少在真正需要世界建模的情况下挑战模型。为推动空间超级感知的进展，我们提出了VSI-SUPER，这是一项包含两部分的基准测试：VSR（长期视觉空间回忆）和VSC（持续视觉空间计数）。这些任务需要任意长的视频输入，但对强制扩展上下文的方式具有抵抗性。我们通过整理VSI-590K并训练Cambrian-S来测试数据扩展的极限，在VSI-Bench上实现了30%的绝对提升而不牺牲整体能力。然而，在VSI-SUPER上的表现仍然有限，表明规模本身不足以实现空间超级感知。我们提出预测感知作为未来的发展方向，展示了一个概念验证，其中一个自监督的下一个潜在帧预测器利用惊奇（预测误差）来驱动记忆和事件分割。在VSI-SUPER上，这种方法显著优于领先的专有基线，表明空间超级感知需要不仅能看见，还能预测、选择和组织体验的模型。",
        "地址": "https://arxiv.org/pdf/2511.04670.pdf"
    },
    {
        "名称": "2025 [2511.04307] GUI-360: A Comprehensive Dataset and Benchmark for Computer-Using Agents.pdf",
        "作者": "Jian Mu, Chaoyun Zhang, Chiming Ni, Lu Wang, Bo Qiao, Kartik Mathur, Qianhui Wu, Yuhang Xie, Xiaojun Ma, Mengyu Zhou, Si Qin, Liqun Li, Yu Kang, Minghua Ma, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang",
        "摘要": "摘要: 我们介绍了GUI-360°，一个大规模、全面的数据集和基准套件，旨在促进计算机使用代理(CUAs)的发展。CUAs呈现出独特的挑战，其发展受限于三个持续性缺口：缺乏现实世界的CUA任务、没有用于多模态轨迹的自动收集和注释管道，以及缺乏一个统一的基准来联合评估GUI基础、屏幕解析和动作预测。GUI-360°通过一个增强的、自动化程度高的管道，结合查询来源、环境模板构建、任务实例化、批处理执行以及驱动质量过滤来解决这些缺口。发布的语料库包含在流行的Windows办公应用程序中执行的超过120万的动作步骤，涵盖数千个任务轨迹，并包括全分辨率截图、可访问性元数据（如有）、实例化目标、中间推理轨迹，以及成功和失败的动作轨迹。数据集支持三个典型任务：GUI基础、屏幕解析和动作预测，以及反映现代代理设计的混合GUI+API动作空间。在GUI-360°上对最先进的视觉语言模型进行基准测试显露出在基础和动作预测中的显著不足；监督微调和强化学习虽然能取得显著进展，但仍无法达到人类级别的可靠性。我们发布GUI-360°及配套代码，以促进可重复的研究并加速在稳健的桌面CUAs上的进展。完整数据集已通过此https URL公开发布。",
        "地址": "https://arxiv.org/pdf/2511.04307.pdf"
    },
    {
        "名称": "2025 [2511.03929] NVIDIA Nemotron Nano V2 VL.pdf",
        "作者": "NVIDIA: Amala Sanjay Deshmukh, Kateryna Chumachenko, Tuomas Rintamaki, Matthieu Le, Tyler Poon, Danial Mohseni Taheri, Ilia Karmanov, Guilin Liu, Jarno Seppanen, Guo Chen, Karan Sapra, Zhiding Yu, Adi Renduchintala, Charles Wang, Peter Jin, Arushi Goel, Mike Ranzinger, Lukas Voegtle, Philipp Fischer, Timo Roman, Wei Ping, Boxin Wang, Zhuolin Yang, Nayeon Lee, Shaokun Zhang, Fuxiao Liu, Zhiqi Li, Di Zhang, Greg Heinrich, Hongxu (Danny)Yin, Song Han, Pavlo Molchanov, Parth Mannan, Yao Xu, Jane Polak Scowcroft, Tom Balough, Subhashree Radhakrishnan, Paris Zhang, Sean Cha, Ratnesh Kumar, Zaid Pervaiz Bhat, Jian Zhang, Darragh Hanley, Pritam Biswas, Jesse Oliver, Kevin Vasques, Roger Waleffe, Duncan Riach, Oluwatobi Olabiyi, Ameya Sunil Mahabaleshwarkar, Bilal Kartal, Pritam Gundecha, Khanh Nguyen, Alexandre Milesi, Eugene Khvedchenia, Ran Zilberstein, Ofri Masad, Natan Bagrov, Nave Assaf, Tomer Asida, Daniel Afrimi, Amit Zuker, Netanel Haber, Zhiyu Cheng, Jingyu (Justin)Xin, Di (Allan)Wu, Nik Spirin, Maryam Moosaei, Roman Ageev, Vanshil Atul Shah, Yuting Wu, Daniel Korzekwa, Unnikrishnan Kizhakkemadam Sreekumar, Wanli Jiang, Padmavathy Subramanian, Alejandra Rico, Sandip Bhaskar, Saeid Motiian, Kedi Wu, Annie Surla, Chia-Chih Chen, Hayden Wolff, Matthew Feinberg, Melissa Corpuz, Marek Wawrzos, Eileen Long, Aastha Jhunjhunwala, Paul Hendricks, Farzan Memarian, Benika Hall, Xin-Yu Wang, David Mosallanezhad, Soumye Singhal, Luis Vega, Katherine Cheung, Krzysztof Pawelec, Michael Evans, Katherine Luna, Jie Lou, Erick Galinkin\n\n\n        , Akshay Hazare, Kaustubh Purandare, Ann Guan, Anna Warno, Chen Cui, Yoshi Suhara, Shibani Likhite, Seph Mard, Meredith Price, Laya Sleiman, Saori Kaji, Udi Karpas, Kari Briski, Joey Conway, Michael Lightstone, Jan Kautz, Mohammad Shoeybi, Mostofa Patwary, Jonathen Cohen, Oleksii Kuchaiev, Andrew Tao, Bryan Catanzaro\n\n\n    et al. (22 additional authors not shown)\n You must enable JavaScript to view entire author list.",
        "摘要": "摘要：我们介绍了Nemotron Nano V2 VL，这是Nemotron视觉语言系列的最新型号，旨在加强现实世界文档理解、长视频理解和推理任务。Nemotron Nano V2 VL在所有视觉和文本领域都比我们之前的型号Llama-3.1-Nemotron-Nano-VL-8B有了显著提升，这些提升主要来自模型架构、数据集和训练方法的重大改进。Nemotron Nano V2 VL基于Nemotron Nano V2，一个混合Mamba-Transformer的大规模语言模型，并采用创新的标记减少技术，在长文档和视频场景中实现了更高的推理吞吐量。我们将发布BF16、FP8和FP4格式的模型检查点，并共享我们的大部分数据集、训练方法和训练代码。",
        "地址": "https://arxiv.org/pdf/2511.03929.pdf"
    },
    {
        "名称": "2025 [2511.03774] Contamination Detection for VLMs using Multi-Modal Semantic Perturbation.pdf",
        "作者": "Jaden Park, Mu Cai, Feng Yao, Jingbo Shang, Soochahn Lee, Yong Jae Lee",
        "摘要": "摘要：近年来，视觉-语言模型（VLMs）在许多基准任务中取得了最先进的性能。然而，使用互联网规模、通常是专有的预训练语料库引发了从业者和用户的一个关键问题：测试集泄漏导致的性能膨胀。尽管先前的工作已经提出了诸如预训练数据去污和长语言模型（LLMs）基准重设计等缓解策略，但开发用于检测受污染的VLMs的方法这一补充方向仍未得到充分探讨。为了填补这一空白，我们故意在流行基准上污染开源的VLMs，并显示现有的检测方法要么完全失败，要么表现出不一致的行为。随后，我们提出了一种基于多模态语义扰动的新颖、简单而有效的检测方法，证明了受污染的模型在受控扰动下无法泛化。最后，我们通过多种现实的污染策略验证了我们的方法，确认了其鲁棒性和有效性。代码和受扰动的数据集将公开发布。",
        "地址": "https://arxiv.org/pdf/2511.03774.pdf"
    },
    {
        "名称": "2025 [2511.04217] The Strong Lottery Ticket Hypothesis for Multi-Head Attention Mechanisms.pdf",
        "作者": "Hikari Otsuka, Daiki Chijiwa, Yasuyuki Okoshi, Daichi Fujiki, Susumu Takeuchi, Masato Motomura",
        "摘要": "摘要：强彩票票理论假设（SLTH）认为高性能的子网络，称为强彩票票（SLT），隐藏在随机初始化的神经网络中。尽管最近的理论研究已经在各种神经架构中建立了SLTH，但针对变压器架构的SLTH仍然缺乏理论理解。特别是，目前的SLTH理论尚未涵盖变压器的核心组件——多头注意力机制（MHA）。为了解决这一差距，我们引入了对MHA中SLT存在情况的理论分析。我们证明了，如果一个随机初始化的具有$H$个头和输入维度$d$的MHA在键和值中具有隐含维度$O(d\\\\log(Hd^{3/2}))$，它包含一个近似具有相同输入维度的任意MHA的SLT，并且具有很高的概率。此外，通过利用该理论分析，我们将SLTH扩展到没有归一化层的变压器中。我们通过实验验证了我们的理论发现，证明在源模型（MHA和变压器）中的SLT与近似目标估计模型间的近似误差会随着源模型隐含维度的增加而呈指数下降。\n\n作者：光冈晶，大木支治，岡村康幸，藤木大治，武内晋，元村正人\n\n评论：22页，8幅图\n\nURL: https://arxiv.org/pdf/2511.04217.pdf\n\n标题：2025 [2511.04217] 多头注意力机制的强彩票票理论.pdf",
        "地址": "https://arxiv.org/pdf/2511.04217.pdf"
    },
    {
        "名称": "2025 [2511.04655] Benchmark Designers Should \"Train on the Test Set\" to Expose Exploitable Non-Visual Shortcuts.pdf",
        "作者": "Ellis Brown, Jihan Yang, Shusheng Yang, Rob Fergus, Saining Xie",
        "摘要": "摘要：稳健的基准对于评估多模态大语言模型（MLLMs）至关重要。然而，我们发现许多模型可以在多模态基准测试中取得高分，而不需要强大的视觉理解，而是利用了偏差、语言先验和表面模式。这对于本应需要视觉输入的以视觉为中心的基准尤其有问题。我们采用了一个基准设计的诊断原则：如果一个基准可以被利用，它就会被利用。因此，设计者应首先尝试“利用”自己的基准，使用诊断和去偏程序系统地识别和减轻非视觉偏差。有效的诊断需要直接“在测试集上训练”——探测已发布的测试集中的内在、可利用的模式。我们通过两个组成部分实践这一标准。首先，我们使用一种“测试集压力测试”(TsT)方法诊断基准的脆弱性。我们主要的诊断工具是通过$k$-折交叉验证在测试集的非视觉、文本输入上微调一个强大的大语言模型，揭示快捷性能并为每个样本赋予偏差评分$s(x)$。我们还补充了一个基于轻量级随机森林的诊断工具，操作手工制作的特征以实现快速、可解释的审计。其次，我们通过使用“迭代偏差修剪”(IBP)过程过滤高偏差样本来去偏基准。将这一框架应用于四个基准——VSI-Bench、CV-Bench、MMMU和VideoMME——我们发现了广泛存在的非视觉偏差。作为一个案例研究，我们应用我们的完整框架创建了VSI-Bench-Debiased，展示了减少的非视觉可解性和比原版更大的视觉盲区性能差距。\n\n作者：Ellis Brown, Jihan Yang, Shusheng Yang, Rob Fergus, Saining Xie\n\n备注：项目页面：此 https URL\n\nURL：https://arxiv.org/pdf/2511.04655.pdf\n\n标题：基准设计者应该“在测试集上训练”以揭示可利用的非视觉捷径",
        "地址": "https://arxiv.org/pdf/2511.04655.pdf"
    },
    {
        "名称": "2025 [2511.03996] Learning Vision-Driven Reactive Soccer Skills for Humanoid Robots.pdf",
        "作者": "Yushi Wang, Changsheng Luo, Penghui Chen, Jianran Liu, Weijian Sun, Tong Guo, Kechang Yang, Biao Hu, Yangang Zhang, Mingguo Zhao",
        "摘要": "摘要：人形机器人足球对具身智能提出了代表性的挑战，要求机器人在紧密耦合的感知-行动循环中运行。然而，现有系统通常依赖于解耦模块，导致在动态环境中出现延迟响应和不连贯的行为，而现实世界的感知限制进一步加剧了这些问题。在这项工作中，我们提出了一种基于统一强化学习的控制器，使人形机器人通过直接整合视觉感知和运动控制来获得反应型足球技能。我们的方法将对抗性运动先验扩展到现实世界的动态环境中的感知设置，桥接了运动模仿和视觉驱动的动态控制。我们引入了一种与虚拟感知系统相结合的编码解码架构，该系统模拟现实世界的视觉特征，使策略能够从不完美的观察中恢复特权状态，并在感知和行动之间建立主动协调。结果表明，该控制器在各种场景中展示了强大的反应性，能够在真实的RoboCup比赛中始终如一地执行连贯且稳健的足球行为。\n\n作者：王羽石, 罗长生, 陈鹏辉, 刘健然, 孙维健, 郭桐, 杨克昌, 胡彪, 张阳刚, 赵明国\n\n链接：https://arxiv.org/pdf/2511.03996.pdf",
        "地址": "https://arxiv.org/pdf/2511.03996.pdf"
    },
    {
        "名称": "2025 [2511.03295] How to Evaluate Speech Translation with Source-Aware Neural MT Metrics.pdf",
        "作者": "Mauro Cettolo, Marco Gaido, Matteo Negri, Sara Papi, Luisa Bentivogli",
        "摘要": "摘要：自动评估语音到文本翻译（ST）系统通常通过将翻译假设与一个或多个参考翻译进行比较来进行。虽然在某种程度上有效，但这种方法继承了基于参考的评估的局限性，忽略了来自源输入的宝贵信息。在机器翻译（MT）中，最近的进展显示融合源文本的神经度量与人类判断的相关性更强。然而，将这一思想扩展到ST并不容易，因为源语音是音频而不是文本，可靠的转录或源与参考之间的对齐通常不可用。在这项工作中，我们进行了首次系统性研究，探索面向源的ST度量，特别关注在源转录不可用的真实操作条件下的评估。我们探讨了两种补充策略来生成输入音频的文本代理：自动语音识别（ASR）转录和参考翻译的反向翻译，并引入了一种新颖的两步跨语言重新分段算法，以解决合成源与参考翻译之间的对齐不匹配问题。我们的实验在跨79对语言和六个具有不同架构和性能水平的ST系统的两个ST基准上进行显示，当词错误率低于20%时，ASR转录构成比反向翻译更可靠的合成源，而反向翻译始终是计算成本更低但仍然有效的替代方案。此外，我们的跨语言重新分段算法使得在ST评估中稳健使用面向源的MT度量成为可能，为更准确和合理的语音翻译评估方法铺平了道路。",
        "地址": "https://arxiv.org/pdf/2511.03295.pdf"
    },
    {
        "名称": "2025 [2511.04668] SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding.pdf",
        "作者": "Ellis Brown, Arijit Ray, Ranjay Krishna, Ross Girshick, Rob Fergus, Saining Xie",
        "摘要": "摘要：尽管多模态语言模型在高层次视频理解方面取得了令人印象深刻的成果，但在时空维度上的空间推理方面仍存在困难。目前的空间训练方法依赖真实世界的视频数据，然而获取带有精确空间注释的多样化镜头仍然是一个瓶颈。为了解决这一瓶颈问题，我们提出了SIMS-V——一个系统性的数据生成框架，该框架利用3D模拟器的特权信息来创建多模态语言模型的丰富空间视频训练数据。通过这个框架，我们研究了模拟数据的哪些属性能够有效驱动实际世界的迁移，并通过对问题类型、混合和规模的系统性消融研究来探讨这些属性。我们确定了三个最少集合的问题类别（度量测量、依赖视角的推理和时间跟踪），这些类别在发展的可迁移空间智能方面最为有效，尽管使用较少的问题类型，却超越了全面覆盖的效果。这些见解使得训练高度高效：我们的70亿参数视频LLM在仅使用25,000个模拟示例微调的情况下，性能超越了较大的720亿参数模型基线，并在严格的现实世界空间推理基准上与专有模型达到了竞争水平。我们的方法展示了强大的泛化能力，在一般视频理解任务上保持表现，同时在具身和现实世界的空间任务上显示显著改进。\n\nAuthors（作者）：Ellis Brown, Arijit Ray, Ranjay Krishna, Ross Girshick, Rob Fergus, Saining Xie\n\nComment（评论）：项目页面：此HTTPS URL\n\nURL：https://arxiv.org/pdf/2511.04668.pdf\n\nTitle（标题）：2025 [2511.04668] SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding.pdf",
        "地址": "https://arxiv.org/pdf/2511.04668.pdf"
    },
    {
        "名称": "2025 [2510.27656] RDMA Point-to-Point Communication for LLM Systems.pdf",
        "作者": "Nandor Licker (1), Kevin Hu (1), Vladimir Zaytsev (1), Lequn Chen (1) ((1) Perplexity AI)",
        "摘要": "摘要：新兴的大型语言模型（LLM）系统模式，如解耦推理、专家混合（MoE）路由和异步强化微调，要求灵活的点对点通信，而不仅仅是简单的集体操作。现有的实现方法依赖于特定的网络接口控制器（NIC），阻碍了其在推理引擎中的集成以及在不同硬件提供商之间的可移植性。我们提出了TransferEngine，它桥接了常见的NIC功能，提供了统一的接口。TransferEngine公开了一侧的WriteImm操作以及用于完成通知的ImmCounter原语，而无需网络传输顺序假设，且透明地管理每个GPU的多个NIC。我们展示了在NVIDIA ConnectX-7和AWS Elastic Fabric Adapter（EFA）上达到400 Gbps的峰值吞吐量。我们通过三个生产系统展示了TransferEngine的应用：（1）具有动态扩展能力的解耦推理中的KvCache传输，（2）实现了1.3秒的万亿参数模型强化学习权重更新，（3）在ConnectX-7上超过DeepEP解码延迟的MoE调度/组合实现，并且在EFA上首次实现了可行的延迟。我们证明了我们的便携式点对点通信既补充了集体操作，又避免了锁定问题。",
        "地址": "https://arxiv.org/pdf/2510.27656.pdf"
    },
    {
        "名称": "2025 [2511.02280] SAIL-RL: Guiding MLLMs in When and How to Think via Dual-Reward RL Tuning.pdf",
        "作者": "Fangxun Shu, Yongjie Ye, Yue Liao, Zijian Kang, Weijie Yin, Jiacong Wang, Xiao Liang, Shuicheng Yan, Chao Feng",
        "摘要": "摘要：我们介绍了一种强化学习（RL）后训练框架SAIL-RL，它通过教授多模态大型语言模型（MLLMs）何时以及如何思考，从而增强其推理能力。现有方法受限于仅结果监督，这种方法奖励正确答案而不保证合理推理，并且受限于统一的思维策略，导致在简单任务上过度思考而在复杂任务上思考不足。SAIL-RL通过双重奖励系统解决了这些问题：思考奖励通过事实基础、逻辑一致性和答案一致性评估推理质量，而判断奖励自适应地确定深层推理或直接回答是否合适。对最先进的SAIL-VL2的实验表明，SAIL-RL在4B和8B规模上均改善了推理和多模态理解基准，达到了与商用闭源模型（如GPT-4o）竞争的性能，并大幅减少了幻觉现象，确立了其作为构建更可靠和适应性更强的MLLMs的原则框架。代码将在此URL提供。",
        "地址": "https://arxiv.org/pdf/2511.02280.pdf"
    },
    {
        "名称": "2025 [2511.00956] EVTAR: End-to-End Try on with Additional Unpaired Visual Reference.pdf",
        "作者": "Liuzhuozheng Li, Yue Gong, Shanyuan Liu, Bo Cheng, Yuhang Ma, Liebucha Wu, Dengyang Jiang, Zanyi Wang, Dawei Leng, Yuhui Yin",
        "摘要": "摘要：我们提出了EVTAR，一个附加参考的端到端虚拟试穿模型，通过整合参考图像来增强试穿精度，直接将目标服装适配到人物图像上。大多数现有的虚拟试穿方法依赖于复杂的输入，如去身份化人物图像、人体姿势、密种标图或身体关键点，导致其在实际应用中劳神费力且不现实。相反，EVTAR采用了两阶段的训练策略，使得推理变得简单，仅需源图像和目标服装输入即可生成试穿结果，而无需使用面具、密种标图或分割图。此外，EVTAR利用穿着相同衣物的不同个体的参考图像，更好地保留了服装的纹理和细粒度细节。这个机制类似于人类在选择服装时考虑参考模型，从而模拟出更真实和高质量的穿衣效果。我们通过补充参考和未配对的人物图像丰富了训练数据以支持这些能力。我们在两个广泛使用的基准和多样化任务上评估EVTAR，结果一致验证了我们方法的有效性。",
        "地址": "https://arxiv.org/pdf/2511.00956.pdf"
    }
]