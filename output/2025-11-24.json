[
    {
        "名称": "2025 [2511.16334] OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe.pdf",
        "作者": "Kaichen Zhang, Keming Wu, Zuhao Yang, Kairui Hu, Bin Wang, Ziwei Liu, Xingxuan Li, Lidong Bing",
        "摘要": "摘要: 最近在大规模推理模型方面的进展激发了将这些能力扩展到多模态领域的兴趣。然而，尽管在视觉推理方面取得了显著进展，缺乏透明和可重复的数据整理和训练策略仍然是可扩展研究的主要障碍。在这项工作中，我们介绍了OpenMMReasoner，这是一种完全透明的，涉及监督微调（SFT）和强化学习（RL）的两阶段多模态推理配方。在SFT阶段，我们构建了一个包含874K样本的冷启动数据集，并进行了严格的逐步验证，为推理能力提供了坚实的基础。随后的RL阶段利用了一个涵盖不同领域的74K样本数据集，进一步磨练和稳定了这些能力，从而实现了更稳健和高效的学习过程。广泛的评估表明，我们的训练配方不仅超越了强基线，还突出了数据质量和训练设计在塑造多模态推理性能中的关键作用。值得注意的是，我们的方法在九个多模态推理基准测试中比Qwen2.5-VL-7B-Instruct基线提高了11.6%，为未来大规模多模态推理研究奠定了坚实的经验基础。我们在此公开了我们的所有代码、管道和数据。",
        "地址": "https://arxiv.org/pdf/2511.16334.pdf"
    },
    {
        "名称": "2025 [2511.15210] Unveiling Intrinsic Dimension of Texts: from Academic Abstract to Creative Story.pdf",
        "作者": "Vladislav Pedashenko, Laida Kushnareva, Yana Khassan Nibal, Eduard Tulchinskii, Kristian Kuznetsov, Vladislav Zharchinskii, Yury Maximov, Irina Piontkovskaya",
        "摘要": "摘要：内在维度（ID）是现代大型语言模型（LLM）分析中的一种重要工具，它为训练动态、规模行为和数据集结构的研究提供了信息。然而，ID的文本决定因素却未被充分探索。通过跨编码器分析、语言特征和稀疏自动编码器（SAE），我们提供了第一个将ID根植于可解释文本属性中的综合研究。在本文中，我们确立了三个关键发现。首先，ID与基于熵的度量是互补的：在控制文本长度后，两者不相关，ID捕捉的是与预测质量正交的几何复杂性。其次，ID表现出强烈的类别分层：科学散文显示低ID（约8），百科内容显示中等ID（约9），创作/意见写作在所有测试模型中显示高ID（约10.5）。这表明当代LLM发现科学文本“表示简单”，而小说则需要更多的自由度。第三，通过SAE，我们识别出因果特征：科学信号（正式语调、报告模板、统计数据）降低ID；人性化信号（个性化、情感、叙述）则增加ID。操控实验确认这些效应是因果性的。因此，对于当代模型来说，科学写作显得相对“容易”，而小说、意见和情感则增加了表示自由度。我们多方面的分析为正确使用ID和对基于ID结果的合理解释提供了实用指导。",
        "地址": "https://arxiv.org/pdf/2511.15210.pdf"
    },
    {
        "名称": "2025 [2511.15705] GeoVista: Web-Augmented Agentic Visual Reasoning for Geolocalization.pdf",
        "作者": "Yikun Wang, Zuyan Liu, Ziyi Wang, Pengfei Liu, Han Hu, Yongming Rao",
        "摘要": "摘要：当前的智能视觉推理研究使深度多模态理解成为可能，但主要集中在图像处理工具上，尚未解决更通用的智能模型。在这项工作中，我们重新探讨了地理定位任务，这不仅需要细致的视觉定位，还需要通过网络搜索来确认或细化推理过程中的假设。由于现有的地理定位基准未能满足高分辨率图像和智能深入推理中的定位挑战，我们创建了GeoBench，这是一项基准，包含来自世界各地的照片和全景图，以及不同城市的卫星图像子集，以严格评估智能模型的地理定位能力。我们还提出了GeoVista，一种无缝集成工具调用的智能模型，包括放大感兴趣区域的图像放大工具和检索相关网络信息的网络搜索工具。我们为其开发了完整的训练管道，包括冷启动监督微调（SFT）阶段，以学习推理模式和工具使用先验知识，然后通过强化学习（RL）阶段进一步增强推理能力。我们采用分层奖励来利用多级地理信息并改善总体地理定位性能。实验结果表明，GeoVista在地理定位任务上远远优于其他开源智能模型，并且在大多数指标上表现可与Gemini-2.5-flash和GPT-5等闭源模型相媲美。\n\n作者：王奕坤、刘作言、王子艺、刘鹏飞、胡函、饶永鸣\n\n论文标题：GeoVista：基于网络增强的智能视觉推理地理定位\n\n论文网址：https://arxiv.org/pdf/2511.15705.pdf",
        "地址": "https://arxiv.org/pdf/2511.15705.pdf"
    },
    {
        "名称": "2025 [2511.16719] SAM 3: Segment Anything with Concepts.pdf",
        "作者": "Nicolas Carion, Laura Gustafson, Yuan-Ting Hu, Shoubhik Debnath, Ronghang Hu, Didac Suris, Chaitanya Ryali, Kalyan Vasudev Alwala, Haitham Khedr, Andrew Huang, Jie Lei, Tengyu Ma, Baishan Guo, Arpit Kalla, Markus Marks, Joseph Greer, Meng Wang, Peize Sun, Roman Rädle, Triantafyllos Afouras, Effrosyni Mavroudi, Katherine Xu, Tsung-Han Wu, Yu Zhou, Liliane Momeni, Rishi Hazra, Shuangrui Ding, Sagar Vaze, Francois Porcher, Feng Li, Siyuan Li, Aishwarya Kamath, Ho Kei Cheng, Piotr Dollár, Nikhila Ravi, Kate Saenko, Pengchuan Zhang, Christoph Feichtenhofer",
        "摘要": "摘要：\n我们介绍了Segment Anything Model (SAM) 3，这是一种统一的模型，可以基于概念提示检测、分割和跟踪图像和视频中的对象。我们将概念提示定义为简短的名词短语（例如“黄色校车”）、图像实例或两者的组合。可提示的概念分割（Promptable Concept Segmentation，PCS）接收这些提示，并返回所有匹配对象实例的分割掩码和唯一标识。为了推进PCS，我们构建了一个可扩展的数据引擎，该引擎在图像和视频中生成了具有400万唯一概念标签（包括难负样本）的高质量数据集。我们的模型由一个图像级别的检测器和一个基于内存的视频跟踪器组成，它们共享一个单一的骨干网。通过分离识别和定位，并引入一个存在头（presence head），提高了检测精度。SAM 3在图像和视频PCS中将现有系统的准确度提高了一倍，并改进了之前SAM在视觉分割任务上的能力。我们开源了SAM 3以及我们的新可提示概念分割（SA-Co）基准。\n\n原文链接：\n[https://arxiv.org/pdf/2511.16719.pdf](https://arxiv.org/pdf/2511.16719.pdf)\n文章标题：\n2025 [2511.16719] SAM 3: Segment Anything with Concepts.pdf",
        "地址": "https://arxiv.org/pdf/2511.16719.pdf"
    },
    {
        "名称": "2025 [2511.13593] O-Mem: Omni Memory System for Personalized, Long Horizon, Self-Evolving Agents.pdf",
        "作者": "Piaohong Wang, Motong Tian, Jiaxian Li, Yuan Liang, Yuqing Wang, Qianben Chen, Tiannan Wang, Zhicong Lu, Jiawei Ma, Yuchen Eleanor Jiang, Wangchunshu Zhou",
        "摘要": "摘要：最近在使用大型语言模型驱动的代理方面取得的进展表明，这些代理在生成类人回应方面具有显著潜力；然而，由于上下文一致性和动态个性化方面的限制，它们在复杂环境中保持长期互动仍然面临挑战。现有的记忆系统通常依赖于在检索前进行语义分组，这可能忽略语义上无关但至关重要的用户信息，并引入检索噪音。在本报告中，我们提出了O-Mem的初步设计，一种基于主动用户分析的新型记忆框架，该框架能够从用户与代理的主动互动中动态提取和更新用户特征和事件记录。O-Mem支持对人物属性和主题相关上下文的分层检索，能够生成更具适应性和连贯性的个性化回应。在公共的LoCoMo基准测试中，O-Mem取得了51.67%的成绩，比以前的最先进系统LangMem提高了近3%；在PERSONAMEM测试中，O-Mem达到62.99%，比以前的最先进系统A-Mem提高了3.5%。与以前的记忆框架相比，O-Mem还提升了标记和互动响应时间的效率。我们的工作为未来开发高效且类人的个性化人工智能助手开辟了令人期待的方向。",
        "地址": "https://arxiv.org/pdf/2511.13593.pdf"
    },
    {
        "名称": "2025 [2511.17502] RynnVLA-002: A Unified Vision-Language-Action and World Model.pdf",
        "作者": "Jun Cen, Siteng Huang, Yuqian Yuan, Kehan Li, Hangjie Yuan, Chaohui Yu, Yuming Jiang, Jiayan Guo, Xin Li, Hao Luo, Fan Wang, Deli Zhao, Hao Chen",
        "摘要": "摘要：我们介绍了RynnVLA-002，一个统一的视觉-语言-动作（VLA）和世界模型。该世界模型利用动作和视觉输入来预测未来的图像状态，通过学习环境的基本物理来优化动作生成。另一方面，VLA模型从图像观测中产生后续动作，增强了视觉理解并支持世界模型的图像生成。RynnVLA-002的统一框架使得环境动态和动作规划的联合学习成为可能。我们的实验表明，RynnVLA-002优于单独的VLA和世界模型，展示了它们的相互增强。我们在模拟和真实世界的机器人任务中评估了RynnVLA-002。在LIBERO仿真基准测试中，RynnVLA-002在无需预训练的情况下达到了97.4%的成功率，而在真实世界的LeRobot实验中，其集成的世界模型将总体成功率提高了50%。",
        "地址": "https://arxiv.org/pdf/2511.17502.pdf"
    },
    {
        "名称": "2025 [2511.17220] Parrot: Persuasion and Agreement Robustness Rating of Output Truth -- A Sycophancy Robustness Benchmark for LLMs.pdf",
        "作者": "Yusuf Çelebi, Mahmoud El Hussieni, Özay Ezerceli",
        "摘要": "摘要：本研究提出了PARROT（Persuasion and Agreement Robustness Rating of Output Truth），一个专注于稳健性的框架，旨在衡量大型语言模型（LLMs）在用户受到权威和说服的社会压力下发生的准确性下降现象（即过度一致性现象）。PARROT通过双盲评估将同一问题的中性版本与权威性错误版本进行比较，分离因果效应；利用基于对数似然的校准跟踪量化对正确和强加的错误响应的信心转变；并使用八种行为分类对失败模式（如稳健正确，阿谀同意，加强错误，顽固错误，自我纠正等）进行系统分类。我们使用1,302个MMLU风格的选择题在13个领域和特定领域的权威模板中评估了22个模型。研究结果显示显著的异质性：先进模型（例如GPT-5, GPT-4.1, Claude Sonnet 4.5）表现出低“追随率”（$\\\\leq 11\\\\%， GPT-5: 4\\\\%）和最小的准确性损失，而较旧/较小的模型显示出严重的认知崩溃（GPT-4: 80\\\\%， Qwen 2.5-1.5B: 94\\\\%）。危害不仅限于响应变化；弱模型在正确响应上的信心减弱，同时对强加的错误响应的信心增加。虽然国际法和领域级别的全球知识表现出高度脆弱性，基础数学则相对强韧。因此，我们认为“抗压过拟合”的目标应当作为与准确性、避免伤害和隐私一样的重要指标，以确保在实际部署中的安全性。",
        "地址": "https://arxiv.org/pdf/2511.17220.pdf"
    },
    {
        "名称": "2025 [2511.17344] Loomis Painter: Reconstructing the Painting Process.pdf",
        "作者": "Markus Pobitzer, Chang Liu, Chenyi Zhuang, Teng Long, Bin Ren, Nicu Sebe",
        "摘要": "摘要: 步骤性绘画教程对于学习艺术技巧至关重要，但现有的视频资源（例如YouTube）缺乏互动性和个性化。尽管近期生成模型在艺术图像合成方面取得了进展，但它们难以跨媒体泛化，且常表现出时间或结构不一致，从而阻碍了人类创作流程的真实再现。为了解决这一问题，我们提出了一个统一框架，用于多媒体绘画过程生成，其语义驱动样式控制机制将多种媒体嵌入扩散模型的条件空间，并使用跨媒体样式增强。这使得纹理演变和过程转换在各个风格之间保持一致。一种反向绘画训练策略进一步确保顺滑和符合人类创作习惯的生成。我们还构建了一个大规模的真实绘画过程数据集，并评估了跨媒体一致性、时间连贯性和最终图像保真度，在LPIPS、DINO和CLIP指标上取得了优异的成绩。最后，我们的感知距离曲线（PDP）定量模型了创作序列，即构图、色彩分区和细节完善，反映了人类艺术创作的进程。",
        "地址": "https://arxiv.org/pdf/2511.17344.pdf"
    },
    {
        "名称": "2025 [2511.16825] WorldGen: From Text to Traversable and Interactive 3D Worlds.pdf",
        "作者": "Dilin Wang, Hyunyoung Jung, Tom Monnier, Kihyuk Sohn, Chuhang Zou, Xiaoyu Xiang, Yu-Ying Yeh, Di Liu, Zixuan Huang, Thu Nguyen-Phuoc, Yuchen Fan, Sergiu Oprea, Ziyan Wang, Roman Shapovalov, Nikolaos Sarafianos, Thibault Groueix, Antoine Toisoul, Prithviraj Dhar, Xiao Chu, Minghao Chen, Geon Yeong Park, Mahima Gupta, Yassir Azziz, Rakesh Ranjan, Andrea Vedaldi",
        "摘要": "摘要: 我们介绍了WorldGen系统，它可以直接从文本提示自动创建大型互动3D世界。我们的方法将自然语言描述转化为可探索、完全贴图的环境，这些环境可以立即在标准游戏引擎中进行探索或编辑。通过结合大语言模型驱动的场景布局推理、程序生成、基于扩散的3D生成、和对象感知的场景分解，WorldGen弥合了创造意图和功能虚拟空间之间的差距，使得创作者无需手动建模或专业3D知识就可以设计出一致的、可导航的世界。该系统是完全模块化的，并支持对布局、规模和风格的精细控制，生成几何一致、视觉丰富和实时渲染效率高的世界。这项工作代表了在规模上可访问的生成式世界构建迈出了一步，推进了3D生成式人工智能在游戏、模拟和沉浸式社交环境应用中的前沿。",
        "地址": "https://arxiv.org/pdf/2511.16825.pdf"
    },
    {
        "名称": "2025 [2511.11007] VisMem: Latent Vision Memory Unlocks Potential of Vision-Language Models.pdf",
        "作者": "Xinlei Yu, Chengming Xu, Guibin Zhang, Zhangquan Chen, Yudong Zhang, Yongbo He, Peng-Tao Jiang, Jiangning Zhang, Xiaobin Hu, Shuicheng Yan",
        "摘要": "摘要：尽管视觉语言模型（VLMs）取得了显著成功，但它们在一系列复杂视觉任务中的性能常因“视觉处理瓶颈”而受阻：即在长时间生成过程中倾向于失去视觉证据的基础，同时表现出上下文化视觉体验的不足。受人类认知记忆理论的启发，该理论区分了短期的视觉主导记忆和长期的语义主导记忆，我们提出了VisMem，一个认知对齐框架，为VLMs配备了动态潜在视觉记忆，包括一个用于精细感知保持的短期模块和一个用于抽象语义整合的长期模块。这些记忆在推理过程中无缝调用，使VLMs在思考和生成过程中保持感知的真实度和语义的一致性。在不同视觉基准上的理解、推理和生成的广泛实验表明，VisMem在相较于基础模型平均性能提升11.8%，并且优于所有对比模型，确立了潜在空间记忆增强的新范式。代码将会发布：此网址。",
        "地址": "https://arxiv.org/pdf/2511.11007.pdf"
    },
    {
        "名称": "2025 [2511.16175] Mantis: A Versatile Vision-Language-Action Model with Disentangled Visual Foresight.pdf",
        "作者": "Yi Yang, Xueqi Li, Yiyang Chen, Jin Song, Yihan Wang, Zipeng Xiao, Jiadi Su, You Qiaoben, Pengfei Liu, Zhijie Deng",
        "摘要": "摘要：视觉-语言-动作（VLA）模型的最新进展表明，视觉信号可以有效补充稀疏的动作监督。然而，让VLA直接预测高维视觉状态会分散模型容量并带来高昂的训练成本，而将视觉状态压缩为更紧凑的监督信号不可避免地会带来信息瓶颈。此外，由于忽视了语言监督，现有方法往往在理解和推理能力方面表现较差。本文介绍了Mantis，一个新颖的框架，采用了解耦的视觉预见（DVF）来解决这些问题。具体而言，Mantis通过元查询和扩散Transformer（DiT）头的结合，将视觉预见预测从主干网络中分离出来。通过残差连接将当前视觉状态提供给DiT，一个简单的下一个状态预测目标使元查询能够自动捕捉描述视觉轨迹的潜在动作，从而促进显式动作的学习。这种解耦减少了VLA主干的负担，使其能够通过语言监督保持理解和推理能力。实证研究表明，通过在人类操作视频、机器人演示和图文对上进行预训练，Mantis在微调后在LIBERO基准测试中达到了96.7%的成功率，超越了强大的基线并表现出高收敛速度。现实世界的评估显示，Mantis在跟随指令能力、对未见过指令的泛化能力和推理能力上优于领先的开源VLA模型$\\\\pi_{0.5}$.代码和权重已发布以支持开源社区。\n\n作者：杨毅、李雪琪、陈一扬、宋瑾、王艺涵、肖子鹏、苏家迪、乔本友、刘鹏飞、邓志杰\n\n链接：https://arxiv.org/pdf/2511.16175.pdf\n\n标题：2025 [2511.16175] Mantis: 一个具有解耦视觉预见的多功能视觉-语言-动作模型",
        "地址": "https://arxiv.org/pdf/2511.16175.pdf"
    },
    {
        "名称": "2025 [2511.14899] InstructMix2Mix: Consistent Sparse-View Editing Through Multi-View Model Personalization.pdf",
        "作者": "Daniel Gilo, Or Litany",
        "摘要": "摘要：我们研究了从稀疏输入视图进行多视角图像编辑的任务，这些输入视图可以看作是从不同视角捕捉到场景的图像混合。目标是根据文本指令修改场景，同时保持所有视图之间的一致性。现有的方法基于每个场景的神经场或时间注意机制，在这种设置下往往会产生伪影和不一致的编辑。我们提出了InstructMix2Mix (I-Mix2Mix) 框架，它将2D扩散模型的编辑能力提炼到一个预训练的多视角扩散模型中，利用其数据驱动的3D先验来实现跨视角一致性。一个关键贡献是用多视角扩散学生替换Score Distillation Sampling (SDS)中的传统神经场整合器，这需要新的改编：跨时间步的增量学生更新、专门的教师噪声调度器以防止退化，以及一种增强跨视角一致性的注意力修饰，无需额外成本。实验表明，I-Mix2Mix显著改善了多视角一致性，同时保持了高质量的逐帧编辑。\n\n翻译文献链接：https://arxiv.org/pdf/2511.14899.pdf",
        "地址": "https://arxiv.org/pdf/2511.14899.pdf"
    },
    {
        "名称": "2025 [2511.14806] MergeDNA: Context-aware Genome Modeling with Dynamic Tokenization through Token Merging.pdf",
        "作者": "Siyuan Li, Kai Yu, Anna Wang, Zicheng Liu, Chang Yu, Jingbo Zhou, Qirong Yang, Yucheng Guo, Xiaoming Zhang, Stan Z. Li",
        "摘要": "摘要：对基因组序列进行建模面临两个未解决的挑战：信息密度在不同区域之间差异很大，且没有明确定义的最小词汇单元。依靠四种基本碱基或独立设计的DNA标记器，现有使用朴素掩码语言模型预训练的方法往往不能适应基因组序列的变化复杂性。本文利用Token Merging技术，介绍了一种分层结构，通过上下文感知的预训练任务联合优化动态基因组标记器和潜在的Transformers。在网络结构方面，分词模块通过堆叠多个具有局部窗口约束的可微标记合并块的层，自动将相邻的碱基分块为词，然后一个潜在编码器通过全注意力块捕捉这些合并词的全局上下文。对称地使用潜在解码器和局部解码器，MergeDNA通过两个预训练任务进行学习：合并标记重建同时训练动态分词模块并自适应过滤重要标记，而自适应掩码标记建模学习预测这些过滤的标记以捕捉信息内容。大量实验表明，MergeDNA在三大流行DNA基准测试和多个多组学任务上，通过微调或零样本评估，均表现出优异性能，优于典型的分词方法和大规模DNA基础模型。\n\n作者：Siyuan Li, Kai Yu, Anna Wang, Zicheng Liu, Chang Yu, Jingbo Zhou, Qirong Yang, Yucheng Guo, Xiaoming Zhang, Stan Z. Li\n\n评论：AAAI 2026 （口头报告） 预印本\n\n网址：https://arxiv.org/pdf/2511.14806.pdf\n\n题目：2025 [2511.14806] MergeDNA: 通过标记合并实现动态分词的上下文感知基因组建模",
        "地址": "https://arxiv.org/pdf/2511.14806.pdf"
    },
    {
        "名称": "2025 [2511.16931] OmniScientist: Toward a Co-evolving Ecosystem of Human and AI Scientists.pdf",
        "作者": "Chenyang Shao, Dehao Huang, Yu Li, Keyu Zhao, Weiquan Lin, Yining Zhang, Qingbin Zeng, Zhiyu Chen, Tianxing Li, Yifei Huang, Taozhong Wu, Xinyang Liu, Ruotong Zhao, Mengsheng Zhao, Xuhua Zhang, Yue Wang, Yuanyi Zhen, Fengli Xu, Yong Li, Tie-Yan Liu",
        "摘要": "摘要：随着大规模语言模型（LLM）的快速发展，AI 代理在科学任务中展示了越来越高的熟练度，从假设生成和实验设计到手稿撰写。这类代理系统通常被称为“AI 科学家”。然而，现有的 AI 科学家主要将科学发现表述为一个独立的搜索或优化问题，忽视了科学研究本质上是一个社会性的合作行为。现实世界的科学依赖于由合作机制、贡献归因、同行评审和结构化的科学知识网络组成的复杂科学基础设施。由于缺乏对这些关键维度的建模，当前系统难以建立真正的研究生态系统或与人类科学共同体进行深度互动。为弥补这一差距，我们引入了 OmniScientist 框架，该框架显式地在 AI 科学工作流程中编码了人类研究的基础机制。OmniScientist 不仅实现了数据基础、文献综述、研究构思、实验自动化、科学写作和同行评审的端到端自动化，还通过模拟人类科学系统提供全面的基础设施支持，包括：（1）基于引用网络和概念关联构建的结构化知识系统；（2）一个促进无缝多代理协作和人类研究者参与的合作研究协议（OSP）；（3）一个基于盲对对用户投票和 Elo 排名的开放评估平台（ScienceArena）。这一基础设施不仅使代理能够理解和利用人类的知识系统，还能合作和共同进化，促进可持续和可扩展的创新生态系统。",
        "地址": "https://arxiv.org/pdf/2511.16931.pdf"
    },
    {
        "名称": "2025 [2511.17487] Downscaling Intelligence: Exploring Perception and Reasoning Bottlenecks in Small Multimodal Models.pdf",
        "作者": "Mark Endo, Serena Yeung-Levy",
        "摘要": "摘要: 扩展多模态模型在视觉理解和推理方面取得了显著的进展，但实际需求呼唤更小、更高效的系统。在这项工作中，我们对多模态模型中智能下调进行了系统分析，研究减少大型语言模型（LLM）容量如何影响多模态能力。我们的初步研究揭示了一个有趣的趋势：LLM的缩减不成比例地影响视觉能力，而不是LLM继承的能力。我们接着探讨这种下降主要反映的是视觉推理能力的预期下降还是感知能力的更基本丧失。隔离LLM缩减对感知的影响后，我们发现性能仍然急剧下降，往往与推理的影响相当甚至更大。为了解决这一瓶颈，我们引入了视觉提取调优，它显式地训练模型在任务间一致地提取与指令相关的视觉细节。通过这些提取的视觉细节，我们然后应用逐步推理来生成答案。综合这些组件，形成了我们的Extract+Think方法，为该领域的效率和性能设立了新的标准。",
        "地址": "https://arxiv.org/pdf/2511.17487.pdf"
    },
    {
        "名称": "2025 [2511.17490] Video-R4: Reinforcing Text-Rich Video Reasoning with Visual Rumination.pdf",
        "作者": "Yolo Yunlong Tang, Daiki Shimada, Hang Hua, Chao Huang, Jing Bi, Rogerio Feris, Chenliang Xu",
        "摘要": "摘要：理解富含文本的视频需要阅读小而瞬时的文本提示，这些往往需要反复检查。然而，大多数视频问答模型依赖于对固定帧的一次性感知，导致在细粒度证据上出现幻觉和失败。受到人类在关键区域暂停、放大和重读的启发，我们引入了Video-R4（通过视觉反刍强化文本富集视频推理），这是一种进行视觉反刍的视频推理LMM：迭代选择帧，放大信息丰富的区域，重新编码检索到的像素，并更新其推理状态。我们构建了两个具有可执行反刍轨迹的数据集：用于监督练习的Video-R4-CoT-17k和用于强化学习的Video-R4-RL-30k。我们提出了一个多阶段反刍学习框架，通过SFT和基于GRPO的RL逐步微调7B LMM，以学习原子和混合视觉操作。Video-R4-7B在M4-ViteVQA上取得了最先进的结果，并进一步推广到多页文档QA、幻灯片QA和通用视频QA，证明迭代反刍是像素为基础的多模态推理的有效范式。",
        "地址": "https://arxiv.org/pdf/2511.17490.pdf"
    },
    {
        "名称": "2025 [2511.17074] Diversity Has Always Been There in Your Visual Autoregressive Models.pdf",
        "作者": "Tong Wang, Guanyu Yang, Nian Liu, Kai Wang, Yaxing Wang, Abdelrahman M Shaker, Salman Khan, Fahad Shahbaz Khan, Senmao Li",
        "摘要": "摘要: 视觉自回归（VAR）模型因其创新的下一尺度预测范式，较传统的多步自回归（AR）和扩散模型在推理效率和图像质量方面具有显著优势，最近受到了极大的关注。然而，尽管VAR模型具有高效性，但它们往往会遭遇多样性崩溃，即输出变异性的减少，这类似于少步蒸馏扩散模型中观察到的情况。在本文中，我们介绍了一种简单而有效的方法——DiverseVAR，可以在不需要额外训练的情况下恢复VAR模型的生成多样性。我们的分析表明，特征图的关键组成部分是决定早期尺度多样性形成的关键因素。通过在模型输入中抑制该关键组成部分并在模型输出中放大它，DiverseVAR有效地释放了VAR模型的内在生成潜力，同时保持了高保真合成。实验证明，我们的方法在几乎不影响性能的情况下显著增强了生成多样性。我们的代码将在此HTTPS URL上公开发布。",
        "地址": "https://arxiv.org/pdf/2511.17074.pdf"
    },
    {
        "名称": "2025 [2511.15462] Insights from the ICLR Peer Review and Rebuttal Process.pdf",
        "作者": "Amir Hossein Kargaran, Nafiseh Nikeghbal, Jing Yang, Nedjma Ousidhoum",
        "摘要": "摘要：同行评审是科学出版的基石，包括在顶级机器学习会议如ICLR。随着投稿量的增加，了解评审过程的性质和动态对提高其效率、有效性以及发表论文的质量至关重要。我们对ICLR 2024和2025的同行评审过程进行了大规模分析，重点关注反驳前后的得分和评审员与作者的互动。我们研究了评审得分、作者-评审员的互动、评审提交的时间模式及共同评审员的影响效应。结合定量分析和基于LLM分类的评审文本和反驳讨论，我们识别了每个评级组的共同优势和弱点，以及与得分变化最密切相关的反驳策略趋势。我们的发现显示，初始得分和共同评审员的评分是反驳期间得分变化的最强预测因子，表明存在一定程度的评审员影响。反驳在改善边界论文的结果方面起到宝贵作用，深思熟虑的作者回应可以有意义地改变评审员的观点。更广泛地说，我们的研究提供了基于证据的见解，以改进同行评审过程，指导作者有效的反驳策略，并帮助社区设计更公平、更高效的评审过程。我们的代码和得分变化数据可在此链接获取。\n\n来自论文《Insight from the ICLR Peer Review and Rebuttal Process》的摘要，该文由Amir Hossein Kargaran、Nafiseh Nikeghbal、Jing Yang、Nedjma Ousidhoum撰写，可以通过以下链接获取：https://arxiv.org/pdf/2511.15462.pdf",
        "地址": "https://arxiv.org/pdf/2511.15462.pdf"
    },
    {
        "名称": "2025 [2511.17450] Planning with Sketch-Guided Verification for Physics-Aware Video Generation.pdf",
        "作者": "Yidong Huang, Zun Wang, Han Lin, Dong-Ki Kim, Shayegan Omidshafiei, Jaehong Yoon, Yue Zhang, Mohit Bansal",
        "摘要": "摘要：近年来的视频生成方法越来越多地依赖于规划中间控制信号，如物体轨迹，以提高时间一致性和运动逼真度。然而，这些方法大多采用一次性规划，只适用于简单运动，或者迭代式优化，需要多次调用视频生成器，计算成本高。为克服这些限制，我们提出了SketchVerify，这是一种无需训练、基于草图验证的规划框架，通过引入测试时采样和验证循环，在完整视频生成之前，改进动态一致轨迹（即物理上合理且与指令一致的运动）的运动规划质量。给定提示和参考图像，我们的方法预测多个候选运动计划，并使用视觉-语言验证器对其进行排名，该验证器联合评估与指令的语义对齐和物理可行性。为了有效地评分候选运动计划，我们通过在静态背景上合成物体，将每个轨迹渲染为轻量级视频草图，从而避免了昂贵且重复的基于扩散的合成，同时达到相当的性能。我们迭代优化运动计划，直到找到满意的计划，然后将其传递给轨迹调条件生成器进行最终合成。在WorldModelBench和PhyWorldBench上的实验表明，与竞争基线相比，我们的方法显著提高了运动质量、物理现实性和长期一致性，同时效率大大提高。我们的消融研究进一步表明，增加轨迹候选数量可以一贯提升整体性能。",
        "地址": "https://arxiv.org/pdf/2511.17450.pdf"
    },
    {
        "名称": "2025 [2511.17199] VLA-4D: Embedding 4D Awareness into Vision-Language-Action Models for SpatioTemporally Coherent Robotic Manipulation.pdf",
        "作者": "Hanyu Zhou, Chuanhao Ma, Gim Hee Lee",
        "摘要": "摘要: 视觉-语言-动作(VLA)模型在通用机器人任务中展现了潜力，但在时空一致的操作中仍存在挑战，这要求细粒度的表示。现有方法通常将3D位置嵌入视觉表示中以增强动作的空间精度。然而，这些方法在动作执行的时间一致性控制方面仍困难重重。在这项工作中，我们提出了VLA-4D，一个具有4D意识的通用VLA模型用于时空一致的机器人操作。我们的模型由两个关键设计引导：1) 4D意识视觉表示。我们提取视觉特征，将1D时间嵌入3D位置形成4D嵌入，并通过交叉关注机制将它们融合成统一的视觉表示。2) 时空动作表示。我们扩展传统的空间动作表示，加入时间信息以实现时空规划，并将多模态表示对齐到LLM进行时空动作预测。在这个统一框架内，设计的视觉和动作表示共同使机器人操作在空间上平滑并在时间上连续。此外，我们通过时间动作注释扩展了VLA数据集，以微调我们的模型。我们进行了广泛的实验，以验证我们方法在不同机器人操作任务上的优越性。\n\n网上链接：https://arxiv.org/pdf/2511.17199.pdf\n\n作者: 周涵宇, 马传昊, 李锦喜\n\n标题: VLA-4D: 在视觉-语言-动作模型中嵌入4D意识用于时空一致的机器人操作",
        "地址": "https://arxiv.org/pdf/2511.17199.pdf"
    },
    {
        "名称": "2025 [2511.16110] Multi-Faceted Attack: Exposing Cross-Model Vulnerabilities in Defense-Equipped Vision-Language Models.pdf",
        "作者": "Yijun Yang, Lichao Wang, Jianping Zhang, Chi Harold Liu, Lanqing Hong, Qiang Xu",
        "摘要": "摘要：视觉语言模型（VLMs）的滥用问题日益严重，供应商们为此部署了多重保障措施，包括对齐调优、系统提示和内容审核。然而，这些防御措施在对抗攻击下的实际表现仍缺乏深入研究。我们提出了多方面攻击（MFA）这一框架，系统性地揭示了配备领先防御措施的VLMs（如GPT-4o、Gemini-Pro和Llama-4）在通用安全性上的漏洞。MFA的核心组件是注意力转移攻击（ATA），该方法将有害指令隐藏在具有竞争目标的元任务中。我们基于奖励劫持提供了一个理论角度来解释该攻击为何能成功。为了提高跨模型的可迁移性，我们进一步引入了一个轻量级的迁移增强算法，结合简单的重复策略，共同绕过了输入级和输出级的过滤器，而无需针对具体模型的微调。实验证明，为一个视觉编码器优化的对抗图像能够广泛迁移到未见过的VLMs上，这表明共享的视觉表示创造了跨模型的安全漏洞。总体而言，MFA的成功率达到58.5%，并且持续优于现有方法。在最先进的商业模型上，MFA成功率达到52.8%，超过第二佳攻击方法34%。这些结果挑战了当前防御机制的稳健性，并凸显了现代VLMs中持续存在的安全弱点。代码链接：https://arxiv.org/pdf/2511.16110.pdf",
        "地址": "https://arxiv.org/pdf/2511.16110.pdf"
    },
    {
        "名称": "2025 [2511.15299] Taming Generative Synthetic Data for X-ray Prohibited Item Detection.pdf",
        "作者": "Jialong Sun, Hongguang Zhu, Weizhe Liu, Yunda Sun, Renshuai Tao, Yunchao Wei",
        "摘要": "摘要：\n训练禁用物品检测模型需要大量的X射线安检图像，但是收集和标注这些图像耗时且费力。为了应对数据不足，X射线安检图像合成方法通过合成图像来扩大数据集。然而，之前的方法主要遵循两阶段的流程，在第一阶段实施劳动密集型的前景提取，然后在第二阶段合成图像。这样的流程引入了不可避免的额外劳动成本，并且效率不高。本文提出了一种基于文本生成图像的一阶段X射线安检图像合成流程（Xsyn），并采用了两种有效策略来提高合成图像的实用性。跨注意力优化（CAR）策略利用扩散模型中的跨注意力图来优化边界框标注。背景遮挡建模（BOM）策略在潜在空间中显式建模背景遮挡以增强成像复杂性。据我们所知，与之前的方法相比，Xsyn是第一个在没有额外劳动成本的情况下实现高质量X射线安检图像合成的方法。实验表明，我们的方法比所有之前的方法提高了1.2％的mAP，并且我们的方法生成的合成图像有助于提高各种X射线安检数据集和检测器的禁用物品检测性能。代码可在此https URL获得。",
        "地址": "https://arxiv.org/pdf/2511.15299.pdf"
    },
    {
        "名称": "2025 [2511.13081] Rethinking Saliency Maps: A Cognitive Human Aligned Taxonomy and Evaluation Framework for Explanations.pdf",
        "作者": "Yehonatan Elisha, Seffi Cohen, Oren Barkan, Noam Koenigstein",
        "摘要": "摘要：显著性图在深度学习中的视觉解释中被广泛使用，但其预期目的和与不同用户查询的一致性方面存在根本性共识缺失。这种模糊性阻碍了解释方法的有效评估和实际效用。我们通过引入参考框架×粒度（RFxG）分类法来解决这一问题，这是一种将显著性解释按两个基本轴组织的原则性概念框架：参考框架：区分逐点（“为什么这个预测？”）和对比（“为什么这个而不是其他？”）解释。粒度：从细粒度的类别级别（例如，“为什么是哈士奇？”）到粗粒度的组级别（例如，“为什么是狗？”）的解释。利用RFxG视角，我们展示了现有评估指标的关键局限性，这些指标过分注重逐点忠实性，却忽视了对比推理和语义粒度。为了系统地评估跨RFxG维度的解释质量，我们提出了四个新的忠实度指标。我们的综合评估框架将这些指标应用于十种最先进的显著性方法、四种模型架构和三个数据集。通过倡导向用户意图驱动评估的转变，我们的工作不仅提供了概念基础，还提供了实际工具，必要时开发出不仅忠实于底层模型行为且与人类理解和探究的复杂性相一致的视觉解释。\n\n翻译作者: Yehonatan Elisha, Seffi Cohen, Oren Barkan, Noam Koenigstein\n\n链接: https://arxiv.org/pdf/2511.13081.pdf\n\n题目: 2025 [2511.13081] 重新思考显著性图：一种认知的人类对齐分类法和解释评估框架",
        "地址": "https://arxiv.org/pdf/2511.13081.pdf"
    }
]