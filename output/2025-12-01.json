[
    {
        "名称": "2025 [2511.22699] Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer.pdf",
        "作者": "Z-Image Team, Huanqia Cai, Sihan Cao, Ruoyi Du, Peng Gao, Steven Hoi, Shijie Huang, Zhaohui Hou, Dengyang Jiang, Xin Jin, Liangchen Li, Zhen Li, Zhong-Yu Li, David Liu, Dongyang Liu, Junhan Shi, Qilong Wu, Feng Yu, Chi Zhang, Shifeng Zhang, Shilin Zhou",
        "摘要": "摘要：当前高性能图像生成模型领域由专有系统主导，如Nano Banana Pro和Seedream 4.0。主要的开源替代方案包括Qwen-Image、Hunyuan-Image-3.0和FLUX.2，这些模型的参数数目庞大（20B到80B），在消费级硬件上进行推理和微调变得不切实际。为了解决这一问题，我们提出了Z-Image，这是一个高效的6B参数基础生成模型，基于可扩展的单流扩散变压器（S3-DiT）架构，挑战了“全成本规模化”的范式。通过系统优化整个模型生命周期——从精心挑选的数据基础设施到简化的训练课程——我们在仅314K H800 GPU小时（约63万美元）内完成了完整的训练流程。我们的少步蒸馏方案通过奖励后训练进一步产生Z-Image-Turbo，在企业级H800 GPU上提供亚秒推理延迟，并兼容消费级硬件（<16GB VRAM）。此外，我们的全能预训练模式还支持高效训练Z-Image-Edit，这是一种具有出色指令跟随能力的编辑模型。定性和定量实验均表明，我们的模型在各个维度上的性能与领先竞争对手相当或超越，尤其是在写实图像生成和双语文本渲染方面表现出色，提供的结果可媲美顶级商业模型，从而证明以显著减少的计算开销即可实现最先进的结果。我们公开发布了代码、权重和在线演示，以促进开发负担得起的、但具有最先进技术水平的生成模型。",
        "地址": "https://arxiv.org/pdf/2511.22699.pdf"
    },
    {
        "名称": "2025 [2511.22625] REASONEDIT: Towards Reasoning-Enhanced Image Editing Models.pdf",
        "作者": "Fukun Yin, Shiyu Liu, Yucheng Han, Zhibo Wang, Peng Xing, Rui Wang, Wei Cheng, Yingming Wang, Aojie Li, Zixin Yin, Pengtao Chen, Xiangyu Zhang, Daxin Jiang, Xianfang Zeng, Gang Yu",
        "摘要": "摘要：近年来，图像编辑模型取得了显著进展。一个常见的架构设计是将多模态大语言模型（MLLM）编码器与扩散解码器结合，例如在Step1X-Edit和Qwen-Image-Edit系统中，MLLM对参考图像和指令进行编码，但在训练过程中保持不变。在这项工作中，我们展示了解锁MLLM的推理能力可以进一步推动编辑模型的界限。具体而言，我们探索了两种推理机制，思考和反思，这增强了指令理解和编辑准确性。基于此，我们提出的框架在思考-编辑-反思循环中实现了图像编辑：思考机制利用MLLM的世界知识解释抽象指令，而反思则审核编辑结果，自动纠正意外操作，并确定停止轮次。大量实验表明，我们的推理方法显著提升了性能，当从Step1X-Edit（ReasonEdit-S）初始化我们的DiT时，ImgEdit（+4.3%）、GEdit（+4.7%）和Kris（+8.2%）均有所提高，并且在与Qwen-Image-Edit（ReasonEdit-Q）结合时，在GEdit和Kris上也优于以前的开源方法。",
        "地址": "https://arxiv.org/pdf/2511.22625.pdf"
    },
    {
        "名称": "2025 [2511.23475] AnyTalker: Scaling Multi-Person Talking Video Generation with Interactivity Refinement.pdf",
        "作者": "Zhizhou Zhong, Yicheng Ji, Zhe Kong, Yiying Liu, Jiarui Wang, Jiasun Feng, Lupeng Liu, Xiangyi Wang, Yanjia Li, Yuqing She, Ying Qin, Huan Li, Shuiyang Mao, Wei Liu, Wenhan Luo",
        "摘要": "摘要：最近，多人视频生成开始受到关注。虽然一些初步工作已经探索了音频驱动的多人说话视频生成，但它们往往面临由于多样化的多人数据收集成本高以及难以驱动多个身份并保持一致互动性等挑战。为了解决这些挑战，我们提出了AnyTalker，一个具有可扩展多流处理架构的多人生成框架。具体而言，我们通过一种新颖的身份感知注意机制扩展了扩散变换器的注意块，该机制迭代处理身份-音频对，允许任意扩展可驱动身份。此外，训练多人生成模型需要大量的多人数据。我们提出的训练流程完全依赖单人视频来学习多人说话模式，并通过少量真实的多人片段优化互动性。此外，我们贡献了一个用于评估生成多人视频的自然性和互动性的针对性指标和数据集。大量实验表明，AnyTalker在唇部同步、视觉质量和自然互动性方面取得了显著成果，在数据成本与身份扩展性之间取得了有利平衡。",
        "地址": "https://arxiv.org/pdf/2511.23475.pdf"
    },
    {
        "名称": "2025 [2511.23199] Vision Bridge Transformer at Scale.pdf",
        "作者": "Zhenxiong Tan, Zeqing Wang, Xingyi Yang, Songhua Liu, Xinchao Wang",
        "摘要": "摘要：我们引入了Vision Bridge Transformer (ViBT)，这是布朗桥模型在大规模条件生成上的表现。不同于传统的将噪声转化为数据的扩散模型，桥模型直接模拟输入和输出之间的轨迹，创造了一种高效的数据到数据的转换模式。通过将这些模型扩展到20B和1.3B参数，我们展示了它们在图像和视频翻译任务中的有效性。为了支持这种规模，我们采用了Transformer架构，并提出了一种方差稳定的速度匹配目标以实现稳健的训练。这些进步共同凸显了桥模型在基于指令的图像编辑和复杂视频翻译中的强大性能。",
        "地址": "https://arxiv.org/pdf/2511.23199.pdf"
    },
    {
        "名称": "2025 [2511.22663] Architecture Decoupling Is Not All You Need For Unified Multimodal Model.pdf",
        "作者": "Dian Zheng, Manyuan Zhang, Hongyu Li, Kai Zou, Hongbo Liu, Ziyu Guo, Kaituo Feng, Yexin Liu, Ying Luo, Yan Feng, Peng Pei, Xunliang Cai, Hongsheng Li",
        "摘要": "摘要：统一的用于图像生成和理解的多模态模型代表了向通用人工智能迈出的一大步，吸引了广泛的研究关注。这项任务的主要挑战在于由于理解和生成任务之间固有的相互冲突目标，很难建立最佳的训练范式。为了减轻这些冲突并追求更高性能，许多研究人员采用不同程度的模型解耦（例如，双图像编码器、MOE/MOT架构或冻结的MLLM）。然而，过度的模型解耦会导致生成能力的丧失，破坏统一模型的初衷。在这项工作中，我们旨在探索如何在不求助于模型解耦的情况下减轻任务冲突。首先，我们分析为什么解耦可以通过研究模型的跨模态注意力行为来缓解冲突。我们观察到模型解耦本质上是驱使模型走向特定任务的多模态交互模式，如Qwen-VL和HunyuanImage中所见，并且解耦越彻底，行为越一致。受到这一观察的启发，我们提出了注意力交互对齐（AIA）损失，它在训练过程中显式地学习特定任务的多模态交互模式。为了证明我们的AIA损失的泛化性，我们在SFT和后训练阶段分别将其应用于Emu3和Janus-Pro。不加任何技术细节，AIA不仅优化了跨模态注意力模式，还提高了生成和理解性能。\n\n作者：郑电，张曼远，李宏宇，邹凯，刘宏波，郭子瑜，冯开拓，刘业新，罗英，冯炎，裴鹏，蔡迅良，李鸿盛\n\n评论: 项目页面: this https URL 代码: this https URL\n\n链接：https://arxiv.org/pdf/2511.22663.pdf\n\n标题：2025 [2511.22663] Architecture Decoupling Is Not All You Need For Unified Multimodal Model.pdf",
        "地址": "https://arxiv.org/pdf/2511.22663.pdf"
    },
    {
        "名称": "2025 [2511.22570] DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning.pdf",
        "作者": "Zhihong Shao, Yuxiang Luo, Chengda Lu, Z.Z. Ren, Jiewen Hu, Tian Ye, Zhibin Gou, Shirong Ma, Xiaokang Zhang",
        "摘要": "摘要：大型语言模型在数学推理方面取得了显著进展，这对于人工智能是一个重要的测试平台，如果进一步发展，将对科学研究产生影响。通过扩展使用强化学习奖励最终正确答案的推理，LLMs的表现从较差提高到在一年的时间内饱和了如AIME和HMMT等定量推理竞赛。然而，这种方法存在根本性的限制。追求更高的最终答案准确性并不能解决一个关键问题：正确答案并不能保证推理过程的正确。此外，许多数学任务如定理证明需要严格的逐步推导，而不是数值答案，这使得最终答案的奖励不可适用。为了突破深度推理的极限，我们认为有必要验证数学推理的全面性和严谨性。自我验证对于扩展测试时的计算尤其重要，特别是对于没有已知解的开放问题。为了实现可以自我验证的数学推理，我们研究了如何训练一个准确且可信赖的基于LLM的定理验证器。然后我们使用验证器作为奖励模型训练一个证明生成器，并激励生成器在最终定稿前识别并解决其自身证据中的尽可能多的问题。为了在生成器变得更强时保持生成-验证差距，我们提议扩展验证计算能力，以自动标记新的难以验证的证明，从而创建训练数据以进一步改进验证器。我们最终的模型DeepSeekMath-V2展示了强大的定理证明能力，在IMO 2025和CMO 2024上取得了金牌水平的成绩，并在Putnam 2024上以扩展的测试时计算获得了接近完美的118/120分。\n\n（中文翻译的摘要可能会有细微调整，以确保准确性和流畅性。）",
        "地址": "https://arxiv.org/pdf/2511.22570.pdf"
    },
    {
        "名称": "2025 [2511.22134] DualVLA: Building a Generalizable Embodied Agent via Partial Decoupling of Reasoning and Action.pdf",
        "作者": "Zhen Fang, Zhuoyang Liu, Jiaming Liu, Hao Chen, Yu Zeng, Shiting Huang, Zehui Chen, Lin Chen, Shanghang Zhang, Feng Zhao",
        "摘要": "摘要：\n为了构建具有较强推理能力的可泛化视觉-语言-行为（VLA）模型，一种常见策略是首先在机器人演示上训练一个专长VLA以获取可靠的操作技能，然后结合混合标注的机器人数据和多模态数据来恢复更广泛的推理能力。然而，我们观察到，经过微调后，其推理VLA模型的操作性能往往比专长模型有所下降，这一现象我们称之为操作退化。为了解决这一问题，我们提出了DualVLA，通过精心设计的训练后方法在仍然保留推理能力的同时增强操作性能。我们首先引入了一种双层数据剪枝方法，该方法移除冗余的具体推理，防止其对操作学习产生不利影响。为了进一步强化操作生成，我们设计了一种双教师自适应蒸馏策略，为不同的数据域分配不同的监督信号，同时保持推理能力。为了填补通用VLA的评估空白，我们还提出了VLA评分，它将VLA能力分为推理、意图、动作和对齐四个维度进行更细粒度的评估。实验表明，DualVLA在SimplerEnv中的平均成功率达到61.0，在八个具有竞争力的多模态基准上的平均得分为65.4，展示了在精确动作执行和多模态理解之间更强的平衡。\n\n项目网站：此 https URL。",
        "地址": "https://arxiv.org/pdf/2511.22134.pdf"
    },
    {
        "名称": "2025 [2511.18822] DiP: Taming Diffusion Models in Pixel Space.pdf",
        "作者": "Zhennan Chen, Junwei Zhu, Xu Chen, Jiangning Zhang, Xiaobin Hu, Hanzhen Zhao, Chengjie Wang, Jian Yang, Ying Tai",
        "摘要": "摘要：扩散模型在生成质量和计算效率之间面临着根本的权衡。潜在扩散模型（LDMs）提供了一种高效的解决方案，但却可能面临信息丢失和非端到端训练的问题。相比之下，现有的像素空间模型绕过了VAE，但对于高分辨率合成来说计算量过大。为了解决这一难题，我们提出了DiP，一种高效的像素空间扩散框架。DiP将生成分解为全球和局部两个阶段：一个扩散变压器（DiT）骨干网在大块区域上运行以高效地构建全球结构，同时一个共同训练的轻量级块细化头部利用上下文特征恢复细粒度的局部细节。这种协同设计实现了可比于LDMs的计算效率，而无需依赖VAE。DiP在推理速度上比之前的方法快至10倍，同时总参数量仅增加了0.3%，并在ImageNet 256×256上取得了1.79的FID得分。",
        "地址": "https://arxiv.org/pdf/2511.18822.pdf"
    },
    {
        "名称": "2025 [2511.23319] Every Token Counts: Generalizing 16M Ultra-Long Context in Large Language Models.pdf",
        "作者": "Xiang Hu, Zhanchao Zhou, Ruiqi Liang, Zehuan Li, Wei Wu, Jianguo Li",
        "摘要": "摘要：本研究探讨了构建\"能记住的机器\"的挑战，将长期记忆框定为高效超长上下文建模的问题。我们认为这需要三个关键特性：稀疏性、随机访问灵活性和长度泛化。为了应对超长上下文建模，我们利用了一种新的注意机制——分层稀疏注意力（Hierarchical Sparse Attention，HSA），它满足所有三个特性。我们将HSA集成到Transformers中，构建了HSA-UltraLong，这是一种拥有80亿参数的专家混合（MoE）模型，在超过8万亿个token上进行了训练，并在具有域内和域外上下文长度的不同任务上进行了严格评估，以展示其处理超长上下文的能力。结果表明，我们的模型在域内长度上表现与全注意力基线相当，而在上下文长度最长达1600万的多数上下文内检索任务上达到了90%以上的准确率。本报告概述了我们的实验见解和开放问题，为超长上下文建模的未来研究提供了基础。",
        "地址": "https://arxiv.org/pdf/2511.23319.pdf"
    },
    {
        "名称": "2025 [2511.21025] CaptionQA: Is Your Caption as Useful as the Image Itself?.pdf",
        "作者": "Shijia Yang, Yunong Liu, Bohan Zhai, Ximeng Sun, Zicheng Liu, Emad Barsoum, Manling Li, Chenfeng Xu",
        "摘要": "摘要: 图像标题在多模式系统中（如检索、推荐和多步骤代理推理管道）作为视觉内容的有效替代品。然而，目前的评价方法忽视了一个基本问题：标题能否在实际的下游任务中替代图像？我们提出了一个基于效用的基准测试CaptionQA，以评估模型生成的标题，其中标题质量通过其对下游任务的支持程度来衡量。CaptionQA是一个可扩展的、域依赖的基准测试，覆盖了四个领域——自然、文档、电商和体现AI——每个领域都有细粒度的分类（25个顶级分类和69个子分类），识别出对特定领域任务有用的信息。CaptionQA构建了33,027个密集注释的选择题（平均每张图像50.3个），这些问题明确要求视觉信息来回答，提供了对标题效用的全面探讨。在我们的评估协议中，一个大型语言模型（LLM）仅使用标题来回答这些问题，直接衡量标题是否保留了图像级的效用且可被下游LLM利用。对最先进的多模态语言模型（MLLMs）进行的评估显示出图像与其标题效用之间的明显差距。值得注意的是，在传统的图像问答基准测试中几乎相同的模型在标题效用方面下降了最多32%。我们发布了CaptionQA以及一个扩展到新领域的开源管道。代码可在此https URL中获取。",
        "地址": "https://arxiv.org/pdf/2511.21025.pdf"
    },
    {
        "名称": "2025 [2511.22475] Adversarial Flow Models.pdf",
        "作者": "Shanchuan Lin, Ceyuan Yang, Zhijie Lin, Hao Chen, Haoqi Fan",
        "摘要": "摘要: 我们提出了对抗性流模型，这是一类统一了对抗性模型和流模型的生成模型。我们的方法支持原生的一步或多步生成，并使用对抗目标进行训练。与传统的生成对抗网络（GAN）不同，我们的生成器学习的是一个确定性的从噪声到数据的映射，这与流匹配模型中的最优传输相同，这显著地稳定了对抗训练。此外，与基于一致性的方法不同，我们的模型直接学习一步或少步生成，而无需学习概率流传播的中间时间步。这样节省了模型容量，减少了训练迭代次数，并避免了错误累积。在 ImageNet-256px 上的相同 1NFE 设置下，我们的 B/2 模型接近基于一致性方法的 XL/2 模型的性能，而我们的 XL/2 模型创造了新的最佳 FID 2.38。我们还展示了通过深度重复进行 56 层和 112 层模型的端到端训练的可能性，而无需任何中间监督，并且在使用单次前向传递的情况下实现了 2.08 和 1.94 的 FID，超越了其 2NFE 和 4NFE 的对应模型。",
        "地址": "https://arxiv.org/pdf/2511.22475.pdf"
    },
    {
        "名称": "2025 [2511.22677] Decoupled DMD: CFG Augmentation as the Spear, Distribution Matching as the Shield.pdf",
        "作者": "Dongyang Liu, Peng Gao, David Liu, Ruoyi Du, Zhen Li, Qilong Wu, Xin Jin, Sihan Cao, Shifeng Zhang, Hongsheng Li, Steven Hoi",
        "摘要": "摘要：扩散模型蒸馏已成为创建高效的少步和单步生成器的强大技术。其中，分布匹配蒸馏（DMD）及其变体因其出色的性能而脱颖而出，这通常归因于其核心机制，即将学生模型的输出分布与预训练教师模型的输出分布相匹配。在这项工作中，我们挑战了这一传统理解。通过对DMD训练目标的严格分解，我们揭示了在诸如文本到图像生成的复杂任务中，通常需要CFG才能实现理想的几步性能，少步蒸馏的主要驱动因素不是分布匹配，而是我们发现的一个之前被忽视的组件，即CFG增强（CA）。我们证明了这个术语是蒸馏的核心“发动机”，而分布匹配（DM）术语则作为“正则化器”，确保训练的稳定性并减少伪影。我们进一步验证了这种分离，证明了尽管DM术语是一个非常有效的正则化器，但它不是唯一的；较简单的非参数约束或基于GAN的目标可以实现相同的稳定功能，尽管会有不同的权衡。这种劳动力的分离促使我们更有原则地分析这两个术语的属性，从而更系统和深入地理解。这种新理解还使我们能够提出对蒸馏过程的原则性修改，例如分离发动机和正则化器的噪声时间表，从而带来进一步的性能提升。值得注意的是，我们的方法已被Z-Image项目采用，以开发顶级的8步图像生成模型，实验证实了我们发现的泛化性和鲁棒性。",
        "地址": "https://arxiv.org/pdf/2511.22677.pdf"
    },
    {
        "名称": "2025 [2511.22173] RefineBench: Evaluating Refinement Capability of Language Models via Checklists.pdf",
        "作者": "Young-Jun Lee, Seungone Kim, Byung-Kwan Lee, Minkyeong Moon, Yechan Hwang, Jong Myoung Kim, Graham Neubig, Sean Welleck, Ho-Jin Choi",
        "摘要": "摘要：语言模型（LMs）是否能够自我改进其响应？随着各种实际用户互动涉及到改进请求，这个问题变得越来越相关。然而，之前的研究大多测试了 LMs 在可验证任务上的改进能力，例如竞赛数学或使用简化支架的符号推理，而用户通常会提出开放性查询并提供不同程度的反馈。最近出现的在思维链中表现出自我反思模式的推理模型进一步激发了这个问题的研究。为了解析这一问题，我们引入了 RefineBench，这是一个包含 11 个领域的 1000 个具有挑战性问题的基准，并配有基于检查表的评估框架。我们评估了两种改进模式：（1）引导改进，即 LM 被提供自然语言反馈；（2）自我改进，即 LMs 尝试在没有指导的情况下进行改进。在自我改进设置中，即使是前沿的 LMs，如 Gemini 2.5 Pro 和 GPT-5，其基线得分也仅为 31.3% 和 29.1%，大多数模型无法在多次迭代中持续改进（例如，Gemini-2.5-Pro 仅增加了+ 1.8%，而 DeepSeek-R1 更是下降了 -0.1%）。相比之下，在引导改进中，专有的 LMs 和大型开源 LMs（>70B）可以利用有针对性的反馈在五次回合内将响应改进到接近完美的水平。这些发现表明，前沿的 LMs 需要突破才能自我改进其错误响应，而 RefineBench 提供了一个有价值的测试平台来跟踪进展。\n\n作者：Young-Jun Lee, Seungone Kim, Byung-Kwan Lee, Minkyeong Moon, Yechan Hwang, Jong Myoung Kim, Graham Neubig, Sean Welleck, Ho-Jin Choi\n\n评论：项目网站：此 HTTPS URL\n\n链接：https://arxiv.org/pdf/2511.22173.pdf\n\n标题：2025 [2511.22173] RefineBench: 通过检查表评估语言模型的改进能力.pdf",
        "地址": "https://arxiv.org/pdf/2511.22173.pdf"
    },
    {
        "名称": "2025 [2511.18890] Nemotron-Flash: Towards Latency-Optimal Hybrid Small Language Models.pdf",
        "作者": "Yonggan Fu, Xin Dong, Shizhe Diao, Matthijs Van keirsbilck, Hanrong Ye, Wonmin Byeon, Yashaswi Karnati, Lucas Liebenwein, Hannah Zhang, Nikolaus Binder, Maksim Khadkevich, Alexander Keller, Jan Kautz, Yingyan Celine Lin, Pavlo Molchanov",
        "摘要": "摘要：小语言模型（SLMs）的高效部署对于许多具有严格延迟要求的实际应用至关重要。尽管以前的SLM设计工作主要集中在减少参数数量以实现参数优化的SLMs，但参数效率并不一定能转化为相应的实际设备速度提升。本研究旨在确定SLMs实际设备延迟的关键决定因素，并在实际设备延迟是主要考虑因素时，为SLM的设计和训练提供可推广的原则和方法。具体而言，我们确定了两个核心体系结构因素：深度-宽度比和算子选择。前者对于小批量尺寸的延迟至关重要，而后者则会影响延迟和大批量尺寸的吞吐量。有鉴于此，我们首先研究了延迟优化的深度-宽度比，发现尽管深薄模型在相同参数预算下通常能达到更好的准确性，但它们可能不在准确性-延迟权衡的前沿。接下来，我们探索了新兴的高效注意力替代方案，以评估它们作为候选构造算子的潜力。利用这些有前途的算子，我们构建了一个进化搜索框架，自动发现这些算子在混合SLMs中的延迟优化组合，从而推进准确性-延迟前沿。除了架构改进外，我们还通过权重归一化技术增强SLM的训练，该技术能够实现更有效的权重更新并改善最终收敛。结合这些方法，我们介绍了一系列新的混合SLMs，称为Nemotron-Flash，与现有的最先进SLMs相比，Nemotron-Flash显著提升了准确性-效率前沿，例如相对于Qwen3-1.7B/0.6B，分别实现了超过+5.5%的平均准确性提升，1.3倍/1.9倍更低的延迟，和18.7倍/45.6倍更高的吞吐量。\n",
        "地址": "https://arxiv.org/pdf/2511.18890.pdf"
    },
    {
        "名称": "2025 [2511.22815] Captain Safari: A World Engine.pdf",
        "作者": "Yu-Cheng Chou, Xingrui Wang, Yitong Li, Jiahao Wang, Hanting Liu, Cihang Xie, Alan Yuille, Junfei Xiao",
        "摘要": "摘要：世界引擎旨在合成支持在用户控制的摄像机运动下进行场景交互探索的长时间3D一致视频。然而，现有系统在面对激烈的6自由度轨迹和复杂的室外布局时会遇到困难：它们失去了长距离的几何一致性，偏离目标路径，或陷入过于保守的运动模式。为此，我们介绍了Captain Safari，这是一种姿态条件的世界引擎，通过从持久的世界记忆中检索数据来生成视频。给定摄像机路径后，我们的方法会维护一个动态的局部记忆，并使用检索器来提取姿态对齐的世界标记，然后在轨迹上条件生成视频。该设计使模型在执行具有挑战性的摄像机操作时保持稳定的3D结构。为了评估该设置，我们整理了一个新的名为OpenSafari的野外FPV（第一视角）数据集，包含通过多阶段几何和运动验证管道构建的具有验证摄像机轨迹的高动态无人机视频。在视频质量、3D一致性和轨迹跟踪方面，Captain Safari显著优于最新的摄像机控制生成器。它将MEt3R从0.3703减少到0.3690，AUC@30从0.181提高到0.200，并且在所有摄像机控制基准中产出显著更低的FVD（频率-视觉散度）。更重要的是，在一项50名参与者的5选1人类研究中，注释者在五个匿名模型中选择最佳结果，67.6%的偏好选择我们的方法。我们的结果表明，姿态条件的世界记忆是一种强大的长视距可控视频生成机制，并且提供OpenSafari作为未来世界引擎研究的一个具有挑战性的新的基准。",
        "地址": "https://arxiv.org/pdf/2511.22815.pdf"
    },
    {
        "名称": "2025 [2511.22787] World in a Frame: Understanding Culture Mixing as a New Challenge for Vision-Language Models.pdf",
        "作者": "Eunsu Kim, Junyeong Park, Na Min An, Junseong Kim, Hitesh Laxmichand Patel, Jiho Jin, Julia Kruk, Amit Agarwal, Srikant Panda, Fenal Ashokbhai Ilasariya, Hyunjung Shim, Alice Oh",
        "摘要": "摘要: 在全球化的世界中，来自不同起源的文化元素经常出现在同一个视觉场景中。我们将此类情境称为文化混合场景，但关于大型视觉语言模型（LVLMs）如何感知这些场景的研究仍然不足。我们将文化混合作为LVLMs的一个关键挑战进行研究，并考察当前模型在多地区文化元素同时出现时的表现。为系统分析这些行为，我们构建了CultureMix，这是一种包含23,000个扩散生成并由人类验证的文化混合图像的食品视觉问答（VQA）基准，涵盖四个子任务：（1）仅食品，（2）食品+食品，（3）食品+背景，以及（4）食品+食品+背景。评估了10个LVLMs后，我们发现它们在混合场景中无法保持单独文化身份的一致性。模型表现出对背景的强依赖性，当文化背景添加到仅食品基线中时，准确性下降了14%，并且它们在不同上下文中对相同食品的预测不一致。为解决这些限制，我们探索了三种增强策略。我们发现，使用多样化的文化混合数据集进行监督微调可以显著提高模型的一致性并减少对背景的敏感性。我们呼吁在文化混合场景上增加关注，这是开发能够在文化多样的真实世界环境中可靠运行的LVLMs的关键步骤。\n\n作者: Eunsu Kim, Junyeong Park, Na Min An, Junseong Kim, Hitesh Laxmichand Patel, Jiho Jin, Julia Kruk, Amit Agarwal, Srikant Panda, Fenal Ashokbhai Ilasariya, Hyunjung Shim, Alice Oh\n\n链接: [论文链接](https://arxiv.org/pdf/2511.22787.pdf)\n\n标题: 2025 [2511.22787] 镜框中的世界：理解文化混合作为视觉语言模型的新挑战",
        "地址": "https://arxiv.org/pdf/2511.22787.pdf"
    },
    {
        "名称": "2025 [2511.22281] The Collapse of Patches.pdf",
        "作者": "Wei Guo, Shunqi Mao, Zhuonan Liang, Heng Wang, Weidong Cai",
        "摘要": "论文摘要如下：\n观察图像中的某些区域可以减少对其他区域的不确定性。其实现方式降低了每个剩余区域特征的分布熵，类似于量子力学中粒子波函数的塌缩。这一现象可以直观地称为“区域塌缩”。为了识别在特定区域塌缩过程中最依赖于哪些区域，我们学习了一个自动编码器，该自动编码器可以软选择一部分区域来重建每个目标区域。为每个区域绘制其PageRank评分的依赖关系图，揭示了实现图像的最佳区域顺序。我们展示了遵循这一顺序对各种掩蔽图像建模方法的益处。首先，通过重新训练最先进的模型MAR，可以提升自回归图像生成的性能。接下来，我们介绍了一种新的图像分类设置，只允许视觉变换器看到塌缩顺序中高排名的区域。仅查看这些高排名区域的22%就足以达到高精度。通过这些实验，我们提出了区域塌缩作为一种新颖的图像建模视角，以促进视觉效率。我们的项目在此https URL上可用。\n\n论文信息：\n- 题目: 2025 [2511.22281] The Collapse of Patches.pdf\n- 作者: Wei Guo, Shunqi Mao, Zhuonan Liang, Heng Wang, Weidong Cai\n- 注释: 13页，10幅图\n- 链接: [链接](https://arxiv.org/pdf/2511.22281.pdf)",
        "地址": "https://arxiv.org/pdf/2511.22281.pdf"
    },
    {
        "名称": "2025 [2511.22055] OralGPT-Omni: A Versatile Dental Multimodal Large Language Model.pdf",
        "作者": "Jing Hao, Yuci Liang, Lizhuo Lin, Yuxuan Fan, Wenkai Zhou, Kaixin Guo, Zanting Ye, Yanpeng Sun, Xinyu Zhang, Yanqi Yang, Qiankun Li, Hao Tang, James Kit-Hon Tsoi, Linlin Shen, Kuo Feng Hung",
        "摘要": "摘要：多模态大语言模型（MLLMs）在众多医学专业领域展示了巨大的潜力，但由于缺乏特定领域的数据、稀缺的牙科专家注释、不足的模态特定建模以及可靠性挑战，牙科领域的研究仍然较少。在本文中，我们提出了OralGPT-Omni，这是第一个专门为牙科领域设计的MLLM，能够对不同牙科影像模态和临床任务进行全面且可靠的分析。为了明确捕捉牙医的诊断推理过程，我们构建了TRACE-CoT，这是一个临床基础的链式思维数据集，反映了牙科放射科医生的决策过程。结合我们提出的四阶段训练范式，这种推理监督显著增强了模型对牙科图像的理解和分析能力。同时，我们引入了MMOral-Uni，这是第一个用于牙科图像分析的统一多模态基准。该基准包含了2,809个开放性问答对，涵盖了五种模态和五个任务，是迄今为止MMMLMs在数字牙科领域的全面评估套件。OralGPT-Omni在MMOral-Uni基准上取得了51.84的总分，在MMOral-OPG基准上取得了45.31的成绩，显著优于GPT-5的评分。我们的工作促进了智能牙科的发展，并为牙科图像分析的未来进步铺平了道路。所有代码、基准和模型都将公开提供。\n\n翻译：\n\n摘要：多模态大语言模型（MLLMs）在众多医学专业领域展示了巨大的潜力，但牙科领域的研究仍然较少，部分原因是缺乏特定领域的数据、稀缺的牙科专家注释、模态特定建模不足以及可靠性挑战。在本文中，我们提出了OralGPT-Omni，这是第一个专门为牙科领域设计的MLLM，能够对不同牙科影像模态和临床任务进行全面且可靠的分析。为了明确捕捉牙医的诊断推理过程，我们构建了TRACE-CoT，这是一个临床基础的链式思维数据集，反映了牙科放射科医生的决策过程。结合我们提出的四阶段训练范式，这种推理监督显著增强了模型对牙科图像的理解和分析能力。同时，我们引入了MMOral-Uni，这是第一个用于牙科图像分析的统一多模态基准。该基准包含了2809个开放性问答对，涵盖了五种模态和五个任务，是迄今为止MMMLMs在数字牙科领域的全面评估套件。OralGPT-Omni在MMOral-Uni基准上取得了51.84的总分，在MMOral-OPG基准上取得了45.31的成绩，显著优于GPT-5的评分。我们的工作促进了智能牙科的发展，并为牙科图像分析的未来进步铺平了道路。所有代码、基准和模型都将公开提供。",
        "地址": "https://arxiv.org/pdf/2511.22055.pdf"
    },
    {
        "名称": "2025 [2511.22688] Test-time scaling of diffusions with flow maps.pdf",
        "作者": "Amirmojtaba Sabour, Michael S. Albergo, Carles Domingo-Enrich, Nicholas M. Boffi, Sanja Fidler, Karsten Kreis, Eric Vanden-Eijnden",
        "摘要": "摘要：提高扩散模型在测试时间的常见方法是将用户指定奖励的梯度引入扩散本身的动态中，以使样本在对用户指定奖励评分时表现良好。然而，该过程通常是不合理的，因为用户指定的奖励通常只在生成结束时在数据分布上有明确的定义。虽然解决此问题的常见方法是使用去噪器来估计样本在生成结束时的状态，但我们提出了一种简单的解决方案，即直接使用流动图。通过利用流动图和控制瞬时传输的速度场之间的关系，我们构建了一种算法，称为流动图轨迹倾斜（Flow Map Trajectory Tilting, FMTT），该算法在奖励的标准测试时间方法中表现出比涉及奖励梯度的更好的上升效果。这种方法可用于通过重要性加权执行精确采样或通过本质搜索识别奖励倾斜分布的局部最大值。我们展示了我们的方法相对于其他前瞻技术的有效性，并展示了流动图如何使得复杂的奖励函数的处理成为可能，从而实现了新形式的图像编辑，例如与视觉语言模型的接口。\n\n作者：Amirmojtaba Sabour, Michael S. Albergo, Carles Domingo-Enrich, Nicholas M. Boffi, Sanja Fidler, Karsten Kreis, Eric Vanden-Eijnden\n标题：测试时间使用流动图缩放扩散模型\n年份：2025\n链接：https://arxiv.org/pdf/2511.22688.pdf",
        "地址": "https://arxiv.org/pdf/2511.22688.pdf"
    },
    {
        "名称": "2025 [2511.22176] Focused Chain-of-Thought: Efficient LLM Reasoning via Structured Input Information.pdf",
        "作者": "Lukas Struppek, Dominik Hintersdorf, Hannah Struppek, Daniel Neider, Kristian Kersting",
        "摘要": "摘要：近期的大型语言模型通过生成详细的思维链痕迹来实现强大的推理性能，但这通常会导致过多的标记使用和高推理延迟。现有的提高效率方法通常侧重于模型中心的干预，如强化学习或监督微调，以减少冗长。然而，我们提出了一种无需训练的、以输入为中心的方法。受认知心理学的启发，我们引入了Focused Chain-of-Thought（F-CoT），它将信息提取与推理过程分开。F-CoT首先将查询中的关键信息组织成简明、结构化的上下文，然后引导模型仅对这个上下文进行推理。通过避免注意无关细节，F-CoT自然会生成更短的推理路径。在算术文字问题上，F-CoT在保持与标准零样本CoT相当的准确性的同时，减少了2-3倍的生成标记量。这些结果突出表明，结构化输入是一种简单但有效的提高大型语言模型推理效率的方法。",
        "地址": "https://arxiv.org/pdf/2511.22176.pdf"
    },
    {
        "名称": "2025 [2511.21750] SO-Bench: A Structural Output Evaluation of Multimodal LLMs.pdf",
        "作者": "Di Feng, Kaixin Ma, Feng Nan, Haofeng Chen, Bohan Zhai, David Griffiths, Mingfei Gao, Zhe Gan, Eshan Verma, Yinfei Yang, Zhifeng Chen, Afshin Dehghan",
        "摘要": "摘要：多模态大语言模型（MLLMs）在现实世界的代理环境中应用越来越广泛，其输出不仅需要正确，还需要符合预定义的数据架构。尽管在文本域的结构化生成方面取得了进展，但仍缺少一个能够系统性评估基于架构的信息提取和视觉输入推理的基准。在这项工作中，我们通过精心设计的SO-Bench基准全面研究了MLLMs的视觉结构输出能力。涵盖了包括UI屏幕、自然图像、文档和图表在内的四个视觉领域，SO-Bench由超过6.5K多样化的JSON架构和1.8K精心策划并经过人工验证的图像-架构对构建而成。在开源和前沿专有模型上的基准实验揭示了在预测准确且符合架构的输出方面存在持久的差距，突显了改进多模态结构化推理的必要性。除了基准测试，我们还进一步进行训练实验，大幅提升了模型的结构化输出能力。我们计划将基准测试开放给社区使用。",
        "地址": "https://arxiv.org/pdf/2511.21750.pdf"
    },
    {
        "名称": "2025 [2511.22805] From Pixels to Feelings: Aligning MLLMs with Human Cognitive Perception of Images.pdf",
        "作者": "Yiming Chen, Junlin Han, Tianyi Bai, Shengbang Tong, Filippos Kokkinos, Philip Torr",
        "摘要": "摘要：多模态大型语言模型（MLLMs）虽然擅长回答图像中有什么——识别物体和描述场景——但往往缺乏理解图像对人类观察者感受的能力。这一差距在考虑主观认知特性时尤为明显，例如是什么让图像令人难忘、有趣、具有美感或情感共鸣。为系统解决这一挑战，我们引入了CogIP-Bench，一个综合基准，用于评估MLLMs对这些图像认知特性的表现。我们的评估揭示了一个显著的差距：当前模型与人类对这些细微特性的感知不一致。随后，我们证明了训练后阶段可以有效弥合这一差距，显著增强模型与人类判断的一致性。此外，我们展示了这种学习到的认知一致性不仅具有预测性，还可以转移到下游的创意任务中。通过将我们认知一致的MLLM集成到图像生成管道中，我们可以指导合成过程以产生更具期望特征的图像，比如更令人难忘或更具视觉吸引力。我们的工作提供了一个衡量这种类似人类感知的基准，一个提高其效能的训练后管道，并证明这种一致性能开启更以人为本的人工智能。",
        "地址": "https://arxiv.org/pdf/2511.22805.pdf"
    },
    {
        "名称": "2025 [2511.22659] Geometrically-Constrained Agent for Spatial Reasoning.pdf",
        "作者": "Zeren Chen, Xiaoya Lu, Zhijie Zheng, Pengrui Li, Lehan He, Yijin Zhou, Jing Shao, Bohan Zhuang, Lu Sheng",
        "摘要": "摘要：视觉语言模型（VLMs）在空间推理中展现了语义与几何之间的基本差距：它们擅长定性语义推理，但其推理在有损的语义空间中进行，与高保真几何不匹配。目前的范式未能弥合这一差距。基于训练的方法存在“预言悖论”，从不完美的预言中学习错误的空间逻辑。工具集成方法约束了最终计算，但关键是放任了VLM的规划过程，导致几何上有缺陷的计划。在这项工作中，我们提出了几何约束代理（GCA），这是一种无需训练的代理模型，通过引入正式任务约束来解决这一差距。具体而言，我们战略性地将VLM的角色分为两个阶段。首先，作为语义分析师，VLM将用户的模糊查询翻译成正式的、可验证的任务约束，定义参考框架和目标。其次，作为任务解决者，VLM生成并执行严格在约束定义的确定性范围内的工具调用。此几何约束推理策略成功解决了语义与几何之间的差距，提供了一个稳健且可验证的空间推理途径。综合实验表明，GCA在多个空间推理基准上达到了SOTA性能，超越现有的基于训练和工具集成的方法约27%。请访问我们的主页了解更多信息：https://arxiv.org/pdf/2511.22659.pdf。",
        "地址": "https://arxiv.org/pdf/2511.22659.pdf"
    },
    {
        "名称": "2025 [2511.20809] Layer-Aware Video Composition via Split-then-Merge.pdf",
        "作者": "Ozgur Kara, Yujia Chen, Ming-Hsuan Yang, James M. Rehg, Wen-Sheng Chu, Du Tran",
        "摘要": "摘要: 我们提出了Split-then-Merge (StM)，这是一个旨在增强生成视频组合控制并解决其数据稀缺性问题的新框架。与依赖于标注数据集或手工规则的传统方法不同，StM将大量未标注的视频分割为动态前景和背景层，然后自我组合它们以学习动态主体如何与多样的场景交互。这个过程使模型能够学习生成真实视频所需的复杂组合动态。StM引入了一种新的转化感知训练流程，它利用多层融合和增强来实现感知能力的组合，并通过一种身份保留损失在融合过程中保持前景的保真度。实验表明，StM在定量基准测试和基于人类/VLLM的定性评估方面都优于当前最先进的方法。更多详细信息请访问我们的项目页面：这个 https URL。",
        "地址": "https://arxiv.org/pdf/2511.20809.pdf"
    },
    {
        "名称": "2025 [2511.19990] OmniRefiner: Reinforcement-Guided Local Diffusion Refinement.pdf",
        "作者": "Yaoli Liu, Ziheng Ouyang, Shengtao Lou, Yiren Song",
        "摘要": "摘要:\n参考引导图像生成技术发展迅速，但当前的扩散模型在使用参考图像优化生成图像时，仍难以保留细粒度的视觉细节。这一限制是因为基于VAE的潜在压缩会固有地丢失微妙的纹理信息，导致身份和属性的特定提示消失。此外，基于现有方法增强局部细节的后期编辑方法往往会在光照、纹理或形状方面产生与原始图像不一致的结果。为了解决这个问题，我们引入了\\ourMthd{}，一个细节感知优化框架，通过执行两个连续阶段的参考驱动校正来增强像素级一致性。首先，我们通过微调一个单图像扩散编辑器，使其能够联合处理草图图像和参考图像，从而在保持结构完整性的同时实现全局一致优化。然后，我们应用强化学习来进一步加强局部编辑能力，明确优化细节准确性和语义一致性。大量实验表明，\\ourMthd{}显著提高了参考对齐和细粒度细节保留，在具有挑战性的参考引导恢复基准测试中，生成的编辑结果在忠实性和视觉一致性方面均超过了开源和商业模型。\n\n作者:\nYaoli Liu, Ziheng Ouyang, Shengtao Lou, Yiren Song\n\n标题:\nOmniRefiner: Reinforcement-Guided Local Diffusion Refinement\n\n链接:\nhttps://arxiv.org/pdf/2511.19990.pdf",
        "地址": "https://arxiv.org/pdf/2511.19990.pdf"
    },
    {
        "名称": "2025 [2511.13344] YOLO Meets Mixture-of-Experts: Adaptive Expert Routing for Robust Object Detection.pdf",
        "作者": "Ori Meiraz, Sharon Shalev, Avishai Weizman",
        "摘要": "摘要：本文提出了一种新型的专家混合框架用于物体检测，结合了多种YOLOv9-T专家之间的自适应路由，以实现动态特征专门化。与单个YOLOv9-T模型相比，该方法能够获得更高的平均精度（mAP）和平均召回率（AR）。",
        "地址": "https://arxiv.org/pdf/2511.13344.pdf"
    },
    {
        "名称": "2025 [2511.22533] Fast3Dcache: Training-free 3D Geometry Synthesis Acceleration.pdf",
        "作者": "Mengyu Yang, Yanming Yang, Chenyi Xu, Chenxi Song, Yufan Zuo, Tong Zhao, Ruibo Li, Chi Zhang",
        "摘要": "摘要：扩散模型在2D图像、视频和3D形状等不同模态中实现了令人印象深刻的生成质量，但由于迭代去噪过程，它们的推断仍然计算量大。虽然最近的基于缓存的方法有效地重复使用冗余计算以加速2D和视频生成，但将这些技术直接应用于3D扩散模型可能会严重破坏几何一致性。在3D合成中，即使是缓存潜在特征中的微小数值误差也会积累，导致结构性伪影和拓扑不一致性。为克服这一限制，我们提出了Fast3Dcache，一种无需训练的几何感知缓存框架，可以加速3D扩散推断，同时保持几何保真度。我们的方法引入了一种预测缓存调度约束（PCSC），根据体素稳定模式动态确定缓存配额，并采用时空稳定性标准（SSC），基于速度幅度和加速度标准选择稳定特征进行重用。全面的实验表明，Fast3Dcache显著加速推断，最高可实现27.12%的加速和54.8%的浮点运算减少，同时几何质量仅有最小的降低，分别由Chamfer距离（2.48%）和F评分（1.95%）测量。\n\n翻译作者和其他信息：\n作者：孟羽杨，杨焱明，徐晨奕，宋辰曦，左宇凡，赵彤，李瑞博，张驰\n链接：https://arxiv.org/pdf/2511.22533.pdf\n标题：2025 [2511.22533] Fast3Dcache: 无需训练的3D几何合成加速",
        "地址": "https://arxiv.org/pdf/2511.22533.pdf"
    },
    {
        "名称": "2025 [2511.22265] FedRE: A Representation Entanglement Framework for Model-Heterogeneous Federated Learning.pdf",
        "作者": "Yuan Yao, Lixu Wang, Jiaqi Wu, Jin Song, Simin Chen, Zehua Wang, Zijian Tian, Wei Chen, Huixia Li, Xiaoxiao Li",
        "摘要": "摘要：联邦学习（FL）使客户端能够在不泄露隐私的情况下进行协作训练。虽然大多数现有的FL方法假设模型架构是同质的，但客户在数据和资源方面的异质性使这一假设变得不切实际，促使了模型异构联邦学习的需求。为了解决这个问题，我们提出了联邦表示纠缠（FedRE），这是一个基于一种新型客户知识——纠缠表示——的框架。在FedRE中，每个客户端使用归一化的随机权重将其本地表示聚合成一个纠缠表示，并使用相同的权重将相应的单热标签编码整合到纠缠标签编码中。这些编码随后被上传到服务器以训练全局分类器。在训练过程中，每个纠缠表示通过其纠缠标签编码在各个类别之间进行监督，而随机权重在每轮中重新采样以引入多样性，缓解全局分类器的过度自信并促进更平滑的决策边界。此外，每个客户端上传单个跨类别纠缠表示及其纠缠标签编码，从而减轻了表示反演攻击的风险并减少了通信开销。大量实验表明，FedRE在模型性能、隐私保护和通信开销之间实现了有效的平衡。代码可以在此https URL访问。",
        "地址": "https://arxiv.org/pdf/2511.22265.pdf"
    },
    {
        "名称": "2025 [2511.19496] Xmodel-2.5: 1.3B Data-Efficient Reasoning SLM.pdf",
        "作者": "Yang Liu, Xiaolong Zhong, Ling Jiang",
        "摘要": "摘要: 大型语言模型在推理和工具使用方面表现出色，但其计算需求使得在边缘或成本敏感的部署中不实用。我们提出了Xmodel-2.5，这是一种设计为插入代理核心的13亿参数小型语言模型。通过最大更新参数化（μP）进行训练，使得在20M参数代理上调优的超参数可以直接转移到完整模型，即使在参数绑定的词嵌入架构下也能实现。使用1.4T-token预热—稳定—衰减课程，并进一步显示在衰减阶段从AdamW切换到Muon能够在保持其他所有超参数固定的情况下，将13任务推理的平均性能提高4.58%，验证了早期AdamW的稳定性可以与后期Muon的锐化结合以获得更好的下游性能。FP8混合精度训练平衡了准确性和吞吐量。所有检查点，配方和评估代码均根据Apache-2.0许可发布。训练代码和评估工具：此地址URL。",
        "地址": "https://arxiv.org/pdf/2511.19496.pdf"
    },
    {
        "名称": "2025 [2511.16854] MRI Super-Resolution with Deep Learning: A Comprehensive Survey.pdf",
        "作者": "Mohammad Khateri, Serge Vasylechko, Morteza Ghahremani, Liam Timms, Deniz Kocanaogullari, Simon K. Warfield, Camilo Jaimes, Davood Karimi, Alejandra Sierra, Jussi Tohka, Sila Kurugol, Onur Afacan",
        "摘要": "**摘要**：\n高分辨率（HR）磁共振成像（MRI）在许多临床和研究应用中至关重要。然而，实现HR MRI仍然成本高昂，并受到技术折衷和实验限制的约束。超分辨率（SR）提供了一种有前景的计算方法，通过从更经济的低分辨率（LR）扫描生成HR图像，可能在不需要额外硬件的情况下提高诊断精度和效率。本综述文章回顾了MRI SR技术的最新进展，重点关注深度学习（DL）方法。它从计算机视觉、计算成像、逆问题和磁共振物理的角度审视了基于DL的MRI SR方法，涵盖了理论基础、架构设计、学习策略、基准数据集和性能指标。我们提出了一种系统的分类法对这些方法进行分类，并深入研究了适用于MRI的已建立和新兴的SR技术，考虑了临床和研究背景下的独特挑战。我们还强调了需要社区解决的开放性问题和方向。此外，我们提供了一系列重要的开源资源、工具和教程，可在我们的GitHub上访问：https://arxiv.org/pdf/2511.16854.pdf。\n\n**IEEE 关键词**：MRI, 超分辨率, 深度学习, 计算成像, 逆问题, 综述。",
        "地址": "https://arxiv.org/pdf/2511.16854.pdf"
    },
    {
        "名称": "2025 [2511.13944] Find the Leak, Fix the Split: Cluster-Based Method to Prevent Leakage in Video-Derived Datasets.pdf",
        "作者": "Noam Glazner, Noam Tsfaty, Sharon Shalev, Avishai Weizman",
        "摘要": "摘要：我们提出了一种基于聚类的帧选择策略，以减轻视频帧数据集中的信息泄漏。通过在将数据集划分为培训集、验证集和测试集之前先将视觉上相似的帧分组，该方法能产生更具代表性、平衡且可靠的数据集划分。",
        "地址": "https://arxiv.org/pdf/2511.13944.pdf"
    },
    {
        "名称": "2025 [2511.13276] Recognition of Abnormal Events in Surveillance Videos using Weakly Supervised Dual-Encoder Models.pdf",
        "作者": "Noam Tsfaty, Avishai Weizman, Liav Cohen, Moshe Tshuva, Yehudit Aperstein",
        "摘要": "摘要：我们提出了一种在监控视频中仅使用视频级监督检测罕见且多样异常的新方法。我们的双主干框架通过top-k池化结合卷积和变压器表示，在UCF-Crime数据集上实现了90.7%的曲线下面积（AUC）。",
        "地址": "https://arxiv.org/pdf/2511.13276.pdf"
    }
]