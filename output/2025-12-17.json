[
    {
        "名称": "2025 [2512.14691] MMGR: Multi-Modal Generative Reasoning.pdf",
        "作者": "Zefan Cai, Haoyi Qiu, Tianyi Ma, Haozhe Zhao, Gengze Zhou, Kung-Hsiang Huang, Parisa Kordjamshidi, Minjia Zhang, Wen Xiao, Jiuxiang Gu, Nanyun Peng, Junjie Hu",
        "摘要": "摘要：视频基础模型能够生成视觉上真实且时间上连贯的内容，但它们作为世界模拟器的可靠性取决于是否捕捉到了物理逻辑和空间约束。现有的指标，如Frechet视频距离（FVD），强调感知质量，而忽略了推理失败，包括因果关系、物理和全局一致性的违反。我们引入了MMGR（多模态生成推理评估和基准），这是一种基于五种推理能力（物理、逻辑、3D空间、2D空间和时间）的系统性评估框架。MMGR在三个领域评估生成推理：抽象推理（ARC-AGI，数独）、具身导航（现实世界中的3D导航和定位）以及物理常识（运动和组合交互）。MMGR采用细粒度指标，要求在视频和图像生成中都达到整体正确性。我们对领先的视频模型（Veo-3、Sora-2、Wan-2.2）和图像模型（Nano-banana、Nano-banana Pro、GPT-4o-image、Qwen-image）进行了基准测试，揭示了各领域之间的性能差距。模型在物理常识任务上表现适中，但在抽象推理（ARC-AGI的准确率低于10%）和具身环境中的长周期空间规划上表现不佳。我们的分析强调了当前模型的关键局限性，包括过度依赖感知数据、全局状态一致性差以及目标导向的视觉合理性而非因果正确性。MMGR提供了一个统一的诊断基准，并为构建具备推理能力的生成世界模型指明了方向。",
        "地址": "https://arxiv.org/pdf/2512.14691.pdf"
    },
    {
        "名称": "2025 [2512.13281] Video Reality Test: Can AI-Generated ASMR Videos fool VLMs and Humans?.pdf",
        "作者": "Jiaqi Wang, Weijia Wu, Yi Zhan, Rui Zhao, Ming Hu, James Cheng, Wei Liu, Philip Torr, Kevin Qinghong Lin",
        "摘要": "摘要：最近在视频生成方面的进展已经产生了色彩鲜艳的内容，这些内容常常与真实视频难以区分，使得AI生成的视频检测成为一个新兴的社会挑战。之前的AIGC检测基准主要评估不含音频的视频，目标是广泛的叙事领域，并且只关注分类。然而，尚不清楚最先进的视频生成模型是否能够生成音画紧密结合、能够可靠欺骗人类和视觉语言模型（VLMs）的沉浸式视频。为此，我们引入了“视频现实测试”，这是一个基于ASMR的视频基准套件，用于测试在紧密音画耦合下的感知真实性，具有以下维度：(i) 沉浸式ASMR影音资源。基准建立在精心策划的真实ASMR视频之上，目标是对细粒度的动作-物体交互进行多样性分析，涉及不同的物体、动作和背景。(ii) 同行评审评估。采用对抗性的创作者-审阅者协议，其中视频生成模型作为创作者，试图欺骗审阅者，而VLMs作为审阅者，试图识别欺骗。我们的实验结果显示：最佳创作者Veo3.1-Fast甚至能欺骗大多数VLMs：最强审阅者Gemini 2.5-Pro仅能达到56%的准确率（随机为50%），远低于人类专家的81.25%。添加音频可以改善真伪辨别，但诸如水印等表面线索仍然能够显著误导模型。这些发现勾画出了当前视频生成真实性的边界，并揭示了VLMs在感知保真度和音画一致性方面的限制。我们的代码可在此链接获取。",
        "地址": "https://arxiv.org/pdf/2512.13281.pdf"
    },
    {
        "名称": "2025 [2512.14614] WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling.pdf",
        "作者": "Wenqiang Sun, Haiyu Zhang, Haoyuan Wang, Junta Wu, Zehan Wang, Zhenwei Wang, Yunhong Wang, Jun Zhang, Tengfei Wang, Chunchao Guo",
        "摘要": "摘要: 本文介绍了 WorldPlay，这是一种流媒体视频扩散模型，能够实现实时互动世界建模，并具有长期的几何一致性，解决了当前方法中速度和内存之间的权衡问题。WorldPlay 的强大功能来自于三项关键创新。1) 我们使用双动作表示，以响应用户的键盘和鼠标输入，实现稳健的动作控制。2) 为了强制长期一致性，我们的重构上下文记忆动态地从过去帧重建上下文，并使用时间重新框架保持几何重要但过去很久的帧可访问，有效缓解了记忆衰减。3) 我们还提出了上下文强制，一种为内存感知模型设计的新蒸馏方法，通过对齐教师和学生的记忆上下文，保持学生利用长距离信息的能力，实现实时速度，同时防止误差漂移。总体而言，WorldPlay 能够在 24 FPS 下生成长期一致的 720p 流媒体视频，在多样场景中表现优于现有技术，并显示出强大的泛化能力。项目页面和在线演示可在: this https URL 和 this https URL 找到。",
        "地址": "https://arxiv.org/pdf/2512.14614.pdf"
    },
    {
        "名称": "2025 [2512.12675] Scone: Bridging Composition and Distinction in Subject-Driven Image Generation via Unified Understanding-Generation Modeling.pdf",
        "作者": "Yuran Wang, Bohan Zeng, Chengzhuo Tong, Wenxuan Liu, Yang Shi, Xiaochen Ma, Hao Liang, Yuanxing Zhang, Wentao Zhang",
        "摘要": "摘要: \n以主题驱动的图像生成已经从单一主体的生成发展到多主体的组合，而忽略了在输入包含多个候选对象时识别和生成正确主体的能力。这个限制在复杂现实的视觉环境中降低了效果。我们提出了Scone，这是一种将组合和区分能力整合的统一理解-生成方法。Scone使理解专家充当语义桥梁，传递语义信息并引导生成专家在保留主体身份的同时最大限度地减少干扰。一个分为两个阶段的训练方案先学习组合，然后通过语义对齐和基于注意力的掩蔽增强区分能力。我们还引入了SconeEval，一种用于评估不同场景下组合和区分能力的基准。实验表明，在两个基准上，Scone在组合和区分任务上优于现有的开源模型。我们的模型、基准和训练数据都可以在此HTTPS URL获取。",
        "地址": "https://arxiv.org/pdf/2512.12675.pdf"
    },
    {
        "名称": "2025 [2512.13660] RoboTracer: Mastering Spatial Trace with Reasoning in Vision-Language Models for Robotics.pdf",
        "作者": "Enshen Zhou, Cheng Chi, Yibo Li, Jingkun An, Jiayuan Zhang, Shanyu Rong, Yi Han, Yuheng Ji, Mengzhen Liu, Pengwei Wang, Zhongyuan Wang, Lu Sheng, Shanghang Zhang",
        "摘要": "摘要：空间追踪作为机器人基本的具身互动能力，其本质上具有挑战性，因为它需要多步骤的度量推理，并需结合复杂的空间参照和实际度量。然而，现有方法在处理这一复合任务时表现不佳。为此，我们提出了RoboTracer，这是一种3D感知的视觉语言模型（VLM），通过使用通用空间编码器和回归监督解码器在有监督微调（SFT）过程中提升尺度意识，从而实现3D空间参照和测量。此外，RoboTracer通过具有度量敏感过程奖励的强化微调（RFT）来推进多步骤度量推理，并监督关键的中间感知线索以准确生成空间追踪。为了支持SFT和RFT训练，我们引入了TraceSpatial数据集，该数据集包含3000万个问答对，跨越户外/室内/桌面场景，并支持复杂的推理过程（最多9个步骤）。我们还推出了TraceSpatial-Bench，这是一个填补评估空间追踪空白的具有挑战性的基准测试。实验结果表明，RoboTracer在空间理解、测量和参照方面超过了基准模型，平均成功率达79.1%，并且在TraceSpatial-Bench上表现出色，精度比Gemini-2.5-Pro高36个百分点。值得注意的是，RoboTracer可以与各种控制策略集成，以在复杂的现实场景中执行长时间、动态任务（包括UR5、G1类人机器人）。\n\n作者：周恩申、池程、李逸博、安敬琨、张佳源、荣杉宇、韩毅、季宇恒、刘梦臻、王鹏伟、王中原、盛璐、张上杭\n\n地址：https://arxiv.org/pdf/2512.13660.pdf\n\n项目页面：此 HTTPS URL",
        "地址": "https://arxiv.org/pdf/2512.13660.pdf"
    },
    {
        "名称": "2025 [2512.14051] OpenDataArena: A Fair and Open Arena for Benchmarking Post-Training Dataset Value.pdf",
        "作者": "Mengzhang Cai, Xin Gao, Yu Li, Honglin Lin, Zheng Liu, Zhuoshi Pan, Qizhi Pei, Xiaoran Shang, Mengyuan Sun, Zinan Tang, Xiaoyang Wang, Zhanping Zhong, Yun Zhu, Dahua Lin, Conghui He, Lijun Wu",
        "摘要": "摘要：大型语言模型（LLMs）的快速发展依赖于后训练数据集的质量和多样性。然而，存在一个关键的两难境地：尽管模型被严格基准测试，驱动这些模型的数据依然是一个黑箱——其组成不透明，出处不确定，缺乏系统评估。这种不透明性阻碍了可重复性，并模糊了数据特征与模型行为之间的因果联系。为弥合这一差距，我们引入了OpenDataArena（ODA），一个用于基准测试后训练数据固有价值的全面开放平台。ODA建立了一个包含四个关键支柱的综合生态系统：(i) 一个统一的训练-评估管道，确保在不同模型（如Llama，Qwen）和领域之间进行公平、公开的比较；(ii) 一个多维评分框架，从数十个不同的轴线来剖析数据质量；(iii) 一个交互式数据谱系探索器，用于可视化数据集的谱系并剖析组成来源；以及(iv) 一个完全开源的工具包，用于训练、评估和评分，以促进数据研究。在ODA上的广泛实验——涵盖了22个基准的120多个训练数据集，通过600多次训练运行和处理超过4000万个数据点验证——揭示了一些重要见解。我们的分析揭示了数据复杂性与任务性能之间的内在权衡，通过谱系追踪识别了流行基准中的冗余，并绘制了数据集之间的谱系关系。我们发布了所有结果、工具和配置，以民主化高质量数据评估的访问权限。ODA不仅仅是为了扩展排行榜，而是设想从试错的数据策划转向数据中心AI的科学，为数据混合规律和基础模型的战略性组成的严格研究铺平道路。",
        "地址": "https://arxiv.org/pdf/2512.14051.pdf"
    },
    {
        "名称": "2025 [2512.12980] Reveal Hidden Pitfalls and Navigate Next Generation of Vector Similarity Search from Task-Centric Views.pdf",
        "作者": "Tingyang Chen, Cong Fu, Jiahua Wu, Haotian Wu, Hua Fan, Xiangyu Ke, Yunjun Gao, Yabo Ni, Anxiang Zeng",
        "摘要": "摘要：高维空间中的向量相似搜索（VSS）正迅速成为新一代数据库系统中许多数据密集型服务的核心功能——从大型语言模型（LLM）中的嵌入查询，到语义信息检索和推荐引擎。然而，当前的基准测试主要基于与唯一由距离度量定义的基准真实值的召回-延迟权衡来评估VSS，忽略了检索质量最终对下游任务的影响。这种脱节可能误导学术研究和工业实践。\n\n我们提出了Iceberg，这是一个用于在实际应用上下文中对VSS方法进行端到端评估的整体基准测试套件。从任务中心的角度来看，Iceberg揭示了信息损失漏斗，其识别了三个主要的端到端性能下降来源：（1）在特征提取期间的嵌入损失；（2）度量误用，其中距离不能很好地反映任务的相关性；（3）数据分布敏感性，突出索引在偏移和模态下的鲁棒性。为了进行更全面的评估，Iceberg跨越了八个关键领域的多样化数据集，如图像分类、人脸识别、文本检索和推荐系统。每个数据集，包括从100万到1亿的向量，提供丰富的任务特定标签和评估指标，使得可以在完整的应用管道中而不是孤立地评估检索算法。Iceberg对13种最先进的VSS方法进行了基准测试，并基于应用层指标对它们进行重新排名，揭示了与仅从召回-延迟评估得出的传统排名存在的显著偏差。基于这些见解，我们定义了一组任务中心的元特性，并推导出一个可解释的决策树，以指导实践者在其特定工作负载中选择和调优VSS方法。",
        "地址": "https://arxiv.org/pdf/2512.12980.pdf"
    },
    {
        "名称": "2025 [2512.14336] Vector Prism: Animating Vector Graphics by Stratifying Semantic Structure.pdf",
        "作者": "Jooyeol Yun, Jaegul Choo",
        "摘要": "摘要：可扩展矢量图形（SVG）是现代网页设计的核心，随着网页环境变得越来越动态，对其进行动画处理的需求也在不断增长。然而，尽管代码生成和运动规划方面取得了进展，自动化矢量图形动画在视觉语言模型（VLMs）上仍然具有挑战性。视觉语言模型经常错误处理SVG，因为视觉上相关的部分通常会被分割成低级形状，这些形状对哪些元素应一起移动提供的指导不足。在本文中，我们介绍了一种框架，该框架恢复了可靠的SVG动画所需的语义结构，并揭示了当前视觉语言模型系统忽略的缺失层。这是通过对多个弱部分预测的统计聚合实现的，使系统能够从噪声预测中稳健地推断语义结构。通过将SVG重新组织成语义组，我们的方法使视觉语言模型能够生成更加连贯的动画。我们的实验显示出比现有方法显著的改进，表明语义恢复是实现稳健SVG动画的关键步骤，并支持视觉语言模型与矢量图形之间更可解释的交互。",
        "地址": "https://arxiv.org/pdf/2512.14336.pdf"
    },
    {
        "名称": "2025 [2512.14699] MemFlow: Flowing Adaptive Memory for Consistent and Efficient Long Video Narratives.pdf",
        "作者": "Sihui Ji, Xi Chen, Shuai Yang, Xin Tao, Pengfei Wan, Hengshuang Zhao",
        "摘要": "摘要：流视频生成的核心挑战是保持长上下文中的内容一致性，这对内存设计提出了很高的要求。大多数现有的解决方案通过预定义策略压缩历史帧来维护内存。然而，不同的生成视频块应该参考不同的历史线索，固定策略难以满足这一需求。在这项工作中，我们提出了MemFlow来解决这个问题。具体而言，在生成即将到来的块之前，我们通过检索与该块的文本提示最相关的历史帧来动态更新内存库。这种设计即使在未来帧中发生新事件或场景切换时也能保持叙事连贯性。此外，在生成过程中，我们在注意力层中仅激活内存库中与每个查询最相关的标记，这有效地保证了生成效率。通过这种方式，MemFlow在几乎不增加计算负担的情况下（与无内存基线相比速度减少7.9%）实现了卓越的长上下文一致性，并且与任何具有KV缓存的流视频生成模型保持兼容性。\n\n作者：季思慧，陈希，杨帅，陶欣，万鹏飞，赵恒双\n\n评论：项目页面：此HTTPS URL\n\n标题：MemFlow: 流动自适应内存以实现一致且高效的长视频叙事",
        "地址": "https://arxiv.org/pdf/2512.14699.pdf"
    },
    {
        "名称": "2025 [2512.14503] RecGPT-V2 Technical Report.pdf",
        "作者": "Chao Yi, Dian Chen, Gaoyang Guo, Jiakai Tang, Jian Wu, Jing Yu, Mao Zhang, Wen Chen, Wenjun Yang, Yujie Luo, Yuning Jiang, Zhujin Gao, Bo Zheng, Binbin Cao, Changfa Wu, Dixuan Wang, Han Wu, Haoyi Hu, Kewei Zhu, Lang Tian, Lin Yang, Qiqi Huang, Siqi Yang, Wenbo Su, Xiaoxiao He, Xin Tong, Xu Chen, Xunke Xi, Xiaowei Huang, Yaxuan Wu, Yeqiu Yang, Yi Hu, Yujin Yuan, Yuliang Yan, Zile Zhou",
        "摘要": "摘要：大型语言模型 (LLM) 表现出惊人的潜力，将推荐系统从隐性行为模式匹配转变为显性意图推理。尽管RecGPT-V1首次通过将基于LLM的推理集成到用户兴趣挖掘和项目标签预测中开创了这一范式，但它存在四个基本限制：（1）多路径推理中的计算效率低和认知冗余；（2）固定模板生成中解释多样性不足；（3）监督学习模式下的泛化能力有限；（4）简单的结果导向评估未能符合人类标准。\n\n为解决这些问题，我们提出了RecGPT-V2，并进行了四项重要创新。首先，分层多智能体系统通过协调协作重组意图推理，消除了认知重复，同时实现了多样化意图覆盖。结合用户行为上下文压缩的混合表示推理，我们的框架减少了60％的GPU消耗，并将专属召回从9.39％提高到10.99％。其次，元提示框架动态生成上下文适应性提示，解释多样性提高了+7.3％。第三，约束强化学习缓解了多重奖励冲突，实现了标签预测+24.1％的改善和解释接受度+13.0％的提升。第四，智能体作为裁判框架将评估分解为多步骤推理，提高了人类偏好一致性。淘宝的在线A/B测试显示显著改进：CTR增加了+2.98％，IPV增加了+3.71％，TV增加了+2.19％，NER增加了+11.46％。RecGPT-V2不仅证明了基于LLM的意图推理大规模部署在技术上的可行性和商业上的可行性，还弥合了认知探索与工业实用性之间的差距。",
        "地址": "https://arxiv.org/pdf/2512.14503.pdf"
    },
    {
        "名称": "2025 [2512.13303] ShowTable: Unlocking Creative Table Visualization with Collaborative Reflection and Refinement.pdf",
        "作者": "Zhihang Liu, Xiaoyi Bao, Pandeng Li, Junjie Zhou, Zhaohe Liao, Yefei He, Kaixun Jiang, Chen-Wei Xie, Yun Zheng, Hongtao Xie",
        "摘要": "摘要: 尽管现有的生成和统一模型在一般的图像生成方面表现出色，但在需要深度推理、规划和精确的数据到视觉映射能力的任务中，它们仍然存在不足。为了突破现有的限制，我们引入了一项新的具有挑战性的任务：创意表格可视化，这要求模型生成一个忠实且美观地可视化给定表格数据的信息图。为了解决这一挑战，我们提出了ShowTable，这是一种通过渐进的自我校正过程将多模态语言模型（MLLMs）与扩散模型协同工作的流程。MLLM作为推理视觉计划和判断视觉错误的核心协调者，以提供精炼的指令，扩散模型执行MLLM的命令，达到高保真的结果。为了支持这项任务和我们的流程，我们引入了三种用于训练不同模块的自动化数据构建流程。此外，我们介绍了TableVisBench，这是一个包含800个具有挑战性实例的新基准，涵盖5个评估维度，用于评估该任务的表现。实验表明，我们使用不同模型实例化的流程显著优于基线，突显其有效的多模态推理、生成和错误校正能力。\n\n来源：https://arxiv.org/pdf/2512.13303.pdf",
        "地址": "https://arxiv.org/pdf/2512.13303.pdf"
    },
    {
        "名称": "2025 [2512.13678] Feedforward 3D Editing via Text-Steerable Image-to-3D.pdf",
        "作者": "Ziqi Ma, Hongqiao Chen, Yisong Yue, Georgia Gkioxari",
        "摘要": "摘要: 最近在图像到3D技术方面的进展为设计、AR/VR和机器人技术开辟了巨大的可能性。然而，要在实际应用中使用AI生成的3D资产，一个关键要求是能够轻松编辑它们。我们提出了一种前馈方法Steer3D，为图像到3D模型增加文本可操控性，允许通过语言编辑生成的3D资产。我们的方法受ControlNet的启发，将其适配于图像到3D生成，使得能够在前向传递中直接进行文本操控。我们构建了一个可扩展的数据引擎用于自动数据生成，并开发了基于流匹配训练和直接偏好优化（DPO）的两阶段训练方案。与竞争方法相比，Steer3D更忠实地遵循语言指令，并且更好地保持了与原始3D资产的一致性，同时速度提高了2.4倍到28.5倍。Steer3D展示了在预训练的图像到3D生成模型中可以通过10万数据加入新的（文本）模态进行操控的可能性。项目网站: this https URL",
        "地址": "https://arxiv.org/pdf/2512.13678.pdf"
    },
    {
        "名称": "2025 [2512.13607] Nemotron-Cascade: Scaling Cascaded Reinforcement Learning for General-Purpose Reasoning Models.pdf",
        "作者": "Boxin Wang, Chankyu Lee, Nayeon Lee, Sheng-Chieh Lin, Wenliang Dai, Yang Chen, Yangyi Chen, Zhuolin Yang, Zihan Liu, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping",
        "摘要": "摘要：通过强化学习（RL）构建通用推理模型需要处理大量领域内的异质性，包括推理时间响应长度和验证延迟的巨大变化。这种变异性使RL基础设施复杂化，减慢训练速度，并使训练课程（例如响应长度扩展）和超参数选择变得困难。在这项工作中，我们提出了级联领域强化学习（Cascade RL），以开发通用推理模型Nemotron-Cascade，能够在指导模式和深度思考模式下运行。与传统方法混合不同领域的异质提示不同，级联RL按顺序组织领域RL，减少了工程复杂性，并在广泛的基准上提供了最先进的性能。特别是，当用作预步骤时，RLHF用于对齐大大增强了模型的推理能力，而不仅仅是偏好优化，随后领域RLVR阶段很少会降低在早期领域中达到的基准性能，甚至可能提升它（如图1所示）。我们的14B模型在RL之后，在LiveCodeBench v5/v6/Pro上超越了其SFT教师DeepSeek-R1-0528，并在2025年国际信息学奥林匹克竞赛（IOI）中获得银牌成绩。我们透明地分享了我们的训练和数据配方。",
        "地址": "https://arxiv.org/pdf/2512.13607.pdf"
    },
    {
        "名称": "2025 [2512.13961] Olmo 3.pdf",
        "作者": "Team Olmo: Allyson Ettinger, Amanda Bertsch, Bailey Kuehl, David Graham, David Heineman, Dirk Groeneveld, Faeze Brahman, Finbarr Timbers, Hamish Ivison, Jacob Morrison, Jake Poznanski, Kyle Lo, Luca Soldaini, Matt Jordan, Mayee Chen, Michael Noukhovitch, Nathan Lambert, Pete Walsh, Pradeep Dasigi, Robert Berry, Saumya Malik, Saurabh Shah, Scott Geng, Shane Arora, Shashank Gupta, Taira Anderson, Teng Xiao, Tyler Murray, Tyler Romero, Victoria Graf, Akari Asai, Akshita Bhagia, Alexander Wettig, Alisa Liu, Aman Rangapur, Chloe Anastasiades, Costa Huang, Dustin Schwenk, Harsh Trivedi, Ian Magnusson, Jaron Lochner, Jiacheng Liu, Lester James V. Miranda, Maarten Sap, Malia Morgan, Michael Schmitz, Michal Guerquin, Michael Wilson, Regan Huff, Ronan Le Bras, Rui Xin, Rulin Shao, Sam Skjonsberg, Shannon Zejiang Shen, Shuyue Stella Li, Tucker Wilde, Valentina Pyatkin, Will Merrill, Yapei Chang, Yuling Gu, Zhiyuan Zeng, Ashish Sabharwal, Luke Zettlemoyer, Pang Wei Koh, Ali Farhadi, Noah A. Smith, Hannaneh Hajishirzi",
        "摘要": "摘要:\n我们介绍了Olmo 3，这是一组最先进的、完全开放的语言模型，参数规模为7B和32B。Olmo 3模型的构建旨在实现长文本推理、函数调用、编码、指令跟随、通用聊天和知识回忆。此版本包括了完整的模型流程，即模型家族的全生命周期，包括构建过程中使用的每个阶段、检查点、数据点和依赖项。我们的旗舰模型Olmo 3 Think 32B是迄今为止发布的最强的完全开放的思维模型。",
        "地址": "https://arxiv.org/pdf/2512.13961.pdf"
    },
    {
        "名称": "2025 [2512.13399] Differentiable Evolutionary Reinforcement Learning.pdf",
        "作者": "Sitao Cheng, Tianle Li, Xuhan Huang, Xunjian Yin, Difan Zou",
        "摘要": "摘要：\n强化学习（RL）中的奖励函数设计是一个核心且经常艰难的挑战，特别是在为复杂推理任务开发自主代理时。尽管存在自动化的奖励优化方法，它们通常依赖无衍生的进化启发式方法，将奖励函数视为一个黑箱，未能捕捉到奖励结构与任务性能之间的因果关系。为弥补这一缺陷，我们提出了可微分进化强化学习（DERL），这是一种双层框架，可实现最佳奖励信号的自主发现。在DERL中，元优化器通过组合结构化的原子原语来进化奖励函数（即元奖励），指导内循环策略的训练。与之前的进化不同，DERL在其元优化方面是可微分的：它将内循环验证性能视为一种信号，通过强化学习更新元优化器。这使得DERL能够近似任务成功的“元梯度”，逐步学习生成更密集和更有行动意义的反馈。我们在三个不同领域验证了DERL：机器人代理（ALFWorld）、科学模拟（ScienceWorld）和数学推理（GSM8k，MATH）。实验结果表明，DERL在ALFWorld和ScienceWorld上达到了最先进的性能，显著优于依赖启发式奖励的方法，尤其是在分布外场景中。进化轨迹分析表明，DERL成功捕获了任务的内在结构，实现了无需人为干预的自我改进代理对齐。\n\n作者：Sitao Cheng, Tianle Li, Xuhan Huang, Xunjian Yin, Difan Zou\n\n评论：工作正在进行中。我们在此https URL发布了我们的代码和模型。\n\n链接：https://arxiv.org/pdf/2512.13399.pdf\n\n标题：2025 [2512.13399] 可微分进化强化学习.pdf",
        "地址": "https://arxiv.org/pdf/2512.13399.pdf"
    },
    {
        "名称": "2025 [2512.14531] VersatileFFN: Achieving Parameter Efficiency in LLMs via Adaptive Wide-and-Deep Reuse.pdf",
        "作者": "Ying Nie, Kai Han, Hongguang Li, Hang Zhou, Tianyu Guo, Enhua Wu, Xinghao Chen, Yunhe Wang",
        "摘要": "摘要：大型语言模型（LLMs）的快速扩展取得了显著的性能，但也导致了无法忍受的内存成本。现有的参数高效方法，如剪枝和量化，主要压缩了预训练的模型，而没有增强架构容量，从而达到了基础模型的表示天花板。在这项工作中，我们提出了VersatileFFN，这是一种新颖的前馈网络（FFN），能够在固定参数预算内在宽度和深度维度上灵活重用参数。受认知双过程理论的启发，VersatileFFN包含两条自适应路径：一种宽度多功能路径，从单个共享FFN生成混合子专家，模仿稀疏专家路由而不增加参数；另一种深度多功能路径，递归地应用相同的FFN，以模拟复杂标记的更深处理。一个难度感知的门控机制动态平衡这两条路径，将“简单”的标记引导到高效的宽路径，并为“困难”的标记分配更深的迭代细化。关键是，这两条路径都重用相同的参数，因此所有额外的容量都来自计算而不是内存。通过多种基准测试和模型规模的实验证明了该方法的有效性。代码将在此https URL提供。",
        "地址": "https://arxiv.org/pdf/2512.14531.pdf"
    },
    {
        "名称": "2025 [2512.14442] A4-Agent: An Agentic Framework for Zero-Shot Affordance Reasoning.pdf",
        "作者": "Zixin Zhang, Kanghao Chen, Hanqing Wang, Hongfei Zhang, Harold Haodong Chen, Chenfei Liao, Litao Guo, Ying-Cong Chen",
        "摘要": "摘要：可供性预测是根据语言指令识别物体上的交互区域，这对具身人工智能至关重要。现有的端到端模型将高级推理和低级落实结合到单个单一管道中，并依赖于带注释的数据集进行训练，这导致在新物体和未见过的环境中泛化能力差。在本文中，我们提出了 A4-Agent，超越了这种范式，这是一种无训练的代理框架，将可供性预测解耦为三个阶段的管道。我们的方法在测试时协调专用的基础模型：（1）$\\\\textbf{Dreamer}$ 使用生成模型可视化交互的 $\\\\textit{样貌}$；（2）$\\\\textbf{Thinker}$ 利用大型视觉-语言模型决定交互 $\\\\textit{什么}$ 对象部位；（3）$\\\\textbf{Spotter}$ 协调视觉基础模型精确定位交互区域的 $\\\\textit{位置}$。通过利用预训练模型的互补优势而无需任何针对特定任务的微调，我们的零样本框架在多个基准测试中显著超越了最先进的监督方法，并展示了对现实世界环境的强大泛化能力。",
        "地址": "https://arxiv.org/pdf/2512.14442.pdf"
    },
    {
        "名称": "2025 [2512.14284] SS4D: Native 4D Generative Model via Structured Spacetime Latents.pdf",
        "作者": "Zhibing Li, Mengchen Zhang, Tong Wu, Jing Tan, Jiaqi Wang, Dahua Lin",
        "摘要": "摘要：我们提出了SS4D，这是一种原生的4D生成模型，可以直接从单目视频中合成动态3D对象。与通过优化3D或视频生成模型构建4D表示的先前方法不同，我们直接在4D数据上训练生成器，实现了高保真度、时间一致性和结构一致性。我们方法的核心是一个压缩的结构化时空潜变量集合。具体来说：（1）为了解决4D训练数据的稀缺性，我们建立在预训练的单图像到3D模型的基础上，保持强大的空间一致性。（2）通过引入专门的时间层在帧之间进行推理，确保时间一致性。（3）为了支持长视频序列的高效训练和推理，我们使用因式分解的4D卷积和时间下采样模块沿时间轴压缩潜变量序列。此外，我们采用了精心设计的训练策略，以增强对遮挡的鲁棒性。",
        "地址": "https://arxiv.org/pdf/2512.14284.pdf"
    },
    {
        "名称": "2025 [2512.14008] Sparse-LaViDa: Sparse Multimodal Discrete Diffusion Language Models.pdf",
        "作者": "Shufan Li, Jiuxiang Gu, Kangning Liu, Zhe Lin, Zijun Wei, Aditya Grover, Jason Kuen",
        "摘要": "摘要：马斯克离散扩散模型（MDMs）在包括图像理解、生成和编辑在内的多种多模态任务中取得了优异的表现。然而，由于每次采样步骤需要反复处理冗余的掩码标记，其推理速度仍然不尽如人意。在这项工作中，我们提出了Sparse-LaViDa，一种新的建模框架，通过在每个推理步骤动态截断不必要的掩码标记来加速MDM采样。为了保持生成质量，我们引入了专用的注册标记，作为被截断标记的紧凑表示。此外，为了确保训练和推理之间的一致性，我们设计了一种专门的注意掩码，使其在训练过程中准确匹配截断采样程序。基于最先进的统一MDM LaViDa-O，Sparse-LaViDa在包括文本到图像生成、图像编辑和数学推理在内的多种任务中实现了高达2倍的加速，同时保持了生成质量。",
        "地址": "https://arxiv.org/pdf/2512.14008.pdf"
    },
    {
        "名称": "2025 [2512.14697] Spherical Leech Quantization for Visual Tokenization and Generation.pdf",
        "作者": "Yue Zhao, Hanwen Jiang, Zhenlin Xu, Chutong Yang, Ehsan Adeli, Philipp Krähenbühl",
        "摘要": "摘要：由于参数高效和在大码书上的可扩展性，非参数量化引起了广泛关注。在本文中，我们通过格子编码的视角提出了不同非参数量化方法的统一公式。格子码的几何解释了在训练具有某些现有无查找表量化变体（如BSQ）的自动编码器时辅助损失项的必要性。作为进步的一步，我们探索了一些可能的候选者，包括随机格子、广义斐波那契格子和最密球堆积格子。在所有这些方法中，我们发现基于李奇格子的量化方法，称为球形李奇量化（$\\Lambda_{24}$-SQ），由于其在超球体上的高度对称性和均匀分布，导致了简化的训练方法和改进的重构-压缩权衡。在图像标记化和压缩任务中，这种量化方法在所有指标上的重构质量均优于之前最好的BSQ，同时消耗的比特略少。这种改进也扩展到了最先进的自回归图像生成框架。",
        "地址": "https://arxiv.org/pdf/2512.14697.pdf"
    },
    {
        "名称": "2025 [2512.14696] CRISP: Contact-Guided Real2Sim from Monocular Video with Planar Scene Primitives.pdf",
        "作者": "Zihan Wang, Jiashun Wang, Jeff Tan, Yiwen Zhao, Jessica Hodgins, Shubham Tulsiani, Deva Ramanan",
        "摘要": "摘要：我们介绍了一种称为CRISP的方法，该方法能从单目视频中恢复模拟人类运动和场景几何。之前在人景重建方面的工作依赖数据驱动的先验知识和没有物理约束的联合优化，或者恢复出带有噪声的几何体，导致考虑场景交互的运动跟踪策略失败。相比之下，我们的一个关键见解是通过将平面原语拟合到场景的点云重建来恢复凸的、干净的、准备用于模拟的几何体，具体方法是通过基于深度、法线和流的简单聚类管道。为了重建在交互过程中可能被遮挡的场景几何，我们利用了人景接触建模（例如，我们利用人体姿态来重建椅子的遮挡部分）。最后，我们通过利用这些几何体来驱动一个类人控制器并使用强化学习来确保人和场景的重建具有物理可行性。我们的方法将人类中心视频基准测试（EMDB、PROX）的运动跟踪失败率从55.2%降低到6.9%，同时提供了43%更快的RL模拟吞吐量。我们还在包括随意拍摄的视频、互联网视频，甚至是Sora生成的视频等自然视频上验证了该方法。这展示了CRISP在大规模生成具有物理有效性的人类运动和交互环境的能力，大大推进了机器人和AR/VR的真实到模拟应用。\n\n——作者：王之晗，王佳舜，谭捷，赵逸文，杰西卡·霍奇斯，图施扬·舒布哈姆，德瓦·拉曼南",
        "地址": "https://arxiv.org/pdf/2512.14696.pdf"
    },
    {
        "名称": "2025 [2512.14666] EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models.pdf",
        "作者": "Zechen Bai, Chen Gao, Mike Zheng Shou",
        "摘要": "摘要：实现真正自适应的具身智能需要智能体不仅通过模仿静态演示来学习，还要通过与环境的持续互动进行改进，这类似于人类通过实践掌握技能的方式。视觉-语言-行动（VLA）模型利用大型语言模型，在机器人操作方面取得了进展，但仍然受到监督微调（SFT）的基本限制：每个任务需要成百上千的演示，死板地记忆轨迹，并且在部署条件偏离训练时无法适应。我们介绍了EVOLVE-VLA，一种测试时训练框架，使得VLA可以通过与环境互动进行持续适应，并且几乎无需特定任务的演示。关键技术挑战是将测试时不可用的预期奖励信号替换为自主反馈。我们通过一个学习的进展估计器提供密集反馈来解决这一问题，并且通过两种机制“驯服”这种本质上噪声较大的信号：(1)累积进展估计机制平滑噪声较大的逐点估计，(2)渐进视野拓展策略使策略逐渐进化。EVOLVE-VLA实现了显著提升：在长时间任务上提高了8.6%，在一次学习中提高了22.0%，并实现了跨任务泛化——在未见过的任务上无需特定任务的演示训练即达到了20.8%的成功率（相比纯SFT的0%）。定性分析揭示了演示中不存在的新能力，包括错误恢复和新策略。这项工作为真正能学习和适应的VLA迈出了关键一步，超越了静态模仿，走向持续的自我改进。\n",
        "地址": "https://arxiv.org/pdf/2512.14666.pdf"
    },
    {
        "名称": "2025 [2512.14550] TAT: Task-Adaptive Transformer for All-in-One Medical Image Restoration.pdf",
        "作者": "Zhiwen Yang, Jiaju Zhang, Yang Yi, Jian Liang, Bingzheng Wei, Yan Xu",
        "摘要": "摘要：医学图像恢复 (MedIR) 旨在从低质量的医学图像中恢复出高质量的医学图像。近年来，MedIR 的进展主要集中在能够同时处理多种不同 MedIR 任务的 All-in-One 模型。然而，由于成像模式和退化类型的显著差异，使用共享模型进行这些多样任务需要仔细考虑两个关键的任务间关系：任务干扰，即当同一参数在不同任务中产生冲突的梯度更新方向时；及任务不平衡，即由于每个任务固有的学习难度不同导致的优化不均。为了解决这些挑战，我们提出了一种任务自适应Transformer（TAT），这是一个通过两项关键创新动态适应不同任务的新框架。首先，介绍了一种任务自适应权重生成策略，通过为每个任务生成特定权重参数来减轻任务干扰，从而消除共享权重参数上的潜在梯度冲突。其次，提出了一种任务自适应损失平衡策略，根据任务特定的学习难度动态调整损失权重，防止任务主导或训练不足。大量实验表明，我们提出的 TAT 在三个 MedIR 任务——PET 合成、CT 除噪和 MRI 超分辨率——的任务特定和 All-in-One 设置中均实现了最先进的性能。代码可在此 https URL 获取。\n\n作者：Zhiwen Yang, Jiaju Zhang, Yang Yi, Jian Liang, Bingzheng Wei, Yan Xu\n\n备注：本文已被 MICCAI 2025 接录\n\n网址：https://arxiv.org/pdf/2512.14550.pdf\n\n标题：2025 [2512.14550] TAT: Task-Adaptive Transformer for All-in-One Medical Image Restoration",
        "地址": "https://arxiv.org/pdf/2512.14550.pdf"
    },
    {
        "名称": "2025 [2512.14273] Zoom-Zero: Reinforced Coarse-to-Fine Video Understanding via Temporal Zoom-in.pdf",
        "作者": "Xiaoqian Shen, Min-Hung Chen, Yu-Chiang Frank Wang, Mohamed Elhoseiny, Ryo Hachiuma",
        "摘要": "以下是学术论文的摘要及其中文翻译：\n\n**英文摘要：**\n*Abstract: Grounded video question answering (GVQA) aims to localize relevant temporal segments in videos and generate accurate answers to a given question; however, large video-language models (LVLMs) exhibit limited temporal awareness. Although existing approaches based on Group Relative Policy Optimization (GRPO) attempt to improve temporal grounding, they still struggle to faithfully ground their answers in the relevant video evidence, leading to temporal mislocalization and hallucinations. In this work, we present Zoom-Zero, a coarse-to-fine framework that first localizes query-relevant segments and then temporally zooms into the most salient frames for finer-grained visual verification. Our method addresses the limits of GRPO for the GVQA task with two key innovations: (i) a zoom-in accuracy reward that validates the fidelity of temporal grounding prediction and facilitates fine-grained visual verification on grounded frames; (ii) token-selective credit assignment, which attributes rewards to the tokens responsible for temporal localization or answer generation, mitigating GRPO's issue in handling multi-faceted reward signals. Our proposed method advances grounded video question answering, improving temporal grounding by 5.2% on NExT-GQA and 4.6% on ReXTime, while also enhancing average answer accuracy by 2.4%. Additionally, the coarse-to-fine zoom-in during inference further benefits long-form video understanding by preserving critical visual details without compromising global context, yielding an average improvement of 6.4% on long-video benchmarks.*\n\n**中文翻译：**\n*摘要：基础视频问答 (GVQA) 旨在定位视频中与问题相关的时间段并生成准确的答案；然而，大型视频语言模型 (LVLM) 缺乏时间意识。尽管现有基于群体相对策略优化 (GRPO) 的方法试图改善时间定位，但它们在将答案准确定位于相关视频证据时仍然存在问题，导致时间定位错误和幻觉。在这项工作中，我们提出了 Zoom-Zero，这是一个粗到细的框架，首先定位查询相关的片段，然后在时间上放大到最显著的帧以进行更细粒度的视觉验证。我们的方法通过两个关键创新解决了 GRPO 在 GVQA 任务中的局限性：(i) 缩放精度奖励，以验证时间定位预测的保真度，并促进对定位帧的细粒度视觉验证；(ii) 选择性奖励分配，奖励归于负责时间定位或答案生成的标记，缓解了 GRPO 处理多方面奖励信号的问题。我们提出的方法推进了基础视频问答，在 NExT-GQA 上提高了 5.2% 的时间定位精度，在 ReXTime 上提高了 4.6%，同时提高了 2.4% 的平均答案准确率。此外，推理期间从粗到细的缩放通过在不影响整体上下文的情况下保留关键视觉细节，进一步增强了长视频理解，在长视频基准上平均提高了 6.4%。*",
        "地址": "https://arxiv.org/pdf/2512.14273.pdf"
    },
    {
        "名称": "2025 [2512.14067] Efficient-DLM: From Autoregressive to Diffusion Language Models, and Beyond in Speed.pdf",
        "作者": "Yonggan Fu, Lexington Whalen, Zhifan Ye, Xin Dong, Shizhe Diao, Jingyu Liu, Chengyue Wu, Hao Zhang, Enze Xie, Song Han, Maksim Khadkevich, Jan Kautz, Yingyan Celine Lin, Pavlo Molchanov",
        "摘要": "摘要：扩散语言模型（dLMs）已经成为一种有前途的范式，能够实现并行的、非自回归的生成，但其学习效率在从头训练时却落后于自回归（AR）语言模型。为此，我们研究了从AR到dLM的转换，以将预训练的AR模型转化为在速度上具有优势且保留AR模型任务准确性的高效dLMs。我们通过识别现有AR到dLM方法在注意模式和目标上的局限性，提出了更有效的AR到dLM转换的原则和方法。具体来说，首先我们系统地比较了不同的注意模式，发现保持预训练AR权重分布对于有效的AR到dLM转换至关重要。因此，我们引入了具有块状注意模式的连续预训练方案，该方案在跨块过程中保持因果性，同时在每个块内实现双向建模。我们发现，这种方法比完全双向建模更好地保留了预训练的AR模型的权重分布，此外还具有已知的KV缓存能力，进而在准确性和效率上实现双赢。其次，为了减小训练-测试期间掩码令牌分布（均匀与高度从左到右）的差距，我们提出了一种位置依赖的令牌掩码策略，在训练过程中对后面的令牌分配更高的掩码概率，以更好地模拟测试时的行为。在这一框架的支持下，我们对dLMs的注意模式、训练动态和其他设计选择进行了广泛研究，提供了关于可扩展AR到dLM转换的可行见解。这些研究产生了Efficient-DLM家族，其表现优于最先进的AR模型和dLMs，例如我们的Efficient-DLM 8B在准确性上比Dream 7B和Qwen3 4B分别高5.4%和2.7%，吞吐量分别高4.5倍和2.7倍。\n\n作者：Yonggan Fu, Lexington Whalen, Zhifan Ye, Xin Dong, Shizhe Diao, Jingyu Liu, Chengyue Wu, Hao Zhang, Enze Xie, Song Han, Maksim Khadkevich, Jan Kautz, Yingyan Celine Lin, Pavlo Molchanov\n\n链接：https://arxiv.org/pdf/2512.14067.pdf\n\n标题：2025 [2512.14067] Efficient-DLM: 从自回归到扩散语言模型，并超越速度",
        "地址": "https://arxiv.org/pdf/2512.14067.pdf"
    },
    {
        "名称": "2025 [2512.13525] Janus: Disaggregating Attention and Experts for Scalable MoE Inference.pdf",
        "作者": "Zhexiang Zhang, Ye Wang, Xiangyu Wang, Yumiao Zhao, Jingzhe Jiang, Qizhen Weng, Shaohuai Shi, Yin Chen, Minchen Yu",
        "摘要": "摘要：大规模专家混合（MoE）模型推理具有高资源需求和动态工作负载，因此具有挑战性。现有解决方案通常将整个模型作为单一整体部署，对注意力和专家模块应用统一的资源配置，尽管它们的需求不同，这导致了扩展性有限和资源效率低下。本文提出了Janus，这是一种可扩展的MoE推理系统，它将注意力和专家模块在独立的GPU子集群上解耦，实现了每个模块的独立管理和扩展。Janus包括三个关键设计，以实现高效、解耦的MoE推理。首先，它提出了一种自适应的两阶段通信方案，利用节点内和节点间的带宽层次结构进行低延迟数据交换。其次，受限于MoE模块的存储器特性，Janus引入了一个轻量级调度器，并将其实现为GPU内核，以最小的开销平衡跨GPU激活的专家数量，从而减少推理延迟。第三，Janus执行细粒度的资源管理，动态调整专家的分布，并独立扩展注意力和MoE资源以提高整体效率。评估结果表明，Janus的每GPU吞吐量比最先进的系统高出最多3.9倍，同时满足每个token的延迟要求。",
        "地址": "https://arxiv.org/pdf/2512.13525.pdf"
    },
    {
        "名称": "2025 [2512.14391] RePo: Language Models with Context Re-Positioning.pdf",
        "作者": "Huayang Li, Tianyu Zhao, Richard Sproat",
        "摘要": "摘要: \n情境学习是现代大型语言模型（LLMs）的基础；然而，现有架构通过分配线性或恒定的位置索引，强加了一种僵化且固定的情境结构。借鉴认知负荷理论（CLT），我们认为这种不具信息性的结构增加了额外的认知负荷，消耗了有限的工作记忆容量，这些容量本应分配给深入推理和注意力分配。为了解决这个问题，我们提出了RePo，一种通过上下文重新定位来减少额外负荷的新机制。与标准方法不同，RePo利用一个可微模块 $f_\\\\phi$ 来分配捕捉上下文依赖性的词元位置，而不是依赖预定义的整数范围。通过在OLMo-2 1B骨干上持续的预训练，我们证明了RePo显著提高了涉及噪声上下文、结构化数据和较长上下文长度的任务表现，同时在一般短上下文任务上保持了竞争力的表现。详细分析表明，RePo成功地将更高的注意力分配给遥远但相关的信息，在密集和非线性空间中分配位置，并捕捉输入上下文的内在结构。我们的代码可以在此https URL获得。",
        "地址": "https://arxiv.org/pdf/2512.14391.pdf"
    },
    {
        "名称": "2025 [2512.14620] JMMMU-Pro: Image-based Japanese Multi-discipline Multimodal Understanding Benchmark via Vibe Benchmark Construction.pdf",
        "作者": "Atsuyuki Miyai, Shota Onohara, Jeonghun Baek, Kiyoharu Aizawa",
        "摘要": "摘要：本文介绍了JMMMU-Pro，这是一种基于图像的多学科多模态理解基准，以及一种可扩展的基准构建方法Vibe Benchmark Construction。JMMMU-Pro在进化自MMMU到MMMU-Pro的过程中，通过将问题图像和问题文本组合成单个图像，扩展了JMMMU，从而创建了一个需要通过视觉感知实现综合视觉-文本理解的基准。为了构建JMMMU-Pro，我们提出了Vibe Benchmark Construction，一种通过图像生成模型（例如Nano Banana Pro）生成候选视觉问题，并由人工验证输出，如有必要，再生成调整后的提示以确保质量的方法。通过利用Nano Banana Pro的高逼真图像生成能力及其嵌入干净日文文本的能力，我们以低成本构建了一个高质量的基准，覆盖了广泛的背景和布局设计。实验结果表明，所有开源LMMs在JMMMU-Pro上表现不佳，强调了JMMMU-Pro作为指导开源社区未来工作的重要基准。我们相信JMMMU-Pro提供了一个更严格的评估工具来评估LMMs的日语能力，而我们的Vibe Benchmark Construction也为未来基于图像的VQA基准开发提供了一个有效的指导方针。\n\nAuthors: Atsuyuki Miyai, Shota Onohara, Jeonghun Baek, Kiyoharu Aizawa",
        "地址": "https://arxiv.org/pdf/2512.14620.pdf"
    },
    {
        "名称": "2025 [2512.14014] MobileWorldBench: Towards Semantic World Modeling For Mobile Agents.pdf",
        "作者": "Shufan Li, Konstantinos Kallidromitis, Akash Gokul, Yusuke Kato, Kazuki Kozuka, Aditya Grover",
        "摘要": "摘要：世界模型在提高具身代理的任务性能方面显示出很大的实用性。虽然之前的工作主要集中在像素空间世界模型上，但这些方法在图形用户界面（GUI）设置中面临实际限制，在未来状态中预测复杂视觉元素通常很困难。在这项工作中，我们探索了一种针对GUI代理的世界建模替代形式，其中状态转换由自然语言描述，而不是预测原始像素。首先，我们介绍了MobileWorldBench，这是一项评估视觉-语言模型（VLMs）作为移动GUI代理世界模型能力的基准测试。其次，我们发布了MobileWorld，一个包含140万样本的大规模数据集，显著提高了VLMs的世界建模能力。最后，我们提出了一种将VLM世界模型集成到移动代理规划框架中的新颖框架，证明了语义世界模型可以通过提高任务成功率直接使移动代理受益。源码和数据集可通过该URL获取。\n\n文档信息：\n- 标题：MobileWorldBench: Towards Semantic World Modeling For Mobile Agents\n- 作者：Shufan Li, Konstantinos Kallidromitis, Akash Gokul, Yusuke Kato, Kazuki Kozuka, Aditya Grover\n- 年份：2025\n- 评论：21页，13幅图\n- URL: https://arxiv.org/pdf/2512.14014.pdf",
        "地址": "https://arxiv.org/pdf/2512.14014.pdf"
    },
    {
        "名称": "2025 [2512.13655] Comparative Analysis of LLM Abliteration Methods: A Cross-Architecture Evaluation.pdf",
        "作者": "Richard J. Young",
        "摘要": "摘要：大型语言模型中的安全对齐机制通过学习到的拒绝行为防止对有害查询的响应，但这些机制也阻碍了包括认知建模、对抗性测试和安全分析在内的合法研究应用。虽然切割技术可以通过定向正交化来实现拒绝表示的外科手术切除，但现有实现的相对有效性尚未得到表征。本研究评估了四种消除工具（Heretic、DECCP、ErisForge、FailSpy）在十六个指令调优模型（7B-14B参数）中的表现，报告了所有16个模型的工具兼容性以及工具支持的子集的定量指标。单次方法在基准子集上表现出更强的能力保留（在三个模型中的平均GSM8K变化：ErisForge -0.28pp；DECCP -0.13pp），而贝叶斯优化消除则产生了可变的分布偏移（KL散度：0.043-1.646），伴随依赖模型的能力影响。这些发现为研究人员提供了基于证据的选择标准，用于在不同模型架构中部署消除工具。主要发现表明，数学推理能力表现出对消除干预的最高敏感性，GSM8K变化范围从+1.51pp到-18.81pp（相对变化-26.5%），具体取决于工具选择和模型架构。",
        "地址": "https://arxiv.org/pdf/2512.13655.pdf"
    },
    {
        "名称": "2025 [2512.13106] TraPO: A Semi-Supervised Reinforcement Learning Framework for Boosting LLM Reasoning.pdf",
        "作者": "Shenzhi Yang, Guangcheng Zhu, Xing Zheng, Yingfan MA, Zhongqi Chen, Bowen Song, Weiqiang Wang, Junbo Zhao, Gang Chen, Haobo Wang",
        "摘要": "摘要: 基于可验证奖励的强化学习（RLVR）已被证明在利用答案验证信号指导策略优化来训练大型推理模型（LRMs）方面是有效的，但这种方法需要高昂的注释成本。为了解决这一问题，最近的研究探索了从模型内部一致性（如熵和多数投票）中获取奖励的无监督RLVR方法。尽管看似有前途，但这些方法在训练的后期阶段通常会出现模型崩溃，这可能是由于缺乏外部监督而强化了错误的推理模式。在这项工作中，我们研究了一种新颖的半监督RLVR范式，它利用一个小型标注集来指导对未标注样本的RLVR训练。我们的主要观点是，监督奖励对于稳定基于一致性的未标注样本训练是必不可少的，这确保了只有在标注实例上验证过的推理模式才会被纳入强化学习中。从技术上讲，我们提出了一种有效的策略优化算法TraPO，该算法通过匹配未标注样本与标注样本的学习轨迹相似性来识别可靠的未标注样本。在此基础上，TraPO在六个广泛使用的数学推理基准（AIME24/25、AMC、MATH-500、Minerva和Olympiad）和三个分布外任务（ARC-c、GPQA-diamond和MMLU-pro）上表现出显著的数据效率和强大的泛化能力。仅使用1K标注样本和3K未标注样本，TraPO达到平均42.6%的准确率，超过了在45K未标注样本上训练的最佳无监督方法（38.3%）。值得注意的是，当使用4K标注样本和12K未标注样本时，TraPO甚至在所有基准上超越了在44K标注样本上训练的完全监督模型，同时仅使用了10%的标注数据。代码可通过此HTTPS链接获得。",
        "地址": "https://arxiv.org/pdf/2512.13106.pdf"
    },
    {
        "名称": "2025 [2512.12941] UAGLNet: Uncertainty-Aggregated Global-Local Fusion Network with Cooperative CNN-Transformer for Building Extraction.pdf",
        "作者": "Siyuan Yao, Dongxiu Liu, Taotao Li, Shengjie Li, Wenqi Ren, Xiaochun Cao",
        "摘要": "摘要：由于建筑物结构复杂多变，从遥感影像中提取建筑物是一项具有挑战性的任务。现有方法在分割模型中采用卷积或自注意力块来捕捉多尺度特征，但特征金字塔的固有鸿沟以及全球-局部特征整合不足导致提取结果不准确和模糊。为了解决这一问题，本文提出了一种不确定性聚合的全球-局部融合网络（UAGLNet），该网络能够在不确定性建模的指导下，利用高质量的全球-局部视觉语义。具体而言，我们提出了一种新型合作编码器，该编码器在不同阶段采用混合的CNN和Transformer层来分别捕捉局部和全球视觉语义。一个中间合作交互块（CIB）被设计用来在网络加深时缩小局部和全球特征之间的差距。随后，我们提出了一种全球-局部融合（GLF）模块来互补融合全球和局部表征。此外，为了减轻不确定区域中的分割模糊性，我们提出了一种不确定性聚合解码器（UAD），该解码器显式估计逐像素的不确定性以提高分割精度。大量实验表明，我们的方法相比其他最先进的方法具有卓越的性能。我们的代码可以通过此链接获取：https://arxiv.org/pdf/2512.12941.pdf",
        "地址": "https://arxiv.org/pdf/2512.12941.pdf"
    },
    {
        "名称": "2025 [2512.14440] S2D: Sparse-To-Dense Keymask Distillation for Unsupervised Video Instance Segmentation.pdf",
        "作者": "Leon Sick, Lukas Hoyer, Dominik Engel, Pedro Hermosilla, Timo Ropinski",
        "摘要": "摘要:近年来，最先进的无监督视频实例分割严重依赖于从以物体为中心的图像数据集（如ImageNet）生成的合成视频数据。然而，通过人工移动和缩放图像实例掩码进行的视频合成未能准确模拟视频中的真实运动，例如视角变化、一个或多个实例的部分移动或相机运动。为了解决这个问题，我们提出了一种完全在真实视频数据上训练的无监督视频实例分割模型。我们从单个视频帧上的无监督实例分割掩码开始。然而，这些单帧分割显示出时间噪声，并且其质量在视频中有所不同。因此，我们通过利用深度运动先验识别视频中的高质量关键掩码来建立时间连贯性。然后，将稀疏的关键掩码伪注释用于训练隐式掩码传播的分割模型，为此我们提出了一种通过时间丢失数据辅助的Sparse-To-Dense蒸馏方法。在对生成的密集标签集上训练最终模型后，我们的方法在各种基准上超越了当前的最先进水平。",
        "地址": "https://arxiv.org/pdf/2512.14440.pdf"
    },
    {
        "名称": "2025 [2512.11934] Unveiling User Perceptions in the Generative AI Era: A Sentiment-Driven Evaluation of AI Educational Apps' Role in Digital Transformation of e-Teaching.pdf",
        "作者": "Adeleh Mazaherian, Erfan Nourbakhsh",
        "摘要": "摘要：生成式人工智能的快速整合推动了电子教学的数字化转型，但用户对人工智能教育应用程序的看法尚未得到充分探索。本研究通过对Google Play商店中顶级AI教育应用程序用户评论的情感驱动评估，来评估其有效性、挑战和教育意义。我们的流程包括抓取应用数据和评论，使用RoBERTa进行二元情感分类，使用GPT-4进行关键点提取，并使用GPT-5综合最重要的正面/负面主题。应用程序被分类为七种类型（例如家庭作业助手、数学解答器、语言工具），重叠反映了多功能设计。结果显示出主要是正面情感，像Edu AI（95.9%正面）和this http URL（92.7%正面）这样的家庭作业应用在准确性、速度和个性化方面领先，而语言/LMS应用（如Teacher AI，只有21.8%正面）因不稳定和功能有限而落后。正面反馈强调了在头脑风暴、问题解决和参与方面的效率；负面反馈集中在付费墙、不准确、广告和故障方面。趋势显示，家庭作业助手胜过了专门工具，突显了人工智能在带来机遇的同时也带来了依赖和不公的风险。讨论提出了未来的生态系统，包括混合AI-人类模型、VR/AR的沉浸式学习，并为开发人员（自适应个性化）和政策制定者（包括包容性货币化监管）提供了路线图。这强调了生成式人工智能在推进电子教学中的作用，通过实现道德上的改进来促进公平和创新的环境。完整的数据集可在这里找到（this https URL）。",
        "地址": "https://arxiv.org/pdf/2512.11934.pdf"
    },
    {
        "名称": "2025 [2512.10952] Hierarchical Dataset Selection for High-Quality Data Sharing.pdf",
        "作者": "Xiaona Zhou, Yingyan Zeng, Ran Jin, Ismini Lourentzou",
        "摘要": "摘要：现代机器学习的成功依赖于高质量的训练数据。在许多现实世界的场景中，如从公共资源库获取数据或在机构间共享数据，数据自然地组织成离散的数据集，这些数据集在相关性、质量和用途方面各不相同。因此，选择哪些资源库或机构来搜索有用的数据集，以及选择哪些数据集纳入模型训练是至关重要的决策。然而，大多数现有方法选择单个样本并视所有数据为同等相关，忽视了数据集及其来源之间的差异。在这项工作中，我们正式定义了数据集选择任务：在资源受限的情况下，从一个庞大、异构的池中选择整个数据集，以提高下游性能。我们提出了通过层次结构的数据集选择方法（DaSH），该方法在数据集和群组（例如，集合、机构）层面建模效用，从而能够从有限的观察中高效的泛化。在两个公共基准（Digit-Five和DomainNet）上，DaSH在准确率上比最先进的数据选择基线高出最多26.2%，同时探索步骤显著减少。消融实验表明，DaSH对资源匮乏的环境和缺乏相关数据集具有鲁棒性，使其适用于实际的多源学习工作流程中可扩展和自适应的数据集选择。\n\n作者：Xiaona Zhou, Yingyan Zeng, Ran Jin, Ismini Lourentzou\n\n链接：https://arxiv.org/pdf/2512.10952.pdf\n\n标题：2025 [2512.10952] 高质量数据共享的层次化数据集选择",
        "地址": "https://arxiv.org/pdf/2512.10952.pdf"
    },
    {
        "名称": "2025 [2512.10945] MeViS: A Multi-Modal Dataset for Referring Motion Expression Video Segmentation.pdf",
        "作者": "Henghui Ding, Chang Liu, Shuting He, Kaining Ying, Xudong Jiang, Chen Change Loy, Yu-Gang Jiang",
        "摘要": "摘要：本文提出了一个大规模的多模态数据集，用于指代运动表达视频分割，重点是根据目标对象运动的语言描述分割和跟踪视频中的目标对象。现有的指代视频分割数据集通常关注显著对象，并使用富含静态属性的语言表达，可能允许在单帧中识别目标对象。这类数据集往往低估了运动在视频和语言中的作用。为了探索使用运动表达和运动推理线索进行像素级视频理解的可行性，我们引入了MeViS数据集，包含33,072个人工注释的运动表达（包括文本和音频），涉及2,006个复杂场景视频中的8,171个对象。我们基准测试了15种现有方法，涵盖MeViS支持的4项任务，包括6种指代视频对象分割（RVOS）方法，3种音频引导的视频对象分割（AVOS）方法，2种指代多对象跟踪（RMOT）方法，以及4种用于新引入的指代运动表达生成（RMEG）任务的视频字幕方法。结果显示了现有方法在应对运动表达引导的视频理解方面的不足和局限性。我们进一步分析了这些挑战并提出了一种名为LMPM++的方法，用于RVOS/AVOS/RMOT，取得了新的最先进成果。我们的数据集为在复杂视频场景中开发运动表达引导的视频理解算法提供了一个平台。所提出的MeViS数据集及方法的源代码在公开的网址上可获取。",
        "地址": "https://arxiv.org/pdf/2512.10945.pdf"
    },
    {
        "名称": "2025 [2512.10342] CoSPlan: Corrective Sequential Planning via Scene Graph Incremental Updates.pdf",
        "作者": "Shresth Grover, Priyank Pathak, Akash Kumar, Vibhav Vineet, Yogesh S Rawat",
        "摘要": "摘要: 大规模视觉语言模型（VLMs）表现出令人印象深刻的复杂推理能力，但在视觉序列规划（即执行多步骤动作达到目标）上仍未得到深入研究。此外，实际的序列规划通常涉及非最优（错误）步骤，这对VLMs提出了检测和纠正这些步骤的挑战。我们提出了纠错序列规划基准（CoSPlan），以评估VLMs在四个领域（迷宫导航、积木重排、图像重建和物体重组）的容易出错的视觉序列规划任务中的表现。CoSPlan评估两项关键能力：错误检测（识别非最优动作）和步骤完成（纠正并完成动作序列以达到目标）。尽管使用了最新的推理技术如链式思维（Chain-of-Thought）和场景图（Scene Graphs），VLMs（如Intern-VLM和Qwen2）在CoSPlan上表现不佳，无法利用上下文线索达到目标。为了解决这个问题，我们提出了一种新的无训练方法——场景图增量更新（SGI），该方法在初始与目标状态之间引入中间推理步骤。SGI帮助VLMs进行序列推理，平均性能提高了5.2%。除了提高纠错序列规划的可靠性外，SGI还可以推广到传统的规划任务，如Plan-Bench和VQA。\n\n作者: Shresth Grover, Priyank Pathak, Akash Kumar, Vibhav Vineet, Yogesh S Rawat\n\nURL: https://arxiv.org/pdf/2512.10342.pdf\n\n标题: 2025 [2512.10342] CoSPlan: Corrective Sequential Planning via Scene Graph Incremental Updates",
        "地址": "https://arxiv.org/pdf/2512.10342.pdf"
    },
    {
        "名称": "2025 [2512.07328] ContextAnyone: Context-Aware Diffusion for Character-Consistent Text-to-Video Generation.pdf",
        "作者": "Ziyang Mai, Yu-Wing Tai",
        "摘要": "摘要: 文本到视频（T2V）生成技术发展迅速，但在场景间保持一致的角色身份仍然是一个主要挑战。现有的个性化方法通常关注面部身份，但未能保留更广泛的上下文线索，如发型、装束和体型，这对于视觉连贯性至关重要。我们提出了 \\textbf{ContextAnyone}，一个上下文感知扩散框架，可以从文本和单个参考图像生成角色一致的视频。我们的方法共同重建参考图像并生成新的视频帧，使模型能够充分感知和利用参考信息。参考信息通过一种新颖的强调注意模块有效集成到基于DiT的扩散骨干中，该模块选择性地增强参考感知特征，并防止身份漂移。双重引导损失结合了扩散和参考重建目标，以增强外观保真度，而我们提出的Gap-RoPE位置嵌入则分离了参考和视频标记以稳定时间建模。实验表明，ContextAnyone在身份一致性和视觉质量上优于现有的参考到视频方法，生成了贯穿各种动作和场景的连贯且保持上下文的角色视频。项目页面：\\href{this https URL}{this https URL}。",
        "地址": "https://arxiv.org/pdf/2512.07328.pdf"
    }
]