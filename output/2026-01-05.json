[
    {
        "名称": "2025 [2512.24615] Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization.pdf",
        "作者": "Yuchen Shi, Yuzheng Cai, Siqi Cai, Zihan Xu, Lichao Chen, Yulei Qin, Zhijian Zhou, Xiang Fei, Chaofan Qiu, Xiaoyu Tan, Gang Li, Zongyi Li, Haojia Lin, Guocan Cai, Yong Mao, Yunsheng Wu, Ke Li, Xing Sun",
        "摘要": "摘要：现有的大型语言模型（LLM）代理框架面临两个重大挑战：高配置成本和静态能力。构建高质量的代理通常需要大量的手工工作进行工具集成和提示工程，而部署的代理在没有昂贵的微调情况下难以适应动态环境。为了解决这些问题，我们提出了Youtu-Agent，一个面向LLM代理自动生成和持续进化的模块化框架。Youtu-Agent具有一个结构化配置系统，该系统解耦了执行环境、工具包和上下文管理，能够灵活重用和自动合成。我们引入了两种生成范式：用于标准任务的工作流模式和用于复杂、非标准需求的元代理模式，能够自动生成工具代码、提示和配置。此外，Youtu-Agent建立了一个混合策略优化系统：(1)代理实践模块，该模块使代理能够通过上下文优化积累经验并提高性能而无需参数更新；(2)代理强化学习模块，该模块与分布式训练框架集成，能够以端到端、大规模的方式实现任何Youtu-Agent的可扩展、稳定的强化学习。实验表明，Youtu-Agent在WebWalkerQA（71.47%）和GAIA（72.8%）上达到最先进的性能，使用开放权重模型。我们的自动生成管道实现了超过81%的工具合成成功率，而实践模块分别在AIME 2024/2025上提高了+2.7%和+5.4%的性能。此外，我们的代理强化学习训练在7B LLMs上实现了40%的加速，并在数学和一般/多跳QA基准测试中分别将编码/推理和搜索能力提升了35%和21%。",
        "地址": "https://arxiv.org/pdf/2512.24615.pdf"
    },
    {
        "名称": "2026 [2601.00393] NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos.pdf",
        "作者": "Yuxue Yang, Lue Fan, Ziqi Shi, Junran Peng, Feng Wang, Zhaoxiang Zhang",
        "摘要": "摘要：在本文中，我们提出了NeoVerse，一种多功能的4D世界模型，能够进行4D重建、新轨迹视频生成以及丰富的下游应用。我们首先发现当前的4D世界建模方法在扩展性上存在一个共同的限制，要么由于昂贵且专业的多视角4D数据，要么由于繁琐的训练预处理。而我们的NeoVerse则基于使整个流程能够扩展到各种自然单目视频的核心理念而构建。具体而言，NeoVerse具有无姿态无监督的前馈四维重建、在线单目退化模式模拟以及其他良好对齐的技术。这些设计使NeoVerse具有多功能性和对各种领域的泛化能力。同时，NeoVerse在标准重建和生成基准中达到了最先进的性能。我们的项目页面可在此https URL访问。\n\n作者：杨宇学，范略，施子琦，彭君然，王丰，张朝翔\n\n项目页面：此https URL\n\n链接：https://arxiv.org/pdf/2601.00393.pdf\n\n标题：2026 [2601.00393] NeoVerse：通过自然单目视频增强4D世界模型.pdf",
        "地址": "https://arxiv.org/pdf/2601.00393.pdf"
    },
    {
        "名称": "2026 [2601.00664] Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation.pdf",
        "作者": "Taekyung Ki, Sangwon Jang, Jaehyeong Jo, Jaehong Yoon, Sung Ju Hwang",
        "摘要": "摘要: \n谈话头像生成技术能够通过静态肖像创建逼真的头像，用于虚拟通信和内容创作。然而，当前模型尚未传递出真正互动交流的感觉，生成的往往是缺乏情感参与的单向响应。我们识别出实现真正互动头像的两个关键挑战：在因果约束下实时生成动态画面，以及在没有额外标注数据的情况下学习出有表现力和充满活力的反应。为了解决这些挑战，我们提出了Avatar Forcing，这是一种用于互动头像生成的新框架，通过扩散强制建模用户与头像的实时互动。该设计允许头像低延迟处理包括用户的音频和动作在内的实时多模态输入，从而即时反应包括言语、点头和笑声在内的口头和非口头提示。此外，我们提出了一种直接偏好优化方法，通过丢弃用户条件构建的合成失败样本，实现无标签的表现力互动学习。实验结果表明，我们的框架实现了实时互动，具有低延迟（约500毫秒），相较于基线模型提高了6.8倍的速度，并生成了反应迅速且富有表现力的头像动态，且超过80%的偏好选择我们的方法。\n",
        "地址": "https://arxiv.org/pdf/2601.00664.pdf"
    },
    {
        "名称": "2025 [2512.24330] SenseNova-MARS: Empowering Multimodal Agentic Reasoning and Search via Reinforcement Learning.pdf",
        "作者": "Yong Xien Chng, Tao Hu, Wenwen Tong, Xueheng Li, Jiandong Chen, Haojia Yu, Jiefan Lu, Hewei Guo, Hanming Deng, Chengjun Xie, Gao Huang, Dahua Lin, Lewei Lu",
        "摘要": "摘要：尽管视觉语言模型（VLMs）可以通过自主推理解决复杂任务，但其能力主要局限于文本导向的链式思维或独立工具调用。它们未能表现出人类般的熟练程度，无法在知识密集和视觉复杂的场景中无缝地将动态工具操作与连续推理结合，尤其是在需要协调外部工具（如搜索和图像裁剪）的情况下。在这项工作中，我们介绍了SenseNova-MARS，一种新颖的多模态自主推理和搜索框架，通过强化学习（RL）赋予VLMs交织视觉推理和工具使用能力。具体而言，SenseNova-MARS动态整合图像搜索、文本搜索和图像裁剪工具，以解决细粒度和知识密集的视觉理解挑战。在RL阶段，我们提出了批归一化组序列策略优化（BN-GSPO）算法，以提高训练稳定性并提升模型有效调用工具和推理的能力。为了全面评估在复杂视觉任务中的自主VLMs，我们引入了HR-MMSearch基准，这是第一个以搜索为导向的基准，由高分辨率图像和知识密集型、搜索驱动的问题组成。实验表明，SenseNova-MARS在开源搜索和细粒度图像理解基准上实现了最先进的性能。具体而言，在搜索导向的基准上，SenseNova-MARS-8B在MMSearch上获得67.84分，在HR-MMSearch上获得41.64分，超过了专有模型如Gemini-3-Flash和GPT-5。SenseNova-MARS通过提供有效且稳健的工具使用能力，代表了自主VLMs的有希望的一步。为了促进该领域的进一步研究，我们将发布所有代码、模型和数据集。",
        "地址": "https://arxiv.org/pdf/2512.24330.pdf"
    },
    {
        "名称": "2025 [2512.24271] Taming Hallucinations: Boosting MLLMs' Video Understanding via Counterfactual Video Generation.pdf",
        "作者": "Zhe Huang, Hao Wen, Aiming Hao, Bingze Song, Meiqi Wu, Jiahong Wu, Xiangxiang Chu, Sheng Lu, Haoqian Wang",
        "摘要": "摘要：多模态大语言模型（MLLMs）在视频理解方面取得了显著进展。然而，它们存在一个关键的脆弱性：过度依赖语言先验，这可能导致视觉未落地的幻觉，特别是在处理违背常识的反事实视频时。由于文本和视频之间固有的数据不平衡，以及收集和注释反事实数据的成本巨大，解决这一问题具有挑战性。为此，我们提出了DualityForge，这是一种新颖的反事实数据合成框架，采用可控的基于扩散的视频编辑技术，将真实视频转化为反事实场景。通过在视频编辑和问答生成过程中嵌入结构化的上下文信息，该框架自动生成高质量的问答对以及原始-编辑视频对用于对比训练。在此基础上，我们构建了DualityVidQA，这是一个大规模的视频数据集，旨在减少MLLM的幻觉。此外，为了充分利用我们配对数据的对比特性，我们提出了Duality-归一化优势训练（DNA-Train），这是一种两阶段的SFT-RL训练机制，其中RL阶段应用成对的$\\\\ell_1$优势归一化，从而实现更稳定和高效的策略优化。在DualityVidQA-Test上的实验表明，我们的方法显著减少了模型在反事实视频上的幻觉现象，相对比Qwen2.5-VL-7B基线提高了24.0%。此外，我们的方法在幻觉和通用基准上均取得了显著的进步，表明了强大的泛化能力。我们将开源我们的数据集和代码。\n\n作者：黄喆, 文浩, 郝艾铭, 宋秉泽, 武美琪, 武佳宏, 储湘湘, 鲁晟, 王浩谦\n\n评论：18页\n\n网址：https://arxiv.org/pdf/2512.24271.pdf\n\n标题：2025 [2512.24271] 驯服幻觉: 通过反事实视频生成提升MLLMs的视频理解\n",
        "地址": "https://arxiv.org/pdf/2512.24271.pdf"
    },
    {
        "名称": "2026 [2601.00796] AdaGaR: Adaptive Gabor Representation for Dynamic Scene Reconstruction.pdf",
        "作者": "Jiewen Chan, Zhenjun Zhao, Yu-Lun Liu",
        "摘要": "摘要：从单目视频重建动态三维场景需要同时捕捉高频外观细节和时间连续的运动。现有方法使用单一高斯原语因其低通滤波性质而受限，而标准Gabor函数引入能量不稳定性。此外，缺乏时间连续性约束常导致插值过程中的运动伪影。我们提出了AdaGaR，一个统一框架，解决显式动态场景建模中频率适应性和时间连续性问题。我们引入了自适应Gabor表示，通过可学习的频率权重和自适应能量补偿扩展高斯函数，以平衡细节捕捉和稳定性。对于时间连续性，我们采用带有时间曲率正则化的三次Hermite样条以确保平滑的运动演变。一个结合深度估计、点跟踪和前景掩膜的自适应初始化机制在早期训练中建立稳定的点云分布。在Tap-Vid DAVIS上的实验表明，性能达到了最新水平（PSNR 35.49, SSIM 0.9433, LPIPS 0.0723），并在帧插值、深度一致性、视频编辑和立体视图合成方面展现了强大的泛化能力。项目页面: this https URL",
        "地址": "https://arxiv.org/pdf/2601.00796.pdf"
    },
    {
        "名称": "2026 [2601.00417] Deep Delta Learning.pdf",
        "作者": "Yifan Zhang, Yifeng Liu, Mengdi Wang, Quanquan Gu",
        "摘要": "摘要：深度残差网络的效能在根本上依赖于恒等快捷连接。尽管这种机制有效地缓解了梯度消失问题，但其对特征变换施加了严格的加性归纳偏置，从而限制了网络对复杂状态转变进行建模的能力。在本文中，我们引入了一种新的架构——深度Delta学习（Deep Delta Learning，DDL），它通过一个可学习且依赖于数据的几何变换来推广标准的残差连接。这个变换被称为Delta算子，它是对恒等矩阵的秩-1扰动，由反射方向向量$\\mathbf{k}(\\mathbf{X})$和门控标量$\\beta(\\mathbf{X})$参数化。我们对该算子进行了谱分析，证明了门$\\beta(\\mathbf{X})$可以在恒等映射、正交投影和几何反射之间进行动态插值。此外，我们将残差更新重构为同步的秩-1注入，其中门作为一个动态步长，控制旧信息的擦除和新特征的写入。这种统一方式使网络能够显式地控制其层级过渡算子的谱，从而在保持门控残差架构稳定训练特性的同时，对复杂的非单调动态进行建模。\n\n作者：Yifan Zhang, Yifeng Liu, Mengdi Wang, Quanquan Gu\n评论：项目页面：该URL\n链接：https://arxiv.org/pdf/2601.00417.pdf\n标题：2026 [2601.00417] 深度Delta学习.pdf",
        "地址": "https://arxiv.org/pdf/2601.00417.pdf"
    },
    {
        "名称": "2025 [2512.24695] Nested Learning: The Illusion of Deep Learning Architectures.pdf",
        "作者": "Ali Behrouz, Meisam Razaviyayn, Peilin Zhong, Vahab Mirrokni",
        "摘要": "摘要:尽管近年来在开发语言模型方面取得了一些进展，但关于这些模型如何能持续学习/记忆、自我改进以及找到有效的解决方案仍然存在根本性的挑战和未解之谜。在本文中，我们提出了一种新的学习范式，即嵌套学习(Nested Learning, NL)，它通过一组嵌套的、多级的、和/或并行的优化问题来一致地表示机器学习模型，每个优化问题都有其自己的上下文流。从NL的角度来看，现有的深度学习方法通过压缩它们自己的上下文流从数据中学习，在大模型中自然地出现上下文学习。NL建议了一种哲学，即通过更多层次设计出更具表达力的学习算法，从而产生更高阶的上下文学习，并有可能解锁有效的持续学习能力。我们通过提出三个核心贡献为NL辩护：(1) 表达性的优化器:我们表明，已知的基于梯度的优化器，如Adam、带动量的SGD等，实际上是旨在压缩梯度信息（通过梯度下降）的关联记忆模块。基于这一洞察，我们提出了具有深度记忆和/或更强大学习规则的其他更具表达力的优化器; (2) 自我修改学习模块:利用NL对学习算法的见解，我们提出了一个通过学习自己的更新算法来学习如何自我修改的序列模型; (3) 连续体记忆系统:我们提出了一种新的记忆系统公式，该公式概括了传统的长/短期记忆观点。将我们的自我修改序列模型与连续体记忆系统结合，我们提出了一个称为Hope的持续学习模块，在语言建模、知识整合和少样本泛化任务、持续学习以及长上下文推理任务中展示了有希望的结果。",
        "地址": "https://arxiv.org/pdf/2512.24695.pdf"
    },
    {
        "名称": "2026 [2601.00747] The Reasoning-Creativity Trade-off: Toward Creativity-Driven Problem Solving.pdf",
        "作者": "Max Ruiz Luyten, Mihaela van der Schaar",
        "摘要": "摘要：最先进的大型语言模型 (LLM) 管道依赖于启动推理循环：采样多样的思维链条并强化得分最高的链条，主要优化正确性。我们分析了这种设计选择如何对模型在推理路径上的分布崩溃敏感，削减了语义熵并削弱了创造性问题解决。为分析这一失败，我们引入了分布式创造性推理 (DCR)，这是一个将训练视为通过解迹概率测度梯度流动的统一变分目标。STaR、GRPO 和 DPO 以及熵奖励和其他方法均构成了相同损失的特例。该框架提供了三个核心结果：(i) 多样性衰减定理，描述了基于正确性目标如何导致 STaR、GRPO 和 DPO 的不同模式的多样性衰减；(ii) 确保收敛到稳定且多样化策略的设计，实际防止崩溃；(iii) 实际操作中实现这一目标的简单可行的方案。因此，DCR 提供了第一个保持准确性和创造性的 LLM 的理论方案。\n\n作者：Max Ruiz Luyten, Mihaela van der Schaar\n\n备注：56 页，9 幅图，已提交至第二十九届人工智能与统计年会\n\n链接：https://arxiv.org/pdf/2601.00747.pdf\n\n标题：推理-创造力权衡：走向创造力驱动的问题解决",
        "地址": "https://arxiv.org/pdf/2601.00747.pdf"
    },
    {
        "名称": "2025 [2512.22955] Diversity or Precision? A Deep Dive into Next Token Prediction.pdf",
        "作者": "Haoyuan Wu, Hai Wang, Jiajia Wu, Jinxiang Ou, Keyao Wang, Weile Chen, Zihao Zheng, Bei Yu",
        "摘要": "摘要：最近的进展表明，强化学习（RL）可以显著提升大型语言模型（LLMs）的推理能力。然而，这种RL训练的有效性在很大程度上取决于预训练模型的token输出分布所定义的探索空间。在本文中，我们重新审视了标准的交叉熵损失，将其解释为在单步决策中应用策略梯度优化的特定实例。为了系统地研究预训练分布如何塑造后续RL的探索潜力，我们提出了一种广义的预训练目标，将on-policy RL的原理应用于监督学习。通过将下一个token预测框架化为一个随机决策过程，我们引入了一种奖励整形策略，该策略明确平衡了多样性和精度。我们的方法采用正奖励缩放因子来控制真实token上的概率集中，并使用一种等级感知机制，不对称地对待高排名和低排名的负面token。这使我们能够重塑预训练的token输出分布，并探索如何为RL提供更有利的探索空间，从而最终提升端到端的推理性能。与直觉上认为更高分布熵有助于有效探索的观点相反，我们发现，施加精度导向的先验能够为RL提供更优的探索空间。\n\n作者：吴昊元, 王海, 吴佳佳, 区金祥, 王科尧, 陈伟乐, 郑子豪, 于蓓\n\n链接：https://arxiv.org/pdf/2512.22955.pdf\n\n标题：2025 多样性还是精度？深入探讨下一个Token预测",
        "地址": "https://arxiv.org/pdf/2512.22955.pdf"
    },
    {
        "名称": "2026 [2601.00671] Fast-weight Product Key Memory.pdf",
        "作者": "Tianyu Zhao, Llion Jones",
        "摘要": "摘要：现代语言模型中的序列建模层通常在存储容量和计算效率之间存在权衡。尽管Softmax注意力提供了无限存储但代价高昂的二次方成本，线性变体则提供了效率但受限于固定大小的存储。我们提出了一种新颖的架构，称为Fast-weight Product Key Memory (FwPKM)，它通过将稀疏的Product Key Memory (PKM)从静态模块转变为动态的“快速权重”情景记忆来解决这一矛盾。与PKM不同，FwPKM在训练和推理过程中通过局部块级梯度下降动态更新其参数，使模型能够快速记住和检索输入序列中的新键值对。实验显示，FwPKM作为一种有效的情景记忆，补充了标准模块的语义记忆，在长上下文数据集上显著减少了困惑度。值得注意的是，在\"大海捞针\"评估中，FwPKM能够推广到128K-token的上下文，尽管其仅在4K-token序列上训练。\n\n翻译：\n2026年，[2601.00671] Fast-weight Product Key Memory\n\n摘要：现代语言模型中的序列建模层通常面临存储容量和计算效率之间的权衡。虽然Softmax注意力提供了无限的存储容量，但其成本高昂，而线性变体则效率较高，但存储容量有限且为固定大小。我们提出了一种新颖的架构，称为快速权重产品键存储器（FwPKM），通过将稀疏的产品键存储器（PKM）从静态模块转换为动态的“快速权重”情景记忆来解决这一问题。与PKM不同，FwPKM在训练和推理过程中通过局部块级梯度下降动态更新其参数，使模型能够快速记住和检索输入序列中的新键值对。实验结果表明，FwPKM作为一种有效的情景记忆，补充了标准模块的语义记忆，在长上下文数据集上显著减少了困惑度。值得注意的是，在“大海捞针”的评估中，FwPKM能够推广到128K-token的上下文，尽管其仅在4K-token序列上训练。\n\n作者：赵天宇, Llion Jones\n\n文档链接：https://arxiv.org/pdf/2601.00671.pdf",
        "地址": "https://arxiv.org/pdf/2601.00671.pdf"
    },
    {
        "名称": "2026 [2601.00575] InfoSynth: Information-Guided Benchmark Synthesis for LLMs.pdf",
        "作者": "Ishir Garg, Neel Kolhe, Xuandong Zhao, Dawn Song",
        "摘要": "摘要: 大型语言模型（LLMs）在推理和代码生成方面展示了显著的进步。然而，高效创建新的基准以评估这些能力仍然是一个挑战。传统的基准创建依赖于人工努力，这一过程既昂贵又耗时。此外，现有的基准测试往往会污染LLM的训练数据，因此需要新颖且多样的基准来准确评估其真正的能力。本文介绍了InfoSynth，这是一种新颖的框架，用于自动生成和评估基于信息论原理引导的推理基准。我们提出了基于KL散度和熵的指标，以量化基准的新颖性和多样性，而不依赖于昂贵的模型评估。在此框架基础上，我们开发了一个端到端的流水线，通过遗传算法和迭代代码反馈从种子数据集中合成出稳健的Python编码问题。我们的方法在97%的情况下生成了准确的问题测试用例和解决方案，而且合成的基准在新颖性和多样性方面始终高于其种子数据集。此外，我们的算法提供了一种方法来控制生成问题的创新性/多样性和难度。InfoSynth为构建高质量、新颖和多样的LLM基准提供了一个可扩展的自验证流水线。项目页面：this https URL",
        "地址": "https://arxiv.org/pdf/2601.00575.pdf"
    },
    {
        "名称": "2026 [2601.00204] MorphAny3D: Unleashing the Power of Structured Latent in 3D Morphing.pdf",
        "作者": "Xiaokun Sun, Zeyu Cai, Hao Tang, Ying Tai, Jian Yang, Zhenyu Zhang",
        "摘要": "摘要：3D变形依然具有挑战性，因为在不同类别之间生成语义一致且时间平滑的变形具有很大难度。我们提出了MorphAny3D，一个利用结构化潜变量(SLAT) 表现进行高质量3D变形的无需训练的框架。我们的关键见解是通过在3D生成器的注意机制中智能地融合源和目标的SLAT特征，可以自然地产生合理的变形序列。为此，我们引入了变形交叉注意(MCA) 来融合源和目标信息以保持结构一致性，以及时间融合自注意(TFSA)，通过引入前一帧的特征来增强时间一致性。一个方向校正策略进一步缓解了变形步骤中的姿态模糊。大量实验证明，我们的方法生成的变形序列达到了最先进水平，即使在跨类别的复杂情况下也是如此。MorphAny3D还支持高级应用，如解耦变形和3D风格转移，并且可以推广到其他基于SLAT的生成模型。项目页面：此https URL。",
        "地址": "https://arxiv.org/pdf/2601.00204.pdf"
    },
    {
        "名称": "2025 [2512.24146] Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning.pdf",
        "作者": "Chubin Chen, Sujie Hu, Jiashu Zhu, Meiqi Wu, Jintao Chen, Yanxun Li, Nisha Huang, Chengyu Fang, Jiahong Wu, Xiangxiang Chu, Xiu Li",
        "摘要": "以下是该论文的中文摘要翻译：\n\n摘要：最近的研究表明，通过从人类反馈中进行强化学习，文本到图像扩散模型在与人类偏好的对齐方面取得了显著进展。然而，尽管现有方法在自动化奖励指标上获得了高分，它们往往会导致偏好模式崩溃（PMC）——这是一种特定形式的奖励欺骗，其中模型收敛于狭窄的高分输出（例如，具有单一风格或过度曝光的图像），严重降低了生成多样性。在这项工作中，我们引入量化了这一现象，提出了DivGenBench，这是一种新的基准，用于衡量PMC的程度。我们认为这种崩溃是由于沿着奖励模型固有偏见的过度优化所驱动的。在此分析的基础上，我们提出了方向分离对齐（D$^2$-Align），这是一个通过方向性校正奖励信号来缓解PMC的新框架。具体来说，我们的方法首先在奖励模型的嵌入空间中学习一个方向性校正，同时保持模型静止。然后在优化过程中将此校正应用于奖励信号，以防止模型崩溃到特定模式，从而维持多样性。我们的综合评估结合了质量和多样性的定性和定量指标，表明D$^2$-Align在与人类偏好的对齐方面达到了更高的水平。",
        "地址": "https://arxiv.org/pdf/2512.24146.pdf"
    }
]